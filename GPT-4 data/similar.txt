Prompt:
Hello I am a tutor who creates study guides to help engineering students study for STEM subjects. I will provide you with a piece of text describing a subject and you will create 5 practice questions, each with 5 answer choices to choose from. There must only be one factually correct answer, all other options must be falsifiable. The questions should be reasonably challenging, meaning that none of the options should be obviously wrong, however it must still be possible for a knowledgeable student to clearly identify the single correct answer.
Here is a question that was asked about this topic on a previous exam. Please generate 5 other questions written in the same format. The new questions should be similar in difficulty to the example and focus on testing the student's knowledge of fundamental concepts and important facts.
@
Subject:
Modified Newtonian dynamics (MOND) is a hypothesis that proposes a modification of Newton's law of universal gravitation to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.

Created in 1982 and first published in 1983 by Israeli physicist Mordehai Milgrom,[1] the hypothesis' original motivation was to explain why the velocities of stars in galaxies were observed to be larger than expected based on Newtonian mechanics. Milgrom noted that this discrepancy could be resolved if the gravitational force experienced by a star in the outer regions of a galaxy was proportional to the square of its centripetal acceleration (as opposed to the centripetal acceleration itself, as in Newton's second law) or alternatively, if gravitational force came to vary inversely linearly with radius (as opposed to the inverse square of the radius, as in Newton's law of gravity). MOND departs from Newton's laws at extremely small accelerations that are characteristic of the outer regions of galaxies as well as the inter-galaxy forces within galaxy clusters, but which are far below anything encountered in the Solar System or on Earth.

MOND is an example of a class of theories known as modified gravity, and is an alternative to the hypothesis that the dynamics of galaxies are determined by massive, invisible dark matter halos. Since Milgrom's original proposal, proponents of MOND have claimed to successfully predict a variety of galactic phenomena that they state are difficult to understand as consequences of dark matter.[2][3]

Though MOND explains the anomalously great rotational velocities of galaxies at their perimeters, it does not fully explain the velocity dispersions of individual galaxies within galaxy clusters. MOND reduces the discrepancy between the velocity dispersions and clusters' observed missing baryonic mass from a factor of around 10 to a factor of about 2. However, the residual discrepancy cannot be accounted for by MOND, requiring that other explanations close the gap such as the presence of as-yet undetected missing baryonic matter.[4]

The accurate measurement of the speed of gravitational waves compared to the speed of light in 2017 ruled out a certain class of modified gravity theories but concluded that other MOND theories that dispense with the need for dark matter remained viable.[5] Two years later, theories put forth by Constantinos Skordis and Tom Zlosnik were consistent with gravitational waves that always travel at the speed of light. Later still in 2021, Skordis and Zlosnik developed a subclass of their theory called "RMOND", for "relativistic MOND", which had "been shown to reproduce in great detail the main observations in cosmology, including the cosmic-microwave-background power spectrum, and the matter structure power spectrum."
$
5
Who is credited with the creation and first publication of the Modified Newtonian Dynamics (MOND) hypothesis?
A: Constantinos Skordis
B: Tom Zlosnik
C: Isaac Newton
D: Mordehai Milgrom
E: Albert Einstein
Answer: D

MOND was initially proposed to explain which observed anomaly related to galaxies?
A: The presence of black holes at the centers of galaxies.
B: The velocities of stars in galaxies were higher than predicted by Newtonian mechanics.
C: The rapid expansion of galaxies.
D: The presence of gamma-ray bursts in outer regions of galaxies.
E: The phenomenon of galaxy collisions.
Answer: B

In MOND's proposal, how does gravitational force vary in the outer regions of a galaxy compared to Newton's law of gravity?
A: It varies in direct proportion to the radius.
B: It varies inversely linearly with radius.
C: It remains consistent with Newton's law of gravity.
D: It varies with the inverse cube of the radius.
E: It varies based on the electromagnetic radiation emitted by the star.
Answer: B

In the context of Modified Gravity theories, which 2017 observation ruled out certain MOND theories?
A: The discovery of the Higgs boson.
B: The observation of the merging of two neutron stars.
C: The accurate measurement of the speed of gravitational waves compared to the speed of light.
D: The direct imaging of a black hole's event horizon.
E: The measurement of neutrinos emanating from the sun.
Answer: C

What does the "RMOND" in the subclass developed by Constantinos Skordis and Tom Zlosnik in 2021 stand for?
A: Revised MOND
B: Radiative MOND
C: Relativistic MOND
D: Residual MOND
E: Reactive MOND
Answer: C
@
Subject:
Dynamic scaling (sometimes known as Family-Vicsek scaling[1][2]) is a litmus test that shows whether an evolving system exhibits self-similarity. In general a function is said to exhibit dynamic scaling if it satisfies: Many of these systems evolve in a self-similar fashion in the sense that data obtained from the snapshot at any fixed time is similar to the respective data taken from the snapshot of any earlier or later time. That is, the system is similar to itself at different times. The litmus test of such self-similarity is provided by the dynamic scaling.
In mathematics, a self-similar object is exactly or approximately similar to a part of itself (i.e., the whole has the same shape as one or more of the parts). Many objects in the real world, such as coastlines, are statistically self-similar: parts of them show the same statistical properties at many scales.[2] Self-similarity is a typical property of fractals. Scale invariance is an exact form of self-similarity where at any magnification there is a smaller piece of the object that is similar to the whole. For instance, a side of the Koch snowflake is both symmetrical and scale-invariant; it can be continually magnified 3x without changing shape. The non-trivial similarity evident in fractals is distinguished by their fine structure, or detail on arbitrarily small scales. As a counterexample, whereas any portion of a straight line may resemble the whole, further detail is not revealed.
$
5
In the context of self-similarity, what does it mean for a system to be self-similar?
A: The system varies greatly at different times.
B: The system exhibits a uniform shape regardless of magnification.
C: The system displays distinct forms at varying scales.
D: The system remains unchanged in its core properties over time.
E: The whole of the system has the same shape as one or more of its parts.
Answer: E

Which of the following best describes a fractal in terms of self-similarity?
A: A structure that exhibits no similarity at any scale.
B: An object that retains the same statistical properties at many scales, typically with detail on arbitrarily small scales.
C: A figure that displays a unique shape regardless of the magnification applied.
D: A pattern that completely transforms its properties when viewed at different scales.
E: An entity that evolves with time, changing its fundamental structures.
Answer: B

Scale invariance in self-similar objects refers to:
A: An object's ability to change shape when magnified.
B: An object's ability to exhibit different properties at different scales.
C: An exact form of self-similarity where a smaller piece of the object is similar to the whole at any magnification.
D: The tendency of an object to lose its original properties when scaled.
E: The invariable nature of an object when subjected to time-evolution.
Answer: C

The Koch snowflake is an example of an object that exhibits:
A: Exact self-similarity without scale invariance.
B: Dynamic scaling without any form of self-similarity.
C: Both symmetry and scale-invariance, allowing continual 3x magnification without changing its shape.
D: Fractal properties only at its boundary edges.
E: Dissimilarity in its overall structure, without any consistent patterns.
Answer: C

Which statement is true regarding objects that are statistically self-similar?
A: They maintain the same statistical properties only at a single scale.
B: Their statistical properties vary wildly across scales.
C: They reveal further detail as they are magnified, but their core properties remain consistent.
D: They never resemble any part of themselves, regardless of scale.
E: They show the same statistical properties at many scales, such as certain coastlines.
Answer: E
@
Subject:
A triskelion or triskeles is an ancient motif consisting of a triple spiral exhibiting rotational symmetry or other patterns in triplicate that emanate from a common center. The spiral design can be based on interlocking Archimedean spirals, or represent three bent human legs. It is found in artifacts of the European Neolithic and Bronze Age with continuation into the Iron Age especially in the context of the La Tène culture[citation needed] and related Celtic traditions. The actual triskeles symbol of three human legs is found especially in Greek antiquity, beginning in archaic pottery and continued in coinage of the classical period.

In the Hellenistic period, the symbol becomes associated with the island of Sicily, appearing on coins minted under Dionysius I of Syracuse beginning in c. 382 BCE.[1] It later appears in heraldry, and, other than in the flag of Sicily, came to be used in the flag of the Isle of Man (known as ny tree cassyn 'the three legs').[2]

Greek τρισκελής (triskelḗs) means 'three-legged'.[3] While the Greek adjective τρισκελής 'three-legged (e.g., of a table)' is ancient, use of the term for the symbol is modern, introduced in 1835 by Honoré Théodoric d'Albert de Luynes as French triskèle,[4] and adopted in the spelling triskeles following Otto Olshausen (1886).[5] The form triskelion (as it were Greek τρισκέλιον[6]) is a diminutive which entered English usage in numismatics in the late 19th century.[7][8] The form consisting of three human legs (as opposed to the triple spiral) has also been called a "triquetra of legs", also triskelos or triskel.[9]
$
7
What is a defining characteristic of a triskelion or triskeles motif?
A: A symbol based on a quadruple spiral.
B: A single spiral that emerges from a common point.
C: A triple spiral with rotational symmetry from a common center.
D: An emblem that always represents three human legs.
E: A double spiral found in the European Iron Age.
Answer: C

In which historical era is the triskeles symbol notably present in European artifacts?
A: Medieval Age
B: Renaissance
C: Neolithic and Bronze Age
D: Age of Enlightenment
E: Late Antiquity
Answer: C

Which culture is particularly associated with the triskeles symbol during the Iron Age?
A: Roman
B: Norse
C: La Tène
D: Byzantine
E: Egyptian
Answer: C

How did the triskeles symbol primarily manifest in Greek antiquity?
A: Three interconnected stars.
B: Triple lightning bolts.
C: Three bent human legs.
D: Three crossed swords.
E: Triple crescent moon.
Answer: C

What association does the triskeles symbol have during the Hellenistic period?
A: It became the national symbol of Rome.
B: It represented the city of Athens.
C: It was associated with the city of Corinth.
D: It became associated with the island of Sicily.
E: It represented the deity Apollo.
Answer: D

The term “triskelion” entered English usage primarily in the field of:
A: Botany
B: Medicine
C: Numismatics
D: Linguistics
E: Architecture
Answer: C

What does the Greek τρισκελής (triskelḗs) translate to in English?
A: Triple spiral
B: Three-armed
C: Rotational symmetry
D: Three-legged
E: Centered triangle
Answer: D
@
Subject:
In physics, especially quantum field theory, regularization is a method of modifying observables which have singularities in order to make them finite by the introduction of a suitable parameter called the regulator. The regulator, also known as a "cutoff", models our lack of knowledge about physics at unobserved scales (e.g. scales of small size or large energy levels). It compensates for (and requires) the possibility that "new physics" may be discovered at those scales which the present theory is unable to model, while enabling the current theory to give accurate predictions as an "effective theory" within its intended scale of use.

It is distinct from renormalization, another technique to control infinities without assuming new physics, by adjusting for self-interaction feedback.

Regularization was for many decades controversial even amongst its inventors, as it combines physical and epistemological claims into the same equations. However, it is now well understood and has proven to yield useful, accurate predictions.
$
5
What is the main role of the regulator in the process of regularization?
A: To adjust for self-interaction feedback in quantum equations.
B: To determine the fundamental constants of nature.
C: To modify observables with singularities to make them finite.
D: To validate "new physics" discovered at larger scales.
E: To negate the influence of renormalization in quantum calculations.
Answer: C

What does the regulator, or "cutoff", compensate for in regularization?
A: The potential inaccuracies in the existing quantum model.
B: The possibility of discovering "new physics" at unobserved scales.
C: The influence of large gravitational forces in quantum systems.
D: The imperfections in experimental setups and observations.
E: The transition between classical and quantum physics.
Answer: B

How is regularization different from renormalization?
A: Regularization requires an external agent to induce changes, while renormalization is an automatic process.
B: Regularization is a modern technique, while renormalization is an outdated method.
C: Regularization controls infinities by introducing a regulator, while renormalization adjusts for self-interaction feedback.
D: Regularization is used only in classical physics, while renormalization is exclusive to quantum field theory.
E: Regularization and renormalization are the same techniques but with different nomenclature.
Answer: C

Why was regularization controversial amongst its inventors for many years?
A: It contradicted the fundamental postulates of quantum mechanics.
B: It combined physical and epistemological claims within the same equations.
C: It led to inaccurate predictions in quantum field theory.
D: It was considered too complicated for practical use in physics.
E: It disregarded the importance of renormalization.
Answer: B

What does regularization primarily aim to provide in quantum field theory?
A: A method to negate the influence of external forces in quantum systems.
B: A strategy to unify classical and quantum physics.
C: A way to obtain accurate predictions as an "effective theory" within a specific scale.
D: A technique to disprove previously accepted theories in physics.
E: A process to transition between different quantum states.
Answer: C
@
Subject:
Diffraction is the interference or bending of waves around the corners of an obstacle or through an aperture into the region of geometrical shadow of the obstacle/aperture. The diffracting object or aperture effectively becomes a secondary source of the propagating wave. Italian scientist Francesco Maria Grimaldi coined the word diffraction and was the first to record accurate observations of the phenomenon in 1660.[1][2]

In classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets.[3] The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength, as shown in the inserted image. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. If there are multiple, closely spaced openings (e.g., a diffraction grating), a complex pattern of varying intensity can result.

These effects also occur when a light wave travels through a medium with a varying refractive index, or when a sound wave travels through a medium with varying acoustic impedance – all waves diffract,[4] including gravitational waves,[5] water waves, and other electromagnetic waves such as X-rays and radio waves. Furthermore, quantum mechanics also demonstrates that matter possesses wave-like properties and, therefore, undergoes diffraction (which is measurable at subatomic to molecular levels).[6]

The amount of diffraction depends on the size of the gap. Diffraction is greatest when the size of the gap is similar to the wavelength of the wave. In this case, when the waves pass through the gap they become semi-circular.

Several qualitative observations can be made of diffraction in general:

The angular spacing of the features in the diffraction pattern is inversely proportional to the dimensions of the object causing the diffraction. In other words: The smaller the diffracting object, the 'wider' the resulting diffraction pattern, and vice versa. (More precisely, this is true of the sines of the angles.)
The diffraction angles are invariant under scaling; that is, they depend only on the ratio of the wavelength to the size of the diffracting object.
When the diffracting object has a periodic structure, for example in a diffraction grating, the features generally become sharper. The third figure, for example, shows a comparison of a double-slit pattern with a pattern formed by five slits, both sets of slits having the same spacing, between the center of one slit and the next.
$
6
Who was the first to record accurate observations of diffraction?
A: Huygens
B: Fresnel
C: Grimaldi
D: Newton
E: Einstein
Answer: C

According to the Huygens–Fresnel principle, each point in a propagating wavefront is treated as:
A: A single ray of light.
B: A collection of individual spherical wavelets.
C: A diffraction grating.
D: A result of the double-slit experiment.
E: An interference pattern.
Answer: B

Which of the following waves is NOT mentioned as undergoing diffraction?
A: Light waves
B: Sound waves
C: Gravitational waves
D: Seismic waves
E: Radio waves
Answer: D

In quantum mechanics, diffraction is significant because:
A: It demonstrates that matter does not have wave-like properties.
B: It shows that light can only behave as a particle.
C: It reveals that matter possesses wave-like properties and can undergo diffraction at subatomic to molecular levels.
D: It proves that diffraction can only be observed at a macroscopic scale.
E: It confirms that matter and energy are mutually exclusive concepts.
Answer: C

When a wave passes through a gap and the size of the gap is similar to its wavelength, the wave:
A: Disappears.
B: Becomes semi-circular.
C: Retains its original shape.
D: Divides into multiple smaller waves.
E: Amplifies in amplitude.
Answer: B

What happens to the features in the diffraction pattern when the diffracting object has a periodic structure?
A: The features blur and become indistinguishable.
B: The features generally become sharper.
C: The features disappear altogether.
D: The features double in number.
E: The features change in frequency.
Answer: B
@
Subject:
In physics (specifically electromagnetism), Gauss's law, also known as Gauss's flux theorem, (or sometimes simply called Gauss's theorem) is a law relating the distribution of electric charge to the resulting electric field. In its integral form, it states that the flux of the electric field out of an arbitrary closed surface is proportional to the electric charge enclosed by the surface, irrespective of how that charge is distributed. Even though the law alone is insufficient to determine the electric field across a surface enclosing any charge distribution, this may be possible in cases where symmetry mandates uniformity of the field. Where no such symmetry exists, Gauss's law can be used in its differential form, which states that the divergence of the electric field is proportional to the local density of charge.

The law was first[1] formulated by Joseph-Louis Lagrange in 1773,[2] followed by Carl Friedrich Gauss in 1835,[3] both in the context of the attraction of ellipsoids. It is one of Maxwell's equations, which forms the basis of classical electrodynamics.[note 1] Gauss's law can be used to derive Coulomb's law,[4] and vice versa.
$
5
How was Gauss's law originally formulated in the context of?
A: The interaction of magnetic fields within ferrous materials.
B: The repulsion between like-charged particles.
C: The attraction of ellipsoids.
D: The polarization of dielectric materials.
E: The generation of static electricity.
Answer: C

2. What is the primary relationship established by Gauss's law in its integral form?
A: The electric field across a closed surface is inversely proportional to the distance from the charge distribution.
B: The divergence of the electric field across a closed surface is directly related to the charge contained within the surface.
C: The flux of the electric field out of a closed surface is proportional to the charge enclosed by the surface.
D: The curl of the electric field within a closed surface is directly related to the charge contained within that surface.
E: The magnitude of the electric field within a closed surface is determined by the medium the charges are placed in.
Answer: C

3. Which statement accurately describes the differential form of Gauss's law?
A: The divergence of the electric field is inversely proportional to the electric charge enclosed by the surface.
B: The divergence of the electric field is directly proportional to the local density of charge.
C: The divergence of the electric field is unaffected by the local density of charge.
D: The differential form focuses only on the net electric field at a specific point within a charge distribution.
E: The differential form is only valid for symmetric charge distributions.
Answer: B

4. Gauss's law is one of Maxwell's equations, which is fundamental to which field?
A: Quantum mechanics.
B: Thermodynamics.
C: Classical mechanics.
D: Classical electrodynamics.
E: Relativistic physics.
Answer: D

5. Which of the following is true regarding Gauss's law and Coulomb's law?
A: Gauss's law cannot be used to derive Coulomb's law and vice versa.
B: Gauss's law and Coulomb's law are essentially the same thing.
C: Coulomb's law can be derived from Gauss's law, but Gauss's law cannot be derived from Coulomb's law.
D: Gauss's law can be used to derive Coulomb's law, and vice versa.
E: Gauss's law contradicts Coulomb's law in the context of point charges.
Answer: D
@
Subject:
A CW complex (also called cellular complex or cell complex) is a kind of a topological space that is particularly important in algebraic topology.[1] It was introduced by J. H. C. Whitehead[2] to meet the needs of homotopy theory. This class of spaces is broader and has some better categorical properties than simplicial complexes, but still retains a combinatorial nature that allows for computation (often with a much smaller complex). The C stands for "closure-finite", and the W for "weak" topology.[2]

The CW complex construction is a straightforward generalization of the following process:

A 0-dimensional CW complex is just a set of zero or more discrete points (with the discrete topology).
A 1-dimensional CW complex is constructed by taking the disjoint union of a 0-dimensional CW complex with one or more copies of the unit interval. For each copy, there is a map that "glues" its boundary (its two endpoints) to elements of the 0-dimensional complex (the points). The topology of the CW complex is the topology of the quotient space defined by these gluing maps.
In general, an n-dimensional CW complex is constructed by taking the disjoint union of a k-dimensional CW complex (for some 
�
<
�
k<n) with one or more copies of the n-dimensional ball. For each copy, there is a map that "glues" its boundary (the 
(
�
−
1
)
(n-1)-dimensional sphere) to elements of the 
�
k-dimensional complex. The topology of the CW complex is the quotient topology defined by these gluing maps.
An infinite-dimensional CW complex can be constructed by repeating the above process countably many times. Since the topology of the union 
∪
�
�
�
{\displaystyle \cup _{k}X_{k}} is indeterminate, one takes the direct limit topology, since the diagram is highly suggestive of a direct limit. This turns out to have great technical benefits.
$
5
Who introduced the CW complex to meet the needs of homotopy theory?
A: Henri Poincaré
B: Leonard Euler
C: Carl Friedrich Gauss
D: Pierre-Simon Laplace
E: J. H. C. Whitehead
Answer: E

2. Which of the following best describes the difference between CW complexes and simplicial complexes?
A: CW complexes are narrower and have better categorical properties than simplicial complexes.
B: CW complexes and simplicial complexes are essentially the same thing and are interchangeable.
C: CW complexes have better categorical properties and are broader than simplicial complexes, allowing for often simpler computations.
D: CW complexes are strictly used for topological applications, while simplicial complexes are used for algebraic applications.
E: Simplicial complexes always have a "weak" topology, while CW complexes have a "closure-finite" topology.
Answer: C

3. Which term stands for "closure-finite" in the context of CW complex terminology?
A: W
B: C
C: CW
D: HC
E: WH
Answer: B

4. A 1-dimensional CW complex is constructed by taking the disjoint union of a 0-dimensional CW complex with which of the following?
A: A square
B: A cube
C: A unit circle
D: A unit interval
E: A tetrahedron
Answer: D

5. How is the topology of a n-dimensional CW complex typically determined?
A: By the n-dimensional ball enclosed within the complex.
B: By the number of singularities in the CW complex.
C: By the quotient topology defined by the gluing maps.
D: By the highest dimensional sphere present in the CW complex.
E: By the direct limit topology if the complex is infinite-dimensional.
Answer: C
@
Subject:
A spin valve is a device, consisting of two or more conducting magnetic materials, whose electrical resistance can change between two values depending on the relative alignment of the magnetization in the layers. The resistance change is a result of the giant magnetoresistive effect. The magnetic layers of the device align "up" or "down" depending on an external magnetic field. In the simplest case, a spin valve consists of a non-magnetic material sandwiched between two ferromagnets, one of which is fixed (pinned) by an antiferromagnet which acts to raise its magnetic coercivity and behaves as a "hard" layer, while the other is free (unpinned) and behaves as a "soft" layer. Due to the difference in coercivity, the soft layer changes polarity at lower applied magnetic field strength than the hard one. Upon application of a magnetic field of appropriate strength, the soft layer switches polarity, producing two distinct states: a parallel, low-resistance state, and an antiparallel, high-resistance state.
$
5
The giant magnetoresistive effect is responsible for which phenomenon in a spin valve?
A: The creation of magnetic fields.
B: The alignment of the magnetic layers "up" or "down".
C: The change in electrical resistance depending on the relative alignment of the magnetization in the layers.
D: The strength of the magnetic field required to change the polarity of a layer.
E: The amount of magnetic material present in a layer.
Answer: C

2. In a spin valve, the layer with higher coercivity is referred to as:
A: The free layer.
B: The soft layer.
C: The hard layer.
D: The non-magnetic layer.
E: The active layer.
Answer: C

3. When the magnetic layers in a spin valve align in opposite directions, the resistance state is:
A: High and called the parallel state.
B: Low and called the parallel state.
C: High and called the antiparallel state.
D: Low and called the antiparallel state.
E: Intermediate and called the perpendicular state.
Answer: C

4. In a spin valve, the layer that switches polarity at a lower applied magnetic field strength compared to the "hard" layer is:
A: The fixed layer.
B: The pinned layer.
C: The non-magnetic layer.
D: The "soft" layer.
E: The antiferromagnetic layer.
Answer: D

5. The antiferromagnet in a spin valve primarily functions to:
A: Create an external magnetic field.
B: Pin the magnetization direction of a ferromagnetic layer.
C: Reduce the coercivity of a ferromagnetic layer.
D: Enhance the giant magnetoresistive effect.
E: Increase the electrical conductivity of the device.
Answer: B
@
Subject:
In astronomy, extinction is the absorption and scattering of electromagnetic radiation by dust and gas between an emitting astronomical object and the observer. Interstellar extinction was first documented as such in 1930 by Robert Julius Trumpler.[1][2] However, its effects had been noted in 1847 by Friedrich Georg Wilhelm von Struve,[3] and its effect on the colors of stars had been observed by a number of individuals who did not connect it with the general presence of galactic dust. For stars that lie near the plane of the Milky Way and are within a few thousand parsecs of the Earth, extinction in the visual band of frequencies (photometric system) is roughly 1.8 magnitudes per kiloparsec.[4]

For Earth-bound observers, extinction arises both from the interstellar medium (ISM) and the Earth's atmosphere; it may also arise from circumstellar dust around an observed object. Strong extinction in earth's atmosphere of some wavelength regions (such as X-ray, ultraviolet, and infrared) is overcome by the use of space-based observatories. Since blue light is much more strongly attenuated than red light, extinction causes objects to appear redder than expected, a phenomenon referred to as interstellar reddening.[5]
$
5
1. Interstellar extinction primarily results from:
A: The Doppler effect of light emitted from moving astronomical objects.
B: The absorption and scattering of electromagnetic radiation by dust and gas.
C: The bending of light around massive objects, according to general relativity.
D: The inherent fading of starlight over vast intergalactic distances.
E: The emission of light by superheated interstellar plasma.
Answer: B

2. Who first documented interstellar extinction as such?
A: Friedrich Georg Wilhelm von Struve.
B: Robert Julius Trumpler.
C: Albert Einstein.
D: Galileo Galilei.
E: Edwin Hubble.
Answer: B

3. For stars that are within a few thousand parsecs of the Earth and lie near the plane of the Milky Way, the extinction in the visual band of frequencies is approximately:
A: 1.8 magnitudes per parsec.
B: 1.8 magnitudes per kiloparsec.
C: 0.18 magnitudes per kiloparsec.
D: 18 magnitudes per parsec.
E: 0.018 magnitudes per kiloparsec.
Answer: B

4. Why do astronomical objects appear redder due to extinction?
A: Because red light is strongly attenuated compared to blue light.
B: Due to the inherent redshift of light from distant objects.
C: Because of the thermal radiation of interstellar gas.
D: Because blue light is more strongly attenuated than red light.
E: Due to the Doppler effect as the object moves away from the observer.
Answer: D

5. Which of the following regions of the electromagnetic spectrum experience strong extinction in Earth's atmosphere and benefit from space-based observatories?
A: Visible light.
B: Radio waves.
C: Microwave.
D: Ultraviolet.
E: Low-frequency radio waves.
Answer: D
@
Subject:
An axiom, postulate, or assumption is a statement that is taken to be true, to serve as a premise or starting point for further reasoning and arguments. The word comes from the Ancient Greek word ἀξίωμα (axíōma), meaning 'that which is thought worthy or fit' or 'that which commends itself as evident'.[1][2]

The precise definition varies across fields of study. In classic philosophy, an axiom is a statement that is so evident or well-established, that it is accepted without controversy or question.[3] In modern logic, an axiom is a premise or starting point for reasoning.[4]

In mathematics, an axiom may be a "logical axiom" or a "non-logical axiom". Logical axioms are taken to be true within the system of logic they define and are often shown in symbolic form (e.g., (A and B) implies A), while non-logical axioms (e.g., a + b = b + a) are substantive assertions about the elements of the domain of a specific mathematical theory, such as arithmetic.

Non-logical axioms may also be called "postulates" or "assumptions". In most cases, a non-logical axiom is simply a formal logical expression used in deduction to build a mathematical theory, and might or might not be self-evident in nature (e.g., the parallel postulate in Euclidean geometry). To axiomatize a system of knowledge is to show that its claims can be derived from a small, well-understood set of sentences (the axioms), and there are typically many ways to axiomatize a given mathematical domain.

Any axiom is a statement that serves as a starting point from which other statements are logically derived. Whether it is meaningful (and, if so, what it means) for an axiom to be "true" is a subject of debate in the philosophy of mathematics.[5]
$
7
1. The word "axiom" is derived from the Ancient Greek word ἀξίωμα (axíōma), which means:
A: Starting point.
B: Deduction.
C: That which is evident.
D: That which is thought worthy or fit.
E: Foundation of reasoning.
Answer: D

2. In classic philosophy, an axiom is:
A: A premise that always requires evidence.
B: A statement that is frequently debated.
C: A statement accepted without controversy or question due to its evident nature.
D: An experimental validation of a hypothesis.
E: A statement that is universally false.
Answer: C

3. In mathematics, which of the following axioms are substantive assertions about the elements of the domain of a specific mathematical theory?
A: Logical axioms.
B: Philosophical axioms.
C: Deductive axioms.
D: Non-logical axioms.
E: Intuitive axioms.
Answer: D

4. The parallel postulate in Euclidean geometry is an example of:
A: A logical axiom that is always self-evident.
B: A non-logical axiom that might or might not be self-evident.
C: An experimental hypothesis in geometry.
D: A controversial statement in non-Euclidean geometry.
E: A theorem derived without the need for axioms.
Answer: B

5. Axiomatizing a system of knowledge means:
A: Defining the terms used in a mathematical domain.
B: Proving the validity of all statements in the system.
C: Demonstrating that claims in the system can be derived from a set of foundational sentences.
D: Validating the truth of axioms using experimental data.
E: Breaking down complex statements into simpler ones for easier understanding.
Answer: C

6. Which of the following best captures the essence of a non-logical axiom in mathematics?
A: It is always self-evident and uncontroversial.
B: It pertains only to the rules of logic and reasoning.
C: It is a formal logical expression used in building a mathematical theory.
D: It is always depicted in symbolic form, like (A and B) implies A.
E: It only serves as a supplementary statement without much importance.
Answer: C

7. In the context of the philosophy of mathematics, which of the following is debated?
A: The logic used in deducing theorems.
B: The validity of proof theory.
C: The meaning and truth value of an axiom.
D: The domain of a specific mathematical theory.
E: The universality of mathematical theories.
Answer: C
@
Subject:
In physics, total internal reflection (TIR) is the phenomenon in which waves arriving at the interface (boundary) from one medium to another (e.g., from water to air) are not refracted into the second ("external") medium, but completely reflected back into the first ("internal") medium. It occurs when the second medium has a higher wave speed (i.e., lower refractive index) than the first, and the waves are incident at a sufficiently oblique angle on the interface. For example, the water-to-air surface in a typical fish tank, when viewed obliquely from below, reflects the underwater scene like a mirror with no loss of brightness (Fig. 1).

TIR occurs not only with electromagnetic waves such as light and microwaves, but also with other types of waves, including sound and water waves. If the waves are capable of forming a narrow beam (Fig. 2), the reflection tends to be described in terms of "rays" rather than waves; in a medium whose properties are independent of direction, such as air, water or glass, the "rays" are perpendicular to the associated wavefronts.

Refraction is generally accompanied by partial reflection. When waves are refracted from a medium of lower propagation speed (higher refractive index) to a medium of higher propagation speed (lower refractive index) —e.g., from water to air—the angle of refraction (between the outgoing ray and the surface normal) is greater than the angle of incidence (between the incoming ray and the normal). As the angle of incidence approaches a certain threshold, called the critical angle, the angle of refraction approaches 90°, at which the refracted ray becomes parallel to the boundary surface. As the angle of incidence increases beyond the critical angle, the conditions of refraction can no longer be satisfied, so there is no refracted ray, and the partial reflection becomes total. For visible light, the critical angle is about 49° for incidence from water to air, and about 42° for incidence from common glass to air.

Details of the mechanism of TIR give rise to more subtle phenomena. While total reflection, by definition, involves no continuing flow of power across the interface between the two media, the external medium carries a so-called evanescent wave, which travels along the interface with an amplitude that falls off exponentially with distance from the interface. The "total" reflection is indeed total if the external medium is lossless (perfectly transparent), continuous, and of infinite extent, but can be conspicuously less than total if the evanescent wave is absorbed by a lossy external medium ("attenuated total reflectance"), or diverted by the outer boundary of the external medium or by objects embedded in that medium ("frustrated" TIR). Unlike partial reflection between transparent media, total internal reflection is accompanied by a non-trivial phase shift (not just zero or 180°) for each component of polarization (perpendicular or parallel to the plane of incidence), and the shifts vary with the angle of incidence. The explanation of this effect by Augustin-Jean Fresnel, in 1823, added to the evidence in favor of the wave theory of light.

The phase shifts are utilized by Fresnel's invention, the Fresnel rhomb, to modify polarization. The efficiency of the total internal reflection is exploited by optical fibers (used in telecommunications cables and in image-forming fiberscopes), and by reflective prisms, such as image-erecting Porro/roof prisms for monoculars and binoculars.
$
8
1. Total Internal Reflection (TIR) occurs when:
A: The second medium has a lower wave speed than the first.
B: Waves are incident at a direct angle on the interface.
C: The waves are refracted into the second medium.
D: Both media have the same refractive index.
E: The angle of incidence is less than the critical angle.
Answer: A

2. If the angle of incidence is beyond the critical angle, the result is:
A: Partial reflection.
B: Refraction into the external medium.
C: Both reflection and refraction.
D: Total internal reflection.
E: Dispersion of the incoming ray.
Answer: D

3. The evanescent wave is associated with:
A: Refraction of light.
B: Partial reflection between transparent media.
C: Total internal reflection where it travels along the interface.
D: Propagation of sound waves.
E: The mechanism of visible light.
Answer: C

4. For visible light, the critical angle for incidence from common glass to air is approximately:
A: 90°
B: 75°
C: 30°
D: 68°
E: 42°
Answer: E

5. In the context of TIR, what occurs when the angle of incidence approaches the critical angle?
A: The refracted ray becomes perpendicular to the boundary surface.
B: There is no reflection at all.
C: The refracted ray becomes parallel to the boundary surface.
D: The incident ray reverses its direction.
E: The refracted ray moves away from the boundary surface.
Answer: C

6. Optical fibers exploit which property related to total internal reflection?
A: The phase shifts induced by TIR.
B: The change in the angle of incidence.
C: The efficiency of the total internal reflection.
D: The attenuation of the evanescent wave.
E: The refraction at critical angle.
Answer: C

7. What happens to the amplitude of the evanescent wave as it travels further from the interface during total internal reflection?
A: It remains constant.
B: It increases exponentially.
C: It fluctuates randomly.
D: It decreases exponentially.
E: It doubles in magnitude.
Answer: D

8. What did Augustin-Jean Fresnel's explanation of an effect related to total internal reflection contribute to?
A: It disproved the wave theory of light.
B: It confirmed the particle theory of light.
C: It showed the impracticality of using prisms.
D: It supported the wave theory of light.
E: It introduced a new theory of reflection.
Answer: D
@
Subject:
The Wigner quasiprobability distribution (also called the Wigner function or the Wigner–Ville distribution, after Eugene Wigner and Jean-André Ville) is a quasiprobability distribution. It was introduced by Eugene Wigner in 1932[1] to study quantum corrections to classical statistical mechanics. The goal was to link the wavefunction that appears in Schrödinger's equation to a probability distribution in phase space.

It is a generating function for all spatial autocorrelation functions of a given quantum-mechanical wavefunction ψ(x). Thus, it maps[2] on the quantum density matrix in the map between real phase-space functions and Hermitian operators introduced by Hermann Weyl in 1927,[3] in a context related to representation theory in mathematics (see Weyl quantization). In effect, it is the Wigner–Weyl transform of the density matrix, so the realization of that operator in phase space. It was later rederived by Jean Ville in 1948 as a quadratic (in signal) representation of the local time-frequency energy of a signal,[4] effectively a spectrogram.

In 1949, José Enrique Moyal, who had derived it independently, recognized it as the quantum moment-generating functional,[5] and thus as the basis of an elegant encoding of all quantum expectation values, and hence quantum mechanics, in phase space (see Phase-space formulation). It has applications in statistical mechanics, quantum chemistry, quantum optics, classical optics and signal analysis in diverse fields, such as electrical engineering, seismology, time–frequency analysis for music signals, spectrograms in biology and speech processing, and engine design.
$
8
1. The Wigner quasiprobability distribution was initially introduced to:
A: Disprove Schrödinger's equation.
B: Highlight discrepancies in classical statistical mechanics.
C: Study quantum corrections to classical statistical mechanics.
D: Represent quantum expectations in real time.
E: Compare different phase-space functions.
Answer: C

2. Who was responsible for introducing the Wigner quasiprobability distribution?
A: Hermann Weyl.
B: José Enrique Moyal.
C: Jean-André Ville.
D: Eugene Wigner.
E: Jean Ville and Eugene Wigner collectively.
Answer: D

3. The Wigner–Weyl transform of the density matrix results in the:
A: Phase-space formulation.
B: Hermann Weyl function.
C: Wigner quasiprobability distribution.
D: Spectrogram of the signal.
E: Time-frequency energy representation.
Answer: C

4. What did Jean Ville rederive the Wigner function as in 1948?
A: A transformation for density matrix to phase space.
B: A method to represent quantum mechanics.
C: A quadratic representation of the local time-frequency energy of a signal.
D: A function that can decode any quantum mechanical process.
E: An alternative to Schrödinger's equation.
Answer: C

5. In what year did José Enrique Moyal recognize the Wigner quasiprobability distribution as the quantum moment-generating functional?
A: 1927
B: 1932
C: 1948
D: 1949
E: 1951
Answer: D

6. Which of the following is NOT an application of the Wigner function?
A: Quantum chemistry.
B: Classical optics.
C: Quantum optics.
D: Relativity theory.
E: Time–frequency analysis for music signals.
Answer: D

7. Hermann Weyl's contribution in the context of the Wigner function relates to:
A: The introduction of the quantum density matrix.
B: The phase-space formulation.
C: The relationship between real phase-space functions and Hermitian operators.
D: Quantum moment-generating functional.
E: The spectral representation of the signal.
Answer: C

8. The Wigner function is often associated with which of the following in mathematics?
A: Hamiltonian transformations.
B: Weyl quantization.
C: Fourier transformations.
D: Eigenfunction expansion.
E: Moyal bracket representation.
Answer: B
@
Subject:
The ΛCDM (Lambda cold dark matter) or Lambda-CDM model is a parameterization of the Big Bang cosmological model in which the universe contains three major components: first, a cosmological constant denoted by Lambda (Greek Λ) associated with dark energy; second, the postulated cold dark matter (abbreviated CDM); and third, ordinary matter. It is frequently referred to as the standard model of Big Bang cosmology because it is the simplest model that provides a reasonably good account of the following properties of the cosmos:

the existence and structure of the cosmic microwave background
the large-scale structure in the distribution of galaxies
the observed abundances of hydrogen (including deuterium), helium, and lithium
the accelerating expansion of the universe observed in the light from distant galaxies and supernovae
The model assumes that general relativity is the correct theory of gravity on cosmological scales. It emerged in the late 1990s as a concordance cosmology, after a period of time when disparate observed properties of the universe appeared mutually inconsistent, and there was no consensus on the makeup of the energy density of the universe.

The ΛCDM model can be extended by adding cosmological inflation, quintessence, and other areas of speculation and research in cosmology.

Some alternative models challenge the assumptions of the ΛCDM model. Examples of these are modified Newtonian dynamics, entropic gravity, modified gravity, theories of large-scale variations in the matter density of the universe, bimetric gravity, scale invariance of empty space, and decaying dark matter (DDM).[1][2][3][4][5]
$
8
1. What are the three major components of the ΛCDM model?
A: A cosmological constant (Lambda), cold dark matter (CDM), and hot dark matter.
B: A cosmological constant (Lambda), postulated cold dark matter (CDM), and dark energy.
C: A cosmological constant (Lambda), postulated cold dark matter (CDM), and ordinary matter.
D: Dark energy, dark matter, and a cosmological constant.
E: Dark matter, ordinary matter, and quintessence.
Answer: C

2. Why is the ΛCDM model frequently referred to as the standard model of Big Bang cosmology?
A: Because it challenges general relativity.
B: Because it emerged in the early 2000s.
C: Because it is the simplest model that accurately describes various observed properties of the universe.
D: Because it includes quintessence and inflation.
E: Because it assumes the Copernican principle.
Answer: C

3. Which property of the universe does the ΛCDM model NOT explain?
A: The existence and structure of the cosmic microwave background.
B: The large-scale structure in the distribution of galaxies.
C: The exact nature and origin of dark energy.
D: The observed abundances of hydrogen, helium, and lithium.
E: The accelerating expansion of the universe.
Answer: C

4. When did the ΛCDM model emerge as a consensus model of cosmology?
A: Early 2000s
B: Mid-1980s
C: Late 1990s
D: Early 1990s
E: Late 2000s
Answer: C

5. Which of the following is NOT an extension or speculation added to the ΛCDM model?
A: Cosmological inflation
B: Quintessence
C: Baryonic matter
D: Modified gravity
E: Decaying dark matter (DDM)
Answer: C

6. What theory challenges the assumption that the universe is expanding equally in all directions?
A: Modified Newtonian dynamics
B: Entropic gravity
C: Copernican principle
D: Bimetric gravity
E: Scale invariance of empty space
Answer: C

7. The ΛCDM model assumes which of the following as the correct theory of gravity on cosmological scales?
A: Quantum mechanics
B: Newtonian physics
C: General relativity
D: Special relativity
E: Entropic gravity
Answer: C

8. Decaying dark matter (DDM) is:
A: An extension of the ΛCDM model.
B: A principle that explains the existence of the cosmic microwave background.
C: A phenomenon where dark matter loses its gravitational influence over time.
D: An alternative model to ΛCDM that challenges its assumptions.
E: A theoretical proof of the existence of dark energy.
Answer: D
@
Subject:
In celestial mechanics, the Roche limit, also called Roche radius, is the distance from a celestial body within which a second celestial body, held together only by its own force of gravity, will disintegrate because the first body's tidal forces exceed the second body's self-gravitation.[1] Inside the Roche limit, orbiting material disperses and forms rings, whereas outside the limit, material tends to coalesce. The Roche radius depends on the radius of the first body and on the ratio of the bodies' densities.

The term is named after Édouard Roche (French: [ʁɔʃ], English: /rɒʃ/ ROSH), the French astronomer who first calculated this theoretical limit in 1848.

The Roche limit typically applies to a satellite's disintegrating due to tidal forces induced by its primary, the body around which it orbits. Parts of the satellite that are closer to the primary are attracted more strongly by gravity from the primary than parts that are farther away; this disparity effectively pulls the near and far parts of the satellite apart from each other, and if the disparity (combined with any centrifugal effects due to the object's spin) is larger than the force of gravity holding the satellite together, it can pull the satellite apart. Some real satellites, both natural and artificial, can orbit within their Roche limits because they are held together by forces other than gravitation. Objects resting on the surface of such a satellite would be lifted away by tidal forces. A weaker satellite, such as a comet, could be broken up when it passes within its Roche limit.

Since, within the Roche limit, tidal forces overwhelm the gravitational forces that might otherwise hold the satellite together, no satellite can gravitationally coalesce out of smaller particles within that limit. Indeed, almost all known planetary rings are located within their Roche limit. (Notable exceptions are Saturn's E-Ring and Phoebe ring. These two rings could possibly be remnants from the planet's proto-planetary accretion disc that failed to coalesce into moonlets, or conversely have formed when a moon passed within its Roche limit and broke apart.)

The Roche limit is not the only factor that causes comets to break apart. Splitting by thermal stress, internal gas pressure and rotational splitting are other ways for a comet to split under stress.
$
7
2. What happens to material that is inside the Roche limit of a celestial body?
A: It orbits the celestial body indefinitely.
B: It disperses and forms rings around the celestial body.
C: It is absorbed into the celestial body.
D: It is repelled away into space.
E: It remains stationary relative to the celestial body.
Answer: B

3. Which of the following is a reason some real satellites can orbit within their Roche limits?
A: They move at a faster orbital velocity.
B: They are held together by forces other than gravitation.
C: They are made of materials resistant to tidal forces.
D: They orbit at a special angle relative to the primary body.
E: Their Roche limits change dynamically.
Answer: B

4. What is one notable exception of planetary rings located outside their Roche limit around Saturn?
A: C-Ring
B: B-Ring
C: A-Ring
D: E-Ring
E: D-Ring
Answer: D

5. Why can't satellites gravitationally coalesce out of smaller particles within the Roche limit?
A: Because within the Roche limit, centrifugal forces prevent any coalescence.
B: Because the Roche limit prevents satellites from having any form of gravity.
C: Because within the Roche limit, tidal forces dominate over the gravitational forces trying to pull particles together.
D: Because the Roche limit only applies to solid objects and not to particles.
E: Because within the Roche limit, particles repel each other due to their charges.
Answer: C

6. Other than the Roche limit, which of the following factors can cause a comet to break apart?
A: Magnetic resonance
B: Splitting by thermal stress
C: Ionization by solar radiation
D: Centrifugal pressure
E: Orbital inclination changes
Answer: B

7. What might happen if a moon passed within its Roche limit?
A: The moon might grow in size.
B: The moon might shift its orbital plane.
C: The moon might gain a strong magnetic field.
D: The moon might break apart and form rings around its planet.
E: The moon might start rotating in the opposite direction.
Answer: D

8. What determines the specific Roche limit for a celestial body?
A: The age and temperature of the celestial body
B: The magnetic field strength of the celestial body
C: The radius of the first body and the ratio of the bodies' densities
D: The distance of the celestial body from its star
E: The composition and atmospheric pressure of the celestial body
Answer: C
@
Subject:
Martin Heidegger (/ˈhaɪdɛɡər, ˈhaɪdɪɡər/;[1] German: [ˈmaʁtiːn ˈhaɪdɛɡɐ];[1] 26 September 1889 – 26 May 1976) was a German philosopher who is best known for contributions to phenomenology, hermeneutics, and existentialism. He is often considered to be among the most important and influential philosophers of the 20th century. He has been widely criticized for supporting the Nazi Party after his election as rector at the University of Freiburg in 1933, and there has been controversy about the relationship between his philosophy and Nazism.

In Heidegger's first major text, Being and Time (1927), Dasein is introduced as a term for the type of being that humans possess. Heidegger believes that Dasein already has a "pre-ontological" and concrete understanding that shapes how it lives, which he analyzes in terms of "being-in-the-world".

Heidegger uses this analysis to approach the question of the meaning of being, that is, the question of how or why entities appear to us as the specific entities they are. In other words, Heidegger's governing "question of being" is "concerned with what makes beings intelligible as beings".

Being and Time (German: Sein und Zeit) is the 1927 magnum opus of German philosopher Martin Heidegger and a key document of existentialism. Being and Time had a notable impact on subsequent philosophy, literary theory and many other fields. Though controversial, its stature in intellectual history has been compared with works by Kant and Hegel. The book attempts to revive ontology through an analysis of Dasein, or "being-in-the-world." It is also noted for an array of neologisms and complex language, as well as an extended treatment of "authenticity" as a means to grasp and confront the unique and finite possibilities of the individual.

Heidegger believes that time finds its meaning in death, according to Michael Kelley. That is, time is understood only from a finite or mortal vantage. Dasein's fundamental characteristic and mode of "being-in-the-world" is temporal: Having been "thrown" into a world implies a "pastness" in its being. "The present is the nodal moment which makes past and future intelligible," writes Lilian Alweiss.[18] Dasein occupies itself with the present tasks required by goals it has projected on the future.[19]

Dasein as an intertwined subject/object cannot be separated from its objective "historicality," a concept Heidegger credits in the text to Wilhelm Dilthey. Dasein is "stretched along" temporally between birth and death, and thrown into its world; into its future possibilities which Dasein is charged with assuming. Dasein's access to this world and these possibilities is always via a history and a tradition—or "world historicality".
$
8
1. Which major work of Martin Heidegger is considered a key document of existentialism?
A: Philosophy and History
B: The Critique of Pure Reason
C: Phenomenology of Spirit
D: Sein und Zeit (Being and Time)
E: Thus Spoke Zarathustra
Answer: D

2. What does Heidegger use the term "Dasein" to describe?
A: The divine essence of human beings.
B: The socio-political conditions of 20th century Germany.
C: The type of being that humans possess.
D: The moral compass guiding human actions.
E: The inevitable end of all existence.
Answer: C

3. How did Martin Heidegger approach the "question of being"?
A: By determining the moral ethics of beings.
B: By examining what makes beings intelligible as beings.
C: By understanding the evolutionary process of beings.
D: By questioning the religious origins of beings.
E: By contrasting beings to inanimate objects.
Answer: B

4. The complex language and extended treatment of "authenticity" in Heidegger's "Being and Time" is intended to...
A: Establish a unified theory of relativity.
B: Contradict previous philosophical stances on the nature of reality.
C: Grasp and confront the unique and finite possibilities of the individual.
D: Highlight the primary role of human consciousness in the universe.
E: Prove the superiority of existentialism over other philosophical doctrines.
Answer: C

5. According to Heidegger, Dasein's fundamental characteristic and mode of "being-in-the-world" is:
A: Static.
B: Cyclical.
C: Infinite.
D: Temporal.
E: Spatial.
Answer: D

6. Which philosopher is credited by Heidegger in "Being and Time" for the concept of "historicality"?
A: Friedrich Nietzsche
B: Immanuel Kant
C: Jean-Paul Sartre
D: Georg Wilhelm Friedrich Hegel
E: Wilhelm Dilthey
Answer: E

7. Heidegger's belief regarding time can best be summarized as:
A: Time being a philosophical construct that only exists in human consciousness.
B: Time being a constant factor, external to human beings and uncontrollable.
C: Time being understood from a finite or mortal vantage, where "pastness" implies being "thrown" into a world.
D: Time being an illusion which humans need to transcend to achieve enlightenment.
E: Time being a linear progression that dictates the evolutionary trajectory of humans.
Answer: C

8. Why has Martin Heidegger been a controversial figure in the 20th century?
A: Because of his rejection of existentialism as a philosophical school.
B: Because of his deep exploration of the concept of "Dasein".
C: Because of his support for the Nazi Party after becoming rector at the University of Freiburg.
D: Because of his criticism of other influential philosophers of his time.
E: Because of his views on the non-existence of time.
Answer: C
@
Subject:
The ultraviolet catastrophe, also called the Rayleigh–Jeans catastrophe, was the prediction of late 19th century/early 20th century classical physics that an ideal black body at thermal equilibrium would emit an unbounded quantity of energy as wavelength decreased into the ultraviolet range.[1]: 6–7  The term "ultraviolet catastrophe" was first used in 1911 by Paul Ehrenfest,[2] but the concept originated with the 1900 statistical derivation of the Rayleigh–Jeans law.

The phrase refers to the fact that the empirically derived Rayleigh–Jeans law, which accurately predicted experimental results at large wavelengths, failed to do so for short wavelengths. (See the image for further elaboration.) As the theory diverged from empirical observations when these frequencies reached the ultraviolet region of the electromagnetic spectrum, there was a problem.[3] This problem was later found to be due to a property of quanta as proposed by Max Planck: There could be no fraction of a discrete energy package already carrying minimal energy.

Since the first use of this term, it has also been used for other predictions of a similar nature, as in quantum electrodynamics and such cases as ultraviolet divergence.
$
8
1. The ultraviolet catastrophe was a problem associated with the predictions of which of the following?
A: Quantum physics
B: Newtonian mechanics
C: Classical physics
D: Relativistic physics
E: Atomic physics
Answer: C

2. Which law, derived from classical physics, failed to accurately predict experimental results for short wavelengths?
A: Planck's law
B: Newton's third law
C: Ohm's law
D: Rayleigh–Jeans law
E: Coulomb's law
Answer: D

3. Who first used the term "ultraviolet catastrophe"?
A: Max Planck
B: James Clerk Maxwell
C: Lord Rayleigh
D: Albert Einstein
E: Paul Ehrenfest
Answer: E

4. The discrepancy between the theory and experimental observations in the ultraviolet catastrophe occurred in which region of the electromagnetic spectrum?
A: Infrared
B: Radio wave
C: Visible
D: X-ray
E: Ultraviolet
Answer: E

5. How was the problem of the ultraviolet catastrophe resolved?
A: By the introduction of the quantum theory of light.
B: By altering the wavelength of the black body radiation.
C: By increasing the temperature of the black body.
D: By developing a new statistical derivation of the Rayleigh–Jeans law.
E: By proving the Rayleigh–Jeans law as a flawed concept.
Answer: A

6. According to Max Planck's proposal, which helped resolve the ultraviolet catastrophe, what cannot exist?
A: Electromagnetic waves in the visible spectrum
B: Fraction of a discrete energy package already carrying minimal energy
C: The existence of the Rayleigh–Jeans law
D: High-frequency electromagnetic waves
E: Black body radiation at high temperatures
Answer: B

7. Which phenomenon or concept was the ultraviolet catastrophe primarily concerned with?
A: The motion of planets
B: The behavior of charged particles in a magnetic field
C: The emission of energy from an ideal black body at thermal equilibrium
D: The reflection of light off a surface
E: The absorption of energy by molecules
Answer: C

8. The ultraviolet catastrophe has been referenced in other areas of physics, such as quantum electrodynamics, mainly due to which kind of issue?
A: Predictions of infinite energy
B: Predictions of zero energy
C: Predictions of cyclic motion
D: Predictions of linear progression of energy
E: Predictions of quantized states
Answer: A
@
Subject:
The shower-curtain effect in physics describes the phenomenon of a shower curtain being blown inward when a shower is running. The problem of identifying the cause of this effect has been featured in Scientific American magazine, with several theories given to explain the phenomenon but no definite conclusion.

The shower-curtain effect may also be used to describe the observation how nearby phase front distortions of an optical wave are more severe than remote distortions of the same amplitude.

Buoyancy hypothesis
See also: Cooling tower
Also called Chimney effect or Stack effect, observes that warm air (from the hot shower) rises out over the shower curtain as cooler air (near the floor) pushes in under the curtain to replace the rising air. By pushing the curtain in towards the shower, the (short range) vortex and Coandă effects become more significant. However, the shower-curtain effect persists when cold water is used, implying that this cannot be the only mechanism at work.[1]

Bernoulli effect hypothesis
The most popular explanation given for the shower-curtain effect is Bernoulli's principle.[1] Bernoulli's principle states that an increase in velocity results in a decrease in pressure. This theory presumes that the water flowing out of a shower head causes the air through which the water moves to start flowing in the same direction as the water. This movement would be parallel to the plane of the shower curtain. If air is moving across the inside surface of the shower curtain, Bernoulli's principle says the air pressure there will drop. This would result in a pressure differential between the inside and outside, causing the curtain to move inward. It would be strongest when the gap between the bather and the curtain is smallest, resulting in the curtain attaching to the bather.[citation needed]

Hypotheses

Horizontal vortex hypothesis
A computer simulation of a typical bathroom found that none of the above theories pan out in their analysis, but instead found that the spray from the shower-head drives a horizontal vortex. This vortex has a low-pressure zone in the centre, which sucks the curtain.[2][1]

David Schmidt of the University of Massachusetts was awarded the 2001 Ig Nobel Prize in Physics for his partial solution to the question of why shower curtains billow inwards. He used a computational fluid dynamics code to achieve the results. Professor Schmidt is adamant that this was done "for fun" in his own free time without the use of grants.[3]

Coandă effect
The Coandă effect, also known as "boundary layer attachment", is the tendency of a moving fluid to adhere to an adjacent wall.[1]

Condensation
A hot shower will produce steam that condenses on the shower side of the curtain, lowering the pressure there. In a steady state the steam will be replaced by new steam delivered by the shower but in reality the water temperature will fluctuate and lead to times when the net steam production is negative.[citation needed]

Air pressure
Colder dense air outside and hot less dense air inside causes higher air pressure on the outside to force the shower curtain inwards to equalise the air pressure, this can be observed simply when the bathroom door is open allowing cold air into the bathroom.
$
8
1. Which hypothesis suggests that warm air from the shower rises out over the curtain while cooler air pushes in under the curtain?
A: Chimney effect hypothesis
B: Bernoulli effect hypothesis
C: Horizontal vortex hypothesis
D: Coandă effect
E: Condensation
Answer: A

2. Who was awarded the 2001 Ig Nobel Prize in Physics for a partial solution to why shower curtains billow inwards using computational fluid dynamics?
A: Bernoulli
B: David Schmidt
C: Coandă
D: Charles Darwin
E: Albert Einstein
Answer: B

3. Which effect or phenomenon describes the tendency of a moving fluid to stick to an adjacent wall?
A: Horizontal vortex
B: Stack effect
C: Coandă effect
D: Bernoulli's principle
E: Condensation
Answer: C

4. In the context of the shower-curtain effect, what is the outcome of Bernoulli's principle when water flows from the shower head causing air to move in the same direction?
A: Increase in air pressure on the inside of the curtain
B: Decrease in air pressure on the inside of the curtain
C: No change in air pressure on either side of the curtain
D: Uniform pressure on both sides of the curtain
E: A horizontal vortex that pushes the curtain outwards
Answer: B

5. A hypothesis suggesting that the spray from the shower-head drives a horizontal vortex, creating a low-pressure zone in the center that draws the curtain inwards, is known as:
A: Chimney effect
B: Bernoulli effect
C: Horizontal vortex hypothesis
D: Coandă effect
E: Condensation effect
Answer: C

6. Which of the following hypotheses suggests that steam from a hot shower condenses on the shower side of the curtain, leading to a reduction in pressure there?
A: Stack effect
B: Bernoulli effect
C: Horizontal vortex
D: Coandă effect
E: Condensation
Answer: E

7. What does the Coandă effect also refer to in the context of the shower-curtain phenomenon?
A: Air circulation due to heat
B: The curtain's attraction to a bather
C: A horizontal vortex driven by the shower-head
D: "Boundary layer attachment"
E: Condensation of steam on the curtain
Answer: D

8. Why, according to one hypothesis, might a shower curtain be forced inwards when the bathroom door is open?
A: Due to the Coandă effect
B: Because of the Bernoulli's principle
C: Owing to the influx of cold air creating a pressure differential
D: Due to the horizontal vortex created by the shower-head
E: Because of the condensation of steam on the outside of the curtain
Answer: C
@
Subject:
In chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.

The term is closely associated with the work of mathematician and meteorologist Edward Norton Lorenz. He noted that the butterfly effect is derived from the metaphorical example of the details of a tornado (the exact time of formation, the exact path taken) being influenced by minor perturbations such as a distant butterfly flapping its wings several weeks earlier. Lorenz originally used a seagull causing a storm but was persuaded to make it more poetic with the use of a butterfly and tornado by 1972.[1][2] He discovered the effect when he observed runs of his weather model with initial condition data that were rounded in a seemingly inconsequential manner. He noted that the weather model would fail to reproduce the results of runs with the unrounded initial condition data. A very small change in initial conditions had created a significantly different outcome.[3]

The idea that small causes may have large effects in weather was earlier acknowledged by French mathematician and engineer Henri Poincaré. American mathematician and philosopher Norbert Wiener also contributed to this theory. Lorenz's work placed the concept of instability of the Earth's atmosphere onto a quantitative base and linked the concept of instability to the properties of large classes of dynamic systems which are undergoing nonlinear dynamics and deterministic chaos.[4]

The butterfly effect concept has since been used outside the context of weather science as a broad term for any situation where a small change is supposed to be the cause of larger consequences.
$
8
1. Who is most closely associated with the term "butterfly effect" in the context of chaos theory?
A: Henri Poincaré
B: Norbert Wiener
C: Edward Norton Lorenz
D: Sir Isaac Newton
E: Albert Einstein
Answer: C

2. The butterfly effect originates from the idea that the details of which natural disaster could be influenced by minor disturbances like a butterfly flapping its wings?
A: Earthquake
B: Tsunami
C: Tornado
D: Volcanic eruption
E: Hurricane
Answer: C

3. Edward Lorenz initially used which creature causing a storm before transitioning to the more poetic butterfly?
A: Seagull
B: Sparrow
C: Dragonfly
D: Hawk
E: Hummingbird
Answer: A

4. What did Lorenz notice when he ran his weather model with slightly different initial condition data?
A: The results were marginally different
B: The model predicted weather with 100% accuracy
C: The weather model failed to reproduce the results of runs with unrounded initial condition data
D: The data showed a linear trend in all conditions
E: The butterfly effect was invalidated
Answer: C

5. Which French mathematician and engineer previously acknowledged the idea that small causes may have large effects in weather before Lorenz?
A: Blaise Pascal
B: Pierre-Simon Laplace
C: René Descartes
D: Henri Poincaré
E: Jean-Baptiste Joseph Fourier
Answer: D

6. What did Lorenz's work link the concept of instability to?
A: The properties of large classes of dynamic systems undergoing linear changes
B: The idea of changing weather patterns due to human intervention
C: The properties of large classes of dynamic systems undergoing nonlinear dynamics and deterministic chaos
D: The shift in global weather patterns due to the Earth's rotation
E: The inevitable outcome of weather regardless of initial conditions
Answer: C

7. The butterfly effect concept has been used as a general term to represent what?
A: A broad idea that only large changes can cause significant effects
B: The understanding that everything in nature is linear and predictable
C: Any situation where a small change can cause larger consequences
D: The significance of butterflies in maintaining ecological balance
E: The connection between physics and biological systems
Answer: C

8. Which mathematician and philosopher from the United States also made contributions to the theory of the butterfly effect?
A: George Boole
B: Benjamin Peirce
C: Alan Turing
D: Norbert Wiener
E: David Hilbert
Answer: D
@
Subject:
The Leidenfrost effect is a physical phenomenon in which a liquid, close to a surface that is significantly hotter than the liquid's boiling point, produces an insulating vapor layer that keeps the liquid from boiling rapidly. Because of this repulsive force, a droplet hovers over the surface, rather than making physical contact with it. The effect is named after the German doctor Johann Gottlob Leidenfrost, who described it in A Tract About Some Qualities of Common Water.

This is most commonly seen when cooking, when drops of water are sprinkled onto a hot pan. If the pan's temperature is at or above the Leidenfrost point, which is approximately 193 °C (379 °F) for water, the water skitters across the pan and takes longer to evaporate than it would take if the water droplets had been sprinkled onto a cooler pan.

The effect can be seen as drops of water are sprinkled onto a pan at various times as it heats up. Initially, as the temperature of the pan is just below 100 °C (212 °F), the water flattens out and slowly evaporates, or if the temperature of the pan is well below 100 °C (212 °F), the water stays liquid. As the temperature of the pan rises above 100 °C (212 °F), the water droplets hiss when touching the pan, and these droplets evaporate quickly. When the temperature exceeds the Leidenfrost point, the Leidenfrost effect appears. On contact with the pan, the water droplets bunch up into small balls of water and skitter around, lasting much longer than when the temperature of the pan was lower. This effect works until a much higher temperature causes any further drops of water to evaporate too quickly to cause this effect.

The effect happens because, at temperatures at or above the Leidenfrost point, the bottom part of the water droplet vaporizes immediately on contact with the hot pan. The resulting gas suspends the rest of the water droplet just above it, preventing any further direct contact between the liquid water and the hot pan. As steam has much poorer thermal conductivity than the metal pan, further heat transfer between the pan and the droplet is slowed down dramatically. This also results in the drop being able to skid around the pan on the layer of gas just under it.

Non-volatile materials were discovered in 2015 to also exhibit a 'reactive Leidenfrost effect', whereby solid particles were observed to float above hot surfaces and skitter around erratically.[14] Detailed characterization of the reactive Leidenfrost effect was completed for small particles of cellulose (~0.5 mm) on high temperature polished surfaces by high speed photography. Cellulose was shown to decompose to short-chain oligomers which melt and wet smooth surfaces with increasing heat transfer associated with increasing surface temperature. Above 675 °C (1,247 °F), cellulose was observed to exhibit transition boiling with violent bubbling and associated reduction in heat transfer. Liftoff of the cellulose droplet (depicted at the right) was observed to occur above about 750 °C (1,380 °F), associated with a dramatic reduction in heat transfer.[14]
$
8
1. Who is the Leidenfrost effect named after?
A: Isaac Newton
B: Albert Einstein
C: Johann Gottlob Leidenfrost
D: Richard Feynman
E: James Clerk Maxwell
Answer: C

2. When water droplets are sprinkled onto a pan that is at or above the Leidenfrost point, what behavior do the droplets exhibit?
A: They flatten out and evaporate immediately.
B: They freeze upon contact with the pan.
C: They bunch up into balls and skitter around.
D: They sink into the pan.
E: They expand and cover the entire surface.
Answer: C

3. What temperature is the Leidenfrost point for water?
A: 100 °C (212 °F)
B: 193 °C (379 °F)
C: 75 °C (167 °F)
D: 250 °C (482 °F)
E: 0 °C (32 °F)
Answer: B

4. At temperatures above the Leidenfrost point, why doesn't the water droplet make direct contact with the pan?
A: Because the droplet is repelled by the pan's magnetic field.
B: Because the droplet turns into a solid upon contact.
C: Because the bottom part of the water droplet vaporizes and the gas suspends the droplet.
D: Because the pan absorbs the droplet immediately.
E: Because the droplet turns into a gas before reaching the pan.
Answer: C

5. When a pan's temperature is just below 100 °C (212 °F), how do water droplets behave upon contact?
A: They skitter around like balls.
B: They flatten out and slowly evaporate.
C: They freeze and become solid.
D: They immediately vaporize.
E: They sink into the pan.
Answer: B

6. Why are droplets able to skid around the pan during the Leidenfrost effect?
A: Because they become magnetically charged.
B: Because they are pushed by external air currents.
C: Because of the layer of gas (steam) just under them.
D: Because the pan becomes slippery at high temperatures.
E: Because the droplets turn into solids and roll.
Answer: C

7. What occurs to cellulose above a temperature of 675 °C (1,247 °F)?
A: It remains stable and doesn't change.
B: It exhibits transition boiling with violent bubbling.
C: It turns into a gas without any transition phase.
D: It melts into a pool of liquid cellulose.
E: It freezes and becomes a hard solid.
Answer: B

8. Liftoff of the cellulose droplet, in the context of the reactive Leidenfrost effect, is observed to occur above which temperature?
A: 500 °C (932 °F)
B: 675 °C (1,247 °F)
C: 750 °C (1,380 °F)
D: 212 °C (414 °F)
E: 379 °C (714 °F)
Answer: C
@
Subject:
Reciprocal length or inverse length is a quantity or measurement used in several branches of science and mathematics. As the reciprocal of length, common units used for this measurement include the reciprocal metre or inverse metre (symbol: m−1), the reciprocal centimetre or inverse centimetre (symbol: cm−1).

Quantities measured in reciprocal length include:

absorption coefficient or attenuation coefficient, in materials science
curvature of a line, in mathematics
gain, in laser physics
magnitude of vectors in reciprocal space, in crystallography
more generally any spatial frequency e.g. in cycles per unit length
optical power of a lens, in optics
rotational constant of a rigid rotor, in quantum mechanics
wavenumber, or magnitude of a wavevector, in spectroscopy
density of a linear feature in hydrology and other fields; see kilometre per square kilometre
surface area to volume ratio
In optics, the dioptre is a unit equivalent to reciprocal metre.
$
4
Question 2: In which of the following fields is reciprocal length used to measure the optical power of a lens?
A: Hydrology
B: Laser physics
C: Optics
D: Geography
E: Chemistry
Answer: C

Question 3: What is the unit used to measure the rotational constant of a rigid rotor in quantum mechanics?
A: kg−1
B: m−1
C: s−1
D: km−2
E: %−1
Answer: B

Question 4: Which statement about reciprocal length is true?
A: It measures the reciprocal of mass.
B: The dioptre in optics is equivalent to a reciprocal second.
C: Reciprocal length measures spatial frequency in cycles per unit length.
D: It is the reciprocal of interest rate.
E: Common units include reciprocal second and reciprocal minute.
Answer: C

Question 5: In the context of materials science, what is measured in terms of reciprocal length?
A: Density
B: Absorption or attenuation coefficient
C: Refractive index
D: Thermal conductivity
E: Viscosity
Answer: B
@
Subject:
A planetary system is a set of gravitationally bound non-stellar objects in or out of orbit around a star or star system. Generally speaking, systems with one or more planets constitute a planetary system, although such systems may also consist of bodies such as dwarf planets, asteroids, natural satellites, meteoroids, comets, planetesimals[1][2] and circumstellar disks. The Sun together with the planetary system revolving around it, including Earth, forms the Solar System.[3][4] The term exoplanetary system is sometimes used in reference to other planetary systems.

Orbital dynamics
Planetary systems can be categorized according to their orbital dynamics as resonant, non-resonant-interacting, hierarchical, or some combination of these. In resonant systems the orbital periods of the planets are in integer ratios. The Kepler-223 system contains four planets in an 8:6:4:3 orbital resonance.[41] Giant planets are found in mean-motion resonances more often than smaller planets.[42] In interacting systems the planets orbits are close enough together that they perturb the orbital parameters. The Solar System could be described as weakly interacting. In strongly interacting systems Kepler's laws do not hold.[43] In hierarchical systems the planets are arranged so that the system can be gravitationally considered as a nested system of two-bodies, e.g. in a star with a close-in hot jupiter with another gas giant much further out, the star and hot jupiter form a pair that appears as a single object to another planet that is far enough out.

Other, as yet unobserved, orbital possibilities include: double planets; various co-orbital planets such as quasi-satellites, trojans and exchange orbits; and interlocking orbits maintained by precessing orbital planes.[44]

Orbital Resonance
In celestial mechanics, orbital resonance occurs when orbiting bodies exert regular, periodic gravitational influence on each other, usually because their orbital periods are related by a ratio of small integers. Most commonly, this relationship is found between a pair of objects (binary resonance). The physical principle behind orbital resonance is similar in concept to pushing a child on a swing, whereby the orbit and the swing both have a natural frequency, and the body doing the "pushing" will act in periodic repetition to have a cumulative effect on the motion. Orbital resonances greatly enhance the mutual gravitational influence of the bodies (i.e., their ability to alter or constrain each other's orbits). In most cases, this results in an unstable interaction, in which the bodies exchange momentum and shift orbits until the resonance no longer exists. Under some circumstances, a resonant system can be self-correcting and thus stable. Examples are the 1:2:4 resonance of Jupiter's moons Ganymede, Europa and Io, and the 2:3 resonance between Neptune and Pluto. Unstable resonances with Saturn's inner moons give rise to gaps in the rings of Saturn. The special case of 1:1 resonance between bodies with similar orbital radii causes large planetary system bodies to eject most other bodies sharing their orbits; this is part of the much more extensive process of clearing the neighbourhood, an effect that is used in the current definition of a planet.[1]

A binary resonance ratio in this article should be interpreted as the ratio of number of orbits completed in the same time interval, rather than as the ratio of orbital periods, which would be the inverse ratio. Thus, the 2:3 ratio above means that Pluto completes two orbits in the time it takes Neptune to complete three. In the case of resonance relationships among three or more bodies, either type of ratio may be used (whereby the smallest whole-integer ratio sequences are not necessarily reversals of each other), and the type of ratio will be specified.
$
8
Which of the following is NOT included in the description of a planetary system?
A: Binary stars.
B: Dwarf planets.
C: Natural satellites.
D: Comets.
E: Exoplanets.
Answer: A

How is the term "exoplanetary system" specifically used?
A: To describe the Solar System.
B: In reference to the Earth's planetary system.
C: In reference to other planetary systems.
D: To refer to systems without stars.
E: To describe systems without planets.
Answer: C

In a resonant system, what is the relationship between the orbital periods of the planets?
A: They are in a sequence of even numbers.
B: They are unpredictable.
C: They are in non-integer ratios.
D: They are in integer ratios.
E: They are always in a 1:1 ratio.
Answer: D

What describes the interaction in the Solar System based on its orbital dynamics?
A: Strongly interacting.
B: Hierarchical.
C: Resonant.
D: Weakly interacting.
E: Non-resonant.
Answer: D

What is the concept behind orbital resonance in celestial mechanics?
A: Orbiting bodies repel each other periodically.
B: Orbiting bodies exert no influence on each other.
C: Orbiting bodies exert regular, periodic gravitational influence on each other.
D: Orbiting bodies always stabilize each other's orbits.
E: Orbiting bodies always share the same orbital period.
Answer: C

What results from the unstable interaction in orbital resonance?
A: The bodies form binary stars.
B: The bodies always remain in a stable orbit.
C: The bodies eject each other from their orbits.
D: The bodies exchange momentum and shift orbits.
E: The bodies' orbital periods become identical.
Answer: D

Which of the following is true about the binary resonance ratio as used in the provided article?
A: It represents the ratio of orbital periods.
B: It indicates that Neptune completes two orbits while Pluto completes three.
C: It means that Pluto completes two orbits in the time Neptune completes three.
D: It is always represented in an even number ratio.
E: It cannot be applied to more than two bodies.
Answer: C

Which resonance is responsible for causing gaps in the rings of Saturn?
A: 1:2:4 resonance.
B: Resonance with Saturn's outer moons.
C: 2:3 resonance.
D: 1:1 resonance.
E: Unstable resonances with Saturn's inner moons.
Answer: E
@
Subject:
In physics, electromagnetic radiation (EMR) consists of waves of the electromagnetic (EM) field, which propagate through space and carry momentum and electromagnetic radiant energy.[1] Types of EMR include radio waves, microwaves, infrared, (visible) light, ultraviolet, X-rays, and gamma rays, all of which are part of the electromagnetic spectrum.[2]

The propagation constant of a sinusoidal electromagnetic wave is a measure of the change undergone by the amplitude and phase of the wave as it propagates in a given direction. The quantity being measured can be the voltage, the current in a circuit, or a field vector such as electric field strength or flux density. The propagation constant itself measures the change per unit length, but it is otherwise dimensionless. In the context of two-port networks and their cascades, propagation constant measures the change undergone by the source quantity as it propagates from one port to the next.

The propagation constant's value is expressed logarithmically, almost universally to the base e, rather than the more usual base 10 that is used in telecommunications in other situations. The quantity measured, such as voltage, is expressed as a sinusoidal phasor. The phase of the sinusoid varies with distance which results in the propagation constant being a complex number, the imaginary part being caused by the phase change.
$
7
Which of the following is NOT a type of electromagnetic radiation?
A: Sound waves.
B: Microwaves.
C: Infrared.
D: X-rays.
E: Gamma rays.
Answer: A

Electromagnetic radiation carries:
A: Only momentum.
B: Only electromagnetic radiant energy.
C: Both momentum and electromagnetic radiant energy.
D: Neither momentum nor electromagnetic radiant energy.
E: Only electric field strength.
Answer: C

In the context of two-port networks, the propagation constant measures:
A: The change undergone by the source quantity as it remains in one port.
B: The phase of the sinusoid as it varies with distance.
C: The change undergone by the source quantity as it propagates from one port to the next.
D: The voltage change as it remains constant over distance.
E: The amplitude of the sinusoidal wave that varies with frequency.
Answer: C

The propagation constant's value is expressed:
A: Exclusively to the base 10.
B: Logarithmically, usually to the base e.
C: As a sinusoidal phasor.
D: Only as a real number.
E: Exclusively as a dimensionless quantity.
Answer: B

Which part of the propagation constant is caused by the phase change in the sinusoidal wave?
A: The real part.
B: The imaginary part.
C: Both the real and imaginary parts.
D: The propagation constant does not have a part caused by phase change.
E: The amplitude.
Answer: B

The propagation constant measures the change per:
A: Unit volume.
B: Unit frequency.
C: Unit time.
D: Unit length.
E: Unit amplitude.
Answer: D

Which of the following best describes the relationship between the propagation constant and the quantity it measures, like voltage?
A: The quantity, such as voltage, is expressed logarithmically to the base 10.
B: The quantity remains constant, irrespective of the propagation constant.
C: The quantity is expressed as a sinusoidal phasor.
D: The propagation constant directly determines the phase of the voltage sinusoid.
E: The quantity, such as voltage, is always a real number.
Answer: C
@
Subject:
Gravitoelectromagnetism, abbreviated GEM, refers to a set of formal analogies between the equations for electromagnetism and relativistic gravitation; specifically: between Maxwell's field equations and an approximation, valid under certain conditions, to the Einstein field equations for general relativity. Gravitomagnetism is a widely used term referring specifically to the kinetic effects of gravity, in analogy to the magnetic effects of moving electric charge.[1] The most common version of GEM is valid only far from isolated sources, and for slowly moving test particles.

The analogy and equations differing only by some small factors were first published in 1893, before general relativity, by Oliver Heaviside as a separate theory expanding Newton's law.[2]

According to general relativity, the gravitational field produced by a rotating object (or any rotating mass–energy) can, in a particular limiting case, be described by equations that have the same form as in classical electromagnetism.
$
7
Gravitoelectromagnetism (GEM) primarily draws analogies between which two fields?
A: Classical electromagnetism and quantum mechanics.
B: Classical electromagnetism and relativistic gravitation.
C: Quantum mechanics and relativistic gravitation.
D: Classical mechanics and quantum gravitation.
E: Thermodynamics and classical electromagnetism.
Answer: B

Which of the following best describes the conditions under which the most common version of GEM is valid?
A: Near isolated sources and for rapidly moving test particles.
B: Only in the vicinity of massive celestial bodies and for stationary test particles.
C: Far from isolated sources and for rapidly moving test particles.
D: Far from isolated sources and for slowly moving test particles.
E: Only in a vacuum and for test particles in equilibrium.
Answer: D

Who first published the analogy and equations that led to the idea of Gravitoelectromagnetism before general relativity?
A: Albert Einstein.
B: Isaac Newton.
C: Max Planck.
D: Oliver Heaviside.
E: James Clerk Maxwell.
Answer: D

Gravitoelectromagnetism is an approximation to which field equations?
A: Newton's laws of motion.
B: Maxwell's field equations.
C: Schrödinger's wave equation.
D: Einstein field equations for general relativity.
E: Lorentz force law.
Answer: D

The gravitational field produced by a rotating object, according to general relativity, can be described by equations similar to those in:
A: Quantum mechanics.
B: Thermodynamics.
C: Classical mechanics.
D: Classical electromagnetism.
E: Quantum field theory.
Answer: D

Why is gravitomagnetism often referred to in relation to the magnetic effects of moving electric charge?
A: Because it describes the kinetic effects of gravity in analogy to the magnetic effects of moving electric charge.
B: Because it operates in the same medium as electromagnetism.
C: Because it can be shielded or enhanced by materials in the same way as electromagnetism.
D: Because it has the same force-carrying particle as electromagnetism.
E: Because it is a force that acts against gravity, similar to how magnetism can oppose electric charges.
Answer: A

In what year was the first set of analogies and equations for Gravitoelectromagnetism published?
A: 1600.
B: 1893.
C: 1915.
D: 1985.
E: 2001.
Answer: B
@
Subject:
Sir Isaac Newton FRS (25 December 1642 – 20 March 1726/27)[a] was an English mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher. He was a key figure in the Scientific Revolution and the Enlightenment that followed. His pioneering book Philosophiæ Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy), first published in 1687, consolidated many previous results and established classical mechanics.[17][18] Newton also made seminal contributions to optics, and shares credit with German mathematician Gottfried Wilhelm Leibniz for developing infinitesimal calculus.

In the Principia, Newton formulated the laws of motion and universal gravitation that formed the dominant scientific viewpoint for centuries until it was superseded by the theory of relativity. Newton used his mathematical description of gravity to derive Kepler's laws of planetary motion, account for tides, the trajectories of comets, the precession of the equinoxes and other phenomena, eradicating doubt about the Solar System's heliocentricity. He demonstrated that the motion of objects on Earth and celestial bodies could be accounted for by the same principles. Newton's inference that the Earth is an oblate spheroid was later confirmed by the geodetic measurements of Maupertuis, La Condamine, and others, convincing most European scientists of the superiority of Newtonian mechanics over earlier systems.

Newton built the first practical reflecting telescope and developed a sophisticated theory of colour based on the observation that a prism separates white light into the colours of the visible spectrum. His work on light was collected in his highly influential book Opticks, published in 1704. He also formulated an empirical law of cooling, made the first theoretical calculation of the speed of sound, and introduced the notion of a Newtonian fluid. In addition to his work on calculus, as a mathematician Newton contributed to the study of power series, generalised the binomial theorem to non-integer exponents, developed a method for approximating the roots of a function, and classified most of the cubic plane curves.

Newton was a fellow of Trinity College and the second Lucasian Professor of Mathematics at the University of Cambridge. He was a devout but unorthodox Christian who privately rejected the doctrine of the Trinity. He refused to take holy orders in the Church of England, unlike most members of the Cambridge faculty of the day. Beyond his work on the mathematical sciences, Newton dedicated much of his time to the study of alchemy and biblical chronology, but most of his work in those areas remained unpublished until long after his death. Politically and personally tied to the Whig party, Newton served two brief terms as Member of Parliament for the University of Cambridge, in 1689–1690 and 1701–1702. He was knighted by Queen Anne in 1705 and spent the last three decades of his life in London, serving as Warden (1696–1699) and Master (1699–1727) of the Royal Mint, as well as president of the Royal Society (1703–1727).
$
8
Which of the following publications by Sir Isaac Newton consolidated previous results and established classical mechanics?
A: Opticks.
B: Calculus and its Infinite Series.
C: Philosophiæ Naturalis Principia Mathematica.
D: Newtonian Fluid Dynamics.
E: The Spectrum of Colours.
Answer: C

In the realm of optics, Newton's major contribution was:
A: Developing the law of reflection.
B: Demonstrating the wave nature of light.
C: Building the first practical reflecting telescope.
D: Discovering the concept of total internal reflection.
E: Introducing the double slit experiment.
Answer: C

Newton and which other mathematician are credited with the development of infinitesimal calculus?
A: Blaise Pascal.
B: Carl Friedrich Gauss.
C: Albert Einstein.
D: Gottfried Wilhelm Leibniz.
E: Pierre-Simon Laplace.
Answer: D

One of Newton's significant inferences about Earth was:
A: That the Earth rotates on its axis.
B: That the Earth is a perfect sphere.
C: That the Earth is an oblate spheroid.
D: That the Earth orbits around the sun in an elliptical path.
E: That the Earth has a magnetic core.
Answer: C

Newton's work on which of the following topics remained mostly unpublished during his lifetime?
A: Classical mechanics.
B: Optics.
C: Infinitesimal calculus.
D: Alchemy and biblical chronology.
E: Universal gravitation.
Answer: D

For how many terms did Newton serve as Member of Parliament for the University of Cambridge?
A: One term.
B: Two terms.
C: Three terms.
D: Four terms.
E: Five terms.
Answer: B

Newton was a professor at which esteemed institution?
A: Oxford University.
B: London School of Economics.
C: University of Edinburgh.
D: Trinity College.
E: University of Cambridge.
Answer: E

Under which monarch was Sir Isaac Newton knighted?
A: King Charles II.
B: Queen Elizabeth I.
C: King James II.
D: Queen Victoria.
E: Queen Anne.
Answer: E
@
Subject:
When embedded in an atomic nucleus, neutrons are (usually) stable particles. Outside the nucleus, free neutrons are unstable and have a mean lifetime of 879.6±0.8 s (about 14 min, 39.6 s).[1] Therefore, the half-life for this process (which differs from the mean lifetime by a factor of ln(2) ≈ 0.693) is 611±1 s (about 10 min, 11 s).[2][3] (An article[4] published in October 2021, arrives at 877.75+0.50
−0.44 s for the mean lifetime).

The beta decay of the neutron described in this article can be notated at four slightly different levels of detail, as shown in four layers of Feynman diagrams in a section below.

n^0 → p^+ + e^− + ν^e

The hard-to-observe 
W−
 quickly decays into an electron and its matching antineutrino. The subatomic reaction shown immediately above depicts the process as it was first understood, in the first half of the 20th century. The boson ( 
W−
 ) vanished so quickly that it was not detected until much later. Later, beta decay was understood to occur by the emission of a weak boson ( 
W±
 ), sometimes called a charged weak current. Beta decay specifically involves the emission of a 
W−
 boson from one of the down quarks hidden within the neutron, thereby converting the down quark into an up quark and consequently the neutron into a proton.

For the free neutron, the decay energy for this process (based on the rest masses of the neutron, proton and electron) is 0.782343 MeV. That is the difference between the rest mass of the neutron and the sum of the rest masses of the products. That difference has to be carried away as kinetic energy. The maximal energy of the beta decay electron (in the process wherein the neutrino receives a vanishingly small amount of kinetic energy) has been measured at 0.782±0.013 MeV.[5] The latter number is not well-enough measured to determine the comparatively tiny rest mass of the neutrino (which must in theory be subtracted from the maximal electron kinetic energy); furthermore, neutrino mass is constrained by many other methods.
$
6
Outside the atomic nucleus, the free neutron is:
A: Stable with a half-life of over a year.
B: Stable indefinitely.
C: Unstable with a mean lifetime of around 15 minutes.
D: Unstable with a mean lifetime of around an hour.
E: Stable with a mean lifetime of 879.6 seconds.
Answer: C

The beta decay of a neutron can be represented by which of the following reactions?
A: n^0 → p^− + e^− + ν^e
B: p^0 → n^+ + e^+ + ν^e
C: n^0 → p^+ + e^+ + ν^e
D: n^0 → p^+ + e^− + ν^e
E: n^0 → e^− + ν^e
Answer: D

The
W−
boson that plays a role in beta decay is:
A: Stable and always detectable.
B: Highly unstable and quickly decays into a proton and an antineutrino.
C: Responsible for converting an up quark into a down quark.
D: Quickly decays into an electron and its matching antineutrino.
E: The main product of beta decay.
Answer: D

In beta decay, which quark is converted due to the emission of a
W−
boson?
A: Proton quark.
B: Electron quark.
C: Up quark.
D: Down quark.
E: Neutrino quark.
Answer: D

The maximal energy of the beta decay electron has been measured at approximately:
A: 1 MeV.
B: 0.782343 MeV.
C: 0.782±0.013 MeV.
D: 0.013 MeV.
E: 879.6 MeV.
Answer: C

The neutrino's rest mass:
A: Is well defined and subtracted from the maximal electron kinetic energy.
B: Is extremely large and is the primary contributor to decay energy.
C: Is not well-enough measured to determine from the maximal electron kinetic energy.
D: Is roughly equal to the rest mass of the electron.
E: Plays no role in the beta decay process.
Answer: C
@
Subject:
In geometry, Hesse's principle of transfer (German: Übertragungsprinzip) states that if the points of the projective line P1 are depicted by a rational normal curve in Pn, then the group of the projective transformations of Pn that preserve the curve is isomorphic to the group of the projective transformations of P1 (this is a generalization of the original Hesse's principle, in a form suggested by Wilhelm Franz Meyer).[1][2] It was originally introduced by Otto Hesse in 1866, in a more restricted form. It influenced Felix Klein in the development of the Erlangen program.[3][4][5] Since its original conception, it was generalized by many mathematicians, including Klein, Fano, and Cartan.
$
5
Hesse's principle of transfer was introduced by Otto Hesse in which year?
A: 1866
B: 1901
C: 1956
D: 1700
E: 2000
Answer: A

Who among the following was influenced by Hesse's principle in the development of the Erlangen program?
A: Albert Einstein
B: Wilhelm Franz Meyer
C: Pythagoras
D: Isaac Newton
E: Felix Klein
Answer: E

In its original conception, Hesse's principle was:
A: Applicable to all mathematical concepts.
B: A method for solving quadratic equations.
C: Explained with the help of advanced calculus.
D: Introduced in a more restricted form.
E: A way to measure the area under a curve.
Answer: D

Which of the following statements about Hesse's principle of transfer is FALSE?
A: It was introduced by Otto Hesse in 1866.
B: It deals with the projective transformations that preserve a curve.
C: It has been generalized by mathematicians including Klein, Fano, and Cartan.
D: It explains the transfer of heat between two bodies in thermodynamics.
E: It influenced the development of the Erlangen program.
Answer: D

The generalized form of Hesse's principle, as suggested by whom, claims that the group of the projective transformations of Pn that preserve the curve is isomorphic to the group of the projective transformations of P1?
A: Pythagoras
B: Felix Klein
C: Wilhelm Franz Meyer
D: Albert Einstein
E: Euclid
Answer: C
@
Subject:
The Navier–Stokes equations (/nævˈjeɪ stoʊks/ nav-YAY STOHKS) are partial differential equations which describe the motion of viscous fluid substances, named after French engineer and physicist Claude-Louis Navier and Irish physicist and mathematician George Gabriel Stokes. They were developed over several decades of progressively building the theories, from 1822 (Navier) to 1842-1850 (Stokes).

The Navier–Stokes equations mathematically express momentum balance and conservation of mass for Newtonian fluids. They are sometimes accompanied by an equation of state relating pressure, temperature and density.[1] They arise from applying Isaac Newton's second law to fluid motion, together with the assumption that the stress in the fluid is the sum of a diffusing viscous term (proportional to the gradient of velocity) and a pressure term—hence describing viscous flow. The difference between them and the closely related Euler equations is that Navier–Stokes equations take viscosity into account while the Euler equations model only inviscid flow. As a result, the Navier–Stokes are a parabolic equation and therefore have better analytic properties, at the expense of having less mathematical structure (e.g. they are never completely integrable).

The Navier–Stokes equations are useful because they describe the physics of many phenomena of scientific and engineering interest. They may be used to model the weather, ocean currents, water flow in a pipe and air flow around a wing. The Navier–Stokes equations, in their full and simplified forms, help with the design of aircraft and cars, the study of blood flow, the design of power stations, the analysis of pollution, and many other things. Coupled with Maxwell's equations, they can be used to model and study magnetohydrodynamics.

The Navier–Stokes equations are also of great interest in a purely mathematical sense. Despite their wide range of practical uses, it has not yet been proven whether smooth solutions always exist in three dimensions—i.e., whether they are infinitely differentiable (or even just bounded) at all points in the domain. This is called the Navier–Stokes existence and smoothness problem. The Clay Mathematics Institute has called this one of the seven most important open problems in mathematics and has offered a US$1 million prize for a solution or a counterexample.[2][3]

All non-relativistic balance equations, such as the Navier–Stokes equations, can be derived by beginning with the Cauchy equations and specifying the stress tensor through a constitutive relation. By expressing the deviatoric (shear) stress tensor in terms of viscosity and the fluid velocity gradient, and assuming constant viscosity, the above Cauchy equations will lead to the Navier–Stokes equations below.
$
5
Who are the Navier-Stokes equations named after?
A: Isaac Newton and Claude-Louis Navier
B: George Gabriel Stokes and Isaac Newton
C: Claude-Louis Navier and George Gabriel Stokes
D: George Gabriel Stokes and Albert Einstein
E: Claude-Louis Navier and Albert Einstein
Answer: C

The Euler equations and the Navier-Stokes equations both describe fluid motion. What is a primary difference between them?
A: The Euler equations consider viscosity while the Navier-Stokes equations do not.
B: The Navier-Stokes equations are derived from the Euler equations.
C: The Euler equations model only inviscid flow, whereas the Navier-Stokes equations take viscosity into account.
D: The Euler equations are parabolic while the Navier-Stokes equations are hyperbolic.
E: The Euler equations are only applicable to Newtonian fluids.
Answer: C

What type of phenomena can the Navier-Stokes equations be used to model?
A: Only the flow of water in rivers.
B: Only the air flow around aircraft wings.
C: Only the blood flow in the human body.
D: Weather, ocean currents, water flow in a pipe, and air flow around a wing, among others.
E: Only phenomena that occur in two dimensions.
Answer: D

What is the Navier-Stokes existence and smoothness problem?
A: It is a question about whether the Navier-Stokes equations can always have solutions in four dimensions.
B: It is a problem regarding the integrability of the Navier-Stokes equations.
C: It questions whether smooth solutions always exist for the Navier-Stokes equations in three dimensions.
D: It pertains to the differentiability of the Euler equations.
E: It challenges the assumption of constant viscosity in the Navier-Stokes equations.
Answer: C

The Navier-Stokes equations, when combined with which other equations, can be used to study magnetohydrodynamics?
A: Euler's equations
B: Bernoulli's equation
C: Maxwell's equations
D: The equation of state
E: The Cauchy momentum equation
Answer: C
@
Subject:
X-ray pulsars or accretion-powered pulsars are a class of astronomical objects that are X-ray sources displaying strict periodic variations in X-ray intensity. The X-ray periods range from as little as a fraction of a second to as much as several minutes.

An X-ray pulsar consists of a magnetized neutron star in orbit with a normal stellar companion and is a type of binary star system. The magnetic-field strength at the surface of the neutron star is typically about 108 Tesla, over a trillion times stronger than the strength of the magnetic field measured at the surface of the Earth (60 μT).

Gas is accreted from the stellar companion and is channeled by the neutron star's magnetic field on to the magnetic poles producing two or more localized X-ray hot spots, similar to the two auroral zones on Earth, but far hotter. At these hotspots the infalling gas can reach half the speed of light before it impacts the neutron star surface. So much gravitational potential energy is released by the infalling gas, that the hotspots, which are estimated to about one square kilometer in area, can be ten thousand times, or more, as luminous than the Sun.[1]

Temperatures of millions of degrees are produced so the hotspots emit mostly X-rays. As the neutron star rotates, pulses of X-rays are observed as the hotspots move in and out of view if the magnetic axis is tilted with respect to the spin axis.[1]

X-ray pulsar-based navigation and timing (XNAV) or simply pulsar navigation is a navigation technique whereby the periodic X-ray signals emitted from pulsars are used to determine the location of a vehicle, such as a spacecraft in deep space. A vehicle using XNAV would compare received X-ray signals with a database of known pulsar frequencies and locations. Similar to GPS, this comparison would allow the vehicle to calculate its position accurately (±5 km). The advantage of using X-ray signals over radio waves is that X-ray telescopes can be made smaller and lighter.[1][2][3] Experimental demonstrations have been reported in 2018.[4]

Among pulsars, millisecond pulsars are good candidate to be space-time references.[14] In particular, extraterrestrial intelligence might encode rich information using millisecond pulsar signals, and the metadata about XNAV is likely to be encoded by reference to millisecond pulsars.[15] Finally, it has been suggested that advanced extraterrestrial intelligence might have tweaked or engineered millisecond pulsars for the goals of timing, navigation and communication.[16]
$
5
What is the primary source of X-ray emissions in X-ray pulsars?
A: X-ray emissions are generated from the core of the neutron star.
B: The X-ray emissions result from gas accreting onto the neutron star's magnetic poles, producing hot spots.
C: X-ray emissions are released by the normal stellar companion.
D: The emissions originate from the collision between the neutron star and its companion.
E: X-rays are emitted when the neutron star collides with space debris.
Answer: B

How does the strength of the magnetic field on the surface of a neutron star compare to Earth's magnetic field strength?
A: The neutron star's magnetic field is ten times stronger.
B: They have comparable strengths.
C: The neutron star's magnetic field is weaker by a trillion times.
D: The neutron star's magnetic field is stronger by about a trillion times.
E: Earth's magnetic field is a billion times stronger.
Answer: D

Why are pulses of X-rays observed from the X-ray pulsars?
A: Because the X-rays are emitted in a cyclical pattern by the neutron star.
B: Because the hotspots on the neutron star move in and out of view as the star rotates.
C: Due to the interference caused by the Earth's atmosphere.
D: Because the neutron star pulsates, emitting X-rays at regular intervals.
E: X-rays are pulsed because of fluctuations in the neutron star's core energy.
Answer: B

What is one advantage of using X-ray signals over radio waves in XNAV?
A: X-ray signals can travel faster than radio waves.
B: X-ray telescopes can be made smaller and lighter.
C: X-ray signals can be detected from much farther distances.
D: X-ray signals are unaffected by cosmic interference.
E: Radio waves cannot penetrate deep space.
Answer: B

How might extraterrestrial intelligence relate to millisecond pulsars?
A: Millisecond pulsars are believed to be communications devices created by extraterrestrial intelligence.
B: Extraterrestrial intelligence might encode rich information using millisecond pulsar signals, and the metadata about XNAV is likely to be encoded by reference to millisecond pulsars.
C: Millisecond pulsars have been proven to be navigational markers set by extraterrestrial intelligence.
D: The signals from millisecond pulsars are the language of extraterrestrial beings.
E: Millisecond pulsars are thought to be remnants of extraterrestrial spacecraft.
Answer: B
@
Subject:
Sagittarius A* (/ˈeɪ stɑːr/ AY star), abbreviated Sgr A* (/ˈsædʒ ˈeɪ stɑːr/ SAJ AY star[3]), is the supermassive black hole[4][5][6] at the Galactic Center of the Milky Way. It is located near the border of the constellations Sagittarius and Scorpius, about 5.6° south of the ecliptic,[7] visually close to the Butterfly Cluster (M6) and Lambda Scorpii.

The object is a bright and very compact astronomical radio source. The name Sagittarius A* follows from historical reasons. In 1954,[8] John D. Kraus, Hsien-Ching Ko, and Sean Matt listed the radio sources they identified with the Ohio State University radio telescope at 250 MHz. They arranged these sources by constellation and then assigned capital letters in order of brightness within each constellation, with A denoting the brightest radio source within the constellation. The asterisk * is a later addition and was added because its discovery was considered "exciting",[9] in parallel with the nomenclature for excited state atoms which are denoted with an asterisk (for example, the excited state of helium would be He*). The asterisk was assigned in 1982 by Robert L. Brown,[10] who understood that the strongest radio emission from the center of the galaxy appeared to be due to a compact nonthermal radio object.

The observations of several stars orbiting Sagittarius A*, particularly star S2, have been used to determine the mass and upper limits on the radius of the object. Based on mass and increasingly precise radius limits, astronomers concluded that Sagittarius A* must be the Milky Way's central supermassive black hole.[11] The current value of its mass is 4.154±0.014 million solar masses.[2]

S2, also known as S0–2, is a star in the star cluster close to the supermassive black hole Sagittarius A* (Sgr A*), orbiting it with a period of 16.0518 years, a semi-major axis of about 970 au, and a pericenter distance of 17 light hours (18 Tm or 120 au) – an orbit with a period only about 30% longer than that of Jupiter around the Sun, but coming no closer than about four times the distance of Neptune from the Sun. The mass when the star first formed is estimated by the European Southern Observatory (ESO) to have been approximately 14 M☉.[5] Based on its spectral type (B0V ~ B3V), it probably has a mass of 10 to 15 solar masses.[citation needed]
$
5
How was Sagittarius A* historically named?
A: It was named after the constellation Sagittarius and the designation A* to mark its significance.
B: The name was derived from the Latin term for "star" and the asterisk was added to denote its brightness.
C: Sagittarius A* was named after its discoverer, John D. Kraus.
D: The name was based on its location in the constellation Sagittarius and the designation A was assigned as it was the brightest radio source within the constellation. The asterisk was added later to mark its "exciting" discovery.
E: It was named after its proximity to the Butterfly Cluster.
Answer: D

What is the significance of the star S2 in relation to Sagittarius A*?
A: S2 is a bright star located within the Milky Way but has no direct connection to Sagittarius A*.
B: S2 is a star that orbits Sagittarius A* and has been instrumental in determining the mass and radius of the supermassive black hole.
C: S2 is a distant star that provides backlighting, allowing astronomers to observe Sagittarius A*.
D: S2 is a star that was consumed by Sagittarius A* many years ago.
E: S2 is a neighboring supermassive black hole that is in contention with Sagittarius A*.
Answer: B

Why was the asterisk added to the name "Sagittarius A*"?
A: To denote it as a black hole.
B: It symbolizes that Sagittarius A* is the main attraction in the constellation.
C: The asterisk was added because its discovery was considered "exciting", mirroring the nomenclature for excited state atoms.
D: To indicate that it's the brightest star in the constellation Sagittarius.
E: It signifies the unknown properties of the object at the time.
Answer: C

How is the mass of Sagittarius A* currently estimated?
A: 4.154±0.014 million solar masses.
B: 5.6 million solar masses.
C: 10 to 15 solar masses.
D: Approximately 14 solar masses as estimated by the ESO.
E: 7.96×1036 kg.
Answer: A

Which star has an orbital period of 16.0518 years around Sagittarius A*?
A: Star S14.
B: Lambda Scorpii.
C: Butterfly Cluster star.
D: Star S2 or S0–2.
E: Hsien-Ching Ko star.
Answer: D
@
Subject:
In cardiology, the cardiac skeleton, also known as the fibrous skeleton of the heart, is a high-density homogeneous structure of connective tissue that forms and anchors the valves of the heart, and influences the forces exerted by and through them. The cardiac skeleton separates and partitions the atria (the smaller, upper two chambers) from the ventricles (the larger, lower two chambers).The heart's cardiac skeleton comprises four dense connective tissue rings that encircle the mitral and tricuspid atrioventricular (AV) canals and extend to the origins of the pulmonary trunk and aorta. This provides crucial support and structure to the heart while also serving to electrically isolate the atria from the ventricles.[1]

The unique matrix of connective tissue within the cardiac skeleton isolates electrical influence within these defined chambers. In normal anatomy, there is only one conduit for electrical conduction from the upper chambers to the lower chambers, known as the atrioventricular node. The physiologic cardiac skeleton forms a firewall governing autonomic/electrical influence until bordering the bundle of His which further governs autonomic flow to the bundle branches of the ventricles. Understood as such, the cardiac skeleton efficiently centers and robustly funnels electrical energy from the atria to the ventricles.
$
6
How many connective tissue rings does the cardiac skeleton comprise?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: D

What is the role of the atrioventricular node in the cardiac skeleton?
A: It provides oxygen to the heart muscles.
B: It is the only conduit for electrical conduction from the upper chambers to the lower chambers.
C: It physically separates the atria from the ventricles.
D: It controls the blood flow from atria to ventricles.
E: It produces hormones that regulate heart rate.
Answer: B

What does the cardiac skeleton electrically isolate?
A: The ventricles from each other.
B: The atria from each other.
C: The atria from the external body.
D: The atria from the ventricles.
E: The heart from the rest of the body.
Answer: D

Which structures does the cardiac skeleton anchor and form?
A: The atrioventricular septum.
B: The bundle of His.
C: The pulmonary trunk and aorta.
D: The valves of the heart.
E: The heart chambers.
Answer: D

What effect does the unique matrix of connective tissue within the cardiac skeleton have on electrical influence?
A: It enhances the electrical influence throughout the heart.
B: It suppresses all electrical activities in the heart.
C: It spreads the electrical influence uniformly in all directions.
D: It isolates electrical influence within the defined chambers of the heart.
E: It conducts electricity to the external body.
Answer: D

In what manner can the function of the cardiac skeleton concerning electrical energy be understood?
A: It disrupts the flow of electrical energy from the atria to the ventricles.
B: It generates electrical energy for the heart's functioning.
C: It efficiently centers and robustly funnels electrical energy from the atria to the ventricles.
D: It stores electrical energy for future use.
E: It transforms electrical energy into mechanical energy.
Answer: C
@
Subject:
A Carnot heat engine[2] is a theoretical heat engine that operates on the Carnot cycle. The basic model for this engine was developed by Nicolas Léonard Sadi Carnot in 1824. The Carnot engine model was graphically expanded by Benoît Paul Émile Clapeyron in 1834 and mathematically explored by Rudolf Clausius in 1857, work that led to the fundamental thermodynamic concept of entropy. The Carnot engine is the most efficient heat engine which is theoretically possible.[3] The efficiency depends only upon the absolute temperatures of the hot and cold heat reservoirs between which it operates.

A heat engine acts by transferring energy from a warm region to a cool region of space and, in the process, converting some of that energy to mechanical work. The cycle may also be reversed. The system may be worked upon by an external force, and in the process, it can transfer thermal energy from a cooler system to a warmer one, thereby acting as a refrigerator or heat pump rather than a heat engine.

Every thermodynamic system exists in a particular state. A thermodynamic cycle occurs when a system is taken through a series of different states, and finally returned to its initial state. In the process of going through this cycle, the system may perform work on its surroundings, thereby acting as a heat engine.

The Carnot engine is a theoretical construct, useful for exploring the efficiency limits of other heat engines. An actual Carnot engine, however, would be completely impractical to build.
$
5
How does the efficiency of the Carnot engine relate to other heat engines?
A: It is the least efficient heat engine.
B: It is more efficient than some but not all heat engines.
C: It is equally efficient as all heat engines.
D: It is the most efficient heat engine.
E: Its efficiency is unrelated to other heat engines.
Answer: D

When a system is taken through a series of different states and then returned to its initial state, what is this process called?
A: A thermodynamic conversion
B: A thermodynamic process
C: A thermodynamic cycle
D: A thermodynamic transformation
E: A thermodynamic phase
Answer: C

Which scientist expanded the Carnot engine model graphically?
A: Isaac Newton
B: Rudolf Clausius
C: Nicolas Léonard Sadi Carnot
D: James Joule
E: Benoît Paul Émile Clapeyron
Answer: E

In what context is the Carnot engine primarily useful?
A: For its practical applications in industry.
B: As a prototype for building other engines.
C: For exploring the efficiency limits of other heat engines.
D: For its role in cooling systems.
E: For its conversion of heat into kinetic energy.
Answer: C

If the Carnot cycle is reversed, what function might the system serve?
A: As a solar panel
B: As a battery
C: As a refrigerator or heat pump
D: As a wind turbine
E: As a water heater
Answer: C
@
Subject:
In engineering, a transfer function (also known as system function[1] or network function) of a system, sub-system, or component is a mathematical function that models the system's output for each possible input.[2][3][4] They are widely used in electronic engineering tools like circuit simulators and control systems. In some simple cases, this function can be represented as two-dimensional graph of an independent scalar input versus the dependent scalar output, called a transfer curve or characteristic curve. Transfer functions for components are used to design and analyze systems assembled from components, particularly using the block diagram technique, in electronics and control theory.

The dimensions and units of the transfer function model the output response of the device for a range of possible inputs. For example, the transfer function of a two-port electronic circuit like an amplifier might be a two-dimensional graph of the scalar voltage at the output as a function of the scalar voltage applied to the input; the transfer function of an electromechanical actuator might be the mechanical displacement of the movable arm as a function of electric current applied to the device; the transfer function of a photodetector might be the output voltage as a function of the luminous intensity of incident light of a given wavelength.

The term "transfer function" is also used in the frequency domain analysis of systems using transform methods such as the Laplace transform; here it means the amplitude of the output as a function of the frequency of the input signal. For example, the transfer function of an electronic filter is the voltage amplitude at the output as a function of the frequency of a constant amplitude sine wave applied to the input. For optical imaging devices, the optical transfer function is the Fourier transform of the point spread function (hence a function of spatial frequency).

Transfer functions are commonly used in the analysis of systems such as single-input single-output filters in the fields of signal processing, communication theory, and control theory. The term is often used exclusively to refer to linear time-invariant (LTI) systems. Most real systems have non-linear input/output characteristics, but many systems, when operated within nominal parameters (not "over-driven") have behavior close enough to linear that LTI system theory is an acceptable representation of the input/output behavior.
$
6
The transfer function of a system, sub-system, or component in engineering essentially models:
A: The cost-effectiveness of the system.
B: The system's maintenance routine.
C: The system's output for each possible input.
D: The energy consumption of the system.
E: The system's material composition.
Answer: C

What might the transfer function of an electromechanical actuator represent?
A: The sound frequency generated when turned on.
B: The weight of the movable arm.
C: The temperature of the actuator over time.
D: The mechanical displacement of the movable arm as a function of electric current applied.
E: The duration the actuator can remain active.
Answer: D

In the context of an electronic filter, how is the transfer function typically represented?
A: Output voltage versus input current.
B: Output amplitude versus time of input.
C: Output voltage amplitude versus frequency of input.
D: Output resistance versus input capacitance.
E: Output voltage versus material composition.
Answer: C

The optical transfer function for optical imaging devices is related to which of the following?
A: The square root of the point spread function.
B: The inverse of the point spread function.
C: The derivative of the point spread function.
D: The Fourier transform of the point spread function.
E: The logarithmic scale of the point spread function.
Answer: D

Transfer functions are most notably used in the analysis of which type of systems?
A: Multi-input multi-output systems in mechanical theory.
B: Single-input single-output filters in signal processing.
C: Dual-input dual-output systems in thermodynamics.
D: Multi-input single-output filters in heat transfer.
E: Single-input multi-output systems in quantum physics.
Answer: B

Why is the transfer function often suitable for many real systems, even if they have non-linear input/output characteristics?
A: Because most real systems operate in extreme parameters.
B: Because most systems are inherently linear.
C: Because when operated within nominal parameters, many systems behave close enough to linear.
D: Because transfer functions are universally adaptable to any system.
E: Because non-linear systems are rare in practical applications.
Answer: C
@
Subject:
he second law of thermodynamics is a physical law based on universal experience concerning heat and energy interconversions. One simple statement of the law is that heat always moves from hotter objects to colder objects (or "downhill"), unless energy in some form is supplied to reverse the direction of heat flow. Another definition is: "Not all heat energy can be converted into work in a cyclic process."[1][2][3]

The second law of thermodynamics in other versions establishes the concept of entropy as a physical property of a thermodynamic system. It can be used to predict whether processes are forbidden despite obeying the requirement of conservation of energy as expressed in the first law of thermodynamics and provides necessary criteria for spontaneous processes. The second law may be formulated by the observation that the entropy of isolated systems left to spontaneous evolution cannot decrease, as they always arrive at a state of thermodynamic equilibrium where the entropy is highest at the given internal energy.[4] An increase in the combined entropy of system and surroundings accounts for the irreversibility of natural processes, often referred to in the concept of the arrow of time.[5]

Historically, the second law was an empirical finding that was accepted as an axiom of thermodynamic theory. Statistical mechanics provides a microscopic explanation of the law in terms of probability distributions of the states of large assemblies of atoms or molecules. The second law has been expressed in many ways. Its first formulation, which preceded the proper definition of entropy and was based on caloric theory, is Carnot's theorem, formulated by the French scientist Sadi Carnot, who in 1824 showed that the efficiency of conversion of heat to work in a heat engine has an upper limit.[6][7] The first rigorous definition of the second law based on the concept of entropy came from German scientist Rudolf Clausius in the 1850s and included his statement that heat can never pass from a colder to a warmer body without some other change, connected therewith, occurring at the same time.

The second law of thermodynamics allows the definition of the concept of thermodynamic temperature, relying also on the zeroth law of thermodynamics.
$
6
Heat always moves from __________.
A: hotter objects to colder objects unless energy is provided to reverse the flow.
B: colder objects to hotter objects without any exceptions.
C: hotter objects to colder objects due to an increase in entropy.
D: colder objects to hotter objects as established by the first law of thermodynamics.
E: hotter objects to colder objects depending on the temperature of surroundings.
Answer: A

Which statement about entropy is correct according to the second law of thermodynamics?
A: The entropy of isolated systems can always decrease.
B: Entropy of isolated systems can either increase or remain constant.
C: The entropy of a system is unrelated to thermodynamic equilibrium.
D: Isolated systems left to spontaneous evolution will always decrease entropy.
E: The entropy of a system determines its total energy.
Answer: B

The irreversibility of natural processes is closely tied to:
A: The conversion of all heat to work.
B: The first law of thermodynamics.
C: The decrease in the combined entropy of a system.
D: The increase in the combined entropy of system and surroundings.
E: The caloric theory as proposed by Sadi Carnot.
Answer: D

Which scientist provided the first rigorous definition of the second law based on the concept of entropy?
A: Benoît Paul Émile Clapeyron
B: Sadi Carnot
C: Isaac Newton
D: Albert Einstein
E: Rudolf Clausius
Answer: E

Carnot's theorem, a precursor to the second law, stated that:
A: All heat can be converted to work in a cyclic process.
B: Entropy can decrease in isolated systems.
C: The efficiency of conversion of heat to work in a heat engine has an upper limit.
D: Heat always flows from a colder object to a hotter object.
E: Internal energy is the sole determinant of thermodynamic processes.
Answer: C

What does the second law of thermodynamics contribute to the concept of thermodynamic temperature?
A: It establishes a direct relation between entropy and temperature.
B: It provides a means to measure temperature based on heat flow.
C: It sets the upper limit for all thermodynamic temperatures.
D: It allows the definition of the concept of thermodynamic temperature.
E: It argues against the necessity of a thermodynamic temperature.
Answer: D
@
Subject:
An amorphous metal (also known as metallic glass, glassy metal, or shiny metal) is a solid metallic material, usually an alloy, with disordered atomic-scale structure. Most metals are crystalline in their solid state, which means they have a highly ordered arrangement of atoms. Amorphous metals are non-crystalline, and have a glass-like structure. But unlike common glasses, such as window glass, which are typically electrical insulators, amorphous metals have good electrical conductivity and can show metallic luster.

There are several ways in which amorphous metals can be produced, including extremely rapid cooling, physical vapor deposition, solid-state reaction, ion irradiation, and mechanical alloying.[1][2] Previously, small batches of amorphous metals had been produced through a variety of quick-cooling methods, such as amorphous metal ribbons which had been produced by sputtering molten metal onto a spinning metal disk (melt spinning). The rapid cooling (in the order of millions of degrees Celsius a second) is too fast for crystals to form and the material is "locked" in a glassy state.[3] Currently, a number of alloys with critical cooling rates low enough to allow formation of amorphous structure in thick layers (over 1 millimetre or 0.039 inches) have been produced; these are known as bulk metallic glasses. More recently, batches of amorphous steel with three times the strength of conventional steel alloys have been produced.

Amorphous metal is usually an alloy rather than a pure metal. The alloys contain atoms of significantly different sizes, leading to low free volume (and therefore up to orders of magnitude higher viscosity than other metals and alloys) in molten state. The viscosity prevents the atoms moving enough to form an ordered lattice. The material structure also results in low shrinkage during cooling, and resistance to plastic deformation. The absence of grain boundaries, the weak spots of crystalline materials, leads to better resistance to wear[22] and corrosion. Amorphous metals, while technically glasses, are also much tougher and less brittle than oxide glasses and ceramics. Amorphous metals can be grouped in two categories, as either non-ferromagnetic, if they are composed of Ln, Mg, Zr, Ti, Pd, Ca, Cu, Pt and Au, or ferromagnetic alloys, if they are composed of Fe, Co, and Ni.[23]

Currently the most important application is due to the special magnetic properties of some ferromagnetic metallic glasses. The low magnetization loss is used in high efficiency transformers (amorphous metal transformer) at line frequency and some higher frequency transformers. Amorphous steel is a very brittle material which makes it difficult to punch into motor laminations.[33] Also electronic article surveillance (such as theft control passive ID tags,) often uses metallic glasses because of these magnetic properties.

A commercial amorphous alloy, Vitreloy 1 (41.2% Zr, 13.8% Ti, 12.5% Cu, 10% Ni, and 22.5% Be), was developed at Caltech, as a part of Department of Energy and NASA research of new aerospace materials.[14]

Ti-based metallic glass, when made into thin pipes, have a high tensile strength of 2,100 MPa (300 ksi), elastic elongation of 2% and high corrosion resistance.[34] Using these properties, a Ti–Zr–Cu–Ni–Sn metallic glass was used to improve the sensitivity of a Coriolis flow meter. This flow meter is about 28-53 times more sensitive than conventional meters,[35] which can be applied in fossil-fuel, chemical, environmental, semiconductor and medical science industry.

Zr-Al-Ni-Cu based metallic glass can be shaped into 2.2 to 5 by 4 mm (0.087 to 0.197 by 0.157 in) pressure sensors for automobile and other industries, and these sensors are smaller, more sensitive, and possess greater pressure endurance compared to conventional stainless steel made from cold working. Additionally, this alloy was used to make the world's smallest geared motor with diameter 1.5 and 9.9 mm (0.059 and 0.390 in) to be produced and sold at the time.[36]
$
8
Which of the following best describes an amorphous metal?
A: A crystalline solid with a regular, repeating atomic arrangement.
B: A non-metallic solid with an irregular atomic arrangement.
C: A solid metallic material with a glass-like, disordered atomic structure.
D: A pure metal with an organized atomic lattice.
E: A semi-crystalline metal with partial atomic order.
Answer: C

How does the structure of amorphous metals compare to traditional metals?
A: Amorphous metals are crystalline, while traditional metals have a glass-like structure.
B: Both amorphous and traditional metals are crystalline.
C: Both amorphous and traditional metals are non-crystalline.
D: Amorphous metals are non-crystalline, while traditional metals are crystalline.
E: Amorphous metals have a liquid-like structure, while traditional metals are solid.
Answer: D

One method to produce amorphous metals involves rapid cooling at rates in the order of:
A: Thousands of degrees Celsius a second.
B: Hundreds of degrees Celsius a second.
C: Tens of degrees Celsius a second.
D: Millions of degrees Celsius a second.
E: Billions of degrees Celsius a second.
Answer: D

Which statement about the electrical properties of amorphous metals is true?
A: They are always poor conductors of electricity.
B: They have good electrical conductivity and can show metallic luster.
C: They are semiconductors.
D: They are only conductive when combined with other crystalline metals.
E: They have no electrical properties as they are insulators.
Answer: B

Which of the following is NOT an advantage of amorphous metals?
A: Resistance to wear and corrosion.
B: Absence of grain boundaries.
C: High shrinkage during cooling.
D: Good electrical conductivity.
E: Low resistance to plastic deformation.
Answer: C

What differentiates bulk metallic glasses from earlier forms of amorphous metals?
A: They have a crystalline structure.
B: They have a thickness over 1 millimeter or 0.039 inches.
C: They are produced using slow cooling techniques.
D: They are not really metallic in nature.
E: They are only made from pure metals.
Answer: B

The most important application of ferromagnetic metallic glasses is in:
A: Aerospace materials.
B: High-efficiency transformers.
C: Motor laminations.
D: Pressure sensors for automobiles.
E: Coriolis flow meters.
Answer: B

The commercial amorphous alloy, Vitreloy 1, was developed in collaboration with:
A: MIT and SpaceX.
B: Caltech, Department of Energy, and NASA.
C: Harvard and Boeing.
D: Stanford and Department of Defense.
E: UCLA and Lockheed Martin.
Answer: B
@
Subject:
The Penrose process (also called Penrose mechanism) is theorised by Sir Roger Penrose as a means whereby energy can be extracted from a rotating black hole.[1][2] The process takes advantage of the ergosphere – a region of spacetime around the black hole dragged by its rotation faster than the speed of light, meaning that from the point of an outside observer any matter inside is forced to move in the direction of the rotation of the black hole.

In the process, a working body falls (black thick line in the figure) into the ergosphere (gray region). At its lowest point (red dot) the body fires a propellant backwards; however, to a faraway observer both seem to continue to move forward due to frame-dragging (albeit at different speeds). The propellant, being slowed, falls (thin gray line) to the event horizon of the black hole (black disk). The remains of the body, being sped up, fly away (thin black line) with an excess of energy (that more than offsets the loss of the propellant and the energy used to shoot it).

The maximum amount of energy gain possible for a single particle decay via the original (or classical) Penrose process is 20.7% of its mass in the case of an uncharged black hole (assuming the best case of maximal rotation of the black hole).[3] The energy is taken from the rotation of the black hole, so there is a limit on how much energy one can extract by Penrose process and similar strategies (for an uncharged black hole no more than 29% of its original mass;[4] larger efficiencies are possible for charged rotating black holes[5]).
$
7
Which region of spacetime around a rotating black hole is dragged by its rotation faster than the speed of light?
A: Horizon.
B: Ergosphere.
C: Accretion disk.
D: Photon sphere.
E: Singularity.
Answer: B

How does the matter inside the ergosphere appear to an outside observer?
A: It seems to be frozen in time.
B: It appears to be moving in the direction of the rotation of the black hole.
C: It seems to move in a spiral pattern.
D: It appears to be moving in the opposite direction of the rotation of the black hole.
E: It appears to be stationary.
Answer: B

During the Penrose process, when the working body reaches its lowest point within the ergosphere, what does it do?
A: It emits light.
B: It absorbs energy.
C: It fires a propellant backwards.
D: It accelerates to the speed of light.
E: It splits into two equal halves.
Answer: C

How much energy gain is possible for a single particle decay via the classical Penrose process in the case of an uncharged black hole with maximal rotation?
A: 5% of its mass.
B: 10% of its mass.
C: 20.7% of its mass.
D: 29% of its mass.
E: 50% of its mass.
Answer: C

From where does the energy extracted during the Penrose process come?
A: The event horizon of the black hole.
B: The gravitational pull of the black hole.
C: The rotational energy of the black hole.
D: The magnetic field around the black hole.
E: The radiation emitted by the black hole.
Answer: C

What is the limit on the amount of energy that can be extracted from an uncharged black hole using the Penrose process?
A: No more than 10% of its original mass.
B: No more than 20.7% of its original mass.
C: No more than 29% of its original mass.
D: No more than 50% of its original mass.
E: No more than 75% of its original mass.
Answer: C

In comparison to an uncharged black hole, are there differences in the efficiency of energy extraction from charged rotating black holes using the Penrose process?
A: Larger efficiencies are possible for charged rotating black holes.
B: Smaller efficiencies are possible for charged rotating black holes.
C: There's no difference in efficiency.
D: The Penrose process doesn't apply to charged rotating black holes.
E: Charged rotating black holes can't be slowed down using the Penrose process.
Answer: A
@
Subject:
Gravity Probe B (GP-B) was a satellite-based experiment to test two unverified predictions of general relativity: the geodetic effect and frame-dragging. This was to be accomplished by measuring, very precisely, tiny changes in the direction of spin of four gyroscopes contained in an Earth-orbiting satellite at 650 km (400 mi) of altitude, crossing directly over the poles.

The satellite was launched on 20 April 2004 on a Delta II rocket.[4] The spaceflight phase lasted until 2005;[5] Its aim was to measure spacetime curvature near Earth, and thereby the stress–energy tensor (which is related to the distribution and the motion of matter in space) in and near Earth. This provided a test of general relativity, gravitomagnetism and related models. The principal investigator was Francis Everitt.

Initial results confirmed the expected geodetic effect to an accuracy of about 1%. The expected frame-dragging effect was similar in magnitude to the current noise level (the noise being dominated by initially unmodeled effects due to nonuniform coatings on the gyroscopes). Work continued to model and account for these sources of error, thus permitting extraction of the frame-dragging signal. By August 2008, the frame-dragging effect had been confirmed to within 15% of the expected result,[6] and the December 2008 NASA report indicated that the geodetic effect was confirmed to be better than 0.5%.[7]

In an article published in the journal Physical Review Letters in 2011, the authors reported analysis of the data from all four gyroscopes results in a geodetic drift rate of −6601.8±18.3 mas/yr and a frame-dragging drift rate of −37.2±7.2 mas/yr, in good agreement with the general relativity predictions of −6606.1±0.28% mas/yr and −39.2±0.19% mas/yr, respectively.[8]
$
7
Which two predictions of general relativity were tested by the Gravity Probe B (GP-B) experiment?
A: Frame-shifting and spacetime curvature.
B: Geodetic effect and frame-dragging.
C: Gravitomagnetism and quantum mechanics.
D: Black hole radiation and space dilation.
E: Event horizon and wormhole dynamics.
Answer: B

How did GP-B aim to measure the predictions of general relativity?
A: By observing the cosmic microwave background.
B: By analyzing the spin of planets around the sun.
C: By measuring changes in the direction of spin of four gyroscopes in a satellite.
D: By detecting gravitational waves.
E: By measuring the redshift of distant galaxies.
Answer: C

Where was the GP-B satellite positioned in its Earth orbit?
A: In geosynchronous orbit.
B: Crossing directly over the equator.
C: In low Earth orbit at 200 km.
D: Crossing directly over the poles at 650 km altitude.
E: In a stationary position over the North Pole.
Answer: D

Who was the principal investigator of the Gravity Probe B mission?
A: Stephen Hawking.
B: Richard Feynman.
C: Albert Einstein.
D: Francis Everitt.
E: Neil deGrasse Tyson.
Answer: D

By December 2008, to what accuracy was the geodetic effect confirmed?
A: Better than 2%.
B: Better than 5%.
C: Better than 0.5%.
D: To exactly 0%.
E: The geodetic effect was not confirmed.
Answer: C

The results published in Physical Review Letters in 2011 reported a geodetic drift rate and a frame-dragging drift rate in agreement with which theory's predictions?
A: Newtonian gravity.
B: Quantum mechanics.
C: Special relativity.
D: General relativity.
E: Electromagnetic theory.
Answer: D

What dominated the noise, making it challenging to extract the frame-dragging signal in the GP-B's initial results?
A: Cosmic background radiation interference.
B: Unmodeled effects due to nonuniform coatings on the gyroscopes.
C: Solar radiation pressure.
D: Interference from other satellites.
E: Vibrations from the spacecraft's onboard instruments.
Answer: B
@
Subject:
Fermat's principle, also known as the principle of least time, is the link between ray optics and wave optics. Fermat's principle states that the path taken by a ray between two given points is the path that can be traveled in the least time.

First proposed by the French mathematician Pierre de Fermat in 1662, as a means of explaining the ordinary law of refraction of light (Fig. 1), Fermat's principle was initially controversial because it seemed to ascribe knowledge and intent to nature. Not until the 19th century was it understood that nature's ability to test alternative paths is merely a fundamental property of waves.[1] If points A and B are given, a wavefront expanding from A sweeps all possible ray paths radiating from A, whether they pass through B or not. If the wavefront reaches point B, it sweeps not only the ray path(s) from A to B, but also an infinitude of nearby paths with the same endpoints. Fermat's principle describes any ray that happens to reach point B; there is no implication that the ray "knew" the quickest path or "intended" to take that path.

Fig. 2: Two points P and P′ on a path from A to B. For the purposes of Fermat's principle, the propagation time from P to P′ is taken as for a point-source at P, not (e.g.) for an arbitrary wavefront W passing through P. The surface Σ  (with unit normal n̂ at P′) is the locus of points that a disturbance at P can reach in the same time that it takes to reach P′; in other words, Σ is the secondary wavefront with radius PP′. (The medium is not assumed to be homogeneous or isotropic.)
In its original "strong" form,[2] Fermat's principle states that the path taken by a ray between two given points is the path that can be traveled in the least time. In order to be true in all cases, this statement must be weakened by replacing the "least" time with a time that is "stationary" with respect to variations of the path — so that a deviation in the path causes, at most, a second-order change in the traversal time. To put it loosely, a ray path is surrounded by close paths that can be traversed in very close times. It can be shown that this technical definition corresponds to more intuitive notions of a ray, such as a line of sight or the path of a narrow beam.

For the purpose of comparing traversal times, the time from one point to the next nominated point is taken as if the first point were a point-source.[3] Without this condition, the traversal time would be ambiguous; for example, if the propagation time from P to P′ were reckoned from an arbitrary wavefront W containing P  (Fig. 2), that time could be made arbitrarily small by suitably angling the wavefront.

Treating a point on the path as a source is the minimum requirement of Huygens' principle, and is part of the explanation of Fermat's principle. But it can also be shown that the geometric construction by which Huygens tried to apply his own principle (as distinct from the principle itself) is simply an invocation of Fermat's principle.[4] Hence all the conclusions that Huygens drew from that construction — including, without limitation, the laws of rectilinear propagation of light, ordinary reflection, ordinary refraction, and the extraordinary refraction of "Iceland crystal" (calcite) — are also consequences of Fermat's principle.
$
6
What does Fermat's principle, also known as the principle of least time, bridge?
A: Wave optics and quantum physics.
B: Ray optics and nuclear physics.
C: Ray optics and wave optics.
D: Classical physics and modern physics.
E: Gravitational forces and optical forces.
Answer: C

When was Fermat's principle first proposed?
A: In the 19th century.
B: In 1762.
C: In 1662.
D: In 1902.
E: In 1801.
Answer: C

How was Fermat's principle initially received by the scientific community?
A: Widely accepted because of its immediate evidence.
B: Controversial due to its implications about nature's intent.
C: Supported only by the major scientists of the time.
D: Ignored and remained undiscovered for several decades.
E: Accepted only in certain parts of the world.
Answer: B

What is the relationship between Huygens' principle and Fermat's principle?
A: Huygens' principle is the exact opposite of Fermat's principle.
B: Huygens' principle can be explained as an invocation of Fermat's principle.
C: Huygens' principle and Fermat's principle are unrelated.
D: Huygens' principle is a refined version of Fermat's principle.
E: Fermat's principle is a subcategory of Huygens' principle.
Answer: B

In the context of Fermat's principle, why is it important to treat a point on the path as a source?
A: Because it helps determine the reflection angle of the light.
B: Because it helps in understanding the wavefront.
C: Because without this condition, the traversal time would be ambiguous.
D: Because it helps in calculating the energy of the light.
E: Because it defines the boundaries of the medium.
Answer: C

What does Fermat's principle in its "strong" form state?
A: The path taken by a ray between two points is the path that has the most resistance.
B: The path taken by a ray is determined by the medium's density.
C: The path taken by a ray between two points is arbitrary.
D: The path taken by a ray between two points is the path that can be traveled in the least time.
E: The path taken by a ray between two points is the path that can be traveled in the most time.
Answer: D
@
Subject:
Apparent magnitude (m) is a measure of the brightness of a star or other astronomical object. An object's apparent magnitude depends on its intrinsic luminosity, its distance, and any extinction of the object's light caused by interstellar dust along the line of sight to the observer.

The word magnitude in astronomy, unless stated otherwise, usually refers to a celestial object's apparent magnitude. The magnitude scale dates back to the ancient Roman astronomer Claudius Ptolemy, whose star catalog listed stars from 1st magnitude (brightest) to 6th magnitude (dimmest). The modern scale was mathematically defined in a way to closely match this historical system.

The scale is reverse logarithmic: the brighter an object is, the lower its magnitude number. A difference of 1.0 in magnitude corresponds to a brightness ratio of 
100
5
{\displaystyle {\sqrt[{5}]{100}}}, or about 2.512. For example, a star of magnitude 2.0 is 2.512 times as bright as a star of magnitude 3.0, 6.31 times as bright as a star of magnitude 4.0, and 100 times as bright as one of magnitude 7.0.

Differences in astronomical magnitudes can also be related to another logarithmic ratio scale, the decibel: an increase of one astronomical magnitude is exactly equal to a decrease of 4 decibels (dB).

The brightest astronomical objects have negative apparent magnitudes: for example, Venus at −4.2 or Sirius at −1.46. The faintest stars visible with the naked eye on the darkest night have apparent magnitudes of about +6.5, though this varies depending on a person's eyesight and with altitude and atmospheric conditions.[1] The apparent magnitudes of known objects range from the Sun at −26.832 to objects in deep Hubble Space Telescope images of magnitude +31.5.[2]

The measurement of apparent magnitude is called photometry. Photometric measurements are made in the ultraviolet, visible, or infrared wavelength bands using standard passband filters belonging to photometric systems such as the UBV system or the Strömgren uvbyβ system.

Absolute magnitude is a measure of the intrinsic luminosity of a celestial object, rather than its apparent brightness, and is expressed on the same reverse logarithmic scale. Absolute magnitude is defined as the apparent magnitude that a star or object would have if it were observed from a distance of 10 parsecs (33 light-years; 3.1×1014 kilometres; 1.9×1014 miles). Therefore, it is of greater use in stellar astrophysics since it refers to a property of a star regardless of how close it is to Earth. But in observational astronomy and popular stargazing, unqualified references to "magnitude" are understood to mean apparent magnitude.

Amateur astronomers commonly express the darkness of the sky in terms of limiting magnitude, i.e. the apparent magnitude of the faintest star they can see with the naked eye. This can be useful as a way of monitoring the spread of light pollution.

Apparent magnitude is really a measure of illuminance, which can also be measured in photometric units such as lux.[3]
$
10
Who historically listed stars using a magnitude scale from the brightest to dimmest?
A: Galileo Galilei
B: Johannes Kepler
C: Claudius Ptolemy
D: Edwin Hubble
E: Sir Isaac Newton
Answer: C

How does the magnitude scale in astronomy work with regards to brightness?
A: The brighter an object, the higher its magnitude number.
B: The dimmer an object, the higher its magnitude number.
C: The brighter an object, the more negative its magnitude number.
D: The magnitude number directly corresponds to the distance of the object.
E: The dimmer an object, the more negative its magnitude number.
Answer: C

A star of magnitude 4.0 is how many times brighter than a star of magnitude 6.0?
A: 2.512 times
B: 5.024 times
C: 6.31 times
D: 10 times
E: 15.36 times
Answer: C

What does a difference of 1.0 in astronomical magnitude correspond to in decibels (dB)?
A: An increase of 2 dB
B: A decrease of 2 dB
C: An increase of 4 dB
D: A decrease of 4 dB
E: No change in dB
Answer: D

Which of the following has a negative apparent magnitude?
A: Polaris
B: Mars
C: Venus
D: Neptune
E: Alpha Centauri
Answer: C

What is the apparent magnitude of the Sun?
A: +6.5
B: 0
C: +31.5
D: −26.832
E: +10
Answer: D

What does Absolute magnitude refer to?
A: The apparent brightness of a star from Earth.
B: The intrinsic luminosity of a celestial object.
C: The distance between the star and Earth.
D: The size of a star in relation to the Sun.
E: The total number of stars in a given region.
Answer: B

When amateur astronomers discuss the "limiting magnitude", what are they referring to?
A: The apparent magnitude of the brightest star.
B: The apparent magnitude of the star that's the furthest away.
C: The apparent magnitude of the faintest star they can see with the naked eye.
D: The absolute magnitude of the closest star.
E: The apparent magnitude of the most popular stars.
Answer: C

Apparent magnitude is a measure of:
A: Luminosity.
B: Distance.
C: Mass.
D: Density.
E: Illuminance.
Answer: E

If an object is observed from a distance of 10 parsecs, its brightness is represented by:
A: Apparent magnitude.
B: Relative magnitude.
C: Absolute magnitude.
D: Limiting magnitude.
E: Distance magnitude.
Answer: C
@
Subject:
In physics, the spin quantum number is a quantum number (designated s) that describes the intrinsic angular momentum (or spin angular momentum, or simply spin) of an electron or other particle. It has the same value for all particles of the same type, such as s = 
1
/
2
 for all electrons. It is an integer for all bosons, such as photons, and a half-odd-integer for all fermions, such as electrons and protons. The component of the spin along a specified axis is given by the spin magnetic quantum number, conventionally written ms.[1] The value of ms is the component of spin angular momentum, in units of the reduced Planck constant ħ, parallel to a given direction (conventionally labelled the z–axis). It can take values ranging from +s to −s in integer increments. For an electron, ms can be either ++
1
/
2
 or −+
1
/
2
 .

The phrase spin quantum number was originally used to describe the fourth of a set of quantum numbers (the principal quantum number n, the azimuthal quantum number ℓ, the magnetic quantum number m, and the spin magnetic quantum number ms), which completely describe the quantum state of an electron in an atom. Some introductory chemistry textbooks describe ms as the spin quantum number,[2][3] and s is not mentioned since its value 
1
/
2
 is a fixed property of the electron, sometimes using the variable s in place of ms.[4] Some authors discourage this usage as it causes confusion.[4] At a more advanced level where quantum mechanical operators or coupled spins are introduced, s is referred to as the spin quantum number, and ms is described as the spin magnetic quantum number[5] or as the z-component of spin sz.[6]

Spin quantum numbers apply also to systems of coupled spins, such as atoms that may contain more than one electron. Capitalized symbols are used: S for the total electronic spin, and mS or MS for the z-axis component. A pair of electrons in a spin singlet state has S = 0, and a pair in the triplet state has S = 1, with mS = −1, 0, or +1. Nuclear-spin quantum numbers are conventionally written I for spin, and mI or MI for the z-axis component.

The name "spin" comes from a geometrical spinning of the electron about an axis, as proposed by Uhlenbeck and Goudsmit. However, this simplistic picture was quickly realized to be physically unrealistic, because it would require the electrons to rotate faster than the speed of light.[7] It was therefore replaced by a more abstract quantum-mechanical description.

For some atoms the spins of several unpaired electrons (s1, s2, ...) are coupled to form a total spin quantum number S.[9][10] This occurs especially in light atoms (or in molecules formed only of light atoms) when spin–orbit coupling is weak compared to the coupling between spins or the coupling between orbital angular momenta, a situation known as L S coupling because L and S are constants of motion. Here L is the total orbital angular momentum quantum number.[10]

For atoms with a well-defined S, the multiplicity of a state is defined as 2S + 1 . This is equal to the number of different possible values of the total (orbital plus spin) angular momentum J for a given (L, S) combination, provided that S ≤ L (the typical case). For example, if S = 1, there are three states which form a triplet. The eigenvalues of Sz for these three states are +1ħ, 0, and −1ħ .[9] The term symbol of an atomic state indicates its values of L, S, and J .

As examples, the ground states of both the oxygen atom and the dioxygen molecule have two unpaired electrons and are therefore triplet states. The atomic state is described by the term symbol 3P, and the molecular state by the term symbol 3Σ−
g.
$
10
What is the value of spin quantum number (s) for all electrons?
A: 0
B: 1
C: 1/2
D: 2
E: 1/4
Answer: C

Bosons, like photons, have a spin quantum number that is:
A: An integer.
B: A half-odd-integer.
C: Always zero.
D: Equal to the charge of the boson.
E: Indefinite.
Answer: A

The spin magnetic quantum number (ms) for an electron can have values:
A: 0 or 1
B: 1 or -1
C: +1/2 or -1/2
D: 0
E: 2 or -2
Answer: C

Which of the following is NOT one of the set of quantum numbers that describe the quantum state of an electron in an atom?
A: Principal quantum number (n)
B: Magnetic quantum number (m)
C: Charge quantum number
D: Azimuthal quantum number (ℓ)
E: Spin magnetic quantum number (ms)
Answer: C

When referring to systems of coupled spins with more than one electron, which symbol is used to represent the total electronic spin?
A: s
B: ms
C: I
D: mS
E: S
Answer: E

What does the value S = 0 indicate for a pair of electrons?
A: The electrons are in a triplet state.
B: The electrons are in a spin singlet state.
C: The electrons are in a high-energy state.
D: The electrons are not coupled.
E: The electrons have opposite spins.
Answer: B

The original conception of "spin" suggested that the electron was spinning about an axis. Why was this idea discarded?
A: It conflicted with the laws of conservation of energy.
B: It was purely hypothetical without any experimental basis.
C: It implied that electrons would rotate faster than the speed of light.
D: It did not account for the quantum nature of electrons.
E: It was found to be mathematically inconsistent.
Answer: C

Which quantum number is referred to as the z-component of spin?
A: Principal quantum number (n)
B: Magnetic quantum number (m)
C: Spin quantum number (s)
D: Spin magnetic quantum number (ms)
E: Azimuthal quantum number (ℓ)
Answer: D

Which of the following best describes the spin quantum number in quantum mechanics?
A: It's an abstract quantum-mechanical description, not a measure of physical rotation.
B: It is a measure of the physical rotation of an electron around the nucleus.
C: It describes the orbit in which the electron moves.
D: It corresponds to the energy levels of the atom.
E: It represents the electron's charge.
Answer: A

In a system of coupled spins, what is the symbol used for the z-axis component of nuclear-spin quantum numbers?
A: ms
B: s
C: S
D: mI or MI
E: sz
Answer: D
@
Subject:
A synaptic transistor is an electrical device that can learn in ways similar to a neural synapse.[1] It optimizes its own properties for the functions it has carried out in the past. The device mimics the behavior of the property of neurons called spike-timing-dependent plasticity, or STDP.[2][3]

Its structure is similar to that of a field effect transistor, where an ionic liquid takes the place of the gate insulating layer between the gate electrode and the conducting channel. That channel is composed of samarium nickelate (SmNiO
3, or SNO) rather than the field effect transistor's doped silicon.[3]

A synaptic transistor has a traditional immediate response whose amount of current that passes between the source and drain contacts varies with voltage applied to the gate electrode. It also produces a much slower learned response such that the conductivity of the SNO layer varies in response to the transistor's STDP history, essentially by shuttling oxygen ions between the SNO and the ionic liquid.[3]

The analog of strengthening a synapse is to increase the SNO's conductivity, which essentially increases gain. Similarly, weakening a synapse is analogous to decreasing the SNO's conductivity, lowering the gain.[3]

The input and output of the synaptic transistor are continuous analog values, rather than digital on-off signals. While the physical structure of the device has the potential to learn from history, it contains no way to bias the transistor to control the memory effect. An external supervisory circuit converts the time delay between input and output into a voltage applied to the ionic liquid that either drives ions into the SNO or removes them.[3]

A network of such devices can learn particular responses to "sensory inputs", with those responses being learned through experience rather than explicitly programmed.[3]
$
8
What property of neurons does the synaptic transistor mimic?
A: Neurotransmitter release
B: Neural connectivity
C: Spike-timing-dependent plasticity (STDP)
D: Myelination
E: Resting potential
Answer: C

Which material takes the place of the gate insulating layer in a synaptic transistor?
A: Silicon
B: Samarium nickelate
C: Ionic liquid
D: Doped silicon
E: Oxygen ions
Answer: C

In a synaptic transistor, what is the function of the samarium nickelate (SmNiO₃ or SNO)?
A: It replaces the gate insulating layer.
B: It replaces the gate electrode.
C: It serves as the conducting channel.
D: It shuttles oxygen ions.
E: It acts as the source and drain contacts.
Answer: C

In the synaptic transistor, what is analogous to strengthening a neural synapse?
A: Decreasing the SNO's conductivity
B: Removing ions from the ionic liquid
C: Shutting off the transistor
D: Increasing the SNO's conductivity
E: Applying voltage to the gate electrode
Answer: D

The input and output values of the synaptic transistor are:
A: Digital on-off signals
B: Continuous analog values
C: Discrete digital values
D: Only high voltage signals
E: Only low voltage signals
Answer: B

Which element is shuttled between the SNO layer and the ionic liquid in response to the transistor's STDP history?
A: Silicon ions
B: Nickel ions
C: Hydrogen ions
D: Samarium ions
E: Oxygen ions
Answer: E

How can a network of synaptic transistors be utilized?
A: To strictly follow explicitly programmed paths.
B: To generate digital signals.
C: To learn particular responses to sensory inputs through experience.
D: To amplify sound waves.
E: To store long-term memory.
Answer: C

What is missing in the physical structure of the synaptic transistor for controlling the memory effect?
A: A way to shuttle oxygen ions.
B: A mechanism to respond to STDP history.
C: A mechanism to bias the transistor.
D: A continuous analog value generator.
E: A way to increase the SNO's conductivity.
Answer: C
@
Subject:
Spontaneous symmetry breaking is a spontaneous process of symmetry breaking, by which a physical system in a symmetric state spontaneously ends up in an asymmetric state.[1][2][3] In particular, it can describe systems where the equations of motion or the Lagrangian obey symmetries, but the lowest-energy vacuum solutions do not exhibit that same symmetry. When the system goes to one of those vacuum solutions, the symmetry is broken for perturbations around that vacuum even though the entire Lagrangian retains that symmetry.

By definition, spontaneous symmetry breaking requires the existence of physical laws (e.g. quantum mechanics) which are invariant under a symmetry transformation (such as translation or rotation), so that any pair of outcomes differing only by that transformation have the same probability distribution. For example if measurements of an observable at any two different positions have the same probability distribution, the observable has translational symmetry.

Spontaneous symmetry breaking occurs when this relation breaks down, while the underlying physical laws remain symmetrical.

Conversely, in explicit symmetry breaking, if two outcomes are considered, the probability distributions of a pair of outcomes can be different. For example in an electric field, the forces on a charged particle are different in different directions, so the rotational symmetry is explicitly broken by the electric field which does not have this symmetry.

Phases of matter, such as crystals, magnets, and conventional superconductors, as well as simple phase transitions can be described by spontaneous symmetry breaking. Notable exceptions include topological phases of matter like the fractional quantum Hall effect.

Typically, when spontaneous symmetry breaking occurs, the observable properties of the system change in multiple ways. For example the density, compressibility, coefficient of thermal expansion, and specific heat will be expected to change when a liquid becomes a solid.
$
5
Question 1: Which of the following statements best describes explicit symmetry breaking?

A: Explicit symmetry breaking occurs when the underlying physical laws are no longer symmetrical.
B: Explicit symmetry breaking occurs when the system moves to a high-energy vacuum solution that exhibits symmetry.
C: Explicit symmetry breaking happens when the observable properties of a system remain unchanged.
D: Explicit symmetry breaking means that the symmetry of the physical laws remains, but the outcomes can have different probability distributions based on that symmetry.
E: Explicit symmetry breaking is when the physical laws and the outcomes both have symmetrical distributions.

Answer: D

Question 2: What is an example of a system described by spontaneous symmetry breaking?

A: Fractional quantum Hall effect
B: An electric field affecting a charged particle in multiple directions equally
C: Liquids at room temperature
D: Magnets in a stable state
E: A vacuum with a higher energy solution that maintains symmetry

Answer: D

Question 3: Which condition must be met for spontaneous symmetry breaking to occur?

A: Physical laws must be variant under a symmetry transformation.
B: All vacuum solutions should exhibit the same symmetry as the Lagrangian.
C: Observable properties must remain constant for a system.
D: A system's lowest-energy vacuum solution doesn't exhibit the same symmetry as its Lagrangian.
E: Probability distributions of outcomes must always be different regardless of symmetry transformation.

Answer: D

Question 4: When a liquid becomes a solid due to spontaneous symmetry breaking, what is expected to change?

A: Only the density of the system
B: The compressibility and coefficient of thermal expansion, but not the specific heat
C: The density, compressibility, coefficient of thermal expansion, and specific heat
D: Only the phase of the system
E: The probability distributions of the system's outcomes

Answer: C

Question 5: In the context of spontaneous symmetry breaking, what signifies that an observable has translational symmetry?

A: Measurements of the observable at all positions have varied probability distributions.
B: Measurements of the observable only at one position have a consistent probability distribution.
C: Measurements of the observable at any two different positions yield different outcomes.
D: Measurements of the observable are always in the highest energy vacuum solution.
E: Measurements of the observable at any two different positions have the same probability distribution.

Answer: E
@
Subject:
In physics, a redshift is an increase in the wavelength, and corresponding decrease in the frequency and photon energy, of electromagnetic radiation (such as light). The opposite change, a decrease in wavelength and simultaneous increase in frequency and energy, is known as a negative redshift, or blueshift. The terms derive from the colours red and blue which form the extremes of the visible light spectrum. The main causes of electromagnetic redshift in astronomy and cosmology are the relative motions of radiation sources, which give rise to the relativistic Doppler effect), and gravitational potentials, which gravitationally redshift escaping radiation. All sufficiently distant light sources show cosmological redshift corresponding to recession speeds proportional to their distances from Earth, a fact known as Hubble's law that implies the universe is expanding.

All redshifts can be understood under the umbrella of frame transformation laws. Gravitational waves, which also travel at the speed of light, are subject to the same redshift phenomena. The value of a redshift is often denoted by the letter z, corresponding to the fractional change in wavelength (positive for redshifts, negative for blueshifts), and by the wavelength ratio 1 + z (which is greater than 1 for redshifts and less than 1 for blueshifts).

Examples of strong redshifting are a gamma ray perceived as an X-ray, or initially visible light perceived as radio waves. Subtler redshifts are seen in the spectroscopic observations of astronomical objects, and are used in terrestrial technologies such as Doppler radar and radar guns.

Other physical processes exist that can lead to a shift in the frequency of electromagnetic radiation, including scattering and optical effects; however, the resulting changes are distinguishable from (astronomical) redshift and are not generally referred to as such (see section on physical optics and radiative transfer).

Currently, the objects with the highest known redshifts are galaxies and the objects producing gamma ray bursts. The most reliable redshifts are from spectroscopic data, and the highest-confirmed spectroscopic redshift of a galaxy is that of GN-z11,[65] with a redshift of z = 11.1, corresponding to 400 million years after the Big Bang. The previous record was held by UDFy-38135539[66] at a redshift of z = 8.6, corresponding to 600 million years after the Big Bang. Slightly less reliable are Lyman-break redshifts, the highest of which is the lensed galaxy A1689-zD1 at a redshift z = 7.5[67][68] and the next highest being z = 7.0.[69] The most distant-observed gamma-ray burst with a spectroscopic redshift measurement was GRB 090423, which had a redshift of z = 8.2.[70] The most distant-known quasar, ULAS J1342+0928, is at z = 7.54.[71][72] The highest-known redshift radio galaxy (TGSS1530) is at a redshift z = 5.72[73] and the highest-known redshift molecular material is the detection of emission from the CO molecule from the quasar SDSS J1148+5251 at z = 6.42.[74]

Extremely red objects (EROs) are astronomical sources of radiation that radiate energy in the red and near infrared part of the electromagnetic spectrum. These may be starburst galaxies that have a high redshift accompanied by reddening from intervening dust, or they could be highly redshifted elliptical galaxies with an older (and therefore redder) stellar population.[75] Objects that are even redder than EROs are termed hyper extremely red objects (HEROs).[76]

The cosmic microwave background has a redshift of z = 1089, corresponding to an age of approximately 379,000 years after the Big Bang and a proper distance of more than 46 billion light-years.[77] The yet-to-be-observed first light from the oldest Population III stars, not long after atoms first formed and the CMB ceased to be absorbed almost completely, may have redshifts in the range of 20 < z < 100.[78] Other high-redshift events predicted by physics but not presently observable are the cosmic neutrino background from about two seconds after the Big Bang (and a redshift in excess of z > 1010)[79] and the cosmic gravitational wave background emitted directly from inflation at a redshift in excess of z > 1025.[80]
$
7
Question 1: What does a blueshift represent in terms of wavelength and energy?

A: A decrease in wavelength and an increase in energy.
B: An increase in wavelength and a decrease in energy.
C: A consistent wavelength with a change in energy.
D: A decrease in both wavelength and energy.
E: An increase in both wavelength and energy.

Answer: A

Question 2: What is the primary cause of electromagnetic redshift in astronomy?

A: Reflection of light.
B: Interaction with dust particles.
C: The relative motions of radiation sources and gravitational potentials.
D: Interaction with gamma rays.
E: Scattering of electromagnetic radiation.

Answer: C

Question 3: What symbol often denotes the value of a redshift?

A: r
B: y
C: z
D: x
E: w

Answer: C

Question 4: Which of the following objects have the highest confirmed spectroscopic redshifts?

A: Quasars.
B: Radio galaxies.
C: Elliptical galaxies.
D: Starburst galaxies.
E: Galaxies.

Answer: E

Question 5: Extremely red objects (EROs) radiate energy primarily in which part of the electromagnetic spectrum?

A: Blue and ultraviolet.
B: Infrared and ultraviolet.
C: Red and near infrared.
D: Gamma and X-ray.
E: Visible light and gamma.

Answer: C

Question 6: What is the redshift of the cosmic microwave background?

A: z = 5.72
B: z = 1025
C: z = 1089
D: z = 7.54
E: z = 6.42

Answer: C

Question 7: What might cause an astronomical source to be classified as an Extremely red object (ERO)?

A: Being a newly formed star.
B: Emitting primarily in the gamma spectrum.
C: Having a high redshift and reddening from intervening dust or being a highly redshifted elliptical galaxy.
D: Being a source of frequent gamma-ray bursts.
E: Emitting primarily in the visible light range.

Answer: C
@
Subject:
In physics, a redshift is an increase in the wavelength, and corresponding decrease in the frequency and photon energy, of electromagnetic radiation (such as light). The opposite change, a decrease in wavelength and simultaneous increase in frequency and energy, is known as a negative redshift, or blueshift. The terms derive from the colours red and blue which form the extremes of the visible light spectrum. The main causes of electromagnetic redshift in astronomy and cosmology are the relative motions of radiation sources, which give rise to the relativistic Doppler effect), and gravitational potentials, which gravitationally redshift escaping radiation. All sufficiently distant light sources show cosmological redshift corresponding to recession speeds proportional to their distances from Earth, a fact known as Hubble's law that implies the universe is expanding.

The history of the subject began with the development in the 19th century of classical wave mechanics and the exploration of phenomena associated with the Doppler effect. The effect is named after Christian Doppler, who offered the first known physical explanation for the phenomenon in 1842.[1] The hypothesis was tested and confirmed for sound waves by the Dutch scientist Christophorus Buys Ballot in 1845.[2] Doppler correctly predicted that the phenomenon should apply to all waves, and in particular suggested that the varying colors of stars could be attributed to their motion with respect to the Earth.[3] Before this was verified, however, it was found that stellar colors were primarily due to a star's temperature, not motion. Only later was Doppler vindicated by verified redshift observations.[citation needed]

The first Doppler redshift was described by French physicist Hippolyte Fizeau in 1848, who pointed to the shift in spectral lines seen in stars as being due to the Doppler effect. The effect is sometimes called the "Doppler–Fizeau effect". In 1868, British astronomer William Huggins was the first to determine the velocity of a star moving away from the Earth by this method.[4] In 1871, optical redshift was confirmed when the phenomenon was observed in Fraunhofer lines using solar rotation, about 0.1 Å in the red.[5] In 1887, Vogel and Scheiner discovered the annual Doppler effect, the yearly change in the Doppler shift of stars located near the ecliptic due to the orbital velocity of the Earth.[6] In 1901, Aristarkh Belopolsky verified optical redshift in the laboratory using a system of rotating mirrors.[7]
$
7
Question 1: What change in electromagnetic radiation is represented by a redshift?

A: A decrease in wavelength and increase in frequency.
B: An increase in both wavelength and frequency.
C: A decrease in both wavelength and frequency.
D: An increase in wavelength and decrease in frequency.
E: No change in wavelength or frequency.

Answer: D

Question 2: What does a blueshift in electromagnetic radiation indicate?

A: An increase in wavelength and decrease in frequency.
B: A decrease in both wavelength and frequency.
C: An increase in both wavelength and frequency.
D: A decrease in wavelength and an increase in frequency.
E: No change in wavelength or frequency.

Answer: D

Question 3: Which law suggests that the universe is expanding?

A: Doppler's law
B: Fizeau's principle
C: Hubble's law
D: Vogel and Scheiner's theory
E: Ballot's hypothesis

Answer: C

Question 4: Who provided the first known physical explanation for the Doppler effect?

A: William Huggins
B: Hippolyte Fizeau
C: Christian Doppler
D: Christophorus Buys Ballot
E: Fraunhofer

Answer: C

Question 5: Which scientist confirmed the hypothesis of the Doppler effect for sound waves in 1845?

A: Fraunhofer
B: William Huggins
C: Hippolyte Fizeau
D: Vogel and Scheiner
E: Christophorus Buys Ballot

Answer: E

Question 6: The colors of stars were initially believed to be due to their motion with respect to Earth. Later it was found that their colors were primarily due to what factor?

A: Their size
B: Their distance from the Earth
C: Their age
D: Their temperature
E: Their composition

Answer: D

Question 7: The yearly change in the Doppler shift of stars located near the ecliptic due to the Earth's orbital velocity is known as what?

A: Hubble's effect
B: The Doppler–Fizeau effect
C: The annual Doppler effect
D: The solar redshift effect
E: The stellar motion effect

Answer: C
@
Subject:
The black hole information paradox[1] is a puzzle that appears when the predictions of quantum mechanics and general relativity are combined. The theory of general relativity predicts the existence of black holes that are regions of spacetime from which nothing — not even light — can escape. In the 1970s, Stephen Hawking applied the semi-classical approach of quantum field theory in curved spacetime to such systems and found that an isolated black hole would emit a form of radiation called Hawking radiation. Hawking also argued that the detailed form of the radiation would be independent of the initial state of the black hole,[2] and would depend only on its mass, electric charge and angular momentum.

The information paradox appears when one considers a process in which a black hole is formed through a physical process and then evaporates away entirely through Hawking radiation. Hawking's calculation suggests that the final state of radiation would retain information only about the total mass, electric charge and angular momentum of the initial state. Since many different states can have the same mass, charge and angular momentum, this suggests that many initial physical states could evolve into the same final state. Therefore, information about the details of the initial state would be permanently lost; however, this violates a core precept of both classical and quantum physics—that, in principle, the state of a system at one point in time should determine its value at any other time.[3][4] Specifically, in quantum mechanics the state of the system is encoded by its wave function. The evolution of the wave function is determined by a unitary operator, and unitarity implies that the wave function at any instant of time can be used to determine the wave function either in the past or the future. In 1993, Don Page argued that if a black hole starts in a pure quantum state and evaporates completely by a unitary process, the von Neumann entropy of the Hawking radiation initially increases and then decreases back to zero when the black hole has disappeared.[5] This is called the Page curve.[6]

It is now generally believed that information is preserved in black-hole evaporation.[7][8][9] For many researchers, deriving the Page curve is synonymous with solving the black hole information puzzle.[10]: 291  However, views differ as to how, precisely, Hawking's original semi-classical calculation should be corrected.[8][9][11][12] In recent years, several extensions of the original paradox have been explored. Taken together these puzzles about black hole evaporation have implications for how gravity and quantum mechanics must be combined, leading to the information paradox remaining an active field of research within quantum gravity.
$
7
Question 1: Which theory predicts the existence of black holes as regions in spacetime from which nothing can escape?

A: Quantum mechanics
B: Electromagnetism
C: Thermodynamics
D: General relativity
E: Classical mechanics

Answer: D

Question 2: What type of radiation is emitted by an isolated black hole according to Stephen Hawking's application of quantum field theory in curved spacetime?

A: Gamma radiation
B: Ultraviolet radiation
C: X-ray radiation
D: Microwave radiation
E: Hawking radiation

Answer: E

Question 3: According to Hawking's calculations, the form of radiation emitted by a black hole is independent of which of the following?

A: Its mass
B: Its temperature
C: Its initial state
D: Its volume
E: Its speed

Answer: C

Question 4: In quantum mechanics, the state of a system is encoded by what?

A: Its gravitational pull
B: Its potential energy
C: Its atomic structure
D: Its wave function
E: Its photon count

Answer: D

Question 5: What is the Page curve a representation of?

A: The increase and subsequent decrease of the von Neumann entropy of Hawking radiation as a black hole evaporates.
B: The radiation emitted from a black hole over time.
C: The change in mass of a black hole as it consumes matter.
D: The increase in the gravitational pull of a black hole over time.
E: The speed at which a black hole moves in space.

Answer: A

Question 6: For many researchers, deriving the Page curve is synonymous with solving what?

A: The equation of state for black holes
B: The mystery of dark matter
C: The black hole singularity problem
D: The black hole information puzzle
E: The theory of everything

Answer: D

Question 7: Which paradox suggests a challenge in combining the predictions of quantum mechanics and general relativity?

A: Heisenberg uncertainty principle
B: Schrödinger's cat paradox
C: Twin paradox
D: Quantum entanglement
E: Black hole information paradox

Answer: E
@
Subject:
The Kutta condition is a principle in steady-flow fluid dynamics, especially aerodynamics, that is applicable to solid bodies with sharp corners, such as the trailing edges of airfoils. It is named for German mathematician and aerodynamicist Martin Kutta.

Kuethe and Schetzer state the Kutta condition as follows:[1]: § 4.11 

A body with a sharp trailing edge which is moving through a fluid will create about itself a circulation of sufficient strength to hold the rear stagnation point at the trailing edge.

In fluid flow around a body with a sharp corner, the Kutta condition refers to the flow pattern in which fluid approaches the corner from above and below, meets at the corner, and then flows away from the body. None of the fluid flows around the sharp corner.

The Kutta condition is significant when using the Kutta–Joukowski theorem to calculate the lift created by an airfoil with a sharp trailing edge. The value of circulation of the flow around the airfoil must be that value which would cause the Kutta condition to exist.
$
5
Question: In the context of fluid dynamics, what role does the Kutta condition play in the flow around an airfoil with a sharp trailing edge?

A: It predicts the turbulent regions of the airfoil.
B: It ensures that there's maximum drag experienced by the airfoil.
C: It dictates that the rear stagnation point is held at the trailing edge due to a specific circulation strength.
D: It explains the vortices formed at the leading edge of the airfoil.
E: It sets the boundary condition for compressible flows around the airfoil.

Answer: C

Question 2: According to Kuethe and Schetzer's statement, what will a body with a sharp trailing edge create when moving through a fluid?

A: A circulation of varying strength around itself.
B: A trailing pattern that opposes the fluid's flow.
C: A leading pattern that supports the fluid's flow.
D: A circulation of sufficient strength to hold the rear stagnation point at the trailing edge.
E: A circulation that leads the fluid flow away from the sharp corner.

Answer: D

Question 3: Which theorem is significant when using the Kutta condition for calculating lift?

A: Bernoulli's theorem
B: Newton's third law
C: Kutta–Joukowski theorem
D: Reynolds' theorem
E: Schetzer's theorem

Answer: C

Question 4: In fluid flow around a body with a sharp corner, what does the Kutta condition dictate about the fluid's behavior?

A: Fluid approaches the corner, circulates around the corner, and flows away from the body.
B: Fluid does not approach sharp corners.
C: Fluid approaches the corner from above and below, meets at the corner, and flows away without circulating around the sharp corner.
D: Fluid always stagnates at sharp corners.
E: Fluid always maintains a laminar flow near sharp corners.

Answer: C

Question 5: Which of the following is most closely associated with the Kutta condition?

A: The behavior of fluid in a circular motion.
B: The circulation of fluid in regions far from sharp corners.
C: The movement of fluid around an airfoil's leading edge.
D: The flow pattern at the sharp corners of solid bodies, particularly the trailing edges of airfoils.
E: The vortices created in turbulent flows.

Answer: D
@
Subject:
Classical mechanics is a physical theory describing the motion of macroscopic objects, from projectiles to parts of machinery and astronomical objects, such as spacecraft, planets, stars, and galaxies. For objects governed by classical mechanics, if the present state is known, it is possible to predict how it will move in the future (determinism), and how it has moved in the past (reversibility).

The "classical" in "classical mechanics" does not refer classical antiquity, as it might in, say, classical architecture. On the contrary the development of classical mechanics involved substantial change in the methods and philosophy of physics.[1] Instead, the qualifier distinguishes classical mechanics from physics developed after the revolutions of the early 20th century, which revealed limitations of classical mechanics.[2]

The earliest formulation of classical mechanics is often referred to as Newtonian mechanics. It consists of the physical concepts based on foundational works of Sir Isaac Newton, and the mathematical methods invented by Gottfried Wilhelm Leibniz, Joseph-Louis Lagrange, Leonhard Euler, and other contemporaries in the 17th century to describe the motion of bodies under the influence of forces. Later, more abstract methods were developed, leading to the reformulations of classical mechanics known as Lagrangian mechanics and Hamiltonian mechanics. These advances, made predominantly in the 18th and 19th centuries, extend substantially beyond earlier works, particularly through their use of analytical mechanics. They are, with some modification, also used in all areas of modern physics.

Classical mechanics provides accurate results when studying large objects that are not extremely massive and speeds not approaching the speed of light. When the objects being examined have about the size of an atom diameter, it becomes necessary to introduce the other major sub-field of mechanics: quantum mechanics. To describe velocities that are not small compared to the speed of light, special relativity is needed. In cases where objects become extremely massive, general relativity becomes applicable. However, a number of modern sources do include relativistic mechanics in classical physics, which in their view represents classical mechanics in its most developed and accurate form.
$
5
Question: Which of the following is NOT true regarding classical mechanics?
A: It can accurately predict the motion of macroscopic objects.
B: It was significantly developed during the 17th century, especially by Sir Isaac Newton.
C: It provides accurate results for objects moving close to the speed of light.
D: Lagrangian mechanics and Hamiltonian mechanics are reformulations of classical mechanics.
E: Classical mechanics becomes less applicable when studying objects around the size of an atom diameter.

Answer: C

Question: The development of classical mechanics involved substantial changes in:
A: The methods and philosophy of physics.
B: The structure of atomic nuclei.
C: The understanding of electromagnetic waves.
D: The principles of heat and thermodynamics.
E: The study of chemical reactions.

Answer: A

Question: If one were examining the motion of bodies under the influence of forces based on the foundational works of Sir Isaac Newton, they would be studying:
A: Hamiltonian mechanics.
B: Quantum mechanics.
C: Special relativity.
D: Newtonian mechanics.
E: Thermodynamics.

Answer: D

Question: In cases where velocities are comparable to the speed of light, which theory becomes necessary?
A: Classical mechanics.
B: Lagrangian mechanics.
C: Quantum mechanics.
D: General relativity.
E: Special relativity.

Answer: E

Question: Which mechanics becomes necessary when studying the behavior of objects around the size of an atom diameter?
A: Lagrangian mechanics.
B: Newtonian mechanics.
C: Quantum mechanics.
D: Special relativity.
E: Classical mechanics.

Answer: C

@
Subject:
Spontaneous symmetry breaking is a spontaneous process of symmetry breaking, by which a physical system in a symmetric state spontaneously ends up in an asymmetric state.[1][2][3] In particular, it can describe systems where the equations of motion or the Lagrangian obey symmetries, but the lowest-energy vacuum solutions do not exhibit that same symmetry. When the system goes to one of those vacuum solutions, the symmetry is broken for perturbations around that vacuum even though the entire Lagrangian retains that symmetry.

In theoretical physics, explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion (most typically, to the Lagrangian or the Hamiltonian) that do not respect the symmetry. Usually this term is used in situations where these symmetry-breaking terms are small, so that the symmetry is approximately respected by the theory. An example is the spectral line splitting in the Zeeman effect, due to a magnetic interaction perturbation in the Hamiltonian of the atoms involved.

Explicit symmetry breaking differs from spontaneous symmetry breaking. In the latter, the defining equations respect the symmetry but the ground state (vacuum) of the theory breaks it.[1]

On October 7, 2008, the Royal Swedish Academy of Sciences awarded the 2008 Nobel Prize in Physics to three scientists for their work in subatomic physics symmetry breaking. Yoichiro Nambu, of the University of Chicago, won half of the prize for the discovery of the mechanism of spontaneous broken symmetry in the context of the strong interactions, specifically chiral symmetry breaking. Physicists Makoto Kobayashi and Toshihide Maskawa, of Kyoto University, shared the other half of the prize for discovering the origin of the explicit breaking of CP symmetry in the weak interactions.[14] This origin is ultimately reliant on the Higgs mechanism, but, so far understood as a "just so" feature of Higgs couplings, not a spontaneously broken symmetry phenomenon.
$
5
Question: What does spontaneous symmetry breaking refer to?
A: A process by which a system with no symmetry spontaneously develops symmetry.
B: A process where an asymmetric state transitions into a symmetric state.
C: A spontaneous process where a symmetric system ends up in an asymmetric state.
D: The process where a physical system, regardless of its state, retains its symmetry.
E: The addition of terms to the Lagrangian or Hamiltonian that disrupt the system's symmetry.

Answer: C

Question: In explicit symmetry breaking:
A: The ground state respects the symmetry, but the defining equations break it.
B: Both the ground state and the defining equations respect the symmetry.
C: The defining equations break the symmetry, whereas the ground state respects it.
D: The ground state breaks the symmetry, but the defining equations respect it.
E: The system is always in a symmetric state, regardless of external influences.

Answer: C

Question: The spectral line splitting observed in the Zeeman effect is a result of:
A: Spontaneous symmetry breaking due to the Higgs mechanism.
B: Explicit symmetry breaking due to a magnetic interaction perturbation in the Hamiltonian.
C: Spontaneous symmetry breaking due to external forces.
D: Explicit symmetry breaking due to gravitational influences.
E: Spontaneous symmetry breaking due to the motion of subatomic particles.

Answer: B

Question: Which phenomenon differs from explicit symmetry breaking because its defining equations respect the symmetry but the ground state does not?
A: Quantum entanglement
B: Superconductivity
C: Spontaneous symmetry breaking
D: Quantum tunneling
E: Quantum decoherence

Answer: C

Question: What discovery earned Yoichiro Nambu half of the 2008 Nobel Prize in Physics?
A: The origin of the explicit breaking of CP symmetry in the weak interactions.
B: The discovery of the Higgs boson.
C: The mechanism of spontaneous broken symmetry in the context of strong interactions.
D: The formulation of quantum electrodynamics.
E: The invention of quantum chromodynamics.

Answer: C

@
Subject:
Alternatives to general relativity are physical theories that attempt to describe the phenomenon of gravitation in competition with Einstein's theory of general relativity. There have been many different attempts at constructing an ideal theory of gravity.[1]

These attempts can be split into four broad categories based on their scope. In this article, straightforward alternatives to general relativity are discussed, which do not involve quantum mechanics or force unification. Other theories which do attempt to construct a theory using the principles of quantum mechanics are known as theories of quantized gravity. Thirdly, there are theories which attempt to explain gravity and other forces at the same time; these are known as classical unified field theories. Finally, the most ambitious theories attempt to both put gravity in quantum mechanical terms and unify forces; these are called theories of everything.

None of these alternatives to general relativity have gained wide acceptance. General relativity has withstood many tests,[2] remaining consistent with all observations so far. In contrast, many of the early alternatives have been definitively disproven. However, some of the alternative theories of gravity are supported by a minority of physicists, and the topic remains the subject of intense study in theoretical physics.

Modified Newtonian dynamics (MOND) is a hypothesis that proposes a modification of Newton's law of universal gravitation to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.

Tensor–vector–scalar gravity (TeVeS),[1] developed by Jacob Bekenstein in 2004, is a relativistic generalization of Mordehai Milgrom's Modified Newtonian dynamics (MOND) paradigm.[2][3]

The main features of TeVeS can be summarized as follows:

As it is derived from the action principle, TeVeS respects conservation laws;
In the weak-field approximation of the spherically symmetric, static solution, TeVeS reproduces the MOND acceleration formula;
TeVeS avoids the problems of earlier attempts to generalize MOND, such as superluminal propagation;
As it is a relativistic theory it can accommodate gravitational lensing.
The theory is based on the following ingredients:

A unit vector field;
A dynamical scalar field;
A nondynamical scalar field;
A matter Lagrangian constructed using an alternate metric;
An arbitrary dimensionless function.
These components are combined into a relativistic Lagrangian density, which forms the basis of TeVeS theory.

Entropic gravity, also known as emergent gravity, is a theory in modern physics that describes gravity as an entropic force—a force with macro-scale homogeneity but which is subject to quantum-level disorder—and not a fundamental interaction. The theory, based on string theory, black hole physics, and quantum information theory, describes gravity as an emergent phenomenon that springs from the quantum entanglement of small bits of spacetime information. As such, entropic gravity is said to abide by the second law of thermodynamics under which the entropy of a physical system tends to increase over time.
$
6
Question: Which of the following best defines alternatives to general relativity?
A: Theories that modify Einstein's theory of general relativity to better explain quantum mechanics.
B: Physical theories that compete with Einstein's theory of general relativity to describe gravitation.
C: Theories that adjust the laws of thermodynamics to align with general relativity.
D: Hypotheses that solely focus on quantum mechanical aspects of general relativity.
E: Propositions that deny the validity of Einstein's theory of general relativity.

Answer: B

Question: How can the attempts at alternatives to general relativity be categorized based on their scope?
A: Quantum mechanics, dark matter, and black holes.
B: Relativistic, non-relativistic, and semi-relativistic theories.
C: Straightforward alternatives, theories of quantized gravity, classical unified field theories, and theories of everything.
D: By the scientists who proposed them.
E: Non-relativistic, quantum, unified, and emergent theories.

Answer: C

Question: What is the purpose behind Modified Newtonian dynamics (MOND)?
A: To unify the forces of nature.
B: To propose a modification of Newton's laws that explains why galaxies appear to disobey currently understood laws of physics.
C: To integrate quantum mechanics with general relativity.
D: To explain the nature of dark energy in the universe.
E: To provide a generalization of Einstein's general relativity.

Answer: B

Question: Tensor–vector–scalar gravity (TeVeS) is a generalization of which paradigm?
A: String theory.
B: Quantum entanglement.
C: Mordehai Milgrom's Modified Newtonian dynamics (MOND).
D: Entropic gravity.
E: Quantum field theory.

Answer: C

Question: What does entropic gravity describe gravity as?
A: A primary interaction that's fundamental to all matter.
B: A force with macro-scale homogeneity subjected to quantum-level disorder.
C: An extension of the electromagnetic force.
D: A direct consequence of Einstein's equations of relativity.
E: A macroscopic phenomenon without any quantum considerations.

Answer: B

Question: Which theory is based on the combination of string theory, black hole physics, and quantum information theory and views gravity as an emergent phenomenon?
A: Modified Newtonian dynamics (MOND).
B: Tensor–vector–scalar gravity (TeVeS).
C: Entropic gravity.
D: Classical unified field theories.
E: Theory of everything.

Answer: C
@
Subject:
The ring-imaging Cherenkov, or RICH, detector is a device for identifying the type of an electrically charged subatomic particle of known momentum, that traverses a transparent refractive medium, by measurement of the presence and characteristics of the Cherenkov radiation emitted during that traversal. RICH detectors were first developed in the 1980s and are used in high energy elementary particle- , nuclear- and astro-physics experiments.

Both focusing and proximity-focusing detectors are in use.

In the more compact proximity-focusing design a thin radiator volume emits a cone of Cherenkov light which traverses a small distance, the proximity gap, and is detected on the photon detector plane. The image is a ring of light the radius of which is defined by the Cherenkov emission angle and the proximity gap. The ring thickness is mainly determined by the thickness of the radiator. An example of a proximity gap RICH detector is the High Momentum Particle Identification (HMPID), one of the detectors of ALICE (A Large Ion Collider Experiment), which is one of the five experiments at the LHC (Large Hadron Collider) at CERN.

DIRC detector
In a DIRC (Detection of Internally Reflected Cherenkov light), another design of a RICH detector, light that is captured by total internal reflection inside the solid radiator reaches the light sensors at the detector perimeter, the precise rectangular cross section of the radiator preserving the angular information of the Cherenkov light cone. One example is the DIRC of the BaBar experiment at SLAC.

LHCb detector
The LHCb experiment on the Large Hadron Collider uses two RICH detectors for differentiating between pions and kaons.[10] The first (RICH-1) is located immediately after the Vertex Locator (VELO) around the interaction point and is optimised for low-momentum particles and the second (RICH-2) is located after the magnet and particle-tracker layers and optimised for higher-momentum particles.[8]

AMS-02
The Alpha Magnetic Spectrometer device AMS-02, recently mounted on the International Space Station uses a RICH detector in combination with other devices to analyze cosmic rays.
$
6
Question: When were RICH detectors first developed?
A: 1970s
B: 1980s
C: 1990s
D: 2000s
E: 2010s

Answer: B

Question: What is the primary function of a RICH detector?
A: To measure the speed of light in various mediums.
B: To identify the type of an electrically charged subatomic particle of known momentum.
C: To detect magnetic fields produced by subatomic particles.
D: To measure the mass of charged particles.
E: To measure the energy levels of gamma rays.

Answer: B

Question: How is the ring thickness in the proximity-focusing design of a RICH detector mainly determined?
A: By the Cherenkov emission angle.
B: By the proximity gap.
C: By the radius of the ring.
D: By the thickness of the radiator.
E: By the intensity of the emitted light.

Answer: D

Question: In a DIRC design of a RICH detector, how does the light reach the light sensors at the detector perimeter?
A: Via refraction through the radiator.
B: By being absorbed and re-emitted by the radiator.
C: By total internal reflection inside the solid radiator.
D: By diffraction around the radiator.
E: Through the emission of Bremsstrahlung radiation.

Answer: C

Question: Which of the following experiments uses a RICH detector to differentiate between pions and kaons?
A: BaBar experiment at SLAC.
B: High Momentum Particle Identification (HMPID).
C: Alpha Magnetic Spectrometer (AMS-02).
D: LHCb experiment on the Large Hadron Collider.
E: A Large Ion Collider Experiment (ALICE) at CERN.

Answer: D

Question: Where has the Alpha Magnetic Spectrometer device AMS-02, which uses a RICH detector, been mounted?
A: On the Voyager spacecraft.
B: On the Hubble Space Telescope.
C: On the James Webb Space Telescope.
D: On the Mars Rover.
E: On the International Space Station.

Answer: E
@
Subject:
A light-year, alternatively spelled light year, is a unit of length used to express astronomical distances and is equivalent to about 9.46 trillion kilometers (9.46×1012 km), or 5.88 trillion miles (5.88×1012 mi).[note 1] As defined by the International Astronomical Union (IAU), a light-year is the distance that light travels in a vacuum in one Julian year (365.25 days).[2] Because it includes the word "year", the term is sometimes misinterpreted as a unit of time.[3]

The light-year is most often used when expressing distances to stars and other distances on a galactic scale, especially in non-specialist contexts and popular science publications.[3] The unit most commonly used in professional astronomy is the parsec (symbol: pc, about 3.26 light-years) which derives from astrometry; it is the distance at which one astronomical unit (au) subtends an angle of one second of arc.[2]
$
6
Question: Which organization has defined the light-year as the distance that light travels in a vacuum in one Julian year?
A: NASA
B: European Space Agency
C: International Astronomical Union (IAU)
D: United Nations Office for Outer Space Affairs
E: American Astronomical Society

Answer: C

Question: How is a light-year commonly misinterpreted due to its name?
A: As a unit of energy.
B: As a unit of time.
C: As a unit of mass.
D: As a unit of temperature.
E: As a unit of velocity.

Answer: B

Question: Which unit is most commonly used in professional astronomy and is about 3.26 light-years in length?
A: Kiloparsec
B: Astronomical unit
C: Megaparsec
D: Gigaparsec
E: Parsec

Answer: E

Question: Why is the term parsec derived from astrometry?
A: It represents the distance at which one year subtends an angle of one second of arc.
B: It represents the distance at which one astronomical unit (au) subtends an angle of one second of arc.
C: It represents the time it takes for light to travel one astronomical unit in a vacuum.
D: It represents the distance light travels in one year in a non-vacuum medium.
E: It represents the average distance between Earth and the nearest star.

Answer: B

Question: In which of the following scenarios is the light-year most often used?
A: Measuring distances within our solar system.
B: Calculating the speed of rockets in space.
C: Expressing distances to stars and on a galactic scale in popular science publications.
D: Calculating the size of planets.
E: Measuring the temperature of distant stars.

Answer: C

Question: How long is a light-year in miles?
A: 5.88×10^6 mi
B: 5.88×10^9 mi
C: 5.88×10^10 mi
D: 5.88×10^12 mi
E: 5.88×10^15 mi

Answer: D

@
Subject:
A memristor (/ˈmɛmrɪstər/; a portmanteau of memory resistor) is a non-linear two-terminal electrical component relating electric charge and magnetic flux linkage. It was described and named in 1971 by Leon Chua, completing a theoretical quartet of fundamental electrical components which also comprises the resistor, capacitor and inductor.[1]

Chua and Kang later generalized the concept to memristive systems.[2] Such a system comprises a circuit, of multiple conventional components, which mimics key properties of the ideal memristor component and is also commonly referred to as a memristor. Several such memristor system technologies have been developed, notably ReRAM.

The identification of memristive properties in electronic devices has attracted controversy. Experimentally, the ideal memristor has yet to be demonstrated.[3][4]

The ferroelectric memristor[77] is based on a thin ferroelectric barrier sandwiched between two metallic electrodes. Switching the polarization of the ferroelectric material by applying a positive or negative voltage across the junction can lead to a two order of magnitude resistance variation: ROFF ≫ RON (an effect called Tunnel Electro-Resistance). In general, the polarization does not switch abruptly. The reversal occurs gradually through the nucleation and growth of ferroelectric domains with opposite polarization. During this process, the resistance is neither RON or ROFF, but in between. When the voltage is cycled, the ferroelectric domain configuration evolves, allowing a fine tuning of the resistance value. The ferroelectric memristor's main advantages are that ferroelectric domain dynamics can be tuned, offering a way to engineer the memristor response, and that the resistance variations are due to purely electronic phenomena, aiding device reliability, as no deep change to the material structure is involved.
$
8
A memristor is a combination of which two words?
A: Memory and resistor
B: Metal and resistor
C: Magnetic and transistor
D: Membrane and transistor
E: Metal and memory
Answer: A

Who described and named the memristor in 1971?
A: Nikola Tesla
B: Albert Einstein
C: Leon Chua
D: Richard Feynman
E: Werner Heisenberg
Answer: C

Which of the following is NOT part of the theoretical quartet of fundamental electrical components?
A: Resistor
B: Capacitor
C: Transistor
D: Inductor
E: Memristor
Answer: C

Memristive systems are:
A: Always smaller than conventional memristors.
B: Circuits that mimic the properties of an ideal memristor using multiple conventional components.
C: Systems where memristors are used to store data in binary format.
D: A subtype of memristors that use only organic materials.
E: A theoretical concept that hasn't been realized yet.
Answer: B

The identification of memristive properties in electronic devices has:
A: Been universally accepted.
B: Attracted controversy.
C: Only been identified in ferroelectric memristors.
D: Resulted in the replacement of all other electronic components.
E: Made capacitors obsolete.
Answer: B

The resistance variation in a ferroelectric memristor is due to:
A: Magnetism.
B: The depth of the material structure.
C: Purely electronic phenomena.
D: Heating of the device.
E: The type of metal used in electrodes.
Answer: C

In a ferroelectric memristor, when a voltage is cycled, what happens to the ferroelectric domain configuration?
A: It remains constant.
B: It disappears and needs to be reintroduced.
C: It evolves.
D: It only reacts to positive voltage.
E: It becomes unstable and might cause device malfunction.
Answer: C

The ferroelectric memristor's resistance variation is called:
A: Piezoelectric Resistance.
B: Tunnel Electro-Resistance.
C: Ferroelastic Resistance.
D: Nucleation Resistance.
E: Conventional Resistance.
Answer: B
@
Subject:
Variable-range hopping is a model used to describe carrier transport in a disordered semiconductor or in amorphous solid by hopping in an extended temperature range.

In condensed matter physics and materials science, an amorphous solid (or non-crystalline solid) is a solid that lacks the long-range order that is characteristic of a crystal. The terms "glass" and "glassy solid" are sometimes used synonymously with amorphous solid; however, these terms refer specifically to amorphous materials that undergo a glass transition.[1] Examples of amorphous solids include glasses, metallic glasses, and certain types of plastics and polymers.[2]

In physics, quantum tunnelling, barrier penetration, or simply tunnelling is a quantum mechanical phenomenon in which an object such as an electron or atom passes through a potential energy barrier that, according to classical mechanics, the object does not have sufficient energy to enter or surmount.

Tunneling is a consequence of the wave nature of matter, where the quantum wave function describes the state of a particle or other physical system, and wave equations such as the Schrödinger equation describe their behavior. The probability of transmission of a wave packet through a barrier decreases exponentially with the barrier height, the barrier width, and the tunneling particle's mass, so tunneling is seen most prominently in low-mass particles such as electrons or protons tunneling through microscopically narrow barriers. Tunneling is readily detectable with barriers of thickness about 1–3 nm or smaller for electrons, and about 0.1 nm or smaller for heavier particles such as protons or hydrogen atoms.[1] Some sources describe the mere penetration of a wave function into the barrier, without transmission on the other side, as a tunneling effect, such as in tunneling into the walls of a finite potential well.[2][3]

Tunneling plays an essential role in physical phenomena such as nuclear fusion[4] and alpha radioactive decay of atomic nuclei. Tunneling applications include the tunnel diode,[5] quantum computing, flash memory, and the scanning tunneling microscope. Tunneling limits the minimum size of devices used in microelectronics because electrons tunnel readily through insulating layers and transistors that are thinner than about 1 nm.[6][7]
$
10
An amorphous solid is different from a crystal because it:
A: Has a regular repeating structure.
B: Has a short-range order.
C: Lacks long-range order.
D: Is a liquid at room temperature.
E: Only consists of metals.
Answer: C

Which of the following is NOT synonymous with "amorphous solid"?
A: Glass
B: Non-crystalline solid
C: Glassy solid
D: Crystal
E: Metallic glass
Answer: D

Quantum tunneling allows an object to:
A: Reflect off potential barriers.
B: Increase its kinetic energy.
C: Pass through potential barriers it classically shouldn't.
D: Travel at the speed of light.
E: Increase its potential energy.
Answer: C

The wave nature of matter and its description through wave functions is essential for understanding:
A: Newton's laws of motion.
B: General relativity.
C: The behavior of classical gases.
D: Quantum tunneling.
E: The photoelectric effect.
Answer: D

The probability of tunneling through a barrier decreases exponentially with all EXCEPT:
A: Barrier height.
B: Tunneling particle's mass.
C: Barrier color.
D: Barrier width.
E: Thickness of the barrier.
Answer: C

Tunneling becomes significant in barriers of thickness about 1–3 nm for:
A: Protons.
B: Neutrons.
C: Electrons.
D: Photons.
E: Positrons.
Answer: C

Tunneling affects the limits of:
A: Speed of electrical circuits.
B: Thermal conductivity of metals.
C: Minimum size of devices in microelectronics.
D: Optical properties of materials.
E: Magnetic properties of materials.
Answer: C

The quantum mechanical phenomenon where an electron or atom passes through a potential energy barrier is known as:
A: Reflection.
B: Refraction.
C: Scattering.
D: Diffusion.
E: Tunneling.
Answer: E

In addition to being seen in the context of semiconductors, quantum tunneling plays a crucial role in:
A: Photosynthesis.
B: Brownian motion.
C: Nuclear fusion.
D: Electron drift.
E: Magnetism.
Answer: C

Which device directly leverages the principle of tunneling?
A: Light Emitting Diode.
B: Transistor.
C: Tunnel Diode.
D: Photovoltaic cell.
E: Inductor.
Answer: C
@
Subject:
Electrical resistivity (also called volume resistivity or specific electrical resistance) is a fundamental specific property of a material that measures its electrical resistance or how strongly it resists electric current. A low resistivity indicates a material that readily allows electric current. Resistivity is commonly represented by the Greek letter ρ (rho). The SI unit of electrical resistivity is the ohm-metre (Ω⋅m).[1][2][3] For example, if a 1 m3 solid cube of material has sheet contacts on two opposite faces, and the resistance between these contacts is 1 Ω, then the resistivity of the material is 1 Ω⋅m.

Electrical conductivity (or specific conductance) is the reciprocal of electrical resistivity. It represents a material's ability to conduct electric current. It is commonly signified by the Greek letter σ (sigma), but κ (kappa) (especially in electrical engineering) and γ (gamma) are sometimes used. The SI unit of electrical conductivity is siemens per metre (S/m). Resistivity and conductivity are intensive properties of materials, giving the opposition of a standard cube of material to current. Electrical resistance and conductance are corresponding extensive properties that give the opposition of a specific object to electric current.
$
10
Which property represents a material's ability to conduct electric current?
A: Electrical resistance
B: Electrical inductance
C: Electrical resistivity
D: Electrical capacitance
E: Electrical conductivity
Answer: E

What is the SI unit of electrical resistivity?
A: Ohm
B: Siemens
C: Ohm-meter
D: Siemens per meter
E: Ohm per meter
Answer: C

If a material has high resistivity, it means:
A: It allows electric current easily.
B: It has a high capacity to store electric charge.
C: It has low resistance.
D: It strongly resists electric current.
E: It has a high electrical conductivity.
Answer: D

Which Greek letter is commonly used to represent electrical resistivity?
A: α (alpha)
B: β (beta)
C: γ (gamma)
D: ρ (rho)
E: σ (sigma)
Answer: D

Which of the following is true about electrical conductivity?
A: It is the reciprocal of electrical resistance.
B: It represents a material's opposition to electric current.
C: It has the unit of ohm-meter.
D: It is the reciprocal of electrical resistivity.
E: It decreases as electrical resistivity increases.
Answer: D

If an object has high electrical resistance, it implies:
A: The object has high electrical conductivity.
B: The object has low electrical resistivity.
C: The object does not allow electric current easily.
D: The object is an excellent conductor of electricity.
E: The object's electrical resistivity is dependent on its shape and size.
Answer: C

Which Greek letter sometimes represents electrical conductivity in electrical engineering?
A: α (alpha)
B: μ (mu)
C: γ (gamma)
D: θ (theta)
E: κ (kappa)
Answer: E

What property is described as the opposition of a standard cube of material to current?
A: Electrical resistance
B: Electrical impedance
C: Electrical resistivity
D: Electrical conductivity
E: Electrical reactance
Answer: D

If a 2 m^3 solid cube of material has a resistance of 2 Ω between two opposite faces, its resistivity is:
A: 4 Ω⋅m.
B: 1 Ω⋅m.
C: 0.5 Ω⋅m.
D: 2 Ω⋅m.
E: 0.25 Ω⋅m.
Answer: B

What would indicate a material that does not readily allow electric current?
A: Low resistivity and high conductivity.
B: High resistivity and low conductivity.
C: High resistivity and high conductivity.
D: Low resistivity and low conductivity.
E: Moderate resistivity and moderate conductivity.
Answer: B
@
Subject:
In 1666, Newton observed that the spectrum of colours exiting a prism in the position of minimum deviation is oblong, even when the light ray entering the prism is circular, which is to say, the prism refracts different colours by different angles.[52][53] This led him to conclude that colour is a property intrinsic to light – a point which had, until then, been a matter of debate.

From 1670 to 1672, Newton lectured on optics.[54] During this period he investigated the refraction of light, demonstrating that the multicoloured image produced by a prism, which he named a spectrum, could be recomposed into white light by a lens and a second prism.[55] Modern scholarship has revealed that Newton's analysis and resynthesis of white light owes a debt to corpuscular alchemy.[56]

He showed that coloured light does not change its properties by separating out a coloured beam and shining it on various objects, and that regardless of whether reflected, scattered, or transmitted, the light remains the same colour. Thus, he observed that colour is the result of objects interacting with already-coloured light rather than objects generating the colour themselves. This is known as Newton's theory of colour.[57]

From this work, he concluded that the lens of any refracting telescope would suffer from the dispersion of light into colours (chromatic aberration). As a proof of the concept, he constructed a telescope using reflective mirrors instead of lenses as the objective to bypass that problem.[58][59] Building the design, the first known functional reflecting telescope, today known as a Newtonian telescope,[59] involved solving the problem of a suitable mirror material and shaping technique. Newton ground his own mirrors out of a custom composition of highly reflective speculum metal, using Newton's rings to judge the quality of the optics for his telescopes. In late 1668,[60] he was able to produce this first reflecting telescope. It was about eight inches long and it gave a clearer and larger image. In 1671, the Royal Society asked for a demonstration of his reflecting telescope.[61] Their interest encouraged him to publish his notes, Of Colours,[62] which he later expanded into the work Opticks. When Robert Hooke criticised some of Newton's ideas, Newton was so offended that he withdrew from public debate. Newton and Hooke had brief exchanges in 1679–80, when Hooke, appointed to manage the Royal Society's correspondence, opened up a correspondence intended to elicit contributions from Newton to Royal Society transactions,[63] which had the effect of stimulating Newton to work out a proof that the elliptical form of planetary orbits would result from a centripetal force inversely proportional to the square of the radius vector. But the two men remained generally on poor terms until Hooke's death.[64]

Newton argued that light is composed of particles or corpuscles, which were refracted by accelerating into a denser medium. He verged on soundlike waves to explain the repeated pattern of reflection and transmission by thin films (Opticks Bk. II, Props. 12), but still retained his theory of 'fits' that disposed corpuscles to be reflected or transmitted (Props.13). However, later physicists favoured a purely wavelike explanation of light to account for the interference patterns and the general phenomenon of diffraction. Today's quantum mechanics, photons, and the idea of wave–particle duality bear only a minor resemblance to Newton's understanding of light.
$
10
How did Newton conclude that color is a property of light after his prism experiments in 1666?
A: By observing the rectangular shape of the spectrum exiting the prism.
B: By observing that the spectrum of colours is oblong even if the entering light ray is circular.
C: By observing that light bends at different angles inside the prism.
D: By observing the circular shape of light entering the prism.
E: By observing the total reflection of light within the prism.
Answer: B

Newton's experiment of recomposing white light from a spectrum utilized:
A: Two lenses and a prism.
B: A lens and two prisms.
C: Two mirrors and a prism.
D: A mirror and two prisms.
E: Three prisms in series.
Answer: B

Which of the following problems associated with refracting telescopes did Newton aim to overcome with his reflecting telescope?
A: Chromatic aberration.
B: Diffraction of light.
C: Refractive errors.
D: Polarization of light.
E: Reflected dispersion.
Answer: A

Newton's first functional reflecting telescope was:
A: About eight inches long and produced a larger, clearer image than refracting telescopes.
B: Ten inches long and had a better magnification than other telescopes.
C: Made entirely of prisms.
D: Made of a glass lens.
E: The first telescope ever built.
Answer: A

How did Newton view light in his theories?
A: As a wave that reflects and refracts.
B: As particles or corpuscles that refract by accelerating into a denser medium.
C: As a continuous stream without discrete particles.
D: As solely a wavelike phenomenon.
E: As rays bending uniformly in all media.
Answer: B

In Newton's work on light, which of the following best describes the theory of 'fits'?
A: The predisposition of light waves to interfere with each other.
B: The tendency of light particles to show wave-like characteristics.
C: The tendency of corpuscles to be reflected or transmitted.
D: The inclination of light to bend inwards.
E: The sporadic bursts of light energy.
Answer: C

In his study of optics, what did Newton name the multicoloured image produced by a prism?
A: Dispersion.
B: Refractogram.
C: Spectrum.
D: Chromatic sequence.
E: Rainbow.
Answer: C

Newton's idea of light as composed of particles or corpuscles was later replaced in favor of:
A: A dual nature of light as both particles and waves.
B: A solely wavelike explanation of light.
C: The theory of 'fits' for all electromagnetic radiation.
D: A combination of reflection and refraction theories.
E: The idea of light as energy quanta without a defined nature.
Answer: B

Newton's feud with Robert Hooke resulted in:
A: Newton's withdrawal from public debate.
B: Newton's increased contributions to the Royal Society.
C: A shared Nobel Prize in Physics.
D: The merger of their respective theories.
E: A joint publication on optics.
Answer: A

Newton's discovery that color is a result of objects interacting with already-colored light rather than generating the color themselves is termed as:
A: Newton's color wheel.
B: Newton's color interaction.
C: Newton's color principle.
D: Newton's theory of color.
E: Newton's refraction principle.
Answer: D
@
Subject:
Kapteyn's Star is a class M1 red subdwarf about 12.83 light-years from Earth in the southern constellation Pictor; it is the closest halo star to the Solar System. With an apparent magnitude of nearly 9 it is visible through binoculars or a telescope.[9]

Its diameter is 30% of the Sun's, but its luminosity just 1.2% that of the Sun. It may have once been part of the globular cluster Omega Centauri, itself a likely dwarf galaxy swallowed up by the Milky Way in the distant past. The discovery of two planets—Kapteyn b and Kapteyn c—was announced in 2014,[10] but had a mixed history of rejections and confirmations, until a 2021 study refuted both planets. The "planets" are in fact artifacts of the star's rotation and activity.[7]

Based upon parallax measurements, Kapteyn's Star is 12.83 light-years (3.93 parsecs) from the Earth.[1] It came within 7.0 ly (2.1 pc) of the Sun about 10,900 years ago and has been moving away since that time.[18] Kapteyn's Star is distinctive in a number of regards: it has a high radial velocity,[14] orbits the Milky Way retrograde,[17] and is the nearest-known halo star to the Sun.[19] It is a member of a moving group of stars that share a common trajectory through space, named the Kapteyn moving group.[20] Based upon their element abundances, these stars may once have been members of Omega Centauri, a globular cluster that is thought to be the remnant of a dwarf galaxy that merged with the Milky Way. During this process, the stars in the group, including Kapteyn's Star, may have been stripped away as tidal debris.[17][21][22]

Kapteyn's Star is between one quarter and one third the size and mass of the Sun and has a much cooler effective temperature at about 3500 K, with some disagreement in the exact measurements between different observers.[17] The stellar classification is sdM1,[3] which indicates that it is a subdwarf with a luminosity lower than that of a main-sequence star at the same spectral type of M1. The abundance of elements other than hydrogen and helium, what astronomers term the metallicity, is about 14% of the abundance in the Sun.[23][24] It is a variable star of the BY Draconis type with the identifier VZ Pictoris. This means that the luminosity of the star changes because of magnetic activity in the chromosphere coupled with rotation moving the resulting star spots into and out of the line of sight with respect to the Earth.[6]
$
10
Which of the following best describes Kapteyn's Star's size in comparison to the Sun?
A: It is three times larger than the Sun.
B: It is half the size of the Sun.
C: It is about 30% the diameter of the Sun.
D: It has a diameter equal to that of the Sun.
E: It is twice as large as the Sun.
Answer: C

Which statement best describes Kapteyn's Star's connection with Omega Centauri?
A: Kapteyn's Star was formed within Omega Centauri.
B: Kapteyn's Star and Omega Centauri are both red giants.
C: Kapteyn's Star may have once been part of the globular cluster Omega Centauri.
D: Kapteyn's Star orbits Omega Centauri.
E: Kapteyn's Star and Omega Centauri are both located in the Pictor constellation.
Answer: C

How does the luminosity of Kapteyn's Star compare to that of the Sun?
A: It has the same luminosity as the Sun.
B: It is 30% as luminous as the Sun.
C: It is 50% as luminous as the Sun.
D: It is 1.2% as luminous as the Sun.
E: It is twice as luminous as the Sun.
Answer: D

The discovery of two planets around Kapteyn's Star was:
A: Confirmed in 2014 and has remained unchallenged.
B: Announced in 2014 and later refuted in 2021.
C: Confirmed by the 2021 study.
D: Never reported.
E: Announced in 2014 and confirmed in 2021.
Answer: B

What is distinctive about the orbit of Kapteyn's Star in the Milky Way?
A: It orbits the Milky Way in a prograde direction.
B: It orbits the Milky Way at an unusually high speed.
C: It orbits the Milky Way retrograde.
D: Its orbit is stationary relative to the Milky Way.
E: It orbits in a perpendicular fashion to the plane of the Milky Way.
Answer: C

Why is Kapteyn's Star's luminosity variable?
A: Due to the presence of two large exoplanets.
B: Because of its distance from Earth.
C: Because of magnetic activity in the chromosphere coupled with rotation.
D: Because it is part of the Kapteyn moving group.
E: Due to its high radial velocity.
Answer: C

What is the estimated effective temperature of Kapteyn's Star?
A: 1500 K
B: 3500 K
C: 5000 K
D: 7000 K
E: 10,000 K
Answer: B

Kapteyn's Star is a member of which of the following groups?
A: The Omega Centauri group
B: The Proxima Centauri group
C: The Kapteyn moving group
D: The Solar system group
E: The Pictor constellation group
Answer: C

How did Kapteyn's Star come into proximity with the Sun about 10,900 years ago?
A: It was stationary and the Sun moved closer.
B: They both moved towards a common point.
C: Kapteyn's Star moved closer to the Sun and has been moving away since.
D: The Milky Way's gravitational pull attracted both stars closer.
E: It was due to a superflare from the Sun.
Answer: C

Which of the following statements regarding Kapteyn's Star's classification is correct?
A: It is a main-sequence star with a classification of M1.
B: Its classification is sdM1, indicating it is a giant star.
C: Its classification is sdM1, indicating lower luminosity than a main-sequence star of the same spectral type.
D: It is a supernova remnant with a classification of sdM1.
E: It is a binary star system with a partner star classified as M2.
Answer: C
@
Subject:
The SI base units are the standard units of measurement defined by the International System of Units (SI) for the seven base quantities of what is now known as the International System of Quantities: they are notably a basic set from which all other SI units can be derived. The units and their physical quantities are the second for time, the metre (sometimes spelled meter) for length or distance, the kilogram for mass, the ampere for electric current, the kelvin for thermodynamic temperature, the mole for amount of substance, and the candela for luminous intensity. The SI base units are a fundamental part of modern metrology, and thus part of the foundation of modern science and technology.

The SI base units form a set of mutually independent dimensions as required by dimensional analysis commonly employed in science and technology.[citation needed]

The names and symbols of SI base units are written in lowercase, except the symbols of those named after a person, which are written with an initial capital letter. For example, the metre has the symbol m, but the kelvin has symbol K, because it is named after Lord Kelvin and the ampere with symbol A is named after André-Marie Ampère.
$
10
How many SI base units are there in total?
A: 5
B: 6
C: 7
D: 8
E: 9
Answer: C

Which SI base unit is used for measuring luminous intensity?
A: Metre
B: Kelvin
C: Second
D: Candela
E: Mole
Answer: D

The SI base unit used for electric current is:
A: Joule
B: Ohm
C: Volt
D: Ampere
E: Watt
Answer: D

Which of the following is true regarding the symbols of SI base units named after a person?
A: They are written entirely in uppercase.
B: They are written entirely in lowercase.
C: The first letter is capitalized.
D: The last letter is capitalized.
E: They are italicized.
Answer: C

The mole measures which of the following?
A: Luminous intensity
B: Electric current
C: Thermodynamic temperature
D: Amount of substance
E: Distance
Answer: D

The SI base unit for thermodynamic temperature is named after which person?
A: André-Marie Ampère
B: James Watt
C: Albert Einstein
D: Sir Isaac Newton
E: Lord Kelvin
Answer: E

Which of the following is the SI base unit for mass?
A: Gram
B: Milligram
C: Kilogram
D: Ton
E: Pound
Answer: C

If one wanted to use dimensional analysis in science and technology, why are the SI base units crucial?
A: They provide derived units.
B: They form a set of mutually dependent dimensions.
C: They form a set of mutually independent dimensions.
D: They are interchangeable with non-SI units.
E: They are named after scientists.
Answer: C

Which SI base unit would be written with a symbol starting with a lowercase letter?
A: Kelvin
B: Ampere
C: Mole
D: Candela
E: None of the above
Answer: C

Why are the SI base units considered a fundamental part of modern metrology?
A: Because they are the only units recognized internationally.
B: Because they form the foundation of modern science and technology.
C: Because they have been in existence for over 1000 years.
D: Because they are named after famous scientists.
E: Because they are used only in academic settings.
Answer: B
@
Subject:
A planetary system is a set of gravitationally bound non-stellar objects in or out of orbit around a star or star system. Generally speaking, systems with one or more planets constitute a planetary system, although such systems may also consist of bodies such as dwarf planets, asteroids, natural satellites, meteoroids, comets, planetesimals[1][2] and circumstellar disks. The Sun together with the planetary system revolving around it, including Earth, forms the Solar System.[3][4] The term exoplanetary system is sometimes used in reference to other planetary systems.

As of 1 September 2023, there are 5,506 confirmed exoplanets in 4,065 planetary systems, with 878 systems having more than one planet.[5] Debris disks are also known to be common, though other objects are more difficult to observe.

Of particular interest to astrobiology is the habitable zone of planetary systems where planets could have surface liquid water, and thus the capacity to support Earth-like life.
$
5
Question: Which of the following is NOT necessarily a component of a planetary system?
A: Asteroids
B: Comets
C: Natural satellites
D: Galaxies
E: Dwarf planets
Answer: D

Question: As of 1 September 2023, how many confirmed exoplanets are there in planetary systems?
A: 4,065
B: 878
C: 5,506
D: 7,000
E: 3,000
Answer: C

Question: What term is sometimes used to refer specifically to planetary systems other than our own?
A: Solar System
B: Exoplanetary system
C: Earth-like system
D: Stellar system
E: Galaxy system
Answer: B

Question: Why is the habitable zone of a planetary system of special interest to astrobiology?
A: It contains planets made of gas.
B: It is where planets could have surface liquid water and the potential to support Earth-like life.
C: It is the zone where most asteroids are found.
D: It contains the most number of planets in a planetary system.
E: It is the area farthest from the central star.
Answer: B

Question: Which system consists of the Sun and the planetary system revolving around it, including Earth?
A: The Galaxy
B: The Milky Way
C: The Stellar System
D: The Exoplanetary System
E: The Solar System
Answer: E
@
Subject:
Cavitation is a phenomenon in which the static pressure of a liquid reduces to below the liquid's vapour pressure, leading to the formation of small vapor-filled cavities in the liquid. When subjected to higher pressure, these cavities, called "bubbles" or "voids", collapse and can generate shock waves that may damage machinery. These shock waves are strong when they are very close to the imploded bubble, but rapidly weaken as they propagate away from the implosion. Cavitation is a significant cause of wear in some engineering contexts. Collapsing voids that implode near to a metal surface cause cyclic stress through repeated implosion. This results in surface fatigue of the metal, causing a type of wear also called "cavitation". The most common examples of this kind of wear are to pump impellers, and bends where a sudden change in the direction of liquid occurs. Cavitation is usually divided into two classes of behavior: inertial (or transient) cavitation and non-inertial cavitation.

The process in which a void or bubble in a liquid rapidly collapses, producing a shock wave, is called inertial cavitation. Inertial cavitation occurs in nature in the strikes of mantis shrimp and pistol shrimp, as well as in the vascular tissues of plants. In manufactured objects, it can occur in control valves, pumps, propellers and impellers.[citation needed]

Non-inertial cavitation is the process in which a bubble in a fluid is forced to oscillate in size or shape due to some form of energy input, such as an acoustic field. Such cavitation is often employed in ultrasonic cleaning baths and can also be observed in pumps, propellers, etc.

Since the shock waves formed by collapse of the voids are strong enough to cause significant damage to parts, cavitation is typically an undesirable phenomenon in machinery (although desirable if intentionally used, for example, to sterilize contaminated surgical instruments, break down pollutants in water purification systems, emulsify tissue for cataract surgery or kidney stone lithotripsy, or homogenize fluids). It is very often specifically prevented in the design of machines such as turbines or propellers, and eliminating cavitation is a major field in the study of fluid dynamics. However, it is sometimes useful and does not cause damage when the bubbles collapse away from machinery, such as in supercavitation.
$
6
Question: What causes cavitation to occur?
A: The static pressure of a liquid increasing above the liquid's vapour pressure.
B: The static pressure of a liquid reducing to below the liquid's vapour pressure.
C: The liquid reaching its boiling point.
D: The sudden change in temperature of the liquid.
E: The mixing of two incompatible liquids.
Answer: B

Question: Why is cavitation considered a significant cause of wear in certain engineering contexts?
A: It causes an increase in the liquid's viscosity.
B: It results in the boiling of the liquid.
C: The implosion of the bubbles or voids can generate shock waves that damage machinery.
D: It causes a sudden change in the liquid's density.
E: The expansion of the bubbles exerts pressure on the machinery.
Answer: C

Question: Which of the following is NOT a manufactured object where inertial cavitation can occur?
A: Pumps
B: Turbines
C: Control valves
D: Heat exchangers
E: Propellers
Answer: D

Question: In which cavitation process does a bubble oscillate in size or shape due to an energy input like an acoustic field?
A: Inertial cavitation
B: Oscillatory cavitation
C: Non-inertial cavitation
D: Reactive cavitation
E: Supercavitation
Answer: C

Question: Supercavitation is a situation where cavitation can be:
A: Always undesirable and damaging to machinery.
B: Always useful and does not cause any damage.
C: Undesirable if it occurs close to machinery surfaces.
D: Useful and does not cause damage when bubbles collapse away from machinery.
E: Always prevented in the design of machines like turbines or propellers.
Answer: D

Question: Which natural phenomenon showcases the occurrence of inertial cavitation?
A: The migration of birds
B: The strikes of mantis shrimp
C: The growth of plant roots
D: The photosynthesis process in plants
E: The reflection of sound in water
Answer: B
@
Subject:
Giordano Bruno (/dʒɔːrˈdɑːnoʊ ˈbruːnoʊ/; Italian: [dʒorˈdaːno ˈbruːno]; Latin: Iordanus Brunus Nolanus; born Filippo Bruno, January or February 1548 – 17 February 1600) was an Italian philosopher, poet, cosmological theorist and esotericist.[4] He is known for his cosmological theories, which conceptually extended to include the then novel Copernican model. He proposed that the stars were distant suns surrounded by their own planets (exoplanets), and he raised the possibility that these planets might foster life of their own, a cosmological position known as cosmic pluralism. He also insisted that the universe is infinite and could have no center.

While Bruno began as a Dominican friar, he embraced Calvinism during his time in Geneva.[5] He was later tried for heresy by the Roman Inquisition on charges of denial of several core Catholic doctrines, including eternal damnation, the Trinity, the divinity of Christ, the virginity of Mary, and transubstantiation. Bruno's pantheism was not taken lightly by the church,[6][better source needed] nor was his teaching of metempsychosis regarding the reincarnation of the soul. The Inquisition found him guilty, and he was burned alive at the stake in Rome's Campo de' Fiori in 1600. After his death, he gained considerable fame, being particularly celebrated by 19th- and early 20th-century commentators who regarded him as a martyr for science, although most historians agree that his heresy trial was not a response to his cosmological views but rather a response to his religious and afterlife views.[7][8][9][10][11] Some historians contend that the main reason for Bruno's death was indeed his cosmological views.[12][13][14] Bruno's case is still considered a landmark in the history of free thought and the emerging sciences.[15][16]

In addition to cosmology, Bruno also wrote extensively on the art of memory, a loosely organized group of mnemonic techniques and principles. Historian Frances Yates argues that Bruno was deeply influenced by the presocratic Empedocles, Neoplatonism, Renaissance Hermeticism, and Book of Genesis-like legends surrounding the Hellenistic conception of Hermes Trismegistus.[17] Other studies of Bruno have focused on his qualitative approach to mathematics and his application of the spatial concepts of geometry to language.[18]
$
6
Question: What did Giordano Bruno propose about the stars?
A: They are immovable and fixed points in the night sky.
B: They are distant suns surrounded by their own planets (exoplanets).
C: They are mere illusions and not real physical entities.
D: They are made up of the same material as Earth.
E: They revolve around Earth in a geocentric model.
Answer: B

Question: How did Giordano Bruno view the universe in terms of its size and center?
A: The universe is finite and has Earth as its center.
B: The universe is infinite and has Earth as its center.
C: The universe is finite and has no center.
D: The universe is infinite and could have no center.
E: The universe's size is unknown, but it definitely has a center.
Answer: D

Question: For which religious views was Giordano Bruno tried by the Roman Inquisition?
A: His belief in the heliocentric model of the universe.
B: His denial of core Catholic doctrines, including the Trinity and the divinity of Christ.
C: His unwavering support for the Church's teachings.
D: His belief in the geocentric model of the universe.
E: His contributions to the art of memory.
Answer: B

Question: What was Giordano Bruno's ultimate fate?
A: He was canonized as a saint.
B: He was exiled from Rome and continued his teachings abroad.
C: He was imprisoned for life.
D: He was pardoned by the Church.
E: He was found guilty by the Inquisition and burned alive at the stake.
Answer: E

Question: On what subjects, other than cosmology, did Giordano Bruno write?
A: Only on poetry and fiction.
B: On the art of memory and mnemonic techniques.
C: Only on the Copernican theory and its implications.
D: On early Christian theology and Biblical interpretations.
E: Exclusively on the structure and formation of the Milky Way galaxy.
Answer: B

Question: Which ancient philosophy is Giordano Bruno said to have been influenced by, according to historian Frances Yates?
A: Stoicism
B: Cynicism
C: Aristotelianism
D: Renaissance Hermeticism
E: Epicureanism
Answer: D
@
Subject:
The Navier–Stokes equations (/nævˈjeɪ stoʊks/ nav-YAY STOHKS) are partial differential equations which describe the motion of viscous fluid substances, named after French engineer and physicist Claude-Louis Navier and Irish physicist and mathematician George Gabriel Stokes. They were developed over several decades of progressively building the theories, from 1822 (Navier) to 1842-1850 (Stokes).

The Navier–Stokes equations mathematically express momentum balance and conservation of mass for Newtonian fluids. They are sometimes accompanied by an equation of state relating pressure, temperature and density.[1] They arise from applying Isaac Newton's second law to fluid motion, together with the assumption that the stress in the fluid is the sum of a diffusing viscous term (proportional to the gradient of velocity) and a pressure term—hence describing viscous flow. The difference between them and the closely related Euler equations is that Navier–Stokes equations take viscosity into account while the Euler equations model only inviscid flow. As a result, the Navier–Stokes are a parabolic equation and therefore have better analytic properties, at the expense of having less mathematical structure (e.g. they are never completely integrable).

The Navier–Stokes equations are useful because they describe the physics of many phenomena of scientific and engineering interest. They may be used to model the weather, ocean currents, water flow in a pipe and air flow around a wing. The Navier–Stokes equations, in their full and simplified forms, help with the design of aircraft and cars, the study of blood flow, the design of power stations, the analysis of pollution, and many other things. Coupled with Maxwell's equations, they can be used to model and study magnetohydrodynamics.

The Navier–Stokes equations are also of great interest in a purely mathematical sense. Despite their wide range of practical uses, it has not yet been proven whether smooth solutions always exist in three dimensions—i.e., whether they are infinitely differentiable (or even just bounded) at all points in the domain. This is called the Navier–Stokes existence and smoothness problem. The Clay Mathematics Institute has called this one of the seven most important open problems in mathematics and has offered a US$1 million prize for a solution or a counterexample.[2][3]
$
6
Question: Who are the Navier-Stokes equations named after?
A: French engineer Albert Navier and Irish mathematician Thomas Stokes.
B: French physicist Pierre-Simon Laplace and Irish physicist Richard Stokes.
C: French engineer and physicist Claude-Louis Navier and Irish physicist and mathematician George Gabriel Stokes.
D: French mathematician Henri Poincaré and Irish engineer Lawrence Stokes.
E: French physicist Jean-Baptiste Biot and Irish engineer Michael Stokes.
Answer: C

Question: Which law of Newton's is applied to derive the Navier–Stokes equations?
A: Newton's first law of motion.
B: Newton's second law of motion.
C: Newton's third law of motion.
D: Newton's law of universal gravitation.
E: Newton's law of cooling.
Answer: B

Question: What is the fundamental difference between the Euler equations and the Navier-Stokes equations?
A: The Euler equations consider magnetic fields, while the Navier-Stokes equations do not.
B: The Euler equations consider temperature effects, while the Navier-Stokes equations only consider viscosity.
C: The Euler equations model only inviscid flow, while the Navier-Stokes equations take viscosity into account.
D: The Euler equations model only viscous flow, while the Navier-Stokes equations take inviscid flow into account.
E: The Euler equations are algebraic, while the Navier-Stokes equations are differential.
Answer: C

Question: What kind of phenomena do the Navier-Stokes equations help describe?
A: The motion of stars and galaxies.
B: The spread of forest fires.
C: The motion of viscous fluid substances like weather patterns, ocean currents, and blood flow.
D: The growth of plant roots.
E: The behavior of electromagnetic waves.
Answer: C

Question: What is the Navier-Stokes existence and smoothness problem?
A: A problem related to the ability to find exact solutions to the Navier-Stokes equations in two dimensions.
B: A problem associated with the conservation of energy when using the Navier-Stokes equations.
C: A challenge in proving whether smooth solutions always exist in three dimensions for the Navier-Stokes equations.
D: A difficulty in determining the viscosity of a fluid using the Navier-Stokes equations.
E: A problem related to the ability to apply the Navier-Stokes equations to non-Newtonian fluids.
Answer: C

Question: What prize has the Clay Mathematics Institute offered for a solution or counterexample to the Navier-Stokes existence and smoothness problem?
A: A Nobel Prize in Mathematics.
B: A scholarship to study fluid dynamics.
C: A Fields Medal.
D: US$1 million prize.
E: A research grant to further study the Navier-Stokes equations.
Answer: D
@
Subject:
Chaos theory concerns deterministic systems whose behavior can, in principle, be predicted. Chaotic systems are predictable for a while and then 'appear' to become random. The amount of time for which the behavior of a chaotic system can be effectively predicted depends on three things: how much uncertainty can be tolerated in the forecast, how accurately its current state can be measured, and a time scale depending on the dynamics of the system, called the Lyapunov time. Some examples of Lyapunov times are: chaotic electrical circuits, about 1 millisecond; weather systems, a few days (unproven); the inner solar system, 4 to 5 million years.[18] In chaotic systems, the uncertainty in a forecast increases exponentially with elapsed time. Hence, mathematically, doubling the forecast time more than squares the proportional uncertainty in the forecast. This means, in practice, a meaningful prediction cannot be made over an interval of more than two or three times the Lyapunov time. When meaningful predictions cannot be made, the system appears random.[19]

Chaos theory is a method of qualitative and quantitative analysis to investigate the behavior of dynamic systems that cannot be explained and predicted by single data relationships, but must be explained and predicted by whole, continuous data relationships.
$
6
Question: What does chaos theory concern itself with?
A: Systems that are always random and unpredictable.
B: Systems that are always stable and completely predictable.
C: Deterministic systems whose behavior can be predicted for a certain time before appearing to become random.
D: Systems that exhibit no order and have no time-based predictability.
E: Systems that rely solely on external factors for their state of order.
Answer: C

Question: How does uncertainty in a forecast of a chaotic system change with elapsed time?
A: It remains constant with time.
B: It decreases linearly with elapsed time.
C: It increases linearly with elapsed time.
D: It decreases exponentially with elapsed time.
E: It increases exponentially with elapsed time.
Answer: E

Question: Which of the following best describes the Lyapunov time in the context of chaos theory?
A: The time it takes for a system to become completely ordered.
B: The time over which the behavior of a chaotic system can be effectively predicted.
C: The duration in which the system remains random.
D: The period during which a system remains stable and non-chaotic.
E: The time taken for external factors to influence the system's behavior.
Answer: B

Question: What is the Lyapunov time for chaotic electrical circuits?
A: A few days.
B: 4 to 5 million years.
C: 1 second.
D: 1 millisecond.
E: 1 year.
Answer: D

Question: According to chaos theory, when can a system appear random?
A: When its behavior can be predicted using single data relationships.
B: When its uncertainty decreases over time.
C: When it remains stable for extended periods.
D: When meaningful predictions cannot be made over an interval of more than two or three times the Lyapunov time.
E: When its behavior can be perfectly forecasted indefinitely.
Answer: D

Question: Which of the following best describes chaos theory's approach to investigating dynamic systems?
A: A focus on individual data points without considering the entire system.
B: A preference for qualitative analysis without quantitative verification.
C: Analysis based solely on the Lyapunov time without considering other factors.
D: Qualitative and quantitative analysis to understand behavior that cannot be explained and predicted by single data relationships.
E: An approach that ignores both small-scale and large-scale processes in a system.
Answer: D
@
Subject:
In astronomy, the interstellar medium (ISM) is the matter and radiation that exist in the space between the star systems in a galaxy. This matter includes gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills interstellar space and blends smoothly into the surrounding intergalactic space. The energy that occupies the same volume, in the form of electromagnetic radiation, is the interstellar radiation field. Although the density of atoms in the ISM is usually far below that in the best laboratory vacuums, the mean free path between collisions is short compared to typical interstellar lengths, so on these scales the ISM behaves as a gas (more precisely, as a plasma: it is everywhere at least slightly ionized), responding to pressure forces, and not as a collection of non-interacting particles.

The interstellar medium is composed of multiple phases distinguished by whether matter is ionic, atomic, or molecular, and the temperature and density of the matter. The interstellar medium is composed primarily of hydrogen, followed by helium with trace amounts of carbon, oxygen, and nitrogen.[1] The thermal pressures of these phases are in rough equilibrium with one another. Magnetic fields and turbulent motions also provide pressure in the ISM, and are typically more important, dynamically, than the thermal pressure. In the interstellar medium, matter is primarily in molecular form and reaches number densities of 1012 molecules per m3 (1 trillion molecules per m3). In hot, diffuse regions, gas is highly ionized, and the density may be as low as 100 ions per m3. Compare this with a number density of roughly 1025 molecules per m3 for air at sea level, and 1016 molecules per m3 (10 quadrillion molecules per m3) for a laboratory high-vacuum chamber. By mass, 99% of the ISM is gas in any form, and 1% is dust.[2] Of the gas in the ISM, by number 91% of atoms are hydrogen and 8.9% are helium, with 0.1% being atoms of elements heavier than hydrogen or helium,[3] known as "metals" in astronomical parlance. By mass this amounts to 70% hydrogen, 28% helium, and 1.5% heavier elements. The hydrogen and helium are primarily a result of primordial nucleosynthesis, while the heavier elements in the ISM are mostly a result of enrichment (due to stellar nucleosynthesis) in the process of stellar evolution.

The ISM plays a crucial role in astrophysics precisely because of its intermediate role between stellar and galactic scales. Stars form within the densest regions of the ISM, which ultimately contributes to molecular clouds and replenishes the ISM with matter and energy through planetary nebulae, stellar winds, and supernovae. This interplay between stars and the ISM helps determine the rate at which a galaxy depletes its gaseous content, and therefore its lifespan of active star formation.

The Lyman-alpha line, typically denoted by Ly-α, is a spectral line of hydrogen (or, more generally, of any one-electron atom) in the Lyman series. It is emitted when the atomic electron transitions from an n = 2 orbital to the ground state (n = 1), where n is the principal quantum number. In hydrogen, its wavelength of 1215.67 angstroms (121.567 nm or 1.21567×10−7 m), corresponding to a frequency of about 2.47×1015 Hz, places Lyman-alpha in the ultraviolet (UV) part of the electromagnetic spectrum. More specifically, Ly-α lies in vacuum UV (VUV), characterized by a strong absorption in the air.
$
7
Question: What constitutes the interstellar medium (ISM)?
A: The matter and radiation found between planets in our solar system.
B: The matter and radiation found in the core of stars.
C: The matter and radiation present between star systems in a galaxy.
D: The radiation emitted solely from black holes.
E: The matter found in the central region of galaxies.
Answer: C

Question: In the interstellar medium, what is the primary element by number of atoms?
A: Helium.
B: Carbon.
C: Nitrogen.
D: Hydrogen.
E: Oxygen.
Answer: D

Question: Which statement accurately describes the role of the ISM in astrophysics?
A: The ISM acts as a barrier preventing stellar formations.
B: The ISM plays a role in determining the number of black holes in a galaxy.
C: Stars form in the least dense regions of the ISM.
D: The ISM, with its interplay between stars, determines a galaxy's lifespan of active star formation.
E: The ISM has no significant impact on the processes within a galaxy.
Answer: D

Question: In comparison to air at sea level, how does the density of matter in hot, diffuse regions of the ISM compare?
A: It is about the same.
B: It is roughly 10 times higher.
C: It is approximately 100 times lower.
D: It is vastly lower, with a number density of about 100 ions per m^3.
E: It is nearly infinite, making the ISM impenetrable.
Answer: D

Question: What is a primary distinction between the ISM and the intergalactic space?
A: The ISM contains only cosmic rays, while intergalactic space contains gases.
B: The ISM blends smoothly into the surrounding intergalactic space, with the former having matter and radiation between star systems.
C: Intergalactic space is filled with dust, while the ISM contains gases.
D: The ISM is found inside stars, while intergalactic space is outside.
E: Intergalactic space is a pure vacuum, while the ISM is a dense plasma.
Answer: B

Question: In the context of astronomical parlance, what are "metals"?
A: Elements that can be found in the Earth's crust.
B: Elements heavier than hydrogen or helium.
C: Only elements like iron, gold, and silver.
D: Elements that have a metallic sheen.
E: Elements that are gaseous at room temperature.
Answer: B

Question: What transition results in the emission of the Lyman-alpha line?
A: Transition of an atomic electron from n = 1 to n = 2 in a one-electron atom.
B: Transition of a molecular electron from n = 3 to n = 2 in a two-electron molecule.
C: Transition of an atomic electron from n = 3 to n = 1 in a one-electron atom.
D: Transition of an atomic electron from n = 2 to n = 1 in a one-electron atom.
E: Transition of a molecular electron from n = 2 to n = 1 in a two-electron molecule.
Answer: D
@
Subject:
In Einstein's theory of general relativity, the Schwarzschild metric (also known as the Schwarzschild solution) is an exact solution to the Einstein field equations that describes the gravitational field outside a spherical mass, on the assumption that the electric charge of the mass, angular momentum of the mass, and universal cosmological constant are all zero. The solution is a useful approximation for describing slowly rotating astronomical objects such as many stars and planets, including Earth and the Sun. It was found by Karl Schwarzschild in 1916.

According to Birkhoff's theorem, the Schwarzschild metric is the most general spherically symmetric vacuum solution of the Einstein field equations. A Schwarzschild black hole or static black hole is a black hole that has neither electric charge nor angular momentum. A Schwarzschild black hole is described by the Schwarzschild metric, and cannot be distinguished from any other Schwarzschild black hole except by its mass.

The Schwarzschild black hole is characterized by a surrounding spherical boundary, called the event horizon, which is situated at the Schwarzschild radius, often called the radius of a black hole. The boundary is not a physical surface, and a person who fell through the event horizon (before being torn apart by tidal forces), would not notice any physical surface at that position; it is a mathematical surface which is significant in determining the black hole's properties. Any non-rotating and non-charged mass that is smaller than its Schwarzschild radius forms a black hole. The solution of the Einstein field equations is valid for any mass M, so in principle (according to general relativity theory) a Schwarzschild black hole of any mass could exist if conditions became sufficiently favorable to allow for its formation.

In the vicinity of a Schwarzschild black hole, space curves so much that even light rays are deflected, and very nearby light can be deflected so much that it travels several times around the black hole.[1][2][3]
$
6
Question: What does the Schwarzschild metric describe?
A: The gravitational field around a rotating mass.
B: The gravitational field inside a black hole.
C: The gravitational field outside a spherically symmetric mass with no charge or angular momentum.
D: The gravitational field of a flat universe.
E: The electromagnetic field outside a charged mass.
Answer: C

Question: What is the significance of the event horizon of a Schwarzschild black hole?
A: It is a physical boundary from which nothing can escape.
B: It is a mathematical surface that marks the radius of the black hole.
C: It is the point at which time stops.
D: It is the boundary where the black hole's mass is concentrated.
E: It represents the maximum size the black hole can grow to.
Answer: B

Question: According to Birkhoff's theorem, what is special about the Schwarzschild metric?
A: It represents the least massive black hole.
B: It is the only solution to Einstein's field equations.
C: It is the most general spherically symmetric vacuum solution of the Einstein field equations.
D: It is the only solution for rotating black holes.
E: It is the most specific solution for charged black holes.
Answer: C

Question: What happens to light rays in the vicinity of a Schwarzschild black hole?
A: They speed up due to the gravitational pull.
B: They get absorbed instantly by the black hole.
C: They travel in a straight line unaffected by the black hole's gravity.
D: They are deflected, and very nearby light can even travel multiple times around the black hole.
E: They change color due to gravitational lensing.
Answer: D

Question: Under what conditions can a Schwarzschild black hole form?
A: When the mass has an electric charge and its size is smaller than the Schwarzschild radius.
B: When the mass has angular momentum and its size is smaller than the Schwarzschild radius.
C: When any rotating and charged mass becomes smaller than its Schwarzschild radius.
D: When any non-rotating and non-charged mass is smaller than its Schwarzschild radius.
E: When the mass has neither charge nor angular momentum but is larger than the Schwarzschild radius.
Answer: D

Question: If someone fell into a Schwarzschild black hole, at what point would they notice a physical boundary?
A: Just before reaching the event horizon.
B: Right after crossing the event horizon.
C: They would never notice a physical boundary at the event horizon.
D: At the center of the black hole.
E: At the Schwarzschild radius.
Answer: C
@
Subject:
A memristor (/ˈmɛmrɪstər/; a portmanteau of memory resistor) is a non-linear two-terminal electrical component relating electric charge and magnetic flux linkage. It was described and named in 1971 by Leon Chua, completing a theoretical quartet of fundamental electrical components which also comprises the resistor, capacitor and inductor.[1]

Atomristor is defined as the electrical devices showing memristive behavior in atomically thin nanomaterials or atomic sheets. In 2018, Ge and Wu et al.[72] in the Akinwande group at the University of Texas, first reported a universal memristive effect in single-layer TMD (MX2, M = Mo, W; and X = S, Se) atomic sheets based on vertical metal-insulator-metal (MIM) device structure. The work was later extended to monolayer hexagonal boron nitride, which is the thinnest memory material of around 0.33 nm.[73] These atomristors offer forming-free switching and both unipolar and bipolar operation. The switching behavior is found in single-crystalline and poly-crystalline films, with various conducting electrodes (gold, silver and graphene). Atomically thin TMD sheets are prepared via CVD/MOCVD, enabling low-cost fabrication. Afterwards, taking advantage of the low "on" resistance and large on/off ratio, a high-performance zero-power RF switch is proved based on MoS2 or h-BN atomristors, indicating a new application of memristors for 5G, 6G and THz communication and connectivity systems.[74][75] In 2020, atomistic understanding of the conductive virtual point mechanism was elucidated in an article in nature nanotechnology.[76]
$
7
Question: Who first described and named the memristor?
A: Nikola Tesla.
B: Leon Chua.
C: Robert Noyce.
D: Jack Kilby.
E: Michael Faraday.
Answer: B

Question: What completes the theoretical quartet of fundamental electrical components?
A: Transistor, diode, capacitor, and inductor.
B: Resistor, diode, capacitor, and transformer.
C: Resistor, capacitor, inductor, and memristor.
D: Memristor, diode, capacitor, and transformer.
E: Resistor, diode, inductor, and memristor.
Answer: C

Question: In what year was a universal memristive effect in single-layer TMD atomic sheets first reported?
A: 2015.
B: 2020.
C: 2018.
D: 2016.
E: 2019.
Answer: C

Question: What was one of the applications indicated for MoS2 or h-BN atomristors?
A: Solar power cells.
B: Liquid crystal displays.
C: 5G, 6G and THz communication and connectivity systems.
D: Quantum computing.
E: Fuel cells for electric vehicles.
Answer: C

Question: How thin is the memory material of monolayer hexagonal boron nitride?
A: 0.13 nm.
B: 0.73 nm.
C: 1.33 nm.
D: 0.33 nm.
E: 2.33 nm.
Answer: D

Question: Which method is used to prepare atomically thin TMD sheets, enabling low-cost fabrication?
A: Physical vapor deposition (PVD).
B: Mechanical exfoliation.
C: Molecular beam epitaxy (MBE).
D: CVD/MOCVD.
E: Sputter deposition.
Answer: D

Question: What is the relationship between electric charge and magnetic flux linkage in a memristor?
A: They are inversely proportional.
B: There is no relationship between them.
C: They are linearly proportional.
D: The memristor relates electric charge and magnetic flux linkage in a non-linear manner.
E: The relationship is only observed in high-frequency ranges.
Answer: D
@
Subject:
In physics, electromagnetism is an interaction that occurs between particles with electric charge via electromagnetic fields. The electromagnetic force is one of the four fundamental forces of nature. It is the dominant force in the interactions of atoms and molecules. Electromagnetism can be thought of as a combination of electrostatics and magnetism, two distinct but closely intertwined phenomena. Electromagnetic forces occur between any two charged particles, causing an attraction between particles with opposite charges and repulsion between particles with the same charge, while magnetism is an interaction that occurs exclusively between charged particles in relative motion. These two effects combine to create electromagnetic fields in the vicinity of charged particles, which can accelerate other charged particles via the Lorentz force. At high energy, the weak force and electromagnetic force are unified as a single electroweak force.

The electromagnetic force is responsible for many of the chemical and physical phenomena observed in daily life. The electrostatic attraction between atomic nuclei and their electrons holds atoms together. Electric forces also allow different atoms to combine into molecules, including the macromolecules such as proteins that form the basis of life. Meanwhile, magnetic interactions between the spin and angular momentum magnetic moments of electrons also play a role in chemical reactivity; such relationships are studied in spin chemistry. Electromagnetism also plays a crucial role in modern technology: electrical energy production, transformation and distribution; light, heat, and sound production and detection; fiber optic and wireless communication; sensors; computation; electrolysis; electroplating; and mechanical motors and actuators.

Electromagnetism has been studied since ancient times. Many ancient civilizations, including the Greeks and the Mayans created wide-ranging theories to explain lightning, static electricity, and the attraction between magnetized pieces of iron ore. However, it wasn't until the late 18th century that scientists began to develop a mathematical basis for understanding the nature of electromagnetic interactions. In the 18th and 19th centuries, prominent scientists and mathematicians such as Coulomb, Gauss and Faraday developed namesake laws which helped to explain the formation and interaction of electromagnetic fields. This process culminated in the 1860s with the discovery of Maxwell's equations, a set of four partial differential equations which provide a complete description of classical electromagnetic fields. Besides providing a sound mathematical basis for the relationships between electricity and magnetism that scientists had been exploring for centuries, Maxwell's equations also predicted the existence of self-sustaining electromagnetic waves. Maxwell postulated that such waves make up visible light, which was later shown to be true. Indeed, gamma-rays, x-rays, ultraviolet, visible, infrared radiation, microwaves and radio waves were all determined to be electromagnetic radiation differing only in their range of frequencies.

In the modern era, scientists have continued to refine the theorem of electromagnetism to take into account the effects of modern physics, including quantum mechanics and relativity. Indeed, the theoretical implications of electromagnetism, particularly the establishment of the speed of light based on properties of the "medium" of propagation (permeability and permittivity), helped inspire Einstein's theory of special relativity in 1905. Meanwhile, the field of quantum electrodynamics (QED) has modified Maxwell's equations to be consistent with the quantized nature of matter. In QED, the electromagnetic field is expressed in terms of discrete particles known as photons, which are also the physical quanta of light. Today, there exist many problems in electromagnetism that remain unsolved, such as the existence of magnetic monopoles and the mechanism by which some organisms can sense electric and magnetic fields.
$
10
Question: Which of the following forces is responsible for holding atoms together?
A: Gravitational force
B: Electromagnetic force
C: Nuclear force
D: Weak force
E: Frictional force
Answer: B

Question: Electromagnetism is a combination of which two phenomena?
A: Gravity and Magnetism
B: Gravity and Electrostatics
C: Electrostatics and Gravitation
D: Electrostatics and Magnetism
E: Magnetism and Friction
Answer: D

Question: Which of the following is NOT a form of electromagnetic radiation?
A: Sound waves
B: X-rays
C: Microwaves
D: Infrared radiation
E: Gamma-rays
Answer: A

Question: Which theory helped inspire Einstein's theory of special relativity based on the properties of the "medium" of propagation?
A: Electromagnetism
B: Gravitation
C: Thermodynamics
D: Quantum mechanics
E: Nuclear physics
Answer: A

Question: What is the term used to describe the discrete particles in quantum electrodynamics that represent the electromagnetic field?
A: Electrons
B: Protons
C: Quarks
D: Neutrons
E: Photons
Answer: E

Question: The electromagnetic force causes an attraction between:
A: Particles with the same charge
B: Particles with opposite charges
C: Neutral particles
D: Particles in relative motion
E: Stationary particles
Answer: B

Question: What role does electromagnetism play in chemical reactivity involving the spin and angular momentum magnetic moments of electrons?
A: Thermodynamics
B: Electrolysis
C: Spin chemistry
D: Electroplating
E: Electrostatics
Answer: C

Question: Which ancient civilizations are known to have theories explaining lightning, static electricity, and magnetism?
A: Romans and Persians
B: Greeks and Mayans
C: Egyptians and Mesopotamians
D: Indus and Chinese
E: Norse and Celts
Answer: B

Question: In which century did scientists begin to develop a mathematical basis for understanding electromagnetic interactions?
A: 16th century
B: 17th century
C: 18th century
D: 19th century
E: 20th century
Answer: C

Question: Which field of study modifies Maxwell's equations to align with the quantized nature of matter?
A: Classical Mechanics
B: Quantum Gravity
C: Quantum Electrodynamics (QED)
D: Thermodynamics
E: Nuclear Physics
Answer: C
@
Subject:
In modern physics, the double-slit experiment demonstrates that light and matter can satisfy the seemingly-incongruous classical definitions for both waves and particles, which is considered evidence for the fundamentally probabilistic nature of quantum mechanics. This type of experiment was first performed by Thomas Young in 1801, as a demonstration of the wave behavior of visible light.[1] At that time it was thought that light consisted of either waves or particles. With the beginning of modern physics, about a hundred years later, it was realized that light could in fact show both wave and particle characteristics. In 1927, Davisson and Germer and, independently George Paget Thomson and Alexander Reid demonstrated that electrons show the same behavior, which was later extended to atoms and molecules.[2][3][4] Thomas Young's experiment with light was part of classical physics long before the development of quantum mechanics and the concept of wave–particle duality. He believed it demonstrated that Christiaan Huygens' wave theory of light was correct, and his experiment is sometimes referred to as Young's experiment[5] or Young's slits.[6]

The experiment belongs to a general class of "double path" experiments, in which a wave is split into two separate waves (the wave is typically made of many photons and better referred to as a wave front, not to be confused with the wave properties of the individual photon) that later combine into a single wave. Changes in the path-lengths of both waves result in a phase shift, creating an interference pattern. Another version is the Mach–Zehnder interferometer, which splits the beam with a beam splitter.

In the basic version of this experiment, a coherent light source, such as a laser beam, illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate.[7][8] The wave nature of light causes the light waves passing through the two slits to interfere, producing bright and dark bands on the screen – a result that would not be expected if light consisted of classical particles.[7][9] However, the light is always found to be absorbed at the screen at discrete points, as individual particles (not waves); the interference pattern appears via the varying density of these particle hits on the screen.[10] Furthermore, versions of the experiment that include detectors at the slits find that each detected photon passes through one slit (as would a classical particle), and not through both slits (as would a wave).[11][12][13][14][15] However, such experiments demonstrate that particles do not form the interference pattern if one detects which slit they pass through. These results demonstrate the principle of wave–particle duality.[16][17]

Other atomic-scale entities, such as electrons, are found to exhibit the same behavior when fired towards a double slit.[8] Additionally, the detection of individual discrete impacts is observed to be inherently probabilistic, which is inexplicable using classical mechanics.[8]

The experiment can be done with entities much larger than electrons and photons, although it becomes more difficult as size increases. The largest entities for which the double-slit experiment has been performed were molecules that each comprised 2000 atoms (whose total mass was 25,000 atomic mass units).[18]

The double-slit experiment (and its variations) has become a classic for its clarity in expressing the central puzzles of quantum mechanics. Because it demonstrates the fundamental limitation of the ability of the observer to predict experimental results, Richard Feynman called it "a phenomenon which is impossible […] to explain in any classical way, and which has in it the heart of quantum mechanics. In reality, it contains the only mystery [of quantum mechanics]."[8]

If light consisted strictly of ordinary or classical particles, and these particles were fired in a straight line through a slit and allowed to strike a screen on the other side, we would expect to see a pattern corresponding to the size and shape of the slit. However, when this "single-slit experiment" is actually performed, the pattern on the screen is a diffraction pattern in which the light is spread out. The smaller the slit, the greater the angle of spread. The top portion of the image shows the central portion of the pattern formed when a red laser illuminates a slit and, if one looks carefully, two faint side bands. More bands can be seen with a more highly refined apparatus. Diffraction explains the pattern as being the result of the interference of light waves from the slit.

If one illuminates two parallel slits, the light from the two slits again interferes. Here the interference is a more pronounced pattern with a series of alternating light and dark bands. The width of the bands is a property of the frequency of the illuminating light.[19] (See the bottom photograph to the right.)

When Thomas Young (1773–1829) first demonstrated this phenomenon, it indicated that light consists of waves, as the distribution of brightness can be explained by the alternately additive and subtractive interference of wavefronts.[8] Young's experiment, performed in the early 1800s, played a crucial role in the understanding of the wave theory of light, vanquishing the corpuscular theory of light proposed by Isaac Newton, which had been the accepted model of light propagation in the 17th and 18th centuries.
$
10
Question: Who first conducted the double-slit experiment in 1801?
A: Richard Feynman
B: Isaac Newton
C: Thomas Young
D: George Paget Thomson
E: Christiaan Huygens
Answer: C

Question: The double-slit experiment illustrates the fundamental principle of which phenomenon in quantum mechanics?
A: Gravity waves
B: Particle isolation
C: Wave–particle duality
D: Quantum field theory
E: String theory
Answer: C

Question: If light consisted only of classical particles, what pattern would one expect on the screen in the single-slit experiment?
A: Interference pattern
B: Diffraction pattern
C: A pattern corresponding to the size and shape of the slit
D: Alternating light and dark bands
E: Completely dark pattern
Answer: C

Question: Which theory of light propagation was popular during the 17th and 18th centuries?
A: Quantum theory
B: Electromagnetic theory
C: Wave theory
D: Corpuscular theory
E: Dual theory
Answer: D

Question: Which scientist's work on the double-slit experiment provided evidence against the corpuscular theory of light proposed by Isaac Newton?
A: Richard Feynman
B: George Paget Thomson
C: Christiaan Huygens
D: Thomas Young
E: Davisson and Germer
Answer: D

Question: What is created on a screen when light waves passing through two slits interfere?
A: A single bright spot
B: A series of alternating light and dark bands
C: Randomly scattered light points
D: A diffraction pattern from each slit
E: Two separate bright bands
Answer: B

Question: According to the passage, the double-slit experiment has been performed on entities comprising up to how many atoms?
A: 200
B: 1000
C: 2000
D: 5000
E: 10000
Answer: C

Question: What did Richard Feynman say about the double-slit experiment?
A: It is the foundation of classical mechanics.
B: It contains the only mystery of quantum mechanics.
C: It proves the correctness of the wave theory.
D: It is an outdated concept.
E: It is the most complex quantum experiment ever performed.
Answer: B

Question: Who believed that the double-slit experiment demonstrated the correctness of Christiaan Huygens' wave theory of light?
A: Isaac Newton
B: Richard Feynman
C: George Paget Thomson
D: Thomas Young
E: Davisson and Germer
Answer: D

Question: Which version of the double path experiment splits the beam with a beam splitter?
A: Young's slits
B: Mach–Zehnder interferometer
C: Davisson-Germer setup
D: Feynman's apparatus
E: Thomson-Reid machine
Answer: B
@
Subject:
In physics, canonical quantization is a procedure for quantizing a classical theory, while attempting to preserve the formal structure, such as symmetries, of the classical theory to the greatest extent possible.

Historically, this was not quite Werner Heisenberg's route to obtaining quantum mechanics, but Paul Dirac introduced it in his 1926 doctoral thesis, the "method of classical analogy" for quantization,[1] and detailed it in his classic text Principles of Quantum Mechanics.[2] The word canonical arises from the Hamiltonian approach to classical mechanics, in which a system's dynamics is generated via canonical Poisson brackets, a structure which is only partially preserved in canonical quantization.

This method was further used by Paul Dirac in the context of quantum field theory, in his construction of quantum electrodynamics. In the field theory context, it is also called the second quantization of fields, in contrast to the semi-classical first quantization of single particles.

In mathematics and classical mechanics, the Poisson bracket is an important binary operation in Hamiltonian mechanics, playing a central role in Hamilton's equations of motion, which govern the time evolution of a Hamiltonian dynamical system. The Poisson bracket also distinguishes a certain class of coordinate transformations, called canonical transformations, which map canonical coordinate systems into canonical coordinate systems.

In theoretical physics, the Peierls bracket is an equivalent description[clarification needed] of the Poisson bracket. It can be defined directly from the action and does not require the canonical coordinates and their canonical momenta to be defined in advance.
$
10
Question: Who introduced canonical quantization in his 1926 doctoral thesis as the "method of classical analogy" for quantization?
A: Werner Heisenberg
B: Albert Einstein
C: Paul Dirac
D: Richard Feynman
E: Erwin Schrödinger
Answer: C

Question: Which word arises from the Hamiltonian approach to classical mechanics related to the dynamics of a system?
A: Classical
B: Canonical
C: Poisson
D: Quantum
E: Electrodynamics
Answer: B

Question: What kind of structure is generated by canonical Poisson brackets in the context of Hamiltonian mechanics?
A: Quantum Poisson brackets
B: Canonical structures
C: System's dynamics
D: Peierls brackets
E: Quantum transformations
Answer: C

Question: What is the role of the Poisson bracket in Hamiltonian mechanics?
A: It represents the quotient algebra.
B: It generates the quantum dynamics of the system.
C: It distinguishes the canonical coordinate transformations.
D: It converts the Hamiltonian system into a quantum system.
E: It plays a central role in Hamilton's equations of motion.
Answer: E

Question: In which field did Paul Dirac further use the method of canonical quantization?
A: General Relativity
B: Quantum Gravity
C: Quantum Electrodynamics
D: Quantum Chromodynamics
E: Electromagnetic Theory
Answer: C

Question: Canonical quantization aims to preserve the formal structure of the classical theory, including which aspect?
A: Quantum transitions
B: Symmetries
C: Quotient algebra
D: Electrodynamics
E: Hamiltonian electrodynamics
Answer: B

Question: In the context of quantum field theory, what is another name for the second quantization of fields?
A: First quantization of particles
B: Semi-classical first quantization
C: Canonical Poisson quantization
D: Semi-classical second quantization
E: Poisson field quantization
Answer: B

Question: Which of the following plays a crucial role in governing the time evolution of a Hamiltonian dynamical system?
A: Heisenberg bracket
B: Quantum bracket
C: Euler-Lagrange bracket
D: Peierls bracket
E: Poisson bracket
Answer: E

Question: What does the Peierls bracket provide that doesn't necessarily require the canonical coordinates and their canonical momenta to be defined in advance?
A: An equivalent description of the Hamiltonian bracket
B: An equivalent description of the quantum dynamics
C: An equivalent description of the Euler-Lagrange equations
D: An equivalent description of the Poisson bracket
E: An equivalent description of the quantum transformations
Answer: D

Question: The canonical quantization was detailed by Paul Dirac in which text?
A: Quantum Field Theory
B: Classical Mechanics Principles
C: Principles of Quantum Electrodynamics
D: Principles of Quantum Mechanics
E: Method of Classical Analogy
Answer: D
@
Subject:
Galaxies do not have a definite boundary by their nature, and are characterized by a gradually decreasing stellar density as a function of increasing distance from their center, making measurements of their true extents difficult. Nevertheless, astronomers over the past few decades have made several criteria in defining the sizes of galaxies. As early as the time of Edwin Hubble in 1936, there have been attempts to characterize the diameters of galaxies. With the advent of large sky surveys in the second half of the 20th century, the need for a standard for accurate determination of galaxy sizes has been in greater demand due to its enormous implications in astrophysics, such as the accurate determination of the Hubble constant. Various standards have been adapted over the decades, some more preferred than others. Below are some of these examples.

The isophotal diameter is introduced as a conventional way of measuring a galaxy's size based on its apparent surface brightness.[98] Isophotes are curves in a diagram - such as a picture of a galaxy - that adjoins points of equal brightnesses, and are useful in defining the extent of the galaxy. The apparent brightness flux of a galaxy is measured in units of magnitudes per square arcsecond (mag/arcsec2; sometimes expressed as mag arcsec−2), which defines the brightness depth of the isophote. To illustrate how this unit works, a typical galaxy has a brightness flux of 18 mag/arcsec2 at its central region. This brightness is equivalent to the light of an 18th magnitude hypothetical point object (like a star) being spread out evenly in a one square arcsecond area of the sky.[99] For the purposes of objectivity, the spectrum of light being used is sometimes also given in figures. As an example, the Milky Way has an average surface brightness of 22.1 B-mag/arcsec−2,[100][101][102] where B-mag refers to the brightness at the B-band (445 nm wavelength of light, in the blue part of the visible spectrum).

R.O. Redman in 1936 suggested that the diameters of galaxies (then referred to as "elliptical nebulae") should be defined at the 25.0 mag/arcsec2 isophote at the B-band, which is expected to cover much of the galaxy's light profile.[103] This isophote then became known simply as D25 (short for "diameter 25"), and corresponds to at least 10% of the normal brightness of the night sky, which is very near the limitations of blue filters at that time. This method was particularly used during the creation of the Uppsala General Catalogue using blue filters from the Palomar Observatory Sky Survey in 1972.

This conventional standard, however, is not universally agreed upon. Erik Holmberg in 1958 measured the diameters of at least 300 galaxies at the isophote of about 26.5 mag/arcsec2 (originally defined as where the photographic brightness density with respect to plate background is 0.5%).[104] Various other surveys such that of the ESO in 1989 use isophotes as faint as 27.0 mag/arcsec2.[105] Nevertheless, corrections of these diameters were introduced by both the Second and Third Reference Catalogue of Galaxies (RC2 and RC3), at least to those galaxies being covered by the two catalogues.
$
10
Question: What defines an isophote in a galaxy diagram?
A: Curves that indicate the galaxy's age.
B: Curves that distinguish between different galaxies.
C: Curves that adjoin points of equal brightnesses.
D: Curves that represent the mass of the galaxy.
E: Curves that highlight the galaxy's temperature.
Answer: C

Question: How is the apparent brightness flux of a galaxy typically expressed in units?
A: Mag per square degree.
B: Light-years per square arcsecond.
C: Mag/arcsec2 or mag arcsec−2.
D: Luminosity per square arcsecond.
E: Brightness per astronomical unit.
Answer: C

Question: For what reason did R.O. Redman in 1936 suggest measuring the diameter of galaxies at the 25.0 mag/arcsec2 isophote at the B-band?
A: It was the average brightness of most galaxies.
B: It was the brightness of the galaxy's core.
C: It corresponds to the brightest star in the night sky.
D: It was expected to cover much of the galaxy's light profile.
E: It matched the visibility of galaxies in infrared light.
Answer: D

Question: What does D25 represent in terms of galaxies?
A: The diameter of a galaxy at the 25th percentile of its brightness.
B: The diameter of a galaxy at the 25.0 mag/arcsec2 isophote at the B-band.
C: The average diameter of the 25 largest galaxies.
D: The diameter at which the galaxy emits 25% of its total light.
E: The diameter that represents a galaxy's 25th-year anniversary of discovery.
Answer: B

Question: In 1958, at which isophote did Erik Holmberg measure the diameters of galaxies?
A: 25.0 mag/arcsec2
B: 27.0 mag/arcsec2
C: 26.5 mag/arcsec2
D: 22.1 B-mag/arcsec−2
E: 20.5 mag/arcsec2
Answer: C

Question: Which catalog used blue filters from the Palomar Observatory Sky Survey in 1972?
A: Second Reference Catalogue of Galaxies
B: Third Reference Catalogue of Galaxies
C: ESO Catalogue of 1989
D: Uppsala General Catalogue
E: Edwin Hubble Catalogue
Answer: D

Question: What information does the term "22.1 B-mag/arcsec−2" provide about the Milky Way?
A: The age of the Milky Way in billions of years.
B: The Milky Way's average surface brightness in the blue part of the visible spectrum.
C: The total number of stars in the Milky Way.
D: The Milky Way's distance from Earth in millions of light-years.
E: The mass of the Milky Way in solar masses.
Answer: B

Question: Which term best describes the concept of isophotes?
A: Points of equal distance.
B: Points of equal mass.
C: Points of equal temperature.
D: Points of equal brightness.
E: Points of equal velocity.
Answer: D

Question: What is the significance of the need for a standard for accurate determination of galaxy sizes in astrophysics?
A: It helps determine the number of galaxies in the universe.
B: It provides a uniform measurement for comparing the temperatures of galaxies.
C: It is crucial for the accurate determination of the Hubble constant.
D: It is essential for tracking the movement of galaxies.
E: It helps astronomers predict the future positions of galaxies.
Answer: C

Question: By what factor does the brightness flux of a typical galaxy differ from that of the Milky Way at their central regions?
A: By a factor of 4.1 mag/arcsec2
B: By a factor of 2.1 mag/arcsec2
C: By a factor of 22.1 mag/arcsec2
D: By a factor of 18 mag/arcsec2
E: They have the same brightness flux.
Answer: B
@
Subject:
Maxwell's demon is a thought experiment that would hypothetically violate the second law of thermodynamics. It was proposed by the physicist James Clerk Maxwell in 1867.[1] In his first letter, Maxwell referred to the entity as a "finite being" or a "being who can play a game of skill with the molecules". Lord Kelvin would later call it a "demon".[2]

In the thought experiment, a demon controls a small massless door between two chambers of gas. As individual gas molecules (or atoms) approach the door, the demon quickly opens and closes the door to allow only fast-moving molecules to pass through in one direction, and only slow-moving molecules to pass through in the other. Because the kinetic temperature of a gas depends on the velocities of its constituent molecules, the demon's actions cause one chamber to warm up and the other to cool down. This would decrease the total entropy of the system, without applying any work, thereby violating the second law of thermodynamics.

The concept of Maxwell's demon has provoked substantial debate in the philosophy of science and theoretical physics, which continues to the present day. It stimulated work on the relationship between thermodynamics and information theory. Most scientists argue that, on theoretical grounds, no practical device can violate the second law in this way. Other researchers have implemented forms of Maxwell's demon in experiments, though they all differ from the thought experiment to some extent and none have been shown to violate the second law.

The second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature.[6] The second law is also expressed as the assertion that in an isolated system, entropy never decreases.[6]

Maxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:[6][7]

... if we conceive of a being whose faculties are so sharpened that he can follow every molecule in its course, such a being, whose attributes are as essentially finite as our own, would be able to do what is impossible to us. For we have seen that molecules in a vessel full of air at uniform temperature are moving with velocities by no means uniform, though the mean velocity of any great number of them, arbitrarily selected, is almost exactly uniform. Now let us suppose that such a vessel is divided into two portions, A and B, by a division in which there is a small hole, and that a being, who can see the individual molecules, opens and closes this hole, so as to allow only the swifter molecules to pass from A to B, and only the slower molecules to pass from B to A. He will thus, without expenditure of work, raise the temperature of B and lower that of A, in contradiction to the second law of thermodynamics.

In other words, Maxwell imagines one container divided into two parts, A and B.[6][8] Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from A flies towards the trapdoor, the demon opens it, and the molecule will fly from A to B. Likewise, when a slower-than-average molecule from B flies towards the trapdoor, the demon will let it pass from B to A. The average speed of the molecules in B will have increased while in A they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in A and increases in B, contrary to the second law of thermodynamics. A heat engine operating between the thermal reservoirs A and B could extract useful work from this temperature difference.

The demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from A to B will cause higher temperature and pressure to develop on the B side.
$
9
Question: What would the actions of Maxwell's demon result in?
A: Equalizing the temperature in both chambers.
B: A decrease in the total entropy of the system without applying any work.
C: An increase in the total entropy of the system with applied work.
D: No change in the temperature or entropy of the system.
E: Only the movement of slow-moving molecules from one chamber to the other.
Answer: B

Question: How is the second law of thermodynamics generally expressed in terms of entropy?
A: Entropy always decreases in an isolated system.
B: Entropy remains constant in an isolated system.
C: Entropy always increases in an isolated system.
D: Entropy varies unpredictably in an isolated system.
E: Entropy is irrelevant in an isolated system.
Answer: C

Question: In Maxwell's description, the being with the ability to see and control individual molecules was referred to as a:
A: Guardian
B: Observer
C: God-like entity
D: Finite being
E: Thermodynamic agent
Answer: D

Question: The proposed violation of the second law of thermodynamics in Maxwell's demon thought experiment involves:
A: Producing work without a temperature difference.
B: Generating energy without any source.
C: Raising the temperature of one chamber and lowering that of the other without expenditure of work.
D: Moving molecules from one chamber to another at a constant rate.
E: Stopping all molecular movement in one of the chambers.
Answer: C

Question: Why did Lord Kelvin refer to the entity in Maxwell's thought experiment as a "demon"?
A: Because it represented a malicious force.
B: Because it was seen as a trickster element in the experiment.
C: Because of its small size.
D: Because it had magical abilities.
E: Maxwell's original term for the entity was "demon".
Answer: B

Question: According to Maxwell's thought experiment, why would a temperature difference arise between the two chambers?
A: Because of the presence of the demon.
B: Due to the selective passage of faster-than-average molecules from one chamber to the other and vice versa.
C: Because the gas in one chamber was initially at a higher temperature.
D: Due to the natural properties of the gas molecules.
E: Because of an external heat source affecting one of the chambers.
Answer: B

Question: If Maxwell's demon were to function as described, what could be extracted from the temperature difference between the two chambers?
A: Light
B: Chemical energy
C: Magnetic energy
D: Electrical energy
E: Useful work
Answer: E

Question: According to the second law of thermodynamics, when two bodies of different temperatures are brought into contact, they will evolve to:
A: A state where one body loses all its heat.
B: A thermodynamic equilibrium with the same temperature.
C: A state where both bodies maintain their initial temperatures.
D: A dynamic equilibrium where temperatures fluctuate randomly.
E: A state where one body becomes twice as hot as the other.
Answer: B

Question: Which component is essential for Maxwell's demon to perform its task in the thought experiment?
A: A massless trapdoor.
B: A heat source.
C: A cooling system.
D: A pressure regulator.
E: A gravitational field.
Answer: A
@
Subject:
Superconductivity is a set of physical properties observed in certain materials where electrical resistance vanishes and magnetic fields are expelled from the material. Any material exhibiting these properties is a superconductor. Unlike an ordinary metallic conductor, whose resistance decreases gradually as its temperature is lowered, even down to near absolute zero, a superconductor has a characteristic critical temperature below which the resistance drops abruptly to zero.[1] [2] An electric current through a loop of superconducting wire can persist indefinitely with no power source.[3][4][5][6]

The superconductivity phenomenon was discovered in 1911 by Dutch physicist Heike Kamerlingh Onnes. Like ferromagnetism and atomic spectral lines, superconductivity is a phenomenon which can only be explained by quantum mechanics. It is characterized by the Meissner effect, the complete cancelation of the magnetic field in the interior of the superconductor during its transitions into the superconducting state. The occurrence of the Meissner effect indicates that superconductivity cannot be understood simply as the idealization of perfect conductivity in classical physics.

In 1986, it was discovered that some cuprate-perovskite ceramic materials have a critical temperature above 90 K (−183 °C).[7] Such a high transition temperature is theoretically impossible for a conventional superconductor, leading the materials to be termed high-temperature superconductors. The cheaply available coolant liquid nitrogen boils at 77 K (−196 °C) and thus the existence of superconductivity at higher temperatures than this facilitates many experiments and applications that are less practical at lower temperatures.

Conversely, a spinning superconductor generates a magnetic field, precisely aligned with the spin axis. The effect, the London moment, was put to good use in Gravity Probe B. This experiment measured the magnetic fields of four superconducting gyroscopes to determine their spin axes. This was critical to the experiment since it is one of the few ways to accurately determine the spin axis of an otherwise featureless sphere.
$
7
Question 1: Which of the following statements best describes the electrical resistance of a superconductor at a temperature below its critical temperature?
A. It gradually decreases as the temperature decreases.
B. It remains constant, but very low.
C. It drops abruptly to zero.
D. It becomes negative.
E. It remains the same as that at room temperature.
Answer: C

Question 2: The discovery of superconductivity is credited to:
A. An unknown Russian physicist in 1901.
B. Heike Kamerlingh Onnes in 1911.
C. A team of scientists in 1986 working on cuprate-perovskite ceramic materials.
D. The person who discovered the Meissner effect.
E. Scientists who were studying Gravity Probe B.
Answer: B

Question 3: The Meissner effect is primarily associated with which of the following phenomena?
A. The gradual decrease of resistance in metallic conductors.
B. The expulsion of magnetic fields from a superconducting material.
C. The ability of a superconductor to spin indefinitely.
D. The occurrence of high transition temperatures in cuprate-perovskite ceramic materials.
E. The generation of a gravitational field in a superconducting gyroscope.
Answer: B

Question 4: Which statement about high-temperature superconductors is true?
A. They have a critical temperature below 90 K.
B. Their existence makes experiments less practical due to the high temperature required.
C. They can be cooled with liquid nitrogen, which boils at 77 K.
D. Their discovery disproved the phenomenon of the Meissner effect.
E. They have been in use since the early 1900s.
Answer: C

Question 5: The term "London moment" is most closely associated with:
A. The abrupt drop in resistance of a superconductor.
B. The expulsion of magnetic fields from a superconducting material.
C. The ability of an electric current to persist indefinitely in a superconducting loop.
D. The generation of a magnetic field by a spinning superconductor.
E. The high critical temperature of cuprate-perovskite ceramic materials.
Answer: D

Question 6: What fundamental area of physics is necessary to explain phenomena like superconductivity?
A. Classical physics.
B. Thermodynamics.
C. Relativity.
D. Quantum mechanics.
E. Electromagnetism.
Answer: D

Question 7: In 1986, why was the discovery of superconductors with a critical temperature above 90 K significant?
A. It meant that these materials are not genuine superconductors.
B. Liquid nitrogen could be used as a coolant for them.
C. The Meissner effect could not be observed in these materials.
D. They provided evidence against quantum mechanics.
E. They were the first materials to exhibit superconductivity.
Answer: B
@
Subject:
Dark matter is a hypothetical form of matter thought to account for approximately 85% of the matter in the universe.[1] Dark matter is called "dark" because it does not appear to interact with the electromagnetic field, which means it does not absorb, reflect, or emit electromagnetic radiation and is, therefore, difficult to detect. Various astrophysical observations – including gravitational effects which cannot be explained by currently accepted theories of gravity unless more matter is present than can be seen – imply dark matter's presence. For this reason, most experts think that dark matter is abundant in the universe and has had a strong influence on its structure and evolution.[2]

The primary evidence for dark matter comes from calculations showing that many galaxies would behave quite differently if they did not contain a large amount of unseen matter. Some galaxies would not have formed at all and others would not move as they currently do.[3] Other lines of evidence include observations in gravitational lensing[4] and the cosmic microwave background, along with astronomical observations of the observable universe's current structure, the formation and evolution of galaxies, mass location during galactic collisions,[5] and the motion of galaxies within galaxy clusters. In the standard Lambda-CDM model of cosmology, the total mass–energy content of the universe contains 5% ordinary matter, 26.8% dark matter, and 68.2% of a form of energy known as dark energy.[6][7][8][9] Thus, dark matter constitutes 85%[a] of the total mass, while dark energy and dark matter constitute 95% of the total mass–energy content.[10][11][12][13]

No one has directly observed dark matter yet, primarily because it doesn't usually interact with ordinary baryonic matter and radiation except through gravity. Dark matter is thought to be non-baryonic. It may be composed of some as-yet-undiscovered subatomic particles.[b] A leading candidate for dark matter has been a new kind of elementary particle that has not yet been discovered, such as weakly interacting massive particles (WIMPs) or axions.[14] Other possibilities include black holes such as primordial black holes.[15][16] Many experiments to detect and study dark matter particles directly are being actively undertaken, but none have yet succeeded.[17] Dark matter is classified as "cold", "warm", or "hot" according to its velocity (more precisely, its free streaming length). Recent models favored a cold dark matter scenario, in which structures emerge by the gradual accumulation of particles. But after a half century of fruitless dark matter particle searches, more recent gravitational wave and James Webb Space Telescope observations have considerably strengthened the case for primordial and direct collapse black holes.

If dark matter is made up of subatomic particles, then millions, possibly billions, of such particles must pass through every square centimeter of the Earth each second.[139][140] Many experiments aim to test this hypothesis. Although WIMPs have been the main search candidates,[17] axions have drawn renewed attention, with the Axion Dark Matter Experiment (ADMX) searches for axions and many more planned in the future.[141] Another candidate is heavy hidden sector particles which only interact with ordinary matter via gravity.

These experiments can be divided into two classes: direct detection experiments, which search for the scattering of dark matter particles off atomic nuclei within a detector; and indirect detection, which look for the products of dark matter particle annihilations or decays.

Direct detection experiments aim to observe low-energy recoils (typically a few keVs) of nuclei induced by interactions with particles of dark matter, which (in theory) are passing through the Earth. After such a recoil the nucleus will emit energy in the form of scintillation light or phonons, as they pass through sensitive detection apparatus. To do so effectively, it is crucial to maintain an extremely low background, which is the reason why such experiments typically operate deep underground, where interference from cosmic rays is minimized. Examples of underground laboratories with direct detection experiments include the Stawell mine, the Soudan mine, the SNOLAB underground laboratory at Sudbury, the Gran Sasso National Laboratory, the Canfranc Underground Laboratory, the Boulby Underground Laboratory, the Deep Underground Science and Engineering Laboratory and the China Jinping Underground Laboratory.

These experiments mostly use either cryogenic or noble liquid detector technologies. Cryogenic detectors operating at temperatures below 100 mK, detect the heat produced when a particle hits an atom in a crystal absorber such as germanium. Noble liquid detectors detect scintillation produced by a particle collision in liquid xenon or argon. Cryogenic detector experiments include: CDMS, CRESST, EDELWEISS, EURECA. Noble liquid experiments include LZ, XENON, DEAP, ArDM, WARP, DarkSide, PandaX, and LUX, the Large Underground Xenon experiment. Both of these techniques focus strongly on their ability to distinguish background particles (which predominantly scatter off electrons) from dark matter particles (that scatter off nuclei). Other experiments include SIMPLE and PICASSO.
$
10
Question 1: What percentage of the total matter in the universe is thought to be made up of dark matter?
A. 5%
B. 26.8%
C. 68.2%
D. 85%
E. 95%
Answer: D

Question 2: Why is dark matter referred to as "dark"?
A. Because it emits a black light visible to the naked eye.
B. Because it does not interact with the electromagnetic field.
C. Because it is a source of dark energy.
D. Because it is made up of black holes.
E. Because it is less dense than visible matter.
Answer: B

Question 3: Which of the following is NOT used as evidence for the existence of dark matter?
A. Gravitational lensing.
B. The behavior of certain galaxies.
C. Cosmic microwave background.
D. Emission of gamma radiation from black holes.
E. Motion of galaxies within galaxy clusters.
Answer: D

Question 4: What does the Lambda-CDM model of cosmology suggest about the total mass–energy content of the universe?
A. 85% is made up of dark matter.
B. 5% is made up of ordinary matter.
C. 26.8% is made up of dark energy.
D. 68.2% is made up of ordinary matter.
E. 95% is made up of dark matter and dark energy.
Answer: E

Question 5: Weakly Interacting Massive Particles (WIMPs) and axions are:
A. Examples of baryonic matter.
B. Proposed particles that make up dark energy.
C. Leading candidates for the composition of dark matter.
D. The particles responsible for gravitational lensing.
E. Particles discovered in the cosmic microwave background.
Answer: C

Question 6: Where do direct detection experiments for dark matter typically operate?
A. In outer space.
B. Above ground to maximize cosmic ray interference.
C. Deep underwater.
D. Deep underground to minimize cosmic ray interference.
E. At high altitudes to capture the dark matter wind.
Answer: D

Question 7: Which of the following statements best describes the primary function of noble liquid detectors in dark matter experiments?
A. They detect scintillation produced by particle collisions in gases like helium and neon.
B. They determine the exact mass of dark matter particles.
C. They detect scintillation produced by particle collisions in liquid xenon or argon.
D. They operate at extremely high temperatures to capture particle interactions.
E. They focus on distinguishing between types of dark matter particles.
Answer: C

Question 8: Which experiment is NOT associated with the cryogenic detector technique for detecting dark matter?
A. CDMS
B. CRESST
C. DEAP
D. EDELWEISS
E. EURECA
Answer: C

Question 9: According to recent models, which scenario is favored for dark matter?
A. A scenario where structures disintegrate due to the repulsion of particles.
B. A warm dark matter scenario, with rapid motion of particles.
C. A cold dark matter scenario, where structures emerge by gradual accumulation of particles.
D. A scenario where dark matter structures are formed by electromagnetic interactions.
E. A hot dark matter scenario where structures form due to the heat generated by particles.
Answer: C

Question 10: The Axion Dark Matter Experiment (ADMX) is primarily focused on searching for:
A. WIMPs.
B. Primordial black holes.
C. Axions.
D. Direct collapse black holes.
E. Heavy hidden sector particles.
Answer: C
@
Subject:
Relative density, sometimes called specific gravity,[1][2] is a dimensionless quantity defined as the ratio of the density (mass of a unit volume) of a substance to the density of a given reference material. Specific gravity for liquids is nearly always measured with respect to water at its densest (at 4 °C or 39.2 °F); for gases, the reference is air at room temperature (20 °C or 68 °F). The term "relative density" (often abbreviated r.d. or RD) is often preferred in scientific usage, whereas the term "specific gravity" is deprecated.[citation needed]

A pycnometer (from Ancient Greek: πυκνός, romanized: puknos, lit. 'dense'), also called pyknometer or specific gravity bottle, is a device used to determine the density of a liquid. A pycnometer is usually made of glass, with a close-fitting ground glass stopper with a capillary tube through it, so that air bubbles may escape from the apparatus. This device enables a liquid's density to be measured accurately by reference to an appropriate working fluid, such as water or mercury, using an analytical balance.[citation needed]

If the flask is weighed empty, full of water, and full of a liquid whose relative density is desired, the relative density of the liquid can easily be calculated. The particle density of a powder, to which the usual method of weighing cannot be applied, can also be determined with a pycnometer. The powder is added to the pycnometer, which is then weighed, giving the weight of the powder sample. The pycnometer is then filled with a liquid of known density, in which the powder is completely insoluble. The weight of the displaced liquid can then be determined, and hence the relative density of the powder.
$
10
Question 1: Relative density is defined as the ratio of the density of a substance to the density of which of the following reference materials for liquids?
A. Mercury
B. Air at room temperature
C. Water at its densest
D. Gas at 0 °C
E. Liquid Nitrogen
Answer: C

Question 2: At what temperature is water considered to be at its densest?
A. 0 °C
B. 20 °C
C. 68 °F
D. 4 °C
E. 32 °F
Answer: D

Question 3: Which of the following terms is often preferred in scientific usage to describe the ratio of the density of a substance to a reference material?
A. Density ratio
B. Weight scale
C. Specific volume
D. Relative density
E. Mass ratio
Answer: D

Question 4: The capillary tube in the ground glass stopper of a pycnometer serves what purpose?
A. To measure the volume of the liquid.
B. To allow air bubbles to escape.
C. To pour the liquid into the pycnometer.
D. To keep the liquid from evaporating.
E. To measure the temperature of the liquid.
Answer: B

Question 5: If you wish to determine the particle density of a powder using a pycnometer, which of the following steps is correct?
A. The powder is added to the pycnometer and then filled with air.
B. The pycnometer is filled with a liquid, then the powder is added.
C. The powder is added to the pycnometer, it's weighed, then filled with a liquid in which the powder is soluble.
D. The powder is added to the pycnometer, it's weighed, then filled with a liquid in which the powder is insoluble.
E. The pycnometer is filled with water, the powder is added, and then the weight is measured.
Answer: D

Question 6: Which of the following statements about a pycnometer is FALSE?
A. It is typically made of glass.
B. It is used to measure the mass of a liquid.
C. It has a close-fitting ground glass stopper.
D. It is used to determine the density of a liquid.
E. The pycnometer has a capillary tube through its stopper.
Answer: B

Question 7: Which of the following is a synonym for pycnometer?
A. Barometer
B. Pyknometer
C. Thermometer
D. Hydrometer
E. Manometer
Answer: B

Question 8: If one were to weigh a pycnometer when it is empty, full of water, and full of another liquid, these measurements would primarily help in calculating:
A. The mass of the liquid.
B. The volume of the liquid.
C. The weight of the water.
D. The relative density of the liquid.
E. The absolute density of the pycnometer.
Answer: D

Question 9: What does the term "relative density" measure?
A. The absolute weight of a substance.
B. The mass of a unit volume of a substance.
C. The ratio of the density of a substance to the density of a given reference material.
D. The temperature at which a substance is densest.
E. The volume occupied by a unit weight of a substance.
Answer: C

Question 10: For gases, what is the reference material when determining relative density?
A. Helium
B. Oxygen
C. Nitrogen
D. Air at room temperature
E. Carbon dioxide
Answer: D
@
Subject:
CEERS-93316 is a high-redshift galaxy with a spectroscopic redshift z=4.9. [3] Notably, the redshift that was initially reported was photometric (z = 16.4), and would have made CEERS-93316 the earliest and most distant known galaxy observed.[1][6][7][8] As of June 2023, the article containing the spectroscopic measurement has not yet been peer-reviewed and published in a journal.

CEERS-93316 has a light-travel distance (lookback time) of 12.6 billion years, and, due to the expansion of the universe, a present proper distance of 25.7 billion light-years.[4]

The candidate high-redshift galaxy CEERS-93316 (RA:14:19:39.48 DEC:+52:56:34.92), in the Boötes constellation,[1][2] was discovered by the CEERS imaging observing program using the Near Infrared Camera of the James Webb Space Telescope (JWST) in July 2022.[1][6][7][8] CEERS stands for "Cosmic Evolution Early Release Science Survey", and is a deep- and wide-field sky survey program developed specifically for JWST image studies, and is conducted by the CEERS Collaboration.
$
10Question 1: What type of redshift was initially reported for CEERS-93316?
A. Spectroscopic
B. Photometric
C. Radio
D. Theoretical
E. Cosmological
Answer: B

Question 2: Which telescope was used to discover CEERS-93316?
A. Hubble Space Telescope
B. Spitzer Space Telescope
C. TESS
D. Kepler
E. James Webb Space Telescope
Answer: E

Question 3: As of June 2023, what is the status of the article containing the spectroscopic measurement of CEERS-93316?
A. Peer-reviewed and published
B. Not peer-reviewed but published
C. Peer-reviewed but not published
D. Not peer-reviewed and not published
E. Retracted after publication
Answer: D

Question 4: In which constellation is the candidate high-redshift galaxy CEERS-93316 located?
A. Orion
B. Ursa Major
C. Cassiopeia
D. Boötes
E. Canis Major
Answer: D

Question 5: CEERS-93316 has a light-travel distance or lookback time of how many billion years?
A. 16.4 billion years
B. 4.9 billion years
C. 10.2 billion years
D. 25.7 billion years
E. 12.6 billion years
Answer: E

Question 6: Due to the expansion of the universe, what is the present proper distance of CEERS-93316?
A. 10.5 billion light-years
B. 16.4 billion light-years
C. 12.6 billion light-years
D. 20.0 billion light-years
E. 25.7 billion light-years
Answer: E

Question 7: What does CEERS stand for in the context of the CEERS imaging observing program?
A. Cosmic Evolution Early Research Study
B. Constellation Evolution Early Response System
C. Cosmic Expansion Energetic Radiation Survey
D. Cosmic Evolution Early Release Science Survey
E. Comprehensive Early Exoplanet Research Study
Answer: D

Question 8: Which instrument of the James Webb Space Telescope was used to observe CEERS-93316?
A. Mid Infrared Instrument
B. Near Infrared Camera
C. Near Infrared Spectrograph
D. Fine Guidance Sensor
E. Optical Telescope Element
Answer: B

Question 9: If the initial photometric redshift measurement of CEERS-93316 was correct, what would it have signified about the galaxy?
A. It would have been one of the latest and nearest galaxies observed.
B. It would have had no significant implications.
C. It would have been the earliest and most distant known galaxy observed.
D. It would have suggested that the galaxy is relatively young.
E. It would have confirmed the presence of black holes at the galaxy's center.
Answer: C

Question 10: Who conducted the CEERS imaging observing program?
A. Hubble Collaboration
B. CEERS Collaboration
C. Spitzer Group
D. NASA's Early Galaxy Exploration Team
E. JWST's Near Infrared Group
Answer: B
@
Subject:
Bollard pull is a conventional measure of the pulling (or towing) power of a watercraft. It is defined as the force (usually in tonnes-force or kilonewtons (kN)) exerted by a vessel under full power, on a shore-mounted bollard through a tow-line, commonly measured in a practical test (but sometimes simulated) under test conditions that include calm water, no tide, level trim, and sufficient depth and side clearance for a free propeller stream.[1] Like the horsepower or mileage rating of a car, it is a convenient but idealized number that must be adjusted for operating conditions that differ from the test. The bollard pull of a vessel may be reported as two numbers, the static or maximum bollard pull – the highest force measured – and the steady or continuous bollard pull, the average of measurements over an interval of, for example, 10 minutes. An equivalent measurement on land is known as drawbar pull, or tractive force, which is used to measure the total horizontal force generated by a locomotive, a piece of heavy machinery such as a tractor, or a truck, (specifically a ballast tractor), which is utilized to move a load.

Bollard pull is primarily (but not only) used for measuring the strength of tugboats, with the largest commercial harbour tugboats in the 2000-2010s having around 60 to 65 short tons-force (530–580 kN; 54–59 tf) of bollard pull, which is described as 15 short tons-force (130 kN; 14 tf) above "normal" tugboats.[2][3] The worlds strongest tug since its delivery in 2020 is Island Victory (Vard Brevik 831) of Island Offshore, with a bollard pull of 477 tonnes-force (526 short tons-force; 4,680 kN).[4] Island Victory is not a typical tug, rather it is a special class of ship used in the petroleum industry called an Anchor Handling Tug Supply vessel.
$
10
Question 1: What is bollard pull a measure of?
A. The speed of a watercraft
B. The depth at which a watercraft can operate
C. The towing or pulling power of a watercraft
D. The storage capacity of a watercraft
E. The fuel efficiency of a watercraft
Answer: C

Question 2: Under what conditions is bollard pull usually measured?
A. Stormy weather, high tide, uneven trim, and shallow waters
B. Calm water, no tide, level trim, and sufficient depth for a free propeller stream
C. Rough seas, low tide, level trim, and minimal side clearance
D. Calm water, high tide, uneven trim, and restricted depth
E. Rough seas, changing tides, tilting trim, and deep waters
Answer: B

Question 3: What are the two types of bollard pull values that might be reported for a vessel?
A. Minimum and Maximum
B. Static and Dynamic
C. Static (or maximum) and Steady (or continuous)
D. Initial and Final
E. Predicted and Actual
Answer: C

Question 4: On land, what is the equivalent measurement of bollard pull known as?
A. Frictional force
B. Net force
C. Push force
D. Drawbar pull or tractive force
E. Rolling resistance
Answer: D

Question 5: What is the typical bollard pull strength for the largest commercial harbour tugboats in the 2000-2010s?
A. 20 to 25 short tons-force
B. 30 to 40 short tons-force
C. 60 to 65 short tons-force
D. 75 to 85 short tons-force
E. 90 to 100 short tons-force
Answer: C

Question 6: Which ship holds the title for the world's strongest tug as of its delivery in 2020?
A. Harbour Master
B. Sea Guardian
C. Island Victory
D. Ocean Puller
E. Tug Titan
Answer: C

Question 7: Island Victory is not just a typical tugboat. What type of special vessel is it?
A. A cruise ship
B. An Icebreaker
C. A Ballast Carrier
D. An Anchor Handling Tug Supply vessel
E. A Container Ship
Answer: D

Question 8: In terms of bollard pull, what amount is described as being above "normal" for tugboats in the 2000-2010s?
A. 5 short tons-force
B. 15 short tons-force
C. 25 short tons-force
D. 35 short tons-force
E. 45 short tons-force
Answer: B

Question 9: How is bollard pull generally expressed in terms of units?
A. Knots or miles per hour
B. Cubic meters or gallons
C. Tons-force or kilonewtons (kN)
D. Decibels or hertz
E. Watts or joules
Answer: C

Question 10: Why might the bollard pull number need to be adjusted?
A. To account for the age of the ship
B. To consider the experience level of the crew
C. When operating conditions differ from the test conditions
D. Based on the color of the ship
E. In accordance with international maritime regulations
Answer: C
@
Subject:
A crystal oscillator is an electronic oscillator circuit that uses a piezoelectric crystal as a frequency-selective element.[1][2][3] The oscillator frequency is often used to keep track of time, as in quartz wristwatches, to provide a stable clock signal for digital integrated circuits, and to stabilize frequencies for radio transmitters and receivers. The most common type of piezoelectric resonator used is a quartz crystal, so oscillator circuits incorporating them became known as crystal oscillators.[1] However, other piezoelectricity materials including polycrystalline ceramics are used in similar circuits.

A crystal oscillator relies on the slight change in shape of a quartz crystal under an electric field, a property known as inverse piezoelectricity. A voltage applied to the electrodes on the crystal causes it to change shape; when the voltage is removed, the crystal generates a small voltage as it elastically returns to its original shape. The quartz oscillates at a stable resonant frequency, behaving like an RLC circuit, but with a much higher Q factor (less energy loss on each cycle of oscillation). Once a quartz crystal is adjusted to a particular frequency (which is affected by the mass of electrodes attached to the crystal, the orientation of the crystal, temperature and other factors), it maintains that frequency with high stability.[4]

Quartz crystals are manufactured for frequencies from a few tens of kilohertz to hundreds of megahertz. As of 2003, around two billion crystals are manufactured annually.[5] Most are used for consumer devices such as wristwatches, clocks, radios, computers, and cellphones. However, in applications where small size and weight is needed crystals can be replaced by thin-film bulk acoustic resonators, specifically if ultra-high frequency (more than roughly 1.5 GHz) resonance is needed. Quartz crystals are also found inside test and measurement equipment, such as counters, signal generators, and oscilloscopes.

Crystals for AT-cut are the most common in mass production of oscillator materials; the shape and dimensions are optimized for high yield of the required wafers. High-purity quartz crystals are grown with especially low content of aluminium, alkali metal and other impurities and minimal defects; the low amount of alkali metals provides increased resistance to ionizing radiation. Crystals for wrist watches, for cutting the tuning fork 32768 Hz crystals, are grown with very low etch channel density.
$
10
Question 1: What is the primary use of a crystal oscillator?
A. Amplifying radio signals.
B. Keeping track of time and providing a stable clock signal.
C. Modulating frequency in a transmitter.
D. Blocking electromagnetic interference.
E. Converting direct current (DC) to alternating current (AC).
Answer: B

Question 2: What property of quartz is leveraged in crystal oscillators?
A. Electrical conductivity.
B. Inverse piezoelectricity.
C. Magnetic resonance.
D. Optical clarity.
E. Thermal conductivity.
Answer: B

Question 3: How does a quartz crystal in an oscillator behave?
A. Like a basic capacitor.
B. Like an RLC circuit.
C. Like a pure resistor.
D. Like a diode.
E. Like a pure inductor.
Answer: B

Question 4: Which of the following is NOT a typical use of quartz crystals?
A. In digital integrated circuits.
B. In thermal sensors.
C. In radio transmitters and receivers.
D. In wristwatches.
E. In oscilloscopes.
Answer: B

Question 5: What is the primary reason thin-film bulk acoustic resonators might be used in place of quartz crystals?
A. To achieve a louder resonance.
B. To reduce manufacturing costs.
C. To achieve ultra-high frequency resonance.
D. To produce a smoother voltage output.
E. To increase weight and size.
Answer: C

Question 6: As of 2003, approximately how many quartz crystals are manufactured annually?
A. Around two hundred million.
B. Around two billion.
C. Around twenty billion.
D. Around two thousand.
E. Around two million.
Answer: B

Question 7: What characteristic do high-purity quartz crystals possess?
A. High content of aluminium.
B. Increased resistance to ionizing radiation.
C. High etch channel density.
D. Enhanced magnetic properties.
E. Increased thermal conductivity.
Answer: B

Question 8: What resonant frequency is a typical wristwatch quartz crystal tuned to?
A. 1 kHz.
B. 32.768 kHz.
C. 1.5 GHz.
D. 100 MHz.
E. 5 GHz.
Answer: B

Question 9: What primarily affects the adjusted frequency of a quartz crystal?
A. The color of the quartz.
B. The packaging material.
C. Mass of electrodes attached, orientation of the crystal, and temperature.
D. The diameter of the quartz.
E. The voltage applied to the entire circuit.
Answer: C

Question 10: What does AT-cut in the context of quartz crystals refer to?
A. The angle at which the crystal is cut.
B. The atomic composition of the crystal.
C. The altitude at which the crystal was found.
D. The application type for which the crystal is used.
E. The alignment technique employed during manufacturing.
Answer: A
@
Subject:
In probability theory, a probability density function (PDF), density function, or density of an absolutely continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would be equal to that sample.[2][3] Probability density is the probability per unit length, in other words, while the absolute likelihood for a continuous random variable to take on any particular value is 0 (since there is an infinite set of possible values to begin with), the value of the PDF at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would be close to one sample compared to the other sample.

In a more precise sense, the PDF is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one value. This probability is given by the integral of this variable's PDF over that range—that is, it is given by the area under the density function but above the horizontal axis and between the lowest and greatest values of the range. The probability density function is nonnegative everywhere, and the area under the entire curve is equal to 1.

In probability and statistics, a probability mass function is a function that gives the probability that a discrete random variable is exactly equal to some value.[1] Sometimes it is also known as the discrete probability density function. The probability mass function is often the primary means of defining a discrete probability distribution, and such functions exist for either scalar or multivariate random variables whose domain is discrete.

A probability mass function differs from a probability density function (PDF) in that the latter is associated with continuous rather than discrete random variables. A PDF must be integrated over an interval to yield a probability.[2]

The value of the random variable having the largest probability mass is called the mode.
$
10
Question 1: Which of the following statements about a probability density function (PDF) is true?
A. The PDF takes on negative values.
B. The total area under the PDF curve is 5.
C. The PDF provides the absolute likelihood of a continuous random variable taking a specific value.
D. The area under the curve of the PDF represents probabilities.
E. The value of a specific sample in a PDF represents its absolute probability.
Answer: D

Question 2: In the context of a continuous random variable, the absolute likelihood of it taking any specific value is:
A. 0.5.
B. 1.
C. 0.
D. Infinite.
E. Undefined.
Answer: C

Question 3: A probability mass function (PMF) is primarily used for defining:
A. Continuous probability distributions.
B. The mode of a distribution.
C. Discrete probability distributions.
D. The median of a distribution.
E. Non-random variables.
Answer: C

Question 4: The value of the random variable having the largest probability mass is known as:
A. Median.
B. Mean.
C. Mode.
D. Variance.
E. Standard deviation.
Answer: C

Question 5: For a probability density function (PDF), which of the following is true?
A. It always equals 1 at its peak.
B. It is always nonnegative.
C. It can be used directly to find the probability of a specific value.
D. It is primarily used for discrete random variables.
E. It can take on negative values.
Answer: B

Question 6: If you want to find the probability of a continuous random variable falling within a specific range, you would:
A. Multiply the PDF by the range.
B. Integrate the PMF over that range.
C. Take the difference of the PDF values at the two endpoints of the range.
D. Integrate the PDF over that range.
E. Simply use the value of the PDF at the midpoint of the range.
Answer: D

Question 7: In which scenario would you use a probability mass function?
A. When determining the likelihood of a temperature between 20 and 30 degrees.
B. When determining the likelihood of a ball landing within a 2-meter range.
C. When determining the probability of rolling a 4 on a six-sided die.
D. When determining the probability of rainfall between 10mm and 20mm.
E. When determining the likelihood of a river flowing at 5 liters per second.
Answer: C

Question 8: The area under the curve of a probability density function (PDF) always sums up to:
A. 0.
B. 10.
C. 5.
D. 2.
E. 1.
Answer: E

Question 9: A PDF is used to specify:
A. The absolute probability of a continuous random variable taking a particular value.
B. The likelihood that a discrete random variable is exactly equal to some value.
C. The relative likelihood of a continuous random variable taking on values around a specific sample.
D. The specific value where the probability mass is the highest.
E. The variance of a discrete random variable.
Answer: C

Question 10: Which statement about the probability mass function (PMF) is NOT true?
A. It gives the probability that a discrete random variable is exactly equal to some value.
B. It can exist for scalar or multivariate random variables with a discrete domain.
C. The PMF must be integrated to yield a probability.
D. It's often the primary means of defining a discrete probability distribution.
E. The term PMF can sometimes be referred to as the discrete probability density function.
Answer: C
@
Subject:
Radar astronomy is a technique of observing nearby astronomical objects by reflecting radio waves or microwaves off target objects and analyzing their reflections. Radar astronomy differs from radio astronomy in that the latter is a passive observation (i.e., receiving only) and the former an active one (transmitting and receiving). Radar systems have been conducted for six decades applied to a wide range of Solar System studies. The radar transmission may either be pulsed or continuous. The strength of the radar return signal is proportional to the inverse fourth-power of the distance. Upgraded facilities, increased transceiver power, and improved apparatus have increased observational opportunities.

Radar techniques provide information unavailable by other means, such as testing general relativity by observing Mercury[1] and providing a refined value for the astronomical unit.[2] Radar images provide information about the shapes and surface properties of solid bodies, which cannot be obtained by other ground-based techniques.

The NASA Deep Space Network (DSN) is a worldwide network of American spacecraft communication ground segment facilities, located in the United States (California), Spain (Madrid), and Australia (Canberra), that supports NASA's interplanetary spacecraft missions. It also performs radio and radar astronomy observations for the exploration of the Solar System and the universe, and supports selected Earth-orbiting missions. DSN is part of the NASA Jet Propulsion Laboratory (JPL).

Lunar Laser Ranging (LLR) is the practice of measuring the distance between the surfaces of the Earth and the Moon using laser ranging. The distance can be calculated from the round-trip time of laser light pulses travelling at the speed of light, which are reflected back to Earth by the Moon's surface or by one of several retroreflectors installed on the Moon. Three were placed by the United States' Apollo program (11, 14, and 15), two by the Soviet Lunokhod 1 and 2 missions,[1] and one by India's Chandrayaan-3 mission.[2][3]
$
10
Question 1: Which of the following techniques involves actively transmitting and receiving signals to study astronomical objects?
A. Radio astronomy
B. Optical astronomy
C. Radar astronomy
D. Passive astronomy
E. Spectroscopy
Answer: C

Question 2: What advantage does radar astronomy offer over other ground-based observation methods?
A. It can be used to observe only planets.
B. It provides information about the shapes and surface properties of solid bodies.
C. It is the only way to observe Mercury.
D. It depends on the visible light spectrum.
E. It requires no transmission power.
Answer: B

Question 3: Which of the following is a primary function of the NASA Deep Space Network (DSN)?
A. To observe only the Moon.
B. To conduct Lunar Laser Ranging experiments.
C. To support NASA's interplanetary spacecraft missions.
D. To manage Earth's weather satellite systems.
E. To coordinate with radio telescopes around the world.
Answer: C

Question 4: In the Lunar Laser Ranging (LLR) experiment, what is used to reflect the laser light pulses back to Earth?
A. Natural craters on the Moon's surface.
B. A series of mirrors placed on the Earth.
C. A set of retroreflectors installed on the Moon.
D. The Moon's atmosphere.
E. The lunar oceans.
Answer: C

Question 5: How does the strength of the radar return signal in radar astronomy relate to the distance from the target?
A. Proportional to the distance.
B. Inverse square of the distance.
C. Inverse fourth-power of the distance.
D. Directly proportional to the square of the distance.
E. Does not depend on the distance.
Answer: C

Question 6: Which among the following missions did NOT place retroreflectors on the Moon for the Lunar Laser Ranging Experiment?
A. Apollo 11
B. Lunokhod 1
C. Chandrayaan-3
D. Voyager 2
E. Apollo 15
Answer: D

Question 7: Radar astronomy is distinct from radio astronomy because:
A. Radar astronomy cannot observe distant galaxies.
B. Radar astronomy involves transmitting signals, whereas radio astronomy does not.
C. Radio astronomy cannot observe objects within our solar system.
D. Radar astronomy only works with microwaves.
E. Radar astronomy does not require any receiving equipment.
Answer: B

Question 8: The NASA Deep Space Network (DSN) supports all of the following, EXCEPT:
A. Radio astronomy observations for exploring the Solar System.
B. Earth-orbiting missions.
C. Interplanetary spacecraft missions.
D. Monitoring of Earth's geostationary satellites.
E. Radar astronomy observations for exploring the universe.
Answer: D

Question 9: Which facility supports NASA's interplanetary spacecraft missions, performs radio and radar astronomy observations, and is a part of the NASA Jet Propulsion Laboratory?
A. Hubble Space Telescope
B. International Space Station
C. Keck Observatory
D. Deep Space Network (DSN)
E. Large Hadron Collider
Answer: D

Question 10: Lunar Laser Ranging (LLR) involves measuring the distance between:
A. The Moon and Mars.
B. Earth and the Sun.
C. Earth's surface and the Moon's surface.
D. Two specific points on the Moon's surface.
E. The Moon and the nearest star.
Answer: C
@
Subject:
The Ambidextrous Universe is a popular science book by Martin Gardner, covering aspects of symmetry and asymmetry in human culture, science and the wider universe. It culminates in a discussion of whether nature's conservation of parity (the symmetry of mirrored quantum systems) is ever violated, which had been proven experimentally in 1956.

The book was originally published in 1964 with the subtitle Left, Right, and the Fall of Parity, with a revised version following in 1969. A second edition was released in 1979 with the new subtitle Mirror Asymmetry and Time-Reversed Worlds. The third edition was released in 1990 under the title The New Ambidextrous Universe: Symmetry and Asymmetry from Mirror Reflections to Superstrings; this was re-released with minor revisions in 2005.

The book begins with the subject of mirror reflection, and from there passes through symmetry in geometry, poetry, art, music, galaxies, stars, planets and living organisms. It then moves down into the molecular scale and looks at how symmetry and asymmetry have evolved from the beginning of life on Earth. There is a chapter on carbon and its versatility and on chirality in biochemistry.

The last several chapters deal with a conundrum called the Ozma Problem, which examines whether there is any fundamental asymmetry to the universe. This discussion concerns various aspects of atomic and subatomic physics and how they relate to mirror asymmetry and the related concepts of chirality, antimatter, magnetic and electrical polarity, parity, charge and spin. Time invariance (and reversal) is discussed. Implications for particle physics, theoretical physics and cosmology are covered and brought up to date (in later editions of the book) with regard to Grand Unified Theories, theories of everything, superstring theory and M-theory.
$
10
Question 1: Who is the author of "The Ambidextrous Universe"?
A. Stephen Hawking
B. Richard Feynman
C. Martin Gardner
D. Neil deGrasse Tyson
E. Brian Greene
Answer: C

Question 2: In which year was the original edition of "The Ambidextrous Universe" published?
A. 1956
B. 1964
C. 1979
D. 1990
E. 2005
Answer: B

Question 3: What was the main subject of the book's discussion?
A. Evolution of life on Earth
B. Time travel possibilities
C. Symmetry and asymmetry in various facets of existence
D. Biochemical aspects of chirality
E. Theoretical exploration of black holes
Answer: C

Question 4: What subject does the book start its discussion with?
A. Chirality in biochemistry
B. Time reversal
C. Mirror reflection
D. Grand Unified Theories
E. The versatility of carbon
Answer: C

Question 5: Which edition of "The Ambidextrous Universe" was titled "The New Ambidextrous Universe: Symmetry and Asymmetry from Mirror Reflections to Superstrings"?
A. First edition
B. Second edition
C. Third edition
D. Fourth edition
E. Fifth edition
Answer: C

Question 6: Which edition of the book discussed Grand Unified Theories, superstring theory, and M-theory?
A. Original 1964 edition
B. 1969 revised version
C. 1979 second edition
D. 1990 third edition
E. All of the above
Answer: D

Question 7: What theme does the discussion of the Ozma Problem revolve around?
A. The nature of the universe in terms of art and music
B. An examination of whether there exists a basic asymmetry in the universe
C. The literary implications of symmetry in classic texts
D. The role of quantum mechanics in understanding universal beauty
E. The correlation between galaxies and symmetry in biology
Answer: B

Question 8: What concept is NOT directly related to the Ozma Problem, based on the book's description?
A. Antimatter
B. Mirror asymmetry
C. Parity
D. Evolution of species
E. Electrical polarity
Answer: D

Question 9: In which year was a minor revision of "The New Ambidextrous Universe: Symmetry and Asymmetry from Mirror Reflections to Superstrings" released?
A. 1969
B. 1979
C. 1990
D. 2000
E. 2005
Answer: E

Question 10: What does the term "chirality" in the context of the book primarily refer to?
A. The phenomenon of light polarization
B. A specific type of symmetry found in galaxies
C. The left-handed or right-handed orientation of molecules
D. The musical harmonies resulting from symmetric scales
E. The rotation direction of planets around their axes
Answer: C
@
Subject:
In mathematics, Hilbert spaces (named after David Hilbert) allow the methods of linear algebra and calculus to be generalized from (finite-dimensional) Euclidean vector spaces to spaces that may be infinite-dimensional. Hilbert spaces arise naturally and frequently in mathematics and physics, typically as function spaces. Formally, a Hilbert space is a vector space equipped with an inner product that induces a distance function for which the space is a complete metric space.

The earliest Hilbert spaces were studied from this point of view in the first decade of the 20th century by David Hilbert, Erhard Schmidt, and Frigyes Riesz. They are indispensable tools in the theories of partial differential equations, quantum mechanics, Fourier analysis (which includes applications to signal processing and heat transfer), and ergodic theory (which forms the mathematical underpinning of thermodynamics). John von Neumann coined the term Hilbert space for the abstract concept that underlies many of these diverse applications. The success of Hilbert space methods ushered in a very fruitful era for functional analysis. Apart from the classical Euclidean vector spaces, examples of Hilbert spaces include spaces of square-integrable functions, spaces of sequences, Sobolev spaces consisting of generalized functions, and Hardy spaces of holomorphic functions.

Geometric intuition plays an important role in many aspects of Hilbert space theory. Exact analogs of the Pythagorean theorem and parallelogram law hold in a Hilbert space. At a deeper level, perpendicular projection onto a linear subspace or a subspace (the analog of "dropping the altitude" of a triangle) plays a significant role in optimization problems and other aspects of the theory. An element of a Hilbert space can be uniquely specified by its coordinates with respect to an orthonormal basis, in analogy with Cartesian coordinates in classical geometry. When this basis is countably infinite, it allows identifying the Hilbert space with the space of the infinite sequences that are square-summable. The latter space is often in the older literature referred to as the Hilbert space.
$
10
Question 1: Who was the mathematician after whom Hilbert spaces were named?
A. John von Neumann
B. Frigyes Riesz
C. David Hilbert
D. Erhard Schmidt
E. Pythagoras
Answer: C

Question 2: Which statement best captures the essence of Hilbert spaces?
A. They are finite-dimensional vector spaces.
B. They deal primarily with matrices and determinant operations.
C. They allow the methods of linear algebra and calculus to be generalized to possibly infinite-dimensional spaces.
D. They focus on only real-valued vector spaces.
E. They are solely used in thermodynamics.
Answer: C

Question 3: In which century were the earliest Hilbert spaces studied?
A. 18th century
B. 19th century
C. 20th century
D. 21st century
E. 17th century
Answer: C

Question 4: For what purpose is Hilbert space an indispensable tool?
A. Only for quantum mechanics
B. Only for Fourier analysis
C. Only for ergodic theory
D. For partial differential equations, quantum mechanics, Fourier analysis, and ergodic theory.
E. Only for matrix theory
Answer: D

Question 5: Which of the following is NOT a property or concept related to Hilbert spaces?
A. Perpendicular projection
B. Pythagorean theorem
C. Parallelogram law
D. Determinant calculation
E. Orthonormal basis
Answer: D

Question 6: What kind of intuition plays a significant role in aspects of Hilbert space theory?
A. Algebraic intuition
B. Probabilistic intuition
C. Geometric intuition
D. Analytical intuition
E. Topological intuition
Answer: C

Question 7: When discussing the coordinates in a Hilbert space with respect to an orthonormal basis, what is the analogous concept in classical geometry?
A. Polar coordinates
B. Spherical coordinates
C. Cartesian coordinates
D. Cylindrical coordinates
E. Hyperbolic coordinates
Answer: C

Question 8: Which of the following is an example of a Hilbert space?
A. Space of non-integrable functions
B. Space of sequences
C. Space of discontinuous functions
D. Space of finite sequences only
E. Space of random variables
Answer: B

Question 9: Who coined the term "Hilbert space" for the abstract concept encompassing various applications?
A. David Hilbert
B. Frigyes Riesz
C. John von Neumann
D. Erhard Schmidt
E. Isaac Newton
Answer: C

Question 10: Which theorem or law holds true in Hilbert space, similar to classical Euclidean spaces?
A. Central limit theorem
B. Law of thermodynamics
C. Law of large numbers
D. Pythagorean theorem
E. Euler's formula
Answer: D
@
Subject:
The speed of light in vacuum, commonly denoted c, is a universal physical constant that is exactly equal to 299,792,458 metres per second (approximately 300,000 kilometres per second; 186,000 miles per second; 671 million miles per hour).[Note 3] According to the special theory of relativity, c is the upper limit for the speed at which conventional matter or energy (and thus any signal carrying information) can travel through space.[4][5][6]

All forms of electromagnetic radiation, including visible light, travel at the speed of light. For many practical purposes, light and other electromagnetic waves will appear to propagate instantaneously, but for long distances and very sensitive measurements, their finite speed has noticeable effects. Any starlight viewed on Earth is from the distant past, allowing humans to study the history of the universe by viewing distant objects. When communicating with distant space probes, it can take minutes to hours for signals to travel. In computing, the speed of light fixes the ultimate minimum communication delay. The speed of light can be used in time of flight measurements to measure large distances to extremely high precision.

Ole Rømer first demonstrated in 1676 that light does not travel instantaneously by studying the apparent motion of Jupiter's moon Io. Progressively more accurate measurements of its speed came over the following centuries. In a paper published in 1865, James Clerk Maxwell proposed that light was an electromagnetic wave and, therefore, travelled at speed c.[7] In 1905, Albert Einstein postulated that the speed of light c with respect to any inertial frame of reference is a constant and is independent of the motion of the light source.[8] He explored the consequences of that postulate by deriving the theory of relativity and, in doing so, showed that the parameter c had relevance outside of the context of light and electromagnetism.

Massless particles and field perturbations, such as gravitational waves, also travel at speed c in vacuum. Such particles and waves travel at c regardless of the motion of the source or the inertial reference frame of the observer. Particles with nonzero rest mass can be accelerated to approach c but can never reach it, regardless of the frame of reference in which their speed is measured. In the special and general theories of relativity, c interrelates space and time and also appears in the famous equation of mass–energy equivalence, E = mc2.[9]

In some cases, objects or waves may appear to travel faster than light (e.g., phase velocities of waves, the appearance of certain high-speed astronomical objects, and particular quantum effects). The expansion of the universe is understood to exceed the speed of light beyond a certain boundary.

The speed at which light propagates through transparent materials, such as glass or air, is less than c; similarly, the speed of electromagnetic waves in wire cables is slower than c. The ratio between c and the speed v at which light travels in a material is called the refractive index n of the material (n = 
c
/
v
). For example, for visible light, the refractive index of glass is typically around 1.5, meaning that light in glass travels at 
c
/
1.5
 ≈ 200000 km/s (124000 mi/s); the refractive index of air for visible light is about 1.0003, so the speed of light in air is about 90 km/s (56 mi/s) slower than c.
$
12
Question 1: What is the speed of light in vacuum?
A. 300,000,000 meters per second
B. 299,792,458 meters per second
C. 200,000 kilometers per second
D. 671,000 miles per hour
E. Both B and D
Answer: E

Question 2: According to the special theory of relativity, what is the upper limit for the speed at which matter or energy can travel through space?
A. Speed of sound
B. Speed of light (c)
C. Twice the speed of light
D. There is no upper limit
E. 671 million miles per hour
Answer: B

Question 3: Which of the following travels at the speed of light?
A. Sound waves
B. Water waves
C. Electromagnetic radiation
D. Airplanes
E. Cars
Answer: C

Question 4: Why is starlight viewed on Earth from the distant past?
A. Stars emit light slowly.
B. The universe is contracting.
C. Light from stars travels instantly.
D. Light has a finite speed, and stars are very distant.
E. Earth's atmosphere delays the light.
Answer: D

Question 5: Who first demonstrated in 1676 that light does not travel instantaneously?
A. Albert Einstein
B. James Clerk Maxwell
C. Isaac Newton
D. Ole Rømer
E. Galileo Galilei
Answer: D

Question 6: What did James Clerk Maxwell propose in 1865?
A. That light was a particle
B. That light was an electromagnetic wave
C. That light has infinite speed
D. That light has mass
E. That light cannot be refracted
Answer: B

Question 7: In Einstein's theory of relativity, how is the speed of light with respect to any inertial frame of reference?
A. Variable
B. Dependent on the motion of the light source
C. A constant
D. Infinite
E. Dependent on the medium
Answer: C

Question 8: Which particles or waves travel at speed c in vacuum, regardless of the motion of the source or the inertial reference frame of the observer?
A. Particles with nonzero rest mass
B. Sound waves
C. Gravitational waves
D. Water waves
E. Electrons
Answer: C

Question 9: What does the equation E = mc^2 represent?
A. Kinetic energy
B. Mass-energy equivalence
C. Conservation of momentum
D. The principle of relativity
E. Gravitational force
Answer: B

Question 10: When light propagates through materials like glass or air, its speed is:
A. Faster than c
B. Equal to c
C. Slower than c
D. Dependent on the color of the light
E. Unchanged regardless of the material
Answer: C

Question 11: How is the refractive index (n) of a material defined in relation to the speed of light?
A. n = v/c
B. n = c/v
C. n = c+v
D. n = c×v
E. n = v-c
Answer: B

Question 12: What is the approximate speed of light in glass, given that the refractive index of glass is around 1.5?
A. 299,792,458 m/s
B. 200,000 km/s
C. 150,000 km/s
D. 100,000 km/s
E. 50,000 km/s
Answer: B
@
Subject:
The Stefan–Boltzmann law, also known as Stefan's law, describes the intensity of the thermal radiation emitted by matter in terms of that matter's temperature. It is named for Josef Stefan, who empirically derived the relationship, and Ludwig Boltzmann who derived the law theoretically.

For an ideal absorber/emitter or black body, the Stefan–Boltzmann law states that the total energy radiated per unit surface area per unit time (also known as the radiant exitance) is directly proportional to the fourth power of the black body's temperature, T:
{\displaystyle M^{\circ }=\sigma \,T^{4}.}
The constant of proportionality, 
�\sigma , is called the Stefan–Boltzmann constant. It has a value
{\displaystyle \sigma =} 5.670374419...×10−8 W m−2 K−4 .
In the general case, the Stefan–Boltzmann law for radiant exitance takes the form:
{\displaystyle M=\varepsilon \,M^{\circ }=\varepsilon \,\sigma \,T^{4}}
where �\varepsilon  is the emissivity of the matter doing the emitting. The emissivity is generally between zero and one, although some exotic materials may have an emissivity greater than one. An emissivity of one corresponds to a black body.
$
9
Question 2: How is the total energy radiated by a black body per unit surface area per unit time related to its temperature, T?
A. Directly proportional to the square of T
B. Inversely proportional to the cube of T
C. Directly proportional to T
D. Inversely proportional to T
E. Directly proportional to the fourth power of T
Answer: E

Question 3: What is the value of the Stefan–Boltzmann constant,
�\sigma ?
A. 
1.380649
×
1
0
−
23
W m
−
2
K
−
4
1.380649×10 
−23
 W m 
−2
 K 
−4
 
B. 
3.14
×
1
0
−
2
W m
−
2
K
−
4
3.14×10 
−2
 W m 
−2
 K 
−4
 
C. 
5.670374419...
×
1
0
−
8
W m
−
2
K
−
4
5.670374419...×10 
−8
 W m 
−2
 K 
−4
 
D. 
6.62607015
×
1
0
−
2
W m
−
2
K
−
4
6.62607015×10 
−2
 W m 
−2
 K 
−4
 
E. 
9.81
W m
−
2
K
−
4
9.81W m 
−2
 K 
−4
 
Answer: C

Question 4: If an object has an emissivity of one, how is it described in terms of Stefan-Boltzmann law?
A. Grey body
B. Reflective body
C. White body
D. Black body
E. Transparent body
Answer: D

Question 5: The Stefan-Boltzmann law for radiant exitance of non-black bodies includes a factor that accounts for the deviation from ideal behavior. What is this factor called?
A. Reflectance factor
B. Transmission factor
C. Emissivity
D. Absorption coefficient
E. Radiation factor
Answer: C

Question 6: Who derived the Stefan–Boltzmann law theoretically?
A. Werner Heisenberg
B. Richard Feynman
C. Ludwig Boltzmann
D. James Clerk Maxwell
E. Enrico Fermi
Answer: C

Question 7: In the equation 
�
=
�
 
�
∘
=
�
 
�
 
�
4
M=εM 
∘
 =εσT 
4
 , what does 
�
∘
M 
∘
  represent?
A. Radiant exitance of a grey body
B. Reflectance of the material
C. Radiant exitance of a black body
D. Emissivity of the material
E. Stefan–Boltzmann constant
Answer: C

Question 8: For materials with an emissivity value greater than one, how can they be described?
A. Standard materials
B. Black bodies
C. Exotic materials
D. Reflective materials
E. Transparent materials
Answer: C

Question 9: What quantity does the Stefan–Boltzmann law describe in relation to the temperature of matter?
A. Reflectance
B. Transmission
C. Intensity of thermal radiation
D. Heat capacity
E. Thermal conductivity
Answer: C

Question 10: In the context of the Stefan–Boltzmann law, a black body ideally:
A. Reflects all incident radiation
B. Allows all radiation to pass through
C. Absorbs and emits all incident radiation
D. Does not interact with radiation
E. Scatters all incident radiation
Answer: C
@
Subject:
Star formation is the process by which dense regions within molecular clouds in interstellar space, sometimes referred to as "stellar nurseries" or "star-forming regions", collapse and form stars.[1] As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products. It is closely related to planet formation, another branch of astronomy. Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function. Most stars do not form in isolation but as part of a group of stars referred as star clusters or stellar associations.[2]

Spiral galaxies like the Milky Way contain stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 104 to 106 particles per cm3, and is typically composed of roughly 70% hydrogen, 28% helium, and 1.5% heavier elements by mass. The trace amounts of heavier elements were and are produced within stars via stellar nucleosynthesis and ejected as the stars pass beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or diffuse nebulae,[3] where star formation takes place.[4] In contrast to spiral galaxies, elliptical galaxies lose the cold component[definition needed] of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through mergers with other galaxies.[5]

In the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H2) form, so these nebulae are called molecular clouds.[4] The Herschel Space Observatory has revealed that filaments, or elongated dense gas structures, are truly ubiquitous in molecular clouds and central to the star formation process. They fragment into gravitationally bound cores, most of which will evolve into stars. Continuous accretion of gas, geometrical bending[definition needed], and magnetic fields may control the detailed manner in which the filaments are fragmented. Observations of supercritical filaments have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded protostars with outflows.[jargon][6]

Observations indicate that the coldest clouds tend to form low-mass stars, which are first observed via the infrared light they emit inside the clouds, and then as visible light when the clouds dissipate. Giant molecular clouds, which are generally warmer, produce stars of all masses.[7] These giant molecular clouds have typical densities of 100 particles per cm3, diameters of 100 light-years (9.5×1014 km), masses of up to 6 million solar masses (M☉), or six million times the mass of Earth's sun.[8] The average interior temperature is 10 K (−441.7 °F).

About half the total mass of the Milky Way's galactic ISM is found in molecular clouds[9] and the galaxy includes an estimated 6,000 molecular clouds, each with more than 100,000 M☉.[10] The nebula nearest to the Sun where massive stars are being formed is the Orion Nebula, 1,300 light-years (1.2×1016 km) away.[11] However, lower mass star formation is occurring about 400–450 light-years distant in the ρ Ophiuchi cloud complex.[12]

A more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules, so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently.[13] The Bok globules are typically up to a light-year across and contain a few solar masses.[14] They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.[15]
$
10
Question 1: Which of the following is sometimes referred to as "stellar nurseries"?
A. Star clusters
B. Protostars
C. Molecular clouds in interstellar space
D. Planet-forming regions
E. Young stellar objects
Answer: C

Question 2: What branch of astronomy is closely related to star formation?
A. Galaxy formation
B. Supernova study
C. Cosmology
D. Planet formation
E. Nebula classification
Answer: D

Question 3: What is the primary composition of the interstellar medium (ISM)?
A. Mostly helium and a small amount of hydrogen
B. Roughly 70% helium, 28% hydrogen, and 1.5% heavier elements
C. Roughly 70% hydrogen, 28% helium, and 1.5% heavier elements
D. Mostly heavier elements with a trace amount of hydrogen
E. Equal proportions of hydrogen, helium, and heavier elements
Answer: C

Question 4: How do elliptical galaxies differ from spiral galaxies in terms of their interstellar medium?
A. Elliptical galaxies have a higher proportion of hydrogen in their ISM.
B. Elliptical galaxies lose the cold component of their ISM quickly.
C. Elliptical galaxies continually form diffuse nebulae.
D. Elliptical galaxies have more molecular clouds.
E. There is no difference in the ISM of elliptical and spiral galaxies.
Answer: B

Question 5: The Herschel Space Observatory discovered that ______ are central to the star formation process in molecular clouds.
A. Supernovas
B. Black holes
C. Filaments
D. Binary star systems
E. Pulsars
Answer: C

Question 6: Which of the following best describes the temperatures inside giant molecular clouds?
A. Extremely hot, reaching temperatures comparable to the sun's surface
B. Moderate, with temperatures similar to Earth's average
C. Cold, with an average interior temperature of 10 K
D. Extremely variable, ranging from cold to hot
E. Generally warm, comparable to human body temperature
Answer: C

Question 7: Approximately how many molecular clouds, each with more than 100,000 M☉, does the Milky Way galaxy contain?
A. 60
B. 600
C. 6,000
D. 60,000
E. 600,000
Answer: C

Question 8: Which nebula nearest to the Sun is known for the formation of massive stars?
A. ρ Ophiuchi cloud complex
B. Milky Way Nebula
C. Andromeda Nebula
D. Orion Nebula
E. Crab Nebula
Answer: D

Question 9: What are Bok globules?
A. Massive galaxies known for star formation
B. Large, hot stars that emit high levels of radiation
C. Opaque clouds of dense gas and dust associated with star formation
D. Bright emission nebulae silhouetting against the Milky Way
E. Pockets of empty space within molecular clouds
Answer: C

Question 10: Where can Bok globules be observed?
A. As bright clouds in front of emission nebulae or background stars
B. Only through infrared telescopes
C. As dark clouds silhouetted against bright emission nebulae or background stars
D. Exclusively within the Milky Way galaxy
E. Surrounding supernovae remnants
Answer: C
@
Subject:
In chemistry, molecular symmetry describes the symmetry present in molecules and the classification of these molecules according to their symmetry. Molecular symmetry is a fundamental concept in chemistry, as it can be used to predict or explain many of a molecule's chemical properties, such as whether or not it has a dipole moment, as well as its allowed spectroscopic transitions. To do this it is necessary to use group theory. This involves classifying the states of the molecule using the irreducible representations from the character table of the symmetry group of the molecule. Symmetry is useful in the study of molecular orbitals, with applications to the Hückel method, to ligand field theory, and to the Woodward-Hoffmann rules. Many university level textbooks on physical chemistry, quantum chemistry, spectroscopy and inorganic chemistry discuss symmetry.[1][2][3][4][5][6] Another framework on a larger scale is the use of crystal systems to describe crystallographic symmetry in bulk materials.

There are many techniques for determining the symmetry of a given molecule, including X-ray crystallography and various forms of spectroscopy. Spectroscopic notation is based on symmetry considerations.

In group theory, geometry, representation theory and molecular geometry, a symmetry operation is a geometric transformation of an object that leaves the object looking the same after it has been carried out. For example, as transformations of an object in space, rotations, reflections and inversions are all symmetry operations. Such symmetry operations are performed with respect to symmetry elements (for example, a point, line or plane).[1] In the context of molecular symmetry, a symmetry operation is a permutation of atoms such that the molecule or crystal is transformed into a state indistinguishable from the starting state. Two basic facts follow from this definition, which emphasizes its usefulness.

Physical properties must be invariant with respect to symmetry operations.
Symmetry operations can be collected together in groups which are isomorphic to permutation groups.
In the context of molecular symmetry, quantum wavefunctions need not be invariant, because the operation can multiply them by a phase or mix states within a degenerate representation, without affecting any physical property.

The identity operation corresponds to doing nothing to the object. Because every molecule is indistinguishable from itself if nothing is done to it, every object possesses at least the identity operation. The identity operation is denoted by E or I. In the identity operation, no change can be observed for the molecule. Even the most asymmetric molecule possesses the identity operation. The need for such an identity operation arises from the mathematical requirements of group theory.

Reflection operation
The reflection operation is carried out with respect to symmetry elements known as planes of symmetry or mirror planes.[2] Each such plane is denoted as σ (sigma). Its orientation relative to the principal axis of the molecule is indicated by a subscript. The plane must pass through the molecule and cannot be completely outside it.

If the plane of symmetry contains the principal axis of the molecule (i.e., the molecular z-axis), it is designated as a vertical mirror plane, which is indicated by a subscript v (σv).
If the plane of symmetry is perpendicular to the principal axis, it is designated as a horizontal mirror plane, which is indicated by a subscript h (σh).
If the plane of symmetry bisects the angle between two 2-fold axes perpendicular to the principal axis, it is designated as a dihedral mirror plane, which is indicated by a subscript d (σd).
Through the reflection of each mirror plane, the molecule must be able to produce an identical image of itself.
$
10
Question 1: Molecular symmetry is essential in chemistry because it can help predict or explain which of the following properties?
A. The molecule's dipole moment
B. The molecule's melting point
C. The molecule's flammability
D. The molecule's color
E. The molecule's smell
Answer: A

Question 2: Which of the following techniques is NOT mentioned as a way to determine the symmetry of a molecule?
A. UV-Vis spectroscopy
B. Hückel method
C. X-ray crystallography
D. IR spectroscopy
E. Mass spectrometry
Answer: E

Question 3: What does the identity operation (E or I) in molecular symmetry signify?
A. Rotation of the molecule by 90°
B. Reflection of the molecule across a plane
C. Doing nothing to the molecule
D. Inversion of the molecule about a point
E. Rotation of the molecule by 180°
Answer: C

Question 4: What does a vertical mirror plane (σv) in the context of molecular symmetry represent?
A. A plane of symmetry perpendicular to the principal axis
B. A plane of symmetry that bisects the angle between two 2-fold axes perpendicular to the principal axis
C. A plane of symmetry that contains the principal axis of the molecule
D. A plane of symmetry outside the molecule
E. None of the above
Answer: C

Question 5: If a molecule possesses a horizontal mirror plane (σh), how is it oriented with respect to the principal axis of the molecule?
A. Contains the principal axis
B. Perpendicular to the principal axis
C. Parallel to the principal axis
D. Bisects the angle between two 2-fold axes perpendicular to the principal axis
E. None of the above
Answer: B

Question 6: In the context of molecular symmetry, what is the result of performing a symmetry operation?
A. The molecule is transformed into a state completely different from the starting state
B. The molecule disappears
C. The molecule is transformed into a state indistinguishable from the starting state
D. The molecule changes its molecular weight
E. None of the above
Answer: C

Question 7: A symmetry operation that can multiply a quantum wavefunction by a phase or mix states within a degenerate representation without affecting any physical property implies that:
A. The wavefunction must always be invariant
B. Physical properties change with symmetry operations
C. The wavefunction need not be invariant
D. All physical properties remain invariant
E. The wavefunction always remains invariant
Answer: C

Question 8: What is the role of a dihedral mirror plane (σd) in molecular symmetry?
A. It bisects the angle between two 2-fold axes perpendicular to the principal axis
B. It is perpendicular to the principal axis
C. It is completely outside the molecule
D. It contains the principal axis of the molecule
E. None of the above
Answer: A

Question 9: Why is the identity operation (E or I) essential in the context of molecular symmetry?
A. It allows for rotation of molecules
B. It is required for the mathematical principles of group theory
C. It represents the most asymmetrical molecule
D. It is needed to reflect molecules across planes
E. None of the above
Answer: B

Question 10: In group theory, which of the following is NOT an example of a symmetry operation?
A. Rotation
B. Inversion
C. Diffusion
D. Reflection
E. Translation
Answer: C
@
Subject:
In mathematics, a regular polytope is a polytope whose symmetry group acts transitively on its flags, thus giving it the highest degree of symmetry. All its elements or j-faces (for all 0 ≤ j ≤ n, where n is the dimension of the polytope) — cells, faces and so on — are also transitive on the symmetries of the polytope, and are regular polytopes of dimension ≤ n.

Regular polytopes are the generalized analog in any number of dimensions of regular polygons (for example, the square or the regular pentagon) and regular polyhedra (for example, the cube). The strong symmetry of the regular polytopes gives them an aesthetic quality that interests both non-mathematicians and mathematicians.

Classically, a regular polytope in n dimensions may be defined as having regular facets ([n–1]-faces) and regular vertex figures. These two conditions are sufficient to ensure that all faces are alike and all vertices are alike. Note, however, that this definition does not work for abstract polytopes.

A regular polytope can be represented by a Schläfli symbol of the form {a, b, c, ..., y, z}, with regular facets as {a, b, c, ..., y}, and regular vertex figures as {b, c, ..., y, z}.
$
10
Question 1: Regular polygons and regular polyhedra are considered to be:
A. Irregular polytopes
B. Analog of irregular polytopes in two and three dimensions respectively
C. Regular polytopes in two and three dimensions respectively
D. Both are regular polytopes, but dimension is not specific
E. Neither are considered to be regular polytopes
Answer: C

Question 2: What is a defining feature of a regular polytope in terms of its elements or j-faces?
A. They have symmetries that are intransitive on the polytope
B. All its elements are irregular polytopes of dimension ≤ n
C. All its elements are transitive on the symmetries of the polytope
D. The symmetries act differently for different elements
E. Only some of its elements are regular polytopes of dimension ≤ n
Answer: C

Question 3: Which of the following is NOT true about the regular polytope?
A. The strong symmetry makes them aesthetically appealing
B. They can be represented by a Schläfli symbol
C. They have irregular vertex figures
D. All their vertices are alike
E. All their faces are alike
Answer: C

Question 4: For a regular polytope in n dimensions, its regular facets ([n–1]-faces) are represented by which part of the Schläfli symbol?
A. {a, b, c, ..., z}
B. {a, b, c, ..., y}
C. {b, c, ..., z}
D. {a, b, c}
E. {y, z}
Answer: B

Question 5: In the context of the definition for abstract polytopes, what is said about the classic definition of a regular polytope?
A. It works perfectly for abstract polytopes
B. It is slightly adjusted for abstract polytopes
C. It is the same for all polytopes
D. It does not work for abstract polytopes
E. It works but with minor exceptions
Answer: D

Question 6: What can be said about the vertex figures of a regular polytope based on the classic definition?
A. They are irregular
B. They vary in symmetry
C. They are regular
D. They are not needed to define a regular polytope
E. Their dimension is always n-1
Answer: C

Question 7: Regular polytopes of what dimensions are generalized analogs of regular polygons?
A. n dimensions, where n is not fixed
B. Only two dimensions
C. Only three dimensions
D. Either two or three dimensions
E. n-1 dimensions, where n is the dimension of the polytope
Answer: A

Question 8: The highest degree of symmetry in a regular polytope implies that:
A. All faces are distinct
B. Only some elements are transitive on the symmetries of the polytope
C. Every element looks the same under its symmetries
D. The polytope has no symmetry
E. Symmetry is only transitive on its edges
Answer: C

Question 9: What aspect of regular polytopes attracts both mathematicians and non-mathematicians?
A. Their complexity
B. Their irregularity
C. Their strong symmetry giving them an aesthetic quality
D. Their abstract nature
E. Their rarity in nature
Answer: C

Question 10: If a regular polytope has a Schläfli symbol of the form {a, b, c, ..., y, z}, what represents its regular vertex figures?
A. {a, b, c}
B. {a, b, c, ..., z}
C. {b, c, ..., y}
D. {b, c, ..., y, z}
E. {y, z}
Answer: D
@
Subject:
An electric field (sometimes E-field[1]) is the physical field that surrounds electrically charged particles and exerts force on all other charged particles in the field, either attracting or repelling them.[2] It also refers to the physical field for a system of charged particles.[3] Electric fields originate from electric charges and time-varying electric currents. Electric fields and magnetic fields are both manifestations of the electromagnetic field, one of the four fundamental interactions (also called forces) of nature.

Electric fields are important in many areas of physics, and are exploited in electrical technology. In atomic physics and chemistry, for instance, the interaction in the electric field between the atomic nucleus and electrons is the force that holds these particles together in atoms. Similarly, the interaction in the electric field between atoms is the force responsible for chemical bonding that result in molecules.

The electric field is defined as a vector field that associates to each point in space the electrostatic (Coulomb) force per unit of charge exerted on an infinitesimal positive test charge at rest at that point.[4][5][6] The derived SI unit for the electric field is the volt per meter (V/m), which is equal to the newton per coulomb (N/C).[7]

The electric field is defined at each point in space as the force per unit charge that would be experienced by a vanishingly small positive test charge if held stationary at that point.[8]: 469–70  As the electric field is defined in terms of force, and force is a vector (i.e. having both magnitude and direction), it follows that an electric field is a vector field.[8]: 469–70  Fields that may be defined in this manner are sometimes referred to as force fields. The electric field acts between two charges similarly to the way the gravitational field acts between two masses, as they both obey an inverse-square law with distance.[9] This is the basis for Coulomb's law, which states that, for stationary charges, the electric field varies with the source charge and varies inversely with the square of the distance from the source. This means that if the source charge were doubled, the electric field would double, and if you move twice as far away from the source, the field at that point would be only one-quarter its original strength.

The electric field can be visualized with a set of lines whose direction at each point is the same as the field's, a concept introduced by Michael Faraday,[10] whose term 'lines of force' is still sometimes used. This illustration has the useful property that the field's strength is proportional to the density of the lines.[11] Field lines due to stationary charges have several important properties, including always originating from positive charges and terminating at negative charges, they enter all good conductors at right angles, and they never cross or close in on themselves.[8]: 479  The field lines are a representative concept; the field actually permeates all the intervening space between the lines. More or fewer lines may be drawn depending on the precision to which it is desired to represent the field.[10] The study of electric fields created by stationary charges is called electrostatics.

Faraday's law describes the relationship between a time-varying magnetic field and the electric field. One way of stating Faraday's law is that the curl of the electric field is equal to the negative time derivative of the magnetic field.[12]: 327  In the absence of time-varying magnetic field, the electric field is therefore called conservative (i.e. curl-free).[12]: 24, 90–91  This implies there are two kinds of electric fields: electrostatic fields and fields arising from time-varying magnetic fields.[12]: 305–307  While the curl-free nature of the static electric field allows for a simpler treatment using electrostatics, time-varying magnetic fields are generally treated as a component of a unified electromagnetic field. The study of time varying magnetic and electric fields is called electrodynamics.
$
15
Question 1: Which of the following describes the field surrounding electrically charged particles?
A. Gravitational field
B. Magnetic field
C. Electrostatic field
D. Electric field
E. Quantum field
Answer: D

Question 2: Electric fields and magnetic fields are both manifestations of:
A. Gravitational force
B. The electromagnetic field
C. Nuclear forces
D. Quantum fields
E. Residual force
Answer: B

Question 3: What holds the atomic nucleus and electrons together in atoms?
A. Magnetic interaction
B. Gravitational interaction
C. Interaction in the electric field
D. Strong nuclear force
E. Weak nuclear force
Answer: C

Question 4: The derived SI unit for the electric field is:
A. Newton
B. Coulomb
C. Volt per meter (V/m)
D. Ohm
E. Watt
Answer: C

Question 5: The electric field is a vector because:
A. It only has magnitude
B. It only has direction
C. It has both magnitude and direction
D. It is scalar in nature
E. It depends on the test charge
Answer: C

Question 6: Which law states that the electric field varies with the source charge and varies inversely with the square of the distance from the source?
A. Ohm's law
B. Faraday's law
C. Coulomb's law
D. Maxwell's equations
E. Ampere's law
Answer: C

Question 7: Who introduced the concept to visualize the electric field with a set of lines?
A. Isaac Newton
B. James Clerk Maxwell
C. Michael Faraday
D. Albert Einstein
E. Niels Bohr
Answer: C

Question 8: In the absence of a time-varying magnetic field, the electric field is called:
A. Electromagnetic
B. Electrostatic
C. Electrodynamics
D. Conservative
E. Electrokinetic
Answer: D

Question 9: The study of electric fields created by stationary charges is termed as:
A. Electrodynamics
B. Electrostatics
C. Electromagnetism
D. Electromechanics
E. Electrophysiology
Answer: B

Question 10: Which law describes the relationship between a time-varying magnetic field and the electric field?
A. Newton's law
B. Coulomb's law
C. Ohm's law
D. Faraday's law
E. Maxwell's equations
Answer: D

Question 11: Fields that may be defined in terms of force are sometimes referred to as:
A. Scalar fields
B. Quantum fields
C. Electromagnetic fields
D. Gravitational fields
E. Force fields
Answer: E

Question 12: If the source charge is doubled, what happens to the electric field?
A. It becomes half
B. It remains the same
C. It becomes four times
D. It doubles
E. It becomes one-quarter its original strength
Answer: D

Question 13: The electric field is defined at each point in space as the force per unit charge that would be experienced by:
A. A large negative test charge
B. A large positive test charge
C. A vanishingly small negative test charge
D. A vanishingly small positive test charge
E. A neutral test charge
Answer: D

Question 14: The study of time-varying magnetic and electric fields is termed as:
A. Electrostatics
B. Electromagnetism
C. Electromechanics
D. Electrodynamics
E. Electrophysiology
Answer: D

Question 15: Which field is responsible for the force responsible for chemical bonding that results in molecules?
A. Magnetic field
B. Gravitational field
C. Electric field
D. Nuclear force
E. Quantum field
Answer: C
@
Subject:
In physics, angular momentum (sometimes called moment of momentum or rotational momentum) is the rotational analog of linear momentum. It is an important physical quantity because it is a conserved quantity – the total angular momentum of a closed system remains constant. Angular momentum has both a direction and a magnitude, and both are conserved. Bicycles and motorcycles, flying discs,[1] rifled bullets, and gyroscopes owe their useful properties to conservation of angular momentum. Conservation of angular momentum is also why hurricanes[2] form spirals and neutron stars have high rotational rates. In general, conservation limits the possible motion of a system, but it does not uniquely determine it.

The three-dimensional angular momentum for a point particle is classically represented as a pseudovector r × p, the cross product of the particle's position vector r (relative to some origin) and its momentum vector; the latter is p = mv in Newtonian mechanics. Unlike linear momentum, angular momentum depends on where this origin is chosen, since the particle's position is measured from it.

Angular momentum is an extensive quantity; that is, the total angular momentum of any composite system is the sum of the angular momenta of its constituent parts. For a continuous rigid body or a fluid, the total angular momentum is the volume integral of angular momentum density (angular momentum per unit volume in the limit as volume shrinks to zero) over the entire body.

Similar to conservation of linear momentum, where it is conserved if there is no external force, angular momentum is conserved if there is no external torque. Torque can be defined as the rate of change of angular momentum, analogous to force. The net external torque on any system is always equal to the total torque on the system; in other words, the sum of all internal torques of any system is always 0 (this is the rotational analogue of Newton's third law of motion). Therefore, for a closed system (where there is no net external torque), the total torque on the system must be 0, which means that the total angular momentum of the system is constant. The change in angular momentum for a particular interaction is called angular impulse, sometimes twirl.[3] Angular impulse is the angular analog of (linear) impulse.

A rotational analog of Newton's third law of motion might be written, "In a closed system, no torque can be exerted on any matter without the exertion on some other matter of an equal and opposite torque about the same axis."[20] Hence, angular momentum can be exchanged between objects in a closed system, but total angular momentum before and after an exchange remains constant (is conserved).[21]

Seen another way, a rotational analogue of Newton's first law of motion might be written, "A rigid body continues in a state of uniform rotation unless acted by an external influence."[20] Thus with no external influence to act upon it, the original angular momentum of the system remains constant.[22]

The conservation of angular momentum is used in analyzing central force motion. If the net force on some body is directed always toward some point, the center, then there is no torque on the body with respect to the center, as all of the force is directed along the radius vector, and none is perpendicular to the radius.  Therefore, the angular momentum of the body about the center is constant. This is the case with gravitational attraction in the orbits of planets and satellites, where the gravitational force is always directed toward the primary body and orbiting bodies conserve angular momentum by exchanging distance and velocity as they move about the primary. Central force motion is also used in the analysis of the Bohr model of the atom.

For a planet, angular momentum is distributed between the spin of the planet and its revolution in its orbit, and these are often exchanged by various mechanisms. The conservation of angular momentum in the Earth–Moon system results in the transfer of angular momentum from Earth to Moon, due to tidal torque the Moon exerts on the Earth. This in turn results in the slowing down of the rotation rate of Earth, at about 65.7 nanoseconds per day,[23] and in gradual increase of the radius of Moon's orbit, at about 3.82 centimeters per year.[24]


The torque caused by the two opposing forces Fg and −Fg causes a change in the angular momentum L in the direction of that torque (since torque is the time derivative of angular momentum). This causes the top to precess.
The conservation of angular momentum explains the angular acceleration of an ice skater as they bring their arms and legs close to the vertical axis of rotation. By bringing part of the mass of their body closer to the axis, they decrease their body's moment of inertia. Because angular momentum is the product of moment of inertia and angular velocity, if the angular momentum remains constant (is conserved), then the angular velocity (rotational speed) of the skater must increase.

The same phenomenon results in extremely fast spin of compact stars (like white dwarfs, neutron stars and black holes) when they are formed out of much larger and slower rotating stars.

Conservation is not always a full explanation for the dynamics of a system but is a key constraint. For example, a spinning top is subject to gravitational torque making it lean over and change the angular momentum about the nutation axis, but neglecting friction at the point of spinning contact, it has a conserved angular momentum about its spinning axis, and another about its precession axis. Also, in any planetary system, the planets, star(s), comets, and asteroids can all move in numerous complicated ways, but only so that the angular momentum of the system is conserved.

Noether's theorem states that every conservation law is associated with a symmetry (invariant) of the underlying physics. The symmetry associated with conservation of angular momentum is rotational invariance. The fact that the physics of a system is unchanged if it is rotated by any angle about an axis implies that angular momentum is conserved.[25]
$
7
Question 1: Which of the following is the rotational analog of linear momentum?
A: Force
B: Torque
C: Angular Momentum
D: Impulse
E: Velocity
Answer: C

Question 2: What causes the angular acceleration of an ice skater when they bring their arms and legs close to the vertical axis of rotation?
A: Increase in their body weight
B: Decrease in their body's moment of inertia
C: Increase in the external torque applied
D: Change in the external gravitational force
E: Change in the frictional force with the ice
Answer: B

Question 3: The conservation of angular momentum in the Earth–Moon system has what impact on the Earth's rotation and Moon's orbit?
A: Increases the Earth's rotation rate and decreases the radius of Moon's orbit
B: Decreases the Earth's rotation rate and decreases the radius of Moon's orbit
C: Increases the Earth's rotation rate and increases the radius of Moon's orbit
D: Decreases the Earth's rotation rate and increases the radius of Moon's orbit
E: No change to either
Answer: D

Question 4: What does Noether's theorem state about every conservation law?
A: Every conservation law is based on experimental evidence.
B: Every conservation law must have a mathematical proof.
C: Every conservation law is associated with a symmetry of the underlying physics.
D: Every conservation law is time-dependent.
E: Every conservation law is associated with a physical constraint.
Answer: C

Question 5: In the case of gravitational attraction in orbits, why is angular momentum conserved?
A: Because gravitational force is always directed outward from the primary body.
B: Because gravitational force has a torque component that changes angular momentum.
C: Because gravitational force is always directed toward the primary body, resulting in no torque on the orbiting body about the center.
D: Because gravitational force varies with the inverse square of the distance.
E: Because of the gravitational time dilation effects.
Answer: C

Question 6: Why do compact stars like neutron stars and black holes spin extremely fast when formed from larger, slower rotating stars?
A: Because they gain external torque from other celestial objects.
B: Because their mass significantly decreases during formation.
C: Because their moment of inertia decreases, and since angular momentum is conserved, their angular velocity must increase.
D: Because gravitational force acts stronger on them than on larger stars.
E: Because of the intense radiation they emit during formation.
Answer: C

Question 7: If there is no external torque on a system, what can be said about its angular momentum?
A: The angular momentum increases over time.
B: The angular momentum decreases over time.
C: The angular momentum oscillates between high and low values.
D: The angular momentum remains constant.
E: The angular momentum becomes zero.
Answer: D
@
Subject:
In mathematics and physics, the right-hand rule is a common mnemonic for understanding the orientation of axes in three-dimensional space. It is also a convenient method for quickly finding the direction of the cross product of two vectors. Rather than a mathematical fact, it is a convention, closely related to the convention that rotation around an axis is positive if the sense of rotation as seen from the axis is counterclockwise and negative if it is clockwise.

Most left-hand and right-hand rules arise from the fact that the three axes of three-dimensional space have two possible orientations.[1] One can see this by holding one's hands outward and together, palms up, with the thumbs out-stretched to the right and left, and the fingers making a curling motion from straight outward to pointing upward. If the curling motion of the fingers represents a movement from the first (x-axis) to the second (y-axis), then the third (z-axis) can point along either thumb. Left-hand and right-hand rules arise when dealing with coordinate axes. The rule can be used to find the direction of the magnetic field, rotation, spirals, electromagnetic fields, mirror images, and enantiomers in mathematics and chemistry.

The sequence is often: index finger along the first vector, then middle finger along the second, then thumb along the third. Two other sequences also work because they preserve the cyclic nature of the cross product (and the underlying Levi-Civita symbol):

Middle finger, thumb, index finger.
Thumb, index finger, middle finger.

Helices and screws

A helix is a curved line formed by a point rotating around a center while the center moves up or down the z-axis. Helices are either right- or left-handed, with curled fingers giving the direction of rotation and thumb giving the direction of advance along the z-axis.

The threads of a screw are helical and therefore screws can be right- or left-handed. To properly fasten or unfasten a screw, one applies the above rules: if a screw is right-handed, pointing one's right thumb in the direction of the hole and turning in the direction of the right hand's curled fingers (i.e. clockwise) will fasten the screw, while pointing away from the hole and turning in the new direction (i.e. counterclockwise) will unfasten the screw.
$
8
Question 1: The right-hand rule in mathematics and physics primarily helps to determine the orientation of which of the following?
A: Mathematical equations
B: 2D plane
C: Physical constants
D: Axes in three-dimensional space
E: Quantum states
Answer: D

Question 2: If you were to use the right-hand rule for a vector cross product and place your index finger along the first vector, which finger would you place along the second vector?
A: Thumb
B: Ring finger
C: Middle finger
D: Pinky
E: Index finger
Answer: C

Question 3: What characteristic makes a helix right- or left-handed?
A: The length of the curve
B: The curvature degree of the line
C: The direction of rotation around a center while the center moves along the z-axis
D: The width of the helix
E: The helix's overall size
Answer: C

Question 4: For a right-handed screw, in which direction should you turn it (when viewing from above) to fasten or tighten it into a hole?
A: Upwards
B: Downwards
C: Counterclockwise
D: Clockwise
E: Neither, as screws don't follow the right-hand rule
Answer: D

Question 5: The right-hand rule is primarily a result of which of the following?
A: Mathematical fact
B: Physical constant
C: Universal truth
D: Convention
E: Geometry theorem
Answer: D

Question 6: Which sequence of fingers preserves the cyclic nature of the cross product when applying the right-hand rule?
A: Pinky, ring finger, middle finger
B: Thumb, pinky, index finger
C: Middle finger, thumb, index finger
D: Index finger, pinky, ring finger
E: Ring finger, thumb, pinky
Answer: C

Question 7: If one applies the right-hand rule to determine the orientation of helices and screws, the thumb provides the direction for which of the following?
A: The width of the helix
B: The curvature of the helix
C: The rotation around the center
D: Advance along the z-axis
E: Tensile strength of the helix
Answer: D

Question 8: Why are left-hand and right-hand rules important when dealing with three-dimensional space?
A: Because they define the curvature of space
B: Because the three axes of three-dimensional space have two possible orientations
C: Because they provide an alternative to complex calculations
D: Because they unify the laws of physics
E: Because they help in quantum mechanics predictions
Answer: B
@
Subject:
A spacetime diagram is a graphical illustration of objects' locations in space at various times, especially in the special theory of relativity. Spacetime diagrams can show the geometry underlying phenomena like time dilation and length contraction without mathematical equations.

The history of an object's location through time traces out a line or curve on a spacetime diagram, referred to as the object's world line. Each point in a spacetime diagram represents a unique position in space and time and is referred to as an event.

The most well-known class of spacetime diagrams are known as Minkowski diagrams, developed by Hermann Minkowski in 1908. Minkowski diagrams are two-dimensional graphs that depict events as happening in a universe consisting of one space dimension and one time dimension. Unlike a regular distance-time graph, the distance is displayed on the horizontal axis and time on the vertical axis. Additionally, the time and space units of measurement are chosen in such a way that an object moving at the speed of light is depicted as following a 45° angle to the diagram's axes.

The term Minkowski diagram refers to a specific form of spacetime diagram frequently used in special relativity. A Minkowski diagram is a two-dimensional graphical depiction of a portion of Minkowski space, usually where space has been curtailed to a single dimension. The units of measurement in these diagrams are taken such that the light cone at an event consists of the lines of slope plus or minus one through that event.[3] The horizontal lines correspond to the usual notion of simultaneous events for a stationary observer at the origin.

A particular Minkowski diagram illustrates the result of a Lorentz transformation. The Lorentz transformation relates two inertial frames of reference, where an observer stationary at the event (0, 0) makes a change of velocity along the x-axis. As shown in Fig 2-1, the new time axis of the observer forms an angle α with the previous time axis, with α < 
π
/
4
. In the new frame of reference the simultaneous events lie parallel to a line inclined by α to the previous lines of simultaneity. This is the new x-axis. Both the original set of axes and the primed set of axes have the property that they are orthogonal with respect to the Minkowski inner product or relativistic dot product.

Whatever the magnitude of α, the line ct = x forms the universal[4] bisector, as shown in Fig 2-2.

One frequently encounters Minkowski diagrams where the time units of measurement are scaled by a factor of c such that one unit of x equals one unit of t. Such a diagram may have units of

Approximately 30 centimetres length and nanoseconds
Astronomical units and intervals of about 8 minutes and 19 seconds (499 seconds)
Light years and years
Light-second and second
With that, light paths are represented by lines parallel to the bisector between the axes.
$
10
Question 1: Which graphical representation illustrates the history of an object's location through time in spacetime diagrams?
A: Event point
B: Minkowski space
C: World line
D: Light cone
E: Lorentz transformation
Answer: C

Question 2: In Minkowski diagrams, what is typically represented on the horizontal axis?
A: Energy
B: Speed
C: Time
D: Distance
E: Acceleration
Answer: D

Question 3: What unique characteristic does an object moving at the speed of light have on a Minkowski diagram?
A: It follows a horizontal line parallel to the x-axis.
B: It follows a line at a 45° angle to the diagram's axes.
C: It follows a vertical line parallel to the y-axis.
D: It creates a loop in the diagram.
E: It remains stationary at the origin.
Answer: B

Question 4: In a Minkowski diagram, which term refers to a specific position in space and time?
A: Light cone
B: World line
C: Lorentz transformation
D: Event
E: Minkowski space
Answer: D

Question 5: When referring to the light paths in a Minkowski diagram where the time units of measurement are scaled by a factor of c, how are these paths represented?
A: Vertically parallel to the y-axis.
B: Horizontally parallel to the x-axis.
C: In circles around the origin.
D: Parallel to the bisector between the axes.
E: At random angles depending on the speed.
Answer: D

Question 6: A Minkowski diagram specifically portrays a segment of which space, usually reduced to a single dimension?
A: Einsteinian space
B: Quantum space
C: Newtonian space
D: Minkowski space
E: Euclidean space
Answer: D

Question 7: In the context of a Lorentz transformation on a Minkowski diagram, what does the new time axis of an observer depict?
A: A horizontal alignment with the x-axis.
B: A 45° angle with the previous time axis.
C: A vertical alignment with the y-axis.
D: An angle α with the previous time axis, with α < π/4.
E: An angle perpendicular to the previous time axis.
Answer: D

Question 8: In a Minkowski diagram, which lines correspond to simultaneous events for a stationary observer at the origin?
A: The lines parallel to the x-axis.
B: The lines at a 45° angle.
C: The lines perpendicular to the x-axis.
D: The lines forming the light cone.
E: The lines parallel to the bisector between the axes.
Answer: A

Question 9: Which unit pairing does NOT correspond to the units of measurement one might encounter in a Minkowski diagram?
A: Light years and years
B: Approximately 30 centimetres length and nanoseconds
C: Light-second and second
D: Kilometers and milliseconds
E: Astronomical units and intervals of about 8 minutes and 19 seconds (499 seconds)
Answer: D

Question 10: The Minkowski diagram aids in providing a geometrical interpretation to the generalization of which transformation to relativistic mechanics?
A: Quantum transformation
B: Minkowski transformation
C: Newtonian transformation
D: Einsteinian transformation
E: Lorentz transformation
Answer: E
@
Subject:
In physical cosmology, baryogenesis (also known as baryosynthesis[1][2]) is the physical process that is hypothesized to have taken place during the early universe to produce baryonic asymmetry, i.e. the imbalance of matter (baryons) and antimatter (antibaryons) in the observed universe.[3]

One of the outstanding problems in modern physics is the predominance of matter over antimatter in the universe. The universe, as a whole, seems to have a nonzero positive baryon number density. Since it is assumed in cosmology that the particles we see were created using the same physics we measure today, it would normally be expected that the overall baryon number should be zero, as matter and antimatter should have been created in equal amounts. A number of theoretical mechanisms are proposed to account for this discrepancy, namely identifying conditions that favour symmetry breaking and the creation of normal matter (as opposed to antimatter). This imbalance has to be exceptionally small, on the order of 1 in every 1630000000 (≈2×109) particles a small fraction of a second after the Big Bang.[4] After most of the matter and antimatter was annihilated, what remained was all the baryonic matter in the current universe, along with a much greater number of bosons. Experiments reported in 2010 at Fermilab, however, seem to show that this imbalance is much greater than previously assumed.[5] These experiments involved a series of particle collisions and found that the amount of generated matter was approximately 1% larger than the amount of generated antimatter. The reason for this discrepancy is not yet known.

Most grand unified theories explicitly break the baryon number symmetry, which would account for this discrepancy, typically invoking reactions mediated by very massive X bosons (
X
) or massive Higgs bosons (
H0
).[6] The rate at which these events occur is governed largely by the mass of the intermediate 
X
 or 
H0
 particles, so by assuming these reactions are responsible for the majority of the baryon number seen today, a maximum mass can be calculated above which the rate would be too slow to explain the presence of matter today.[7] These estimates predict that a large volume of material will occasionally exhibit a spontaneous proton decay, which has not been observed. Therefore, the imbalance between matter and antimatter remains a mystery.

Baryogenesis theories are based on different descriptions of the interaction between fundamental particles. Two main theories are electroweak baryogenesis (standard model), which would occur during the electroweak phase transition, and the GUT baryogenesis, which would occur during or shortly after the grand unification epoch. Quantum field theory and statistical physics are used to describe such possible mechanisms.

Baryogenesis is followed by primordial nucleosynthesis, when atomic nuclei began to form.
$
10
Question 1: What does baryogenesis attempt to explain in the early universe?
A: The origin of dark matter
B: The uniformity of cosmic microwave background radiation
C: The creation of the first black holes
D: The imbalance of matter and antimatter in the observed universe
E: The rapid expansion of space during the inflationary period
Answer: D

Question 2: What notable discovery was made at Fermilab in 2010 regarding the matter-antimatter imbalance?
A: Matter and antimatter are actually equal in the universe.
B: The imbalance between generated matter and generated antimatter was found to be approximately 1%.
C: Antimatter no longer exists in the universe.
D: Antimatter has the potential to convert into matter.
E: There was no significant difference between the amounts of generated matter and antimatter.
Answer: B

Question 3: Which of the following has NOT been observed, casting doubt on some theories that try to explain the matter-antimatter imbalance?
A: Proton decay
B: Electroweak phase transition
C: Spontaneous breaking of baryon number symmetry
D: Boson creation
E: Quantum field interactions
Answer: A

Question 4: Which event follows baryogenesis in the chronology of the early universe?
A: The grand unification epoch
B: The inflationary period
C: Dark matter generation
D: Primordial nucleosynthesis
E: Cosmic microwave background radiation emission
Answer: D

Question 5: In grand unified theories, which particles are often invoked to explain the breaking of baryon number symmetry?
A: Photons and neutrinos
B: Gluons and leptons
C: X bosons and massive Higgs bosons
D: Quarks and fermions
E: W and Z bosons
Answer: C

Question 6: During which phase would electroweak baryogenesis potentially occur?
A: Grand unification epoch
B: Dark matter formation phase
C: Electroweak phase transition
D: Cosmic inflation phase
E: Primordial nucleosynthesis phase
Answer: C

Question 7: What is the primary reason the imbalance between matter and antimatter remains an unresolved mystery in modern physics?
A: Because the laws of physics should produce an equal amount of matter and antimatter.
B: Because experiments show a larger amount of antimatter than matter.
C: Because the universe began with only antimatter.
D: Because antimatter is invisible to current telescopic technology.
E: Because the inflationary theory disproves baryogenesis.
Answer: A

Question 8: Which fields of physics primarily describe the potential mechanisms for baryogenesis?
A: Classical mechanics and general relativity
B: Thermodynamics and quantum mechanics
C: Quantum field theory and statistical physics
D: Electrodynamics and string theory
E: Solid-state physics and atomic physics
Answer: C

Question 9: Which theory proposes baryogenesis occurring during or shortly after the grand unification epoch?
A: Electroweak baryogenesis
B: Quantum baryogenesis
C: Thermodynamic baryogenesis
D: GUT baryogenesis
E: String baryogenesis
Answer: D

Question 10: How small was the imbalance between matter and antimatter a fraction of a second after the Big Bang, as per the text?
A: 1 in every 1000 particles
B: 1 in every 1 million particles
C: 1 in every 1630000000 particles
D: 1 in every 5000000000 particles
E: 1 in every 100 billion particles
Answer: C
@
Subject:
The Ramsauer–Townsend effect, also sometimes called the Ramsauer effect or the Townsend effect, is a physical phenomenon involving the scattering of low-energy electrons by atoms of a noble gas. The effect can not be explained by classical mechanics, but requires the wave theory of quantum mechanics.

When an electron moves through a gas, its interactions with the gas atoms cause scattering to occur. These interactions are classified as inelastic if they cause excitation or ionization of the atom to occur and elastic if they do not.

The probability of scattering in such a system is defined as the number of electrons scattered, per unit electron current, per unit path length, per unit pressure at 0 °C, per unit solid angle. The number of collisions equals the total number of electrons scattered elastically and inelastically in all angles, and the probability of collision is the total number of collisions, per unit electron current, per unit path length, per unit pressure at 0 °C.

Because noble gas atoms have a relatively high first ionization energy and the electrons do not carry enough energy to cause excited electronic states, ionization and excitation of the atom are unlikely, and the probability of elastic scattering over all angles is approximately equal to the probability of collision.

If one tries to predict the probability of collision with a classical model that treats the electron and atom as hard spheres, one finds that the probability of collision should be independent of the incident electron energy (see Kukolich [1] ). However, Ramsauer and Townsend observed that for slow-moving electrons in argon, krypton, or xenon, the probability of collision between the electrons and gas atoms obtains a minimum value for electrons with a certain amount of kinetic energy (about 1 electron volts for xenon gas[2]). This is the Ramsauer–Townsend effect.

No good explanation for the phenomenon existed until the introduction of quantum mechanics, which explains that the effect results from the wave-like properties of the electron. A simple model of the collision that makes use of wave theory can predict the existence of the Ramsauer–Townsend minimum. Bohr presents one such model that considers the atom as a finite square potential well.

Predicting from theory the kinetic energy that will produce a Ramsauer–Townsend minimum is quite complicated since the problem involves understanding the wave nature of particles. However, the problem has been extensively investigated both experimentally and theoretically and is well understood (see Johnson and Guet).

In 1970 Gryzinski has proposed classical explanation of Ramsauer effect[3] using effective picture of atom as oscillating multipole of electric field (dipole, quadrupole, octupole), which was a consequence of his free-fall atomic model.
$
10
Question 1: What is the difference between elastic and inelastic interactions when an electron moves through a gas?
A: Elastic interactions cause the electron to gain energy, while inelastic interactions cause the electron to lose energy.
B: Elastic interactions involve the absorption of the electron by the atom, while inelastic interactions involve the release of the electron from the atom.
C: Elastic interactions do not cause excitation or ionization of the atom, while inelastic interactions do.
D: Elastic interactions only occur with noble gases, while inelastic interactions occur with all gases.
E: Elastic interactions cause the electron to change direction, while inelastic interactions do not.
Answer: C

Question 2: Which gases did Ramsauer and Townsend primarily observe for the manifestation of the Ramsauer–Townsend effect?
A: Helium, neon, and radon
B: Oxygen, nitrogen, and carbon dioxide
C: Argon, krypton, and xenon
D: Hydrogen, helium, and neon
E: Chlorine, bromine, and iodine
Answer: C

Question 3: According to the classical model that treats the electron and atom as hard spheres, how is the probability of collision related to the incident electron energy?
A: The probability increases with increasing energy.
B: The probability decreases with increasing energy.
C: The probability remains constant regardless of energy.
D: The probability is inversely proportional to the square of the energy.
E: The probability is directly proportional to the square of the energy.
Answer: C

Question 4: In the Ramsauer–Townsend effect, what happens to the probability of collision between electrons and noble gas atoms at around 1 electron volt for xenon gas?
A: The probability reaches its maximum value.
B: The probability is zero.
C: The probability reaches its minimum value.
D: The probability remains constant.
E: The probability starts to oscillate.
Answer: C

Question 5: What crucial element of quantum mechanics helps explain the Ramsauer–Townsend effect?
A: The particle-like properties of the electron
B: The duality of light and matter
C: The wave-like properties of the electron
D: The uncertainty principle
E: Quantum entanglement
Answer: C

Question 6: How does Bohr's model consider the atom in the context of the Ramsauer–Townsend effect?
A: As a particle with a fixed trajectory
B: As a finite square potential well
C: As an infinite square potential well
D: As a continuous energy spectrum
E: As a discrete energy level system
Answer: B

Question 7: In 1970, how did Gryzinski describe the atom in his proposed classical explanation of the Ramsauer effect?
A: As a particle with fixed energy levels
B: As oscillating multipole of electric field (e.g., dipole, quadrupole, octupole)
C: As a continuous wave packet
D: As an object with quantum jumps between states
E: As an entity bound by strings in multiple dimensions
Answer: B

Question 8: What makes predicting the kinetic energy that will produce a Ramsauer–Townsend minimum challenging?
A: The need to consider the gravitational effects on electrons
B: The randomness of electron paths
C: The complexity of the noble gas structures
D: The requirement to understand the wave nature of particles
E: The interactions between different types of noble gases
Answer: D

Question 9: What kind of energy do the electrons in the Ramsauer–Townsend effect typically possess?
A: Zero energy
B: Low-energy
C: Moderate-energy
D: High-energy
E: Infinite energy
Answer: B

Question 10: Why is the probability of elastic scattering over all angles approximately equal to the probability of collision for noble gases in the context of the Ramsauer–Townsend effect?
A: Because noble gas atoms have a low ionization energy
B: Because electrons carry high energy sufficient to cause excited electronic states
C: Because noble gas atoms have a high first ionization energy and the electrons do not carry enough energy to cause excited electronic states
D: Because electrons can easily penetrate the atomic nucleus of noble gases
E: Because noble gases have a strong magnetic field that repels electrons
Answer: C
@
Subject:
In mathematical physics, Minkowski space (or Minkowski spacetime) (/mɪŋˈkɔːfski, -ˈkɒf-/[1]) combines inertial space and time manifolds (x,y) with a non-inertial reference frame of space and time (x',t') into a four-dimensional model relating a position (inertial frame of reference) to the field. A four-vector (x,y,z,t) consists of a coordinate axes such as a Euclidean space plus time. This may be used with the non-inertial frame to illustrate specifics of motion, but should not be confused with the spacetime model generally.

The model helps show how a spacetime interval between any two events is independent of the inertial frame of reference in which they are recorded. Mathematician Hermann Minkowski developed it from the work of Hendrik Lorentz, Henri Poincaré, and others, and said it "was grown on experimental physical grounds."

Minkowski space is closely associated with Einstein's theories of special relativity and general relativity and is the most common mathematical structure by which special relativity is formalized. While the individual components in Euclidean space and time might differ due to length contraction and time dilation, in Minkowski spacetime, all frames of reference will agree on the total interval in spacetime between events.[nb 1] Minkowski space differs from four-dimensional Euclidean space insofar as it treats time differently than the three spatial dimensions.

In 3-dimensional Euclidean space, the isometry group (the maps preserving the regular Euclidean distance) is the Euclidean group. It is generated by rotations, reflections and translations. When time is appended as a fourth dimension, the further transformations of translations in time and Lorentz boosts are added, and the group of all these transformations is called the Poincaré group. Minkowski's model follows special relativity where motion causes time dilation changing the scale applied to the frame in motion and shifts the phase of light.

Spacetime is equipped with an indefinite non-degenerate bilinear form, variously called the Minkowski metric,[2] the Minkowski norm squared or Minkowski inner product depending on the context.[nb 2] The Minkowski inner product is defined so as to yield the spacetime interval between two events when given their coordinate difference vector as argument.[3] Equipped with this inner product, the mathematical model of spacetime is called Minkowski space. The group of transformations for Minkowski space that preserve the spacetime interval (as opposed to the spatial Euclidean distance) is the Poincaré group (as opposed to the isometry group).
$
9
Question 2: How does Minkowski space treat time compared to the three spatial dimensions?
A: They are treated identically.
B: Time is treated as a passive dimension.
C: Time is treated differently than the three spatial dimensions.
D: Time is considered as a derivative of spatial dimensions.
E: The three spatial dimensions are derivatives of time.
Answer: C

Question 3: What is the significance of the spacetime interval in Minkowski spacetime?
A: It varies with the observer's frame of reference.
B: It remains constant only in Euclidean space.
C: It is independent of the inertial frame of reference in which events are recorded.
D: It decreases with the speed of the observer.
E: It determines the curvature of space.
Answer: C

Question 4: Which group of transformations preserve the spacetime interval in Minkowski space?
A: Euclidean group
B: Lorentz group
C: Galilean group
D: Newtonian group
E: Poincaré group
Answer: E

Question 5: In Minkowski space, how does motion affect time?
A: It remains unchanged.
B: It speeds up.
C: It slows down due to time dilation.
D: It reverses direction.
E: It ceases to exist.
Answer: C

Question 6: The Minkowski inner product is used to determine what between two events?
A: Their gravitational attraction
B: Their relative velocity
C: The spacetime interval
D: Their spatial Euclidean distance
E: Their relative position in Euclidean space
Answer: C

Question 7: In the context of Minkowski space, what does the term "four-vector" mean?
A: A vector representing four spatial dimensions.
B: A vector combining three spatial dimensions and one temporal dimension.
C: A vector that includes only time and excludes space.
D: A vector that represents four types of time.
E: A vector combining two spatial dimensions and two temporal dimensions.
Answer: B

Question 8: Which theory closely associates with Minkowski space?
A: Quantum mechanics
B: Thermodynamics
C: Special relativity
D: Electromagnetism
E: Classical mechanics
Answer: C

Question 9: Who are some of the predecessors from whose work Hermann Minkowski developed his spacetime concept?
A: Nikola Tesla and James Maxwell
B: Blaise Pascal and Rene Descartes
C: Hendrik Lorentz, Henri Poincaré, and others
D: Niels Bohr and Werner Heisenberg
E: Benjamin Franklin and Michael Faraday
Answer: C

Question 10: When time is added as a fourth dimension to the 3-dimensional Euclidean space, which further transformation is added beyond rotations, reflections, and translations?
A: Gravitational bending
B: Quantum tunneling
C: Lorentz boosts
D: Hyperbolic rotations
E: Spin inversions
Answer: C
@
Subject:
Signal-to-noise ratio (SNR or S/N) is a measure used in science and engineering that compares the level of a desired signal to the level of background noise. SNR is defined as the ratio of signal power to noise power, often expressed in decibels. A ratio higher than 1:1 (greater than 0 dB) indicates more signal than noise.

SNR is an important parameter that affects the performance and quality of systems that process or transmit signals, such as communication systems, audio systems, radar systems, imaging systems, and data acquisition systems. A high SNR means that the signal is clear and easy to detect or interpret, while a low SNR means that the signal is corrupted or obscured by noise and may be difficult to distinguish or recover. SNR can be improved by various methods, such as increasing the signal strength, reducing the noise level, filtering out unwanted noise, or using error correction techniques.

SNR also determines the maximum possible amount of data that can be transmitted reliably over a given channel, which depends on its bandwidth and SNR. This relationship is described by the Shannon–Hartley theorem, which is a fundamental law of information theory.

SNR can be calculated using different formulas depending on how the signal and noise are measured and defined. The most common way to express SNR is in decibels, which is a logarithmic scale that makes it easier to compare large or small values. Other definitions of SNR may use different factors or bases for the logarithm, depending on the context and application.

Optical signals
Optical signals have a carrier frequency (about 200 THz and more) that is much higher than the modulation frequency. This way the noise covers a bandwidth that is much wider than the signal itself. The resulting signal influence relies mainly on the filtering of the noise. To describe the signal quality without taking the receiver into account, the optical SNR (OSNR) is used. The OSNR is the ratio between the signal power and the noise power in a given bandwidth. Most commonly a reference bandwidth of 0.1 nm is used. This bandwidth is independent of the modulation format, the frequency and the receiver. For instance an OSNR of 20 dB/0.1 nm could be given, even the signal of 40 GBit DPSK would not fit in this bandwidth. OSNR is measured with an optical spectrum analyzer.
$
10
Question 1: What does the Signal-to-Noise Ratio (SNR) measure?
A: The quality of a digital transmission over a network
B: The level of a desired signal in comparison to the level of background noise
C: The bandwidth of a given channel
D: The strength of a battery in electronic devices
E: The frequency of a radio wave
Answer: B

Question 2: How does a high SNR affect the signal?
A: Makes the signal weaker and harder to detect
B: Makes the signal clear and easy to detect or interpret
C: Increases the background noise of the signal
D: Reduces the signal strength
E: Filters out the main signal, leaving only the noise
Answer: B

Question 3: What theorem describes the relationship between SNR and the maximum possible amount of data that can be transmitted reliably over a given channel?
A: Pythagorean theorem
B: Fermat's Last theorem
C: Einstein's relativity theorem
D: Shannon–Hartley theorem
E: Bayes' theorem
Answer: D

Question 4: Why is SNR often expressed in decibels?
A: To make it easier to transmit the data
B: To increase the signal strength
C: To make it easier to compare large or small values
D: To reduce the overall power consumption
E: To emphasize the noise over the signal
Answer: C

Question 5: Which method is NOT a way to improve SNR?
A: Increasing the signal strength
B: Filtering out unwanted noise
C: Using error correction techniques
D: Amplifying the noise
E: Reducing the noise level
Answer: D

Question 6: For optical signals, why does noise cover a bandwidth that is much wider than the signal itself?
A: Because of the interference from other devices
B: Due to the nature of optical signals
C: Because the carrier frequency is much higher than the modulation frequency
D: Due to limitations in optical fiber technology
E: Because of temperature fluctuations in the medium
Answer: C

Question 7: What is the most common reference bandwidth used for measuring OSNR?
A: 0.5 nm
B: 1.0 nm
C: 0.1 nm
D: 0.05 nm
E: 2.0 nm
Answer: C

Question 8: How is OSNR typically measured?
A: With an audio spectrum analyzer
B: With a decibel meter
C: With an optical spectrum analyzer
D: With a frequency counter
E: With a digital multimeter
Answer: C

Question 9: In the context of OSNR, what does a bandwidth of 0.1 nm indicate?
A: The modulation format of the signal
B: The frequency of the optical signal
C: The range within which the signal can be transmitted
D: The dynamic range of the signal
E: The bandwidth is independent of the modulation format, the frequency, and the receiver
Answer: E

Question 10: Why is OSNR used to describe the quality of optical signals?
A: To emphasize the importance of the receiver in signal quality
B: To account for the unpredictable dynamic range of optical systems
C: To describe the signal quality without taking the receiver into account
D: To measure the efficiency of optical fibers in transmitting signals
E: To compare optical signals to non-optical signals
Answer: C
@
Subject:
Supersymmetric theory of stochastic dynamics or stochastics (STS) is an exact theory of stochastic (partial) differential equations (SDEs), the class of mathematical models with the widest applicability covering, in particular, all continuous time dynamical systems, with and without noise. The main utility of the theory from the physical point of view is a rigorous theoretical explanation of the ubiquitous spontaneous long-range dynamical behavior that manifests itself across disciplines via such phenomena as 1/f, flicker, and crackling noises and the power-law statistics, or Zipf's law, of instantonic processes like earthquakes and neuroavalanches. From the mathematical point of view, STS is interesting because it bridges the two major parts of mathematical physics – the dynamical systems theory and topological field theories. Besides these and related disciplines such as algebraic topology and supersymmetric field theories, STS is also connected with the traditional theory of stochastic differential equations and the theory of pseudo-Hermitian operators.

The theory began with the application of BRST gauge fixing procedure to Langevin SDEs,[1][2] that was later adapted to classical mechanics[3][4][5][6] and its stochastic generalization,[7] higher-order Langevin SDEs,[8] and, more recently, to SDEs of arbitrary form,[9] which allowed to link BRST formalism to the concept of transfer operators and recognize spontaneous breakdown of BRST supersymmetry as a stochastic generalization of dynamical chaos.

The main idea of the theory is to study, instead of trajectories, the SDE-defined temporal evolution of differential forms. This evolution has an intrinsic BRST or topological supersymmetry representing the preservation of topology and/or the concept of proximity in the phase space by continuous time dynamics. The theory identifies a model as chaotic, in the generalized, stochastic sense, if its ground state is not supersymmetric, i.e., if the supersymmetry is broken spontaneously. Accordingly, the emergent long-range behavior that always accompanies dynamical chaos and its derivatives such as turbulence and self-organized criticality can be understood as a consequence of the Goldstone theorem.
$
10
Question 1: What does the supersymmetric theory of stochastic dynamics primarily explain?
A: The theory behind hydromagnetic dynamos and their behavior.
B: The statistical analysis of instantaneous processes like earthquakes.
C: The spontaneous long-range dynamical behavior observed across different disciplines.
D: The fundamental principle behind deterministic chaos.
E: The inherent properties of kinematic approximations in differential equations.
Answer: C

Question 2: Which two major areas of mathematical physics are bridged by STS?
A: Algebraic topology and stochastic differential equations.
B: Dynamical systems theory and topological field theories.
C: Supersymmetric field theories and traditional stochastic equations.
D: Kinematic dynamo theory and BRST gauge fixing procedure.
E: Classical mechanics and higher-order Langevin SDEs.
Answer: B

Question 3: The STS is connected to which of the following disciplines?
A: Quantum mechanics and relativistic physics.
B: Kinematic dynamo theory and BRST gauge fixing procedure.
C: Algebraic topology and supersymmetric field theories.
D: Electromagnetic theory and classical mechanics.
E: None of the above.
Answer: C

Question 4: What initiated the development of the supersymmetric theory of stochastic dynamics?
A: The application of kinematic dynamo theory.
B: The discovery of Goldstone theorem.
C: The application of BRST gauge fixing procedure to Langevin SDEs.
D: The study of 1/f and flicker noises in various systems.
E: The exploration of emergent long-range behaviors in physical systems.
Answer: C

Question 5: How does the theory determine if a model is chaotic in a generalized stochastic sense?
A: If the model exhibits hydromagnetic behavior.
B: If its ground state is supersymmetric.
C: If the BRST formalism aligns with the concept of transfer operators.
D: If its ground state is not supersymmetric.
E: If the model showcases kinematic behavior at various phase states.
Answer: D

Question 6: Which phenomena does STS provide explanations for?
A: Hydromagnetic dynamos and kinematic dynamo behaviors.
B: The principles behind the Goldstone theorem and its applications.
C: 1/f, flicker, and crackling noises along with the power-law statistics of instantonic processes.
D: The continuous behaviors observed in deterministic chaos systems.
E: The in-depth study of BRST gauge fixing procedures in classical mechanics.
Answer: C

Question 7: In the context of STS, what does BRST supersymmetry signify?
A: The continuous and unpredictable behavior of chaotic systems.
B: The alignment between dynamical systems theory and topological field theories.
C: The preservation of topology or the concept of proximity in the phase space by continuous time dynamics.
D: The application of the Goldstone theorem to stochastic processes.
E: The balance between algebraic topology and supersymmetric field theories.
Answer: C

Question 8: What is the role of the Goldstone theorem in STS?
A: It explains the principles behind the kinematic dynamo theory.
B: It provides a framework to study the hydromagnetic behaviors in dynamo systems.
C: It underlines the reason for the emergent long-range behavior accompanying dynamical chaos.
D: It describes the behavior of 1/f, flicker, and crackling noises in various systems.
E: It defines the principles of algebraic topology in the context of STS.
Answer: C

Question 9: STS identifies a model as chaotic if:
A: It exhibits Goldstone behavior.
B: Its ground state is supersymmetric.
C: The BRST supersymmetry is actively maintained.
D: It adapts to the classical mechanics of Langevin SDEs.
E: Its supersymmetry is broken spontaneously.
Answer: E

Question 10: What is the primary focus of the STS when studying SDE-defined temporal evolution?
A: Trajectories of differential equations.
B: Effects of the Goldstone theorem on temporal evolution.
C: Continuous behavior of deterministic chaos.
D: Temporal evolution of differential forms.
E: Effects of the BRST gauge fixing procedure on SDEs.
Answer: D
@
Subject:
The scale of a map is the ratio of a distance on the map to the corresponding distance on the ground. This simple concept is complicated by the curvature of the Earth's surface, which forces scale to vary across a map. Because of this variation, the concept of scale becomes meaningful in two distinct ways.

The first way is the ratio of the size of the generating globe to the size of the Earth. The generating globe is a conceptual model to which the Earth is shrunk and from which the map is projected. The ratio of the Earth's size to the generating globe's size is called the nominal scale (also called principal scale or representative fraction). Many maps state the nominal scale and may even display a bar scale (sometimes merely called a "scale") to represent it.

The second distinct concept of scale applies to the variation in scale across a map. It is the ratio of the mapped point's scale to the nominal scale. In this case 'scale' means the scale factor (also called point scale or particular scale).

If the region of the map is small enough to ignore Earth's curvature, such as in a town plan, then a single value can be used as the scale without causing measurement errors. In maps covering larger areas, or the whole Earth, the map's scale may be less useful or even useless in measuring distances. The map projection becomes critical in understanding how scale varies throughout the map.[1][2] When scale varies noticeably, it can be accounted for as the scale factor. Tissot's indicatrix is often used to illustrate the variation of point scale across a map.
$
10
Question 1: Why does the scale vary across a map?
A: Due to different map projections used.
B: Because of the curvature of the Earth's surface.
C: Due to the varying sizes of continents.
D: Because of the inconsistencies in measuring distances.
E: Due to the orientation of the map.
Answer: B

Question 2: What is the generating globe in the context of map scale?
A: A map that serves as the base for all other maps.
B: A conceptual model to which the Earth is shrunk and from which the map is projected.
C: The actual globe representing the Earth in real-time.
D: A tool used to measure distances on a map.
E: A virtual representation of the Earth with enhanced features.
Answer: B

Question 3: Which type of scale is described as the ratio of the Earth's size to the generating globe's size?
A: Topographic scale
B: Mapped point scale
C: Nominal scale
D: Scale factor
E: Bar scale
Answer: C

Question 4: In maps covering small areas where Earth's curvature can be ignored, why is a single scale value sufficient?
A: Because the orientation of the map is constant.
B: Because the map projection changes frequently.
C: Because scale variation due to Earth's curvature is negligible.
D: Because these maps are typically not used for accurate measurements.
E: Because the generating globe and Earth are of similar sizes.
Answer: C

Question 5: What becomes critical in understanding how scale varies in maps covering larger areas or the entire Earth?
A: The color scheme of the map.
B: The orientation of the continents.
C: The map projection.
D: The size of the generating globe.
E: The layout of the map.
Answer: C

Question 6: What is Tissot's indicatrix often used for?
A: Indicating the orientation of a map.
B: Displaying the nominal scale of a map.
C: Illustrating the variation of point scale across a map.
D: Demonstrating the actual size of continents.
E: Showing the ratio between two different map projections.
Answer: C

Question 7: Which type of scale refers to the variation in scale across a map?
A: Topographic scale
B: Mapped point scale
C: Nominal scale
D: Scale factor
E: Bar scale
Answer: D

Question 8: If a map has a bar scale that visually represents 1 cm corresponding to 1 km on the Earth, what is the nominal scale of the map?
A: 1:1,000
B: 1:100
C: 1:10
D: 1:10,000
E: 1:100,000
Answer: D

Question 9: In which kind of map would the scale likely be less useful or even useless for measuring distances due to variations in scale?
A: A map of a small neighborhood.
B: A map of a town's central park.
C: A map of a country.
D: A map of a shopping mall.
E: A floor plan of a house.
Answer: C

Question 10: What do we refer to when we talk about the "scale factor" in the context of a map's scale?
A: The ratio of the size of the map to the area it represents.
B: The ratio of the mapped point's scale to the nominal scale.
C: The relationship between different map projections.
D: The ratio of the Earth's size to the generating globe's size.
E: The distance representation on the bar scale.
Answer: B
@
Subject:
In astronomy, the main sequence is a continuous and distinctive band of stars that appears on plots of stellar color versus brightness. These color-magnitude plots are known as Hertzsprung–Russell diagrams after their co-developers, Ejnar Hertzsprung and Henry Norris Russell. Stars on this band are known as main-sequence stars or dwarf stars. These are the most numerous true stars in the universe and include the Sun.

After condensation and ignition of a star, it generates thermal energy in its dense core region through nuclear fusion of hydrogen into helium. During this stage of the star's lifetime, it is located on the main sequence at a position determined primarily by its mass but also based on its chemical composition and age. The cores of main-sequence stars are in hydrostatic equilibrium, where outward thermal pressure from the hot core is balanced by the inward pressure of gravitational collapse from the overlying layers. The strong dependence of the rate of energy generation on temperature and pressure helps to sustain this balance. Energy generated at the core makes its way to the surface and is radiated away at the photosphere. The energy is carried by either radiation or convection, with the latter occurring in regions with steeper temperature gradients, higher opacity, or both.

The main sequence is sometimes divided into upper and lower parts, based on the dominant process that a star uses to generate energy. The Sun, along with main sequence stars below about 1.5 times the mass of the Sun (1.5 M☉), primarily fuse hydrogen atoms together in a series of stages to form helium, a sequence called the proton–proton chain. Above this mass, in the upper main sequence, the nuclear fusion process mainly uses atoms of carbon, nitrogen, and oxygen as intermediaries in the CNO cycle that produces helium from hydrogen atoms. Main-sequence stars with more than two solar masses undergo convection in their core regions, which acts to stir up the newly created helium and maintain the proportion of fuel needed for fusion to occur. Below this mass, stars have cores that are entirely radiative with convective zones near the surface. With decreasing stellar mass, the proportion of the star forming a convective envelope steadily increases. Main-sequence stars below 0.4 M☉ undergo convection throughout their mass. When core convection does not occur, a helium-rich core develops surrounded by an outer layer of hydrogen.

The more massive a star is, the shorter its lifespan on the main sequence. After the hydrogen fuel at the core has been consumed, the star evolves away from the main sequence on the HR diagram, into a supergiant, red giant, or directly to a white dwarf.
$
10
Question 1: Who are the Hertzsprung–Russell diagrams named after?
A: Isaac Newton and Albert Einstein
B: Stephen Hawking and Carl Sagan
C: Ejnar Hertzsprung and Henry Norris Russell
D: Galileo Galilei and Johannes Kepler
E: Richard Feynman and Paul Dirac
Answer: C

Question 2: How do main-sequence stars generate energy in their core?
A: By nuclear fusion of hydrogen into helium.
B: By the gravitational pull from other stars.
C: Through the absorption of cosmic radiation.
D: By splitting atoms in a process similar to nuclear reactors on Earth.
E: By burning carbon and other heavy elements.
Answer: A

Question 3: What process primarily generates energy in the Sun?
A: The CNO cycle
B: Nuclear fission
C: Black hole radiation
D: The proton–proton chain
E: Carbon burning
Answer: D

Question 4: Which of the following best describes hydrostatic equilibrium in the context of a star's core?
A: It refers to the balance between the energy radiated by the star and the energy absorbed from its surroundings.
B: It is a state where the star's core is neither contracting nor expanding.
C: It refers to the balance between the outward thermal pressure from the hot core and the inward pressure of gravitational collapse from the overlying layers.
D: It is the equilibrium between the fusion processes in a star.
E: It is the balance between the forces of gravity and magnetism within the star.
Answer: C

Question 5: In which part of the main sequence does the nuclear fusion process mainly use carbon, nitrogen, and oxygen as intermediaries?
A: In the lower main sequence
B: In the middle of the main sequence
C: In stars less than 0.4 M☉
D: In the upper main sequence
E: In stars that are white dwarfs
Answer: D

Question 6: What happens when a main-sequence star's core doesn't undergo convection?
A: It undergoes nuclear fission instead of fusion.
B: The star quickly evolves into a black hole.
C: A helium-rich core develops surrounded by an outer layer of hydrogen.
D: The star will expand into a supergiant instantly.
E: The star's brightness and color remain unchanged for its entire lifetime.
Answer: C

Question 7: Which stars have cores that are entirely radiative with convective zones near the surface?
A: Stars with more than two solar masses
B: Stars below 0.4 M☉
C: Stars below about 1.5 M☉ but more than 0.4 M☉
D: Supergiant stars
E: Red giant stars
Answer: C

Question 8: What occurs to a star after the hydrogen fuel at its core has been consumed?
A: It continues to burn hydrogen indefinitely.
B: It evolves into a black hole.
C: It transforms into a nebula.
D: It evolves away from the main sequence into a supergiant, red giant, or directly to a white dwarf.
E: It starts the process of core convection.
Answer: D

Question 9: Which main-sequence stars undergo convection throughout their entire mass?
A: Stars above 1.5 M☉
B: Stars above 2 M☉
C: Stars below 0.4 M☉
D: Stars in the upper main sequence
E: The Sun and its neighboring stars
Answer: C

Question 10: What determines a star's position on the main sequence?
A: Its distance from Earth
B: The number of planets around it
C: Its luminosity alone
D: Primarily its mass, but also its chemical composition and age
E: The type of galaxy it resides in
Answer: D
@
Subject:
Quantum mechanics is a fundamental theory in physics that provides a description of the physical properties of nature at the scale of atoms and subatomic particles.[2]: 1.1  It is the foundation of all quantum physics including quantum chemistry, quantum field theory, quantum technology, and quantum information science.

Quantum mechanics differs from classical physics in that energy, momentum, angular momentum, and other quantities of a bound system are restricted to discrete values (quantization); objects have characteristics of both particles and waves (wave–particle duality); and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (the uncertainty principle).

Eduardo Renato Caianiello (June 25, 1921 – October 22, 1993) was an Italian physicist. He contributed to scientific research, especially in quantum theory and cybernetics. He was also a pioneer in the theory of neural networks. His Caianiello's equation formalized the theory of Hebbian learning.[1]

Caianello founded and directed the Institute of Theoretical Physics of the University of Naples; the Laboratory of Cybernetics of the Consiglio Nazionale delle Ricerche at Arco Felice (Naples), the Faculty of Mathematical, Physical and Natural Sciences of the University of Salerno, the International Institute for Senior Scientific Studies (IIASS) at Vietri sul Mare (Salerno) and the School of Specialization in Cyber and Physical Sciences.

The name of the Hafnian was coined by Cainaniello "to mark the fruitful period of stay in Copenhagen (Hafnia in Latin)."[2]
$
10
Question 1: What is a fundamental difference between quantum mechanics and classical physics?
A: Quantum mechanics does not take into account the behavior of subatomic particles.
B: Classical physics is based on the principles of wave-particle duality.
C: In quantum mechanics, quantities like energy and momentum are restricted to discrete values.
D: Classical physics is a more comprehensive theory than quantum mechanics.
E: Quantum mechanics is only applicable at the macroscopic level.
Answer: C

Question 2: Which principle states there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement?
A: The conservation of energy principle
B: The wave function collapse
C: The relativity principle
D: The uncertainty principle
E: The quantization principle
Answer: D

Question 3: What did Eduardo Renato Caianiello contribute significantly to?
A: The theory of black holes
B: The theory of relativity
C: The theory of Hebbian learning and neural networks
D: The study of planetary motion
E: Quantum field theory
Answer: C

Question 4: Which institute did Caianiello NOT found or direct?
A: The International Institute of Advanced Quantum Mechanics
B: The Institute of Theoretical Physics of the University of Naples
C: The Laboratory of Cybernetics of the Consiglio Nazionale delle Ricerche at Arco Felice
D: The International Institute for Senior Scientific Studies (IIASS) at Vietri sul Mare
E: The School of Specialization in Cyber and Physical Sciences
Answer: A

Question 5: What does the term "Hafnian" commemorate according to Caianiello?
A: The name of a famous physicist
B: A special equation in quantum mechanics
C: The discovery of a new subatomic particle
D: His time in a town called Hafnia
E: The foundation of quantum mechanics
Answer: D

Question 6: In quantum mechanics, what concept speaks to objects having characteristics of both particles and waves?
A: Quantum entanglement
B: Schrödinger's cat
C: Wave-particle duality
D: Quantum teleportation
E: Superposition principle
Answer: C

Question 7: Which field is NOT directly based on the foundation of quantum mechanics?
A: Quantum field theory
B: Quantum chemistry
C: Classical mechanics
D: Quantum technology
E: Quantum information science
Answer: C

Question 8: Where did Caianiello found the International Institute for Senior Scientific Studies (IIASS)?
A: Naples
B: Copenhagen
C: Salerno
D: Vietri sul Mare
E: Rome
Answer: D

Question 9: Eduardo Renato Caianiello was also a pioneer in which field?
A: Quantum teleportation
B: Theory of relativity
C: Neural networks
D: Particle accelerator design
E: Thermodynamics
Answer: C

Question 10: What does "quantization" in quantum mechanics refer to?
A: The process of converting classical information to quantum information
B: The restriction of quantities like energy and momentum to continuous values
C: The behavior of large macroscopic systems
D: The restriction of quantities like energy and momentum to discrete values
E: The study of quantum phenomena in space
Answer: D
@
Subject:
Photophoresis denotes the phenomenon that small particles suspended in gas (aerosols) or liquids (hydrocolloids) start to migrate when illuminated by a sufficiently intense beam of light. The existence of this phenomenon is owed to a non-uniform distribution of temperature of an illuminated particle in a fluid medium.[1] Separately from photophoresis, in a fluid mixture of different kinds of particles, the migration of some kinds of particles may be due to differences in their absorptions of thermal radiation and other thermal effects collectively known as thermophoresis. In laser photophoresis, particles migrate once they have a refractive index different from their surrounding medium. The migration of particles is usually possible when the laser is slightly or not focused. A particle with a higher refractive index compared to its surrounding molecule moves away from the light source due to momentum transfer from absorbed and scattered light photons. This is referred to as a radiation pressure force. This force depends on light intensity and particle size but has nothing to do with the surrounding medium[clarification needed]. Just like in Crookes radiometer, light can heat up one side and gas molecules bounce from that surface with greater velocity, hence push the particle to the other side. Under certain conditions, with particles of diameter comparable to the wavelength of light, the phenomenon of a negative indirect photophoresis occurs, due to the unequal heat generation on the laser irradiation between the back and front sides of particles, this produces a temperature gradient in the medium around the particle such that molecules at the far side of the particle from the light source may get to heat up more, causing the particle to move towards the light source.[2]

If the suspended particle is rotating, it will also experience the Yarkovsky effect.

Discovery of photophoresis is usually attributed to Felix Ehrenhaft in the 1920s, though earlier observations were made by others including Augustin-Jean Fresnel.
$
10
Question 1: What causes the movement of particles during photophoresis?
A: Interaction of the particles with the magnetic field of the light.
B: A non-uniform distribution of temperature of an illuminated particle in a fluid medium.
C: The cohesive nature of particles when subjected to light.
D: The electrostatic attraction between particles.
E: Interaction of the particles with the sound waves produced by light.
Answer: B

Question 2: In laser photophoresis, under what conditions do particles typically migrate?
A: When the laser is highly focused and the particles are large.
B: When they have a refractive index similar to their surrounding medium.
C: When the laser is slightly or not focused and the particles have a refractive index different from their surrounding medium.
D: When the particles are neutral and the surrounding medium is charged.
E: When they absorb all the light and become invisible.
Answer: C

Question 3: What is the driving force behind the movement of a particle with a higher refractive index compared to its surrounding medium in laser photophoresis?
A: Gravity.
B: Osmotic pressure.
C: Centrifugal force.
D: Radiation pressure force.
E: Brownian motion.
Answer: D

Question 4: In the case of negative indirect photophoresis, in what direction does the particle move relative to the light source?
A: Away from the light source.
B: In a random direction.
C: In a direction perpendicular to the light.
D: Towards the light source.
E: It remains stationary.
Answer: D

Question 5: What effect does a rotating suspended particle experience?
A: The Heisenberg effect.
B: The Fresnel effect.
C: The Ehrenhaft effect.
D: The Yarkovsky effect.
E: The Doppler effect.
Answer: D

Question 6: Who is credited with the discovery of photophoresis?
A: Augustin-Jean Fresnel.
B: Felix Ehrenhaft.
C: Isaac Newton.
D: Michael Faraday.
E: Richard Feynman.
Answer: B

Question 7: Which of the following is NOT a cause for particle migration in a fluid mixture?
A: Photophoresis.
B: Thermophoresis.
C: Gravitophoresis.
D: Crookes radiometer effect.
E: Radiation pressure force.
Answer: C

Question 8: What happens in Crookes radiometer regarding light and particle movement?
A: Light uniformly heats up the radiometer, causing it to glow.
B: Light has no effect on the radiometer.
C: Light heats up one side, and gas molecules bounce from that surface with greater velocity, pushing the particle to the other side.
D: Light causes the radiometer to vibrate and produce sound.
E: Light ionizes the radiometer, making it conductive.
Answer: C

Question 9: Which scientist made earlier observations of photophoresis before its official discovery?
A: Max Planck.
B: Galileo Galilei.
C: James Clerk Maxwell.
D: Werner Heisenberg.
E: Augustin-Jean Fresnel.
Answer: E

Question 10: What can be said about the relationship between the refractive index of particles and their movement in laser photophoresis?
A: Particles with a similar refractive index to the surrounding medium move towards the light source.
B: Particles with a higher refractive index than the surrounding medium move towards the light source.
C: Particles with a lower refractive index than the surrounding medium remain stationary.
D: Particles with a similar refractive index to the surrounding medium remain stationary.
E: Particles with a higher refractive index than the surrounding medium move away from the light source.
Answer: E
@
Subject:
Earnshaw's theorem states that a collection of point charges cannot be maintained in a stable stationary equilibrium configuration solely by the electrostatic interaction of the charges. This was first proven by British mathematician Samuel Earnshaw in 1842. It is usually cited in reference to magnetic fields, but was first applied to electrostatic fields.

Earnshaw's theorem applies to classical inverse-square law forces (electric and gravitational) and also to the magnetic forces of permanent magnets, if the magnets are hard (the magnets do not vary in strength with external fields). Earnshaw's theorem forbids magnetic levitation in many common situations.

If the materials are not hard, Braunbeck's extension shows that materials with relative magnetic permeability greater than one (paramagnetism) are further destabilising, but materials with a permeability less than one (diamagnetic materials) permit stable configurations.
$
10
Question 2: Which force does Earnshaw's theorem originally apply to?
A: Gravitational force
B: Magnetic force
C: Electrostatic force
D: Centrifugal force
E: Nuclear force
Answer: C

Question 3: Under what condition are materials considered to permit stable configurations according to Braunbeck's extension?
A: When materials are hard
B: When materials have relative magnetic permeability greater than one
C: When materials have relative magnetic permeability equal to one
D: When materials have a permeability less than one
E: When materials exhibit no magnetic properties at all
Answer: D

Question 4: In many common situations, Earnshaw's theorem forbids what?
A: Electrostatic attraction
B: Magnetic levitation
C: Gravitational equilibrium
D: Centrifugal stabilization
E: Nuclear fusion
Answer: B

Question 5: Which of the following describes hard magnets in the context of Earnshaw's theorem?
A: Magnets that vary in strength with external fields
B: Magnets that do not vary in strength with external fields
C: Magnets that are physically harder than other materials
D: Magnets that have a permeability less than one
E: Magnets that lose their magnetization quickly
Answer: B

Question 6: Which type of materials are further destabilizing according to Braunbeck's extension?
A: Diamagnetic materials
B: Hard magnetic materials
C: Materials with relative magnetic permeability greater than one
D: Materials with relative magnetic permeability less than one
E: Non-magnetic materials
Answer: C

Question 7: What type of forces does Earnshaw's theorem apply to?
A: Only electrostatic forces
B: Only magnetic forces
C: Only gravitational forces
D: Both electrostatic and gravitational forces
E: Electrostatic, gravitational, and certain magnetic forces
Answer: E

Question 8: What is the main implication of Earnshaw's theorem for point charges and electrostatic interaction?
A: They can achieve stable equilibrium
B: They will always repel each other
C: They cannot achieve stable stationary equilibrium
D: They can achieve equilibrium only in motion
E: Their behavior is unpredictable
Answer: C

Question 9: In which year was Earnshaw's theorem first proven?
A: 1865
B: 1912
C: 1842
D: 1889
E: 1901
Answer: C

Question 10: For which type of materials does Earnshaw's theorem NOT forbid magnetic levitation?
A: Hard magnets
B: Diamagnetic materials
C: Paramagnetic materials
D: Materials with a permeability equal to one
E: Soft magnets
Answer: B
@
Subject:
Radiometry is a set of techniques for measuring electromagnetic radiation, including visible light. Radiometric techniques in optics characterize the distribution of the radiation's power in space, as opposed to photometric techniques, which characterize the light's interaction with the human eye. The fundamental difference between radiometry and photometry is that radiometry gives the entire optical radiation spectrum, while photometry is limited to the visible spectrum. Radiometry is distinct from quantum techniques such as photon counting.

The use of radiometers to determine the temperature of objects and gasses by measuring radiation flux is called pyrometry. Handheld pyrometer devices are often marketed as infrared thermometers.

Radiometry is important in astronomy, especially radio astronomy, and plays a significant role in Earth remote sensing. The measurement techniques categorized as radiometry in optics are called photometry in some astronomical applications, contrary to the optics usage of the term.

Spectroradiometry is the measurement of absolute radiometric quantities in narrow bands of wavelength.[1]

In radiometry, radiosity is the radiant flux leaving (emitted, reflected and transmitted by) a surface per unit area, and spectral radiosity is the radiosity of a surface per unit frequency or wavelength, depending on whether the spectrum is taken as a function of frequency or of wavelength.[1] The SI unit of radiosity is the watt per square metre (W/m2), while that of spectral radiosity in frequency is the watt per square metre per hertz (W·m−2·Hz−1) and that of spectral radiosity in wavelength is the watt per square metre per metre (W·m−3)—commonly the watt per square metre per nanometre (W·m−2·nm−1). The CGS unit erg per square centimeter per second (erg·cm−2·s−1) is often used in astronomy. Radiosity is often called intensity[2] in branches of physics other than radiometry, but in radiometry this usage leads to confusion with radiant intensity.
$
10
Question 1: What does radiometry measure?
A: The interaction of light with the human eye.
B: Electromagnetic radiation, including visible light.
C: Only the visible spectrum of light.
D: The psychological interpretation of light.
E: The interaction of light with quantum objects.
Answer: B

Question 2: How does photometry differ from radiometry?
A: Photometry is limited to the visible spectrum, while radiometry gives the entire optical radiation spectrum.
B: Photometry gives the entire optical radiation spectrum, while radiometry is limited to the visible spectrum.
C: Photometry and radiometry both focus on the entire optical radiation spectrum.
D: Photometry deals with photon counting, while radiometry does not.
E: Photometry measures electromagnetic radiation, while radiometry does not.
Answer: A

Question 3: What is the primary use of pyrometry?
A: Measuring visible light.
B: Counting photons.
C: Determining the temperature of objects by measuring radiation flux.
D: Observing the interaction of light with the human eye.
E: Studying electromagnetic waves in quantum mechanics.
Answer: C

Question 4: In what field is radiometry especially important?
A: Cellular biology.
B: Mechanical engineering.
C: Radio astronomy.
D: Human physiology.
E: Chemical kinetics.
Answer: C

Question 5: What does spectroradiometry measure?
A: Interaction of light with the human eye.
B: The speed of light in various media.
C: Absolute radiometric quantities in narrow bands of wavelength.
D: The psychological perception of colors.
E: The phase difference in light waves.
Answer: C

Question 6: Which unit is the SI unit of radiosity?
A: Watt per cubic metre (W/m3).
B: Watt per square metre per nanometre (W·m−2·nm−1).
C: Erg per square centimeter per second (erg·cm−2·s−1).
D: Watt per square metre (W/m2).
E: Joule per square metre (J/m2).
Answer: D

Question 7: What is spectral radiosity when taken as a function of frequency?
A: The radiosity of a surface per unit frequency.
B: The radiosity of a surface per unit wavelength.
C: The interaction of light with the human eye per unit frequency.
D: The radiosity of a surface per unit volume.
E: The luminosity of a surface per unit frequency.
Answer: A

Question 8: In certain branches of physics, what term is often used instead of radiosity?
A: Luminance.
B: Flux.
C: Intensity.
D: Brightness.
E: Spectral radiance.
Answer: C

Question 9: How is the measurement technique of radiometry referred to in some astronomical applications?
A: Spectroradiometry.
B: Pyrometry.
C: Quantum radiometry.
D: Photometry.
E: Radiosity.
Answer: D

Question 10: What does radiometry primarily characterize?
A: The radiation's power in space.
B: The radiation's interaction with solids.
C: The radiation's phase in space.
D: The radiation's power in a vacuum.
E: The radiation's speed in various media.
Answer: A
@
Subject:
A virtual particle is a theoretical transient particle that exhibits some of the characteristics of an ordinary particle, while having its existence limited by the uncertainty principle.[vague] The concept of virtual particles arises in the perturbation theory of quantum field theory where interactions between ordinary particles are described in terms of exchanges of virtual particles. A process involving virtual particles can be described by a schematic representation known as a Feynman diagram, in which virtual particles are represented by internal lines.[1][2]

Virtual particles do not necessarily carry the same mass as the corresponding real particle, although they always conserve energy and momentum. The closer its characteristics come to those of ordinary particles, the longer the virtual particle exists. They are important in the physics of many processes, including particle scattering and Casimir forces. In quantum field theory, forces—such as the electromagnetic repulsion or attraction between two charges—can be thought of as due to the exchange of virtual photons between the charges. Virtual photons are the exchange particle for the electromagnetic interaction.

The term is somewhat loose and vaguely defined, in that it refers to the view that the world is made up of "real particles". "Real particles" are better understood to be excitations of the underlying quantum fields. Virtual particles are also excitations of the underlying fields, but are "temporary" in the sense that they appear in calculations of interactions, but never as asymptotic states or indices to the scattering matrix. The accuracy and use of virtual particles in calculations is firmly established, but as they cannot be detected in experiments, deciding how to precisely describe them is a topic of debate.[3] Although widely used, they are by no means a necessary feature of QFT, but rather are mathematical conveniences - as demonstrated by lattice field theory, which avoids using the concept altogether.
$
10
Question 1: In which theory does the concept of virtual particles arise?
A: Classical mechanics.
B: General relativity.
C: Thermodynamics.
D: Perturbation theory of quantum field theory.
E: Wave optics.
Answer: D

Question 2: How can processes involving virtual particles be schematically represented?
A: Heisenberg diagrams.
B: Newtonian diagrams.
C: Feynman diagrams.
D: Bohr diagrams.
E: Schrödinger diagrams.
Answer: C

Question 3: Which of the following is true regarding the mass of virtual particles?
A: They always have the same mass as the corresponding real particle.
B: They never have mass.
C: They do not necessarily carry the same mass as the corresponding real particle.
D: They have infinite mass.
E: Their mass is always half of the corresponding real particle.
Answer: C

Question 4: What force can be thought of as due to the exchange of virtual photons?
A: Gravitational attraction.
B: Strong nuclear force.
C: Electromagnetic interaction.
D: Weak nuclear force.
E: Frictional force.
Answer: C

Question 5: How are "real particles" better understood in quantum field theory?
A: As permanent particles in space.
B: As excitations of the underlying quantum fields.
C: As physical entities that can be touched.
D: As mathematical illusions.
E: As endpoints of Feynman diagrams.
Answer: B

Question 6: Virtual particles play a role in which of the following?
A: Casimir forces.
B: Inertial forces.
C: Frictional forces.
D: Centrifugal forces.
E: Gravitational forces.
Answer: A

Question 7: Why are virtual particles considered "temporary"?
A: Because they decay into real particles.
B: Because they disappear after a certain time.
C: Because they appear in calculations of interactions but never as asymptotic states.
D: Because they can be transformed into energy.
E: Because they only exist during the daytime.
Answer: C

Question 8: What is the status of virtual particles in experiments?
A: They can be directly detected.
B: They can be seen under a microscope.
C: They cannot be detected in experiments.
D: They leave visible trails in cloud chambers.
E: They produce audible sounds.
Answer: C

Question 9: Which of the following is a true statement about virtual particles in quantum field theory (QFT)?
A: They are the only means to describe interactions in QFT.
B: Their existence has been experimentally proven.
C: They are mathematical conveniences and not a necessary feature of QFT.
D: They are essential for the underlying foundation of QFT.
E: They are the primary components that build up the universe.
Answer: C

Question 10: Which particle is considered the exchange particle for electromagnetic interaction?
A: Gluon.
B: Neutrino.
C: Positron.
D: Photon.
E: Higgs boson.
Answer: D
@
Subject:
Henri Atlan (born 27 December 1931 in Blida, French Algeria) is a French biophysicist and philosopher.
Influenced by Heinz von Foerster, Atlan became interested in applying cybernetics and information theory to living organisms, and went to the Weizmann Institute in Rehovot to work under the biophysicist Aharon Katchalsky.[3] In 1972, he returned to Paris; and, in that year, his 1972 work on information theory and self-organising systems, entitled L'organisation biologique et la théorie de l'information, received a wide readership.[citation needed] In this book, he proposed the principle of "complexity from noise"[4][5] (French: le principe de complexité par le bruit),[6][7] concept taken up in his following book Entre le cristal et la fumée (1979)[8] and development of 1960 Von Foerster's notion of "order from noise". 

Complexity characterises the behaviour of a system or model whose components interact in multiple ways and follow local rules, leading to non-linearity, randomness, collective dynamics, hierarchy, and emergence.[1][2]

The term is generally used to characterize something with many parts where those parts interact with each other in multiple ways, culminating in a higher order of emergence greater than the sum of its parts. The study of these complex linkages at various scales is the main goal of complex systems theory.

The intuitive criterion of complexity can be formulated as follows: a system would be more complex if more parts could be distinguished, and if more connections between them existed.[3]

In signal processing, noise is a general term for unwanted (and, in general, unknown) modifications that a signal may suffer during capture, storage, transmission, processing, or conversion.[1]

Sometimes the word is also used to mean signals that are random (unpredictable) and carry no useful information; even if they are not interfering with other signals or may have been introduced intentionally, as in comfort noise.

Noise reduction, the recovery of the original signal from the noise-corrupted one, is a very common goal in the design of signal processing systems, especially filters. The mathematical limits for noise removal are set by information theory.
$
10
Question 1: Where was Henri Atlan born?
A: Paris, France
B: Blida, French Algeria
C: Rehovot, Israel
D: Berlin, Germany
E: New York, USA
Answer: B

Question 2: Which biophysicist did Henri Atlan work under at the Weizmann Institute?
A: René Descartes
B: Heinz von Foerster
C: Ilya Prigogine
D: Aharon Katchalsky
E: Richard Feynman
Answer: D

Question 3: What does complexity characterize?
A: The linear progression of a system.
B: The behavior of a system whose components interact in simple ways.
C: The behavior of a system whose components interact in multiple ways and follow local rules.
D: The singular dynamics of a model.
E: The predictable behavior of a system.
Answer: C

Question 4: What is the result of components interacting in multiple ways in a system?
A: Simplicity
B: Predictability
C: Linear progression
D: Emergence greater than the sum of its parts
E: Decreased linkage
Answer: D

Question 5: In the context of signal processing, what is noise?
A: A pleasant sound added to improve a signal.
B: A harmonic sound used in music.
C: A general term for unwanted modifications that a signal may undergo.
D: A type of information that improves signal clarity.
E: The intentional removal of certain parts of a signal.
Answer: C

Question 6: Why is noise reduction important in signal processing?
A: To amplify the noise.
B: To intentionally introduce unpredictability.
C: To ensure comfort noise.
D: To recover the original signal from the noise-corrupted one.
E: To degrade the original signal.
Answer: D

Question 7: What sets the mathematical limits for noise removal?
A: Quantum physics.
B: Biological principles.
C: Thermodynamics.
D: Classical mechanics.
E: Information theory.
Answer: E

Question 8: Which of the following books by Henri Atlan discussed the principle of "complexity from noise"?
A: The Structure of Scientific Revolutions
B: L'organisation biologique et la théorie de l'information
C: A Brief History of Time
D: The Nature of Complexity
E: Chaos and Order
Answer: B

Question 9: Which notion from Von Foerster did Henri Atlan develop in his work?
A: "Signal from noise"
B: "Emergence from simplicity"
C: "Order from chaos"
D: "Order from noise"
E: "Complexity from order"
Answer: D

Question 10: What is the main goal of complex systems theory?
A: To understand linear systems.
B: To decrease complexity and increase order.
C: To study the simple linkages at various scales.
D: To study these complex linkages at various scales.
E: To eliminate all noise from systems.
Answer: D
@
Subject:
There are many criteria by which superconductors are classified. The most common are:

Response to a magnetic field
A superconductor can be Type I, meaning it has a single critical field, above which all superconductivity is lost and below which the magnetic field is completely expelled from the superconductor; or Type II, meaning it has two critical fields, between which it allows partial penetration of the magnetic field through isolated points.[8] These points are called vortices.[9] Furthermore, in multicomponent superconductors it is possible to have a combination of the two behaviours. In that case the superconductor is of Type-1.5.[10]

By theory of operation
It is conventional if it can be explained by the BCS theory or its derivatives, or unconventional, otherwise.[11] Alternatively, a superconductor is called unconventional if the superconducting order parameter transforms according to a non-trivial irreducible representation of the point group or space group of the system.

By critical temperature
A superconductor is generally considered high-temperature if it reaches a superconducting state above a temperature of 30 K (−243.15 °C);[12] as in the initial discovery by Georg Bednorz and K. Alex Müller.[7] It may also reference materials that transition to superconductivity when cooled using liquid nitrogen – that is, at only Tc > 77 K, although this is generally used only to emphasize that liquid nitrogen coolant is sufficient. Low temperature superconductors refer to materials with a critical temperature below 30 K, and are cooled mainly by liquid helium (Tc > 4.2 K). One exception to this rule is the iron pnictide group of superconductors which display behaviour and properties typical of high-temperature superconductors, yet some of the group have critical temperatures below 30 K.

By material
Superconductor material classes include chemical elements (e.g. mercury or lead), alloys (such as niobium–titanium, germanium–niobium, and niobium nitride), ceramics (YBCO and magnesium diboride), superconducting pnictides (like fluorine-doped LaOFeAs) or organic superconductors (fullerenes and carbon nanotubes; though perhaps these examples should be included among the chemical elements, as they are composed entirely of carbon).[14][15]

In chemistry, thermodynamics, and other related fields, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.
$
10
Question 1: How is a Type I superconductor characterized in response to a magnetic field?
A: It has no critical field.
B: It has two critical fields and allows partial magnetic field penetration.
C: It has a single critical field above which superconductivity is lost.
D: It only allows magnetic fields when heated above a certain temperature.
E: It completely allows magnetic fields without any restrictions.
Answer: C

Question 2: What occurs between the two critical fields of a Type II superconductor?
A: Complete expulsion of the magnetic field.
B: No interaction with the magnetic field.
C: Partial penetration of the magnetic field through isolated points.
D: Total loss of superconductivity.
E: Magnetic field absorption without any restrictions.
Answer: C

Question 3: What are the isolated points in Type II superconductors called?
A: Electrons
B: Coils
C: Vortices
D: Holes
E: Fluxes
Answer: C

Question 4: If a superconductor is unconventional, which of the following is true?
A: It can be explained by BCS theory.
B: The order parameter transforms according to a non-trivial irreducible representation of the system's point or space group.
C: It operates at a temperature above 30 K.
D: It only responds to magnetic fields.
E: It can only be made from chemical elements.
Answer: B

Question 5: What defines a high-temperature superconductor based on its critical temperature?
A: Tc > 50 K
B: Tc > 4.2 K
C: Tc > 77 K
D: Tc > 30 K
E: Tc < 30 K
Answer: D

Question 6: Which of the following is NOT a class of superconductor materials?
A: Ceramics
B: Alloys
C: Gases
D: Chemical elements
E: Organic superconductors
Answer: C

Question 7: In thermodynamics, what defines a phase transition point?
A: The temperature at which a superconductor loses conductivity.
B: The point at which all physical properties of a system change.
C: The external conditions at which a transformation in a medium occurs.
D: The moment when a system starts emitting energy.
E: The temperature at which a substance reaches its critical field.
Answer: C

Question 8: Which of the following is considered a phase transition between basic states of matter?
A: From solid to superconductor
B: From Type I to Type II superconductor
C: From liquid to gas upon heating to its boiling point
D: From unconventional to conventional superconductor
E: From high-temperature to low-temperature superconductor
Answer: C

Question 9: What characterizes a superconductor of Type-1.5?
A: It has two critical fields and does not allow any penetration of the magnetic field.
B: It is a combination of behaviors of both Type I and Type II superconductors.
C: It operates only at temperatures below 1.5 K.
D: It can only be explained by BCS theory.
E: It is made exclusively of chemical elements.
Answer: B

Question 10: Which pair represents an alloy used in superconductor materials?
A: YBCO and magnesium diboride
B: Fluorine-doped LaOFeAs and carbon nanotubes
C: Niobium–titanium and germanium–niobium
D: Solid and liquid
E: Organic and ceramic
Answer: C
@
Subject:
The Sun emits light across the visible spectrum, so its color is white, with a CIE color-space index near (0.3, 0.3), when viewed from space or when the Sun is high in the sky. The Solar radiance per wavelength peaks in the green portion of the spectrum when viewed from space.[94][95] When the Sun is very low in the sky, atmospheric scattering renders the Sun yellow, red, orange, or magenta, and in rare occasions even green or blue. Despite its typical whiteness (white sunrays, white ambient light, white illumination of the Moon, etc.), some cultures mentally picture the Sun as yellow and some even red; the reasons for this are cultural and exact ones are the subject of debate.[96] The Sun is a G2V star, with G2 indicating its surface temperature of approximately 5,778 K (5,505 °C; 9,941 °F), and V that it, like most stars, is a main-sequence star.[61][97]

The solar constant is the amount of power that the Sun deposits per unit area that is directly exposed to sunlight. The solar constant is equal to approximately 1,368 W/m2 (watts per square meter) at a distance of one astronomical unit (AU) from the Sun (that is, on or near Earth).[98] Sunlight on the surface of Earth is attenuated by Earth's atmosphere, so that less power arrives at the surface (closer to 1,000 W/m2) in clear conditions when the Sun is near the zenith.[99] Sunlight at the top of Earth's atmosphere is composed (by total energy) of about 50% infrared light, 40% visible light, and 10% ultraviolet light.[100] The atmosphere in particular filters out over 70% of solar ultraviolet, especially at the shorter wavelengths.[101] Solar ultraviolet radiation ionizes Earth's dayside upper atmosphere, creating the electrically conducting ionosphere.[102]

Ultraviolet light from the Sun has antiseptic properties and can be used to sanitize tools and water. It also causes sunburn, and has other biological effects such as the production of vitamin D and sun tanning. It is also the main cause of skin cancer. Ultraviolet light is strongly attenuated by Earth's ozone layer, so that the amount of UV varies greatly with latitude and has been partially responsible for many biological adaptations, including variations in human skin color in different regions of the Earth.[103]

High-energy gamma ray photons initially released with fusion reactions in the core are almost immediately absorbed by the solar plasma of the radiative zone, usually after traveling only a few millimeters. Re-emission happens in a random direction and usually at slightly lower energy. With this sequence of emissions and absorptions, it takes a long time for radiation to reach the Sun's surface. Estimates of the photon travel time range between 10,000 and 170,000 years.[104] In contrast, it takes only 2.3 seconds for neutrinos, which account for about 2% of the total energy production of the Sun, to reach the surface. Because energy transport in the Sun is a process that involves photons in thermodynamic equilibrium with matter, the time scale of energy transport in the Sun is longer, on the order of 30,000,000 years. This is the time it would take the Sun to return to a stable state if the rate of energy generation in its core were suddenly changed.[105]

Neutrinos are also released by fusion reactions in the core, but, unlike photons, they rarely interact with matter, so almost all are able to escape the Sun immediately. For many years, measurements of the number of neutrinos produced in the Sun were lower than theories predicted by a factor of 3. This discrepancy was resolved in 2001 through the discovery of the effects of neutrino oscillation: the Sun emits the number of neutrinos predicted by the theory, but neutrino detectors were missing 2⁄3 of them because the neutrinos had changed flavor by the time they were detected.[106]
$
9
Question 1: What color is the Sun when viewed from space or when high in the sky?
A: Yellow
B: Red
C: Blue
D: Green
E: White
Answer: E

Question 2: Where does the Solar radiance per wavelength peak when viewed from space?
A: Red portion
B: Blue portion
C: Green portion
D: Yellow portion
E: Ultraviolet portion
Answer: C

Question 3: What is the solar constant at a distance of one astronomical unit (AU) from the Sun?
A: 1,200 W/m2
B: 1,000 W/m2
C: 1,468 W/m2
D: 1,368 W/m2
E: 1,568 W/m2
Answer: D

Question 4: Which portion of sunlight at the top of Earth's atmosphere has the highest energy composition?
A: Visible light
B: Infrared light
C: Ultraviolet light
D: Gamma ray photons
E: Neutrinos
Answer: B

Question 5: How does ultraviolet light from the Sun affect the skin?
A: It moisturizes the skin
B: It gives the skin a blue tint
C: It is the main cause of skin cancer
D: It reduces the risk of sunburn
E: It helps in skin hydration
Answer: C

Question 6: How long does it take for the neutrinos, which account for about 2% of the total energy production of the Sun, to reach the surface?
A: 30,000,000 years
B: 10,000 years
C: 2.3 seconds
D: 170,000 years
E: 5 minutes
Answer: C

Question 7: Why were neutrino detectors initially detecting fewer neutrinos than theories predicted?
A: The Sun was emitting fewer neutrinos than anticipated
B: The neutrinos were absorbed by Earth's atmosphere
C: The neutrino detectors were malfunctioning
D: Neutrinos had changed flavor by the time they were detected
E: The theory overestimated the neutrino emission
Answer: D

Question 8: Which region of sunlight is filtered out over 70% by the atmosphere?
A: Visible light
B: Infrared light
C: Gamma rays
D: Solar radiance
E: Ultraviolet light
Answer: E

Question 9: What is the main biological effect of ultraviolet light apart from causing sunburn?
A: Enhancing vision
B: Slowing down metabolism
C: Production of vitamin D
D: Increasing hair growth
E: Boosting energy
Answer: C
@
Subject:
Micromagnetics is a field of physics dealing with the prediction of magnetic behaviors at sub-micrometer length scales. The length scales considered are large enough for the atomic structure of the material to be ignored (the continuum approximation), yet small enough to resolve magnetic structures such as domain walls or vortices.

Micromagnetics can deal with static equilibria, by minimizing the magnetic energy, and with dynamic behavior, by solving the time-dependent dynamical equation.

In physics, the Landau–Lifshitz–Gilbert equation, named for Lev Landau, Evgeny Lifshitz, and T. L. Gilbert, is a name used for a differential equation describing the precessional motion of magnetization M in a solid. It is a modification by Gilbert of the original equation of Landau and Lifshitz.

The various forms of the equation are commonly used in micromagnetics to model the effects of a magnetic field on ferromagnetic materials. In particular it can be used to model the time domain behavior of magnetic elements due to a magnetic field.[1] An additional term was added to the equation to describe the effect of spin polarized current on magnets.[2]
$
5
Question 1: Which of the following best describes the scale at which micromagnetics operates?
A: At the atomic scale, considering individual atoms.
B: At a macroscopic scale, similar to everyday objects.
C: At a scale where atomic structures can be ignored but magnetic structures such as domain walls are resolvable.
D: At a scale where domain walls are ignored but atomic structures are considered.
E: At the quantum scale, considering individual electrons and their spins.

Answer: C

Question 2: How does micromagnetics approach the study of static magnetic behavior?
A: By minimizing gravitational energy.
B: By solving the time-dependent dynamical equation.
C: By maximizing the magnetic energy.
D: By minimizing the magnetic energy.
E: By analyzing the magnetic fields in liquids.

Answer: D

Question 3: Who among the following did NOT contribute to the original form of the Landau–Lifshitz–Gilbert equation?
A: Lev Landau
B: T. L. Gilbert
C: Evgeny Lifshitz
D: Albert Einstein
E: Richard Feynman

Answer: D

Question 4: What was the purpose of adding an additional term to the Landau–Lifshitz–Gilbert equation?
A: To describe the effect of electric current on magnets.
B: To account for the effects of gravitational forces on magnetization.
C: To describe the effect of spin polarized current on magnets.
D: To factor in the effects of magnetic fields on non-ferromagnetic materials.
E: To address the macroscopic behavior of magnetic fields.

Answer: C

Question 5: The continuum approximation in micromagnetics implies that:
A: Both atomic structures and magnetic structures such as domain walls are considered.
B: Only atomic structures are considered, while magnetic structures are ignored.
C: Atomic structures are ignored, while macroscopic behaviors are the focus.
D: Atomic structures are ignored, but magnetic structures such as domain walls are resolvable.
E: Only the behavior of magnetic fields in liquids is considered.

Answer: D
@
Subject:
In the physics of continuous media, spatial dispersion is a phenomenon where material parameters such as permittivity or conductivity have dependence on wavevector. Normally, such a dependence is assumed to be absent for simplicity, however spatial dispersion exists to varying degrees in all materials.

Spatial dispersion can be compared to temporal dispersion, the latter often just called dispersion. Temporal dispersion represents memory effects in systems, commonly seen in optics and electronics. Spatial dispersion on the other hand represents spreading effects and is usually significant only at microscopic length scales. Spatial dispersion contributes relatively small perturbations to optics, giving weak effects such as optical activity. Spatial dispersion and temporal dispersion may occur in the same system.

In physics, a wave vector (or wavevector) is a vector used in describing a wave, with a typical unit being cycle per metre. It has a magnitude and direction. Its magnitude is the wavenumber of the wave (inversely proportional to the wavelength), and its direction is perpendicular to the wavefront. In isotropic media, this is also the direction of wave propagation.

A closely related vector is the angular wave vector (or angular wavevector), with a typical unit being radian per metre. The wave vector and angular wave vector are related by a fixed constant of proportionality, 2π radians per cycle.[a]
$
5
Question 1: Which of the following statements about wave vectors is false?
A: A wave vector describes a wave with units typically being cycle per metre.
B: The magnitude of a wave vector is directly proportional to the wavelength of the wave.
C: A wave vector has both magnitude and direction.
D: The direction of a wave vector is perpendicular to the wavefront.
E: In isotropic media, the wave vector direction is also the direction of wave propagation.

Answer: B

Question 2: How is temporal dispersion best described in comparison to spatial dispersion?
A: Temporal dispersion has no effect on optics and electronics.
B: Temporal dispersion is related to the dependence of material parameters on wavevector.
C: Temporal dispersion represents spreading effects, significant at microscopic length scales.
D: Temporal dispersion represents memory effects, commonly seen in optics and electronics.
E: Temporal dispersion and spatial dispersion cannot occur in the same system.

Answer: D

Question 3: How is the angular wave vector related to the wave vector?
A: They are unrelated quantities used in different contexts.
B: The angular wave vector is always smaller than the wave vector in magnitude.
C: The angular wave vector and wave vector are related by a variable constant.
D: They are the same and can be used interchangeably.
E: The wave vector and angular wave vector are related by a fixed constant of proportionality, 2π radians per cycle.

Answer: E

Question 4: Why is spatial dispersion usually significant?
A: It mainly affects optics and electronics at macroscopic scales.
B: It gives strong effects such as optical activity in most materials.
C: It mainly contributes at microscopic length scales.
D: It is a direct result of temporal dispersion.
E: It affects the memory effects in systems.

Answer: C

Question 5: Which statement best describes the typical effects of spatial dispersion on optics?
A: Spatial dispersion has a large and obvious impact on optics.
B: Spatial dispersion does not affect optics at all.
C: Spatial dispersion contributes to memory effects in optics.
D: Spatial dispersion contributes relatively small perturbations to optics.
E: Spatial dispersion primarily enhances the clarity of optical systems.

Answer: D
@
Subject:
In cosmology and physics, cold dark matter (CDM) is a hypothetical type of dark matter. According to the current standard model of cosmology, Lambda-CDM model, approximately 27% of the universe is dark matter and 68% is dark energy, with only a small fraction being the ordinary baryonic matter that composes stars, planets, and living organisms. Cold refers to the fact that the dark matter moves slowly compared to the speed of light, giving it a vanishing equation of state. Dark indicates that it interacts very weakly with ordinary matter and electromagnetic radiation. Proposed candidates for CDM include weakly interacting massive particles, primordial black holes, and axions.

History
The theory of cold dark matter was originally published in 1982 by James Peebles;[1] while the warm dark matter picture was proposed independently at the same time by J. Richard Bond, Alex Szalay, and Michael Turner;[2] and George Blumenthal, H. Pagels, and Joel Primack.[3] A review article in 1984 by Blumenthal, Sandra Moore Faber, Primack, and Martin Rees developed the details of the theory.[4]

Structure formation
In the cold dark matter theory, structure grows hierarchically, with small objects collapsing under their self-gravity first and merging in a continuous hierarchy to form larger and more massive objects. Predictions of the cold dark matter paradigm are in general agreement with observations of cosmological large-scale structure.

In the hot dark matter paradigm, popular in the early 1980s and less so now, structure does not form hierarchically (bottom-up), but forms by fragmentation (top-down), with the largest superclusters forming first in flat pancake-like sheets and subsequently fragmenting into smaller pieces like our galaxy the Milky Way.

Since the late 1980s or 1990s, most cosmologists favor the cold dark matter theory (specifically the modern Lambda-CDM model) as a description of how the universe went from a smooth initial state at early times (as shown by the cosmic microwave background radiation) to the lumpy distribution of galaxies and their clusters we see today—the large-scale structure of the universe. Dwarf galaxies are crucial to this theory, having been created by small-scale density fluctuations in the early universe;[5] they have now become natural building blocks that form larger structures.

Composition
Dark matter is detected through its gravitational interactions with ordinary matter and radiation. As such, it is very difficult to determine what the constituents of cold dark matter are. The candidates fall roughly into three categories:

Axions, very light particles with a specific type of self-interaction that makes them a suitable CDM candidate.[6][7] In recent years, axions have become one of the most promising candidates for dark matter.[8] Axions have the theoretical advantage that their existence solves the strong CP problem in quantum chromodynamics, but axion particles have only been theorized and never detected. Axions are an example of a more general category of particle called a WISP (weakly interacting "slender" or "slim" particle), which are the low-mass counterparts of WIMPs.
Massive compact halo objects (MACHOs), large, condensed objects such as black holes, neutron stars, white dwarfs, very faint stars, or non-luminous objects like planets. The search for these objects consists of using gravitational lensing to detect the effects of these objects on background galaxies. Most experts believe that the constraints from those searches rule out MACHOs as a viable dark matter candidate.[9][10][11][12][13][14]
Weakly interacting massive particles (WIMPs). There is no currently known particle with the required properties, but many extensions of the standard model of particle physics predict such particles. The search for WIMPs involves attempts at direct detection by highly sensitive detectors, as well as attempts at production of WIMPs by particle accelerators. Historically, WIMPs were regarded as one of the most promising candidates for the composition of dark matter,[10][12][14] but in recent years WIMPs have since been supplanted by axions with the non-detection of WIMPs in experiments.[8] The DAMA/NaI experiment and its successor DAMA/LIBRA have claimed to have directly detected dark matter particles passing through the Earth, but many scientists remain skeptical because no results from similar experiments seem compatible with the DAMA results.
$
9
Question 2: What percentage of the universe is composed of dark matter according to the Lambda-CDM model?
A: 10%
B: 50%
C: 27%
D: 68%
E: 40%

Answer: C

Question 3: Why is the term "cold" used in reference to cold dark matter?
A: Because the dark matter emits no heat.
B: Because the dark matter moves slowly compared to the speed of light.
C: Because the dark matter originates from cold regions of space.
D: Because the dark matter is found in cold galaxies.
E: Because it reacts strongly with cold temperatures.

Answer: B

Question 4: Who originally published the theory of cold dark matter?
A: J. Richard Bond
B: George Blumenthal
C: Alex Szalay
D: James Peebles
E: Michael Turner

Answer: D

Question 5: How does the cold dark matter theory describe the formation of structures in the universe?
A: Formation happens hierarchically, with small objects merging to form larger ones.
B: Structures form by fragmentation with large superclusters forming first.
C: Structures form randomly with no distinct pattern.
D: Formation is linear, with medium-sized objects forming first.
E: Structures form based on their temperature.

Answer: A

Question 6: How does the hot dark matter paradigm differ in terms of structure formation?
A: Structures form by fragmentation with small objects forming first.
B: Structures form hierarchically, with large objects collapsing first.
C: Structures form by fragmentation with large superclusters forming first.
D: Structures form in spherical patterns.
E: Structures form due to radiation.

Answer: C

Question 7: Why are dwarf galaxies crucial to the cold dark matter theory?
A: Because they are considered the oldest galaxies in the universe.
B: Because they are composed mostly of hot dark matter.
C: Because they were created by small-scale density fluctuations in the early universe.
D: Because they contain a large number of MACHOs.
E: Because they serve as a counter to the Lambda-CDM model.

Answer: C

Question 8: What advantage do axions have from a theoretical standpoint?
A: They are highly luminous and can be easily detected.
B: They are highly reactive with baryonic matter.
C: Their existence solves the strong CP problem in quantum chromodynamics.
D: They can be detected using gravitational lensing.
E: They have been directly detected in the DAMA experiments.

Answer: C

Question 9: Which candidate for dark matter has been ruled out as a viable dark matter candidate by most experts?
A: WIMPs
B: Axions
C: Brown Dwarfs
D: MACHOs
E: Neutron Stars

Answer: D

Question 10: Which dark matter candidate was historically considered promising but has been supplanted by axions in recent years?
A: Neutron Stars
B: MACHOs
C: WIMPs
D: White Dwarfs
E: Black Holes

Answer: C
@
Subject:
Fourier-transform infrared spectroscopy (FTIR)[1] is a technique used to obtain an infrared spectrum of absorption or emission of a solid, liquid, or gas. An FTIR spectrometer simultaneously collects high-resolution spectral data over a wide spectral range. This confers a significant advantage over a dispersive spectrometer, which measures intensity over a narrow range of wavelengths at a time.

The term Fourier-transform infrared spectroscopy originates from the fact that a Fourier transform (a mathematical process) is required to convert the raw data into the actual spectrum.

The goal of absorption spectroscopy techniques (FTIR, ultraviolet-visible ("UV-vis") spectroscopy, etc.) is to measure how much light a sample absorbs at each wavelength. The most straightforward way to do this, the "dispersive spectroscopy" technique, is to shine a monochromatic light beam at a sample, measure how much of the light is absorbed, and repeat for each different wavelength. (This is how some UV–vis spectrometers work, for example.)

Fourier-transform spectroscopy is a less intuitive way to obtain the same information. Rather than shining a monochromatic beam of light (a beam composed of only a single wavelength) at the sample, this technique shines a beam containing many frequencies of light at once and measures how much of that beam is absorbed by the sample. Next, the beam is modified to contain a different combination of frequencies, giving a second data point. This process is rapidly repeated many times over a short time span. Afterwards, a computer takes all this data and works backward to infer what the absorption is at each wavelength.

The beam described above is generated by starting with a broadband light source—one containing the full spectrum of wavelengths to be measured. The light shines into a Michelson interferometer—a certain configuration of mirrors, one of which is moved by a motor. As this mirror moves, each wavelength of light in the beam is periodically blocked, transmitted, blocked, transmitted, by the interferometer, due to wave interference. Different wavelengths are modulated at different rates, so that at each moment or mirror position the beam coming out of the interferometer has a different spectrum.

As mentioned, computer processing is required to turn the raw data (light absorption for each mirror position) into the desired result (light absorption for each wavelength). The processing required turns out to be a common algorithm called the Fourier transform. The Fourier transform converts one domain (in this case displacement of the mirror in cm) into its inverse domain (wavenumbers in cm−1). The raw data is called an "interferogram".

Advantages
There are three principal advantages for an FT spectrometer compared to a scanning (dispersive) spectrometer.[1]

The multiplex or Fellgett's advantage. This arises from the fact that information from all wavelengths is collected simultaneously. It results in a higher signal-to-noise ratio for a given scan-time for observations limited by a fixed detector noise contribution (typically in the thermal infrared spectral region where a photodetector is limited by generation-recombination noise). For a spectrum with m resolution elements, this increase is equal to the square root of m. Alternatively, it allows a shorter scan-time for a given resolution. In practice multiple scans are often averaged, increasing the signal-to-noise ratio by the square root of the number of scans.
The throughput or Jacquinot's advantage. This results from the fact that in a dispersive instrument, the monochromator has entrance and exit slits which restrict the amount of light that passes through it. The interferometer throughput is determined only by the diameter of the collimated beam coming from the source. Although no slits are needed, FTIR spectrometers do require an aperture to restrict the convergence of the collimated beam in the interferometer. This is because convergent rays are modulated at different frequencies as the path difference is varied. Such an aperture is called a Jacquinot stop.[1] For a given resolution and wavelength this circular aperture allows more light through than a slit, resulting in a higher signal-to-noise ratio.
The wavelength accuracy or Connes' advantage. The wavelength scale is calibrated by a laser beam of known wavelength that passes through the interferometer. This is much more stable and accurate than in dispersive instruments where the scale depends on the mechanical movement of diffraction gratings. In practice, the accuracy is limited by the divergence of the beam in the interferometer which depends on the resolution.
Another minor advantage is less sensitivity to stray light, that is radiation of one wavelength appearing at another wavelength in the spectrum. In dispersive instruments, this is the result of imperfections in the diffraction gratings and accidental reflections. In FT instruments there is no direct equivalent as the apparent wavelength is determined by the modulation frequency in the interferometer.
$
9
Question 2: Which mathematical process is used to convert the raw data of FTIR into the actual spectrum?
A: Differentiation
B: Integration
C: Wave function
D: Fourier transform
E: Matrix multiplication

Answer: D

Question 3: In dispersive spectroscopy, how is the absorption of light at different wavelengths measured?
A: By shining a beam containing many frequencies of light at once.
B: By using a broadband light source with a full spectrum of wavelengths.
C: By shining a monochromatic light beam at a sample for each wavelength.
D: By modifying the beam to contain a different combination of frequencies.
E: By using wave interference patterns.

Answer: C

Question 4: How is the light beam in Fourier-transform spectroscopy modified?
A: By using different chemical reagents.
B: By using a broadband light source.
C: By shining through a series of different colored filters.
D: By a Michelson interferometer and moving a mirror to block and transmit wavelengths.
E: By focusing through a magnifying glass.

Answer: D

Question 5: What is the raw data of FTIR referred to as?
A: Spectrogram
B: Hologram
C: Diffraction pattern
D: Interferogram
E: Modulogram

Answer: D

Question 6: What principal advantage of an FT spectrometer arises from the fact that information from all wavelengths is collected simultaneously?
A: Jacquinot's advantage
B: Connes' advantage
C: Dispersion advantage
D: Fellgett's advantage
E: Inference advantage

Answer: D

Question 7: In a dispersive spectrometer, what restricts the amount of light passing through?
A: The diameter of the collimated beam.
B: The Jacquinot stop.
C: The entrance and exit slits of the monochromator.
D: The wavelength scale.
E: The diffraction gratings.

Answer: C

Question 8: How is the wavelength scale calibrated in an FT spectrometer?
A: By the modulation frequency in the interferometer.
B: By using diffraction gratings.
C: By a laser beam of known wavelength passing through the interferometer.
D: By the resolution of the spectrometer.
E: By the entrance and exit slits of the monochromator.

Answer: C

Question 9: Which of the following is not a principal advantage of an FT spectrometer compared to a scanning (dispersive) spectrometer?
A: Reduced sensitivity to stray light.
B: Increased generation-recombination noise.
C: Higher signal-to-noise ratio due to simultaneous collection of all wavelengths.
D: Higher throughput due to the absence of restrictive slits.
E: Improved wavelength accuracy due to laser calibration.

Answer: B

Question 10: In an FT instrument, the apparent wavelength is determined by what?
A: The broadband light source.
B: The mechanical movement of diffraction gratings.
C: The modulation frequency in the interferometer.
D: The entrance and exit slits of the monochromator.
E: The laser beam of known wavelength.

Answer: C
@
Subject:
Paramagnetism is a form of magnetism whereby some materials are weakly attracted by an externally applied magnetic field, and form internal, induced magnetic fields in the direction of the applied magnetic field. In contrast with this behavior, diamagnetic materials are repelled by magnetic fields and form induced magnetic fields in the direction opposite to that of the applied magnetic field.[1] Paramagnetic materials include most chemical elements and some compounds;[2] they have a relative magnetic permeability slightly greater than 1 (i.e., a small positive magnetic susceptibility) and hence are attracted to magnetic fields. The magnetic moment induced by the applied field is linear in the field strength and rather weak. It typically requires a sensitive analytical balance to detect the effect and modern measurements on paramagnetic materials are often conducted with a SQUID magnetometer.

Paramagnetism is due to the presence of unpaired electrons in the material, so most atoms with incompletely filled atomic orbitals are paramagnetic, although exceptions such as copper exist. Due to their spin, unpaired electrons have a magnetic dipole moment and act like tiny magnets. An external magnetic field causes the electrons' spins to align parallel to the field, causing a net attraction. Paramagnetic materials include aluminium, oxygen, titanium, and iron oxide (FeO). Therefore, a simple rule of thumb is used in chemistry to determine whether a particle (atom, ion, or molecule) is paramagnetic or diamagnetic:[3] if all electrons in the particle are paired, then the substance made of this particle is diamagnetic; if it has unpaired electrons, then the substance is paramagnetic.

Unlike ferromagnets, paramagnets do not retain any magnetization in the absence of an externally applied magnetic field because thermal motion randomizes the spin orientations. (Some paramagnetic materials retain spin disorder even at absolute zero, meaning they are paramagnetic in the ground state, i.e. in the absence of thermal motion.) Thus the total magnetization drops to zero when the applied field is removed. Even in the presence of the field there is only a small induced magnetization because only a small fraction of the spins will be oriented by the field. This fraction is proportional to the field strength and this explains the linear dependency. The attraction experienced by ferromagnetic materials is non-linear and much stronger, so that it is easily observed, for instance, in the attraction between a refrigerator magnet and the iron of the refrigerator itself.

Constituent atoms or molecules of paramagnetic materials have permanent magnetic moments (dipoles), even in the absence of an applied field. The permanent moment generally is due to the spin of unpaired electrons in atomic or molecular electron orbitals (see Magnetic moment). In pure paramagnetism, the dipoles do not interact with one another and are randomly oriented in the absence of an external field due to thermal agitation, resulting in zero net magnetic moment. When a magnetic field is applied, the dipoles will tend to align with the applied field, resulting in a net magnetic moment in the direction of the applied field. In the classical description, this alignment can be understood to occur due to a torque being provided on the magnetic moments by an applied field, which tries to align the dipoles parallel to the applied field. However, the true origins of the alignment can only be understood via the quantum-mechanical properties of spin and angular momentum.

In electromagnetism, the magnetic moment is the magnetic strength and orientation of a magnet or other object that produces a magnetic field, expressed as a vector. Examples of objects that have magnetic moments include loops of electric current (such as electromagnets), permanent magnets, elementary particles (such as electrons), composite particles (such as protons and neutrons), various molecules, and many astronomical objects (such as many planets, some moons, stars, etc).
$
9
Question 2: Which of the following is a property of paramagnetic materials when an externally applied magnetic field is removed?
A: They retain a strong magnetization.
B: Their total magnetization increases.
C: Their total magnetization drops to zero.
D: They act like ferromagnetic materials.
E: Their spins align perpendicular to the field.

Answer: C

Question 3: What determines if a particle (atom, ion, or molecule) is paramagnetic?
A: If all electrons in the particle are paired.
B: If it has an even number of electrons.
C: If it has unpaired electrons.
D: If it has no electrons in its outermost shell.
E: If it has a magnetic dipole moment.

Answer: C

Question 4: Which of the following materials is NOT paramagnetic?
A: Aluminium
B: Oxygen
C: Titanium
D: Copper
E: Iron oxide (FeO)

Answer: D

Question 5: Why don't paramagnetic materials retain any magnetization in the absence of an externally applied magnetic field?
A: Because they act like ferromagnetic materials.
B: Because thermal motion randomizes the spin orientations.
C: Because they have no unpaired electrons.
D: Because they have a linear dependency on the field.
E: Because they interact with other magnetic fields.

Answer: B

Question 6: In the classical description, why do the dipoles in paramagnetic materials tend to align with an applied magnetic field?
A: Because of the interaction with other dipoles.
B: Due to the torque provided on the magnetic moments by the applied field.
C: Because of the quantum-mechanical properties of spin.
D: Due to their attraction to other metals.
E: Because of their intrinsic magnetic field.

Answer: B

Question 7: What is the definition of a magnetic moment in electromagnetism?
A: The ability of a material to repel a magnetic field.
B: The magnetic field produced by an object.
C: The force applied by a magnet on an object.
D: The magnetic strength and orientation of a magnet or other object that produces a magnetic field.
E: The alignment of all the atoms in a magnetic material.

Answer: D

Question 8: In which materials does the spin of unpaired electrons play a role in producing a magnetic moment?
A: Diamagnetic materials
B: Ferromagnetic materials
C: Paramagnetic materials
D: Superconducting materials
E: Non-magnetic materials

Answer: C

Question 9: How do most atoms with incompletely filled atomic orbitals behave in terms of magnetism?
A: They are usually diamagnetic.
B: They are usually ferromagnetic.
C: They are usually non-magnetic.
D: They are usually paramagnetic.
E: Their behavior is unpredictable.

Answer: D

Question 10: Which term best describes materials that are repelled by magnetic fields and form induced magnetic fields in the direction opposite to that of the applied magnetic field?
A: Ferromagnetic
B: Non-magnetic
C: Paramagnetic
D: Diamagnetic
E: Electromagnetic

Answer: D
@
Subject:
Spin is an intrinsic form of angular momentum carried by elementary particles, and thus by composite particles such as hadrons, atomic nuclei, and atoms.[1][2]: 183–184  Spin should not be understood as in the "rotating internal mass" sense: spin is a quantized wave property.[3]

The existence of electron spin angular momentum is inferred from experiments, such as the Stern–Gerlach experiment, in which silver atoms were observed to possess two possible discrete angular momenta despite having no orbital angular momentum.[4] The existence of the electron spin can also be inferred theoretically from the spin–statistics theorem and from the Pauli exclusion principle—and vice versa, given the particular spin of the electron, one may derive the Pauli exclusion principle.

Spin is described mathematically as a vector for some particles such as photons, and as spinors and bispinors for other particles such as electrons. Spinors and bispinors behave similarly to vectors: they have definite magnitudes and change under rotations; however, they use an unconventional "direction". All elementary particles of a given kind have the same magnitude of spin angular momentum, though its direction may change. These are indicated by assigning the particle a spin quantum number.[2]: 183–184 

The SI unit of spin is the same as classical angular momentum (i.e., N·m·s, J·s, or kg·m2·s−1). In practice, spin is usually given as a dimensionless spin quantum number by dividing the spin angular momentum by the reduced Planck constant ħ, which has the same dimensions as angular momentum. Often, the "spin quantum number" is simply called "spin".

The very earliest models for electron spin imagined a rotating charged mass, but this model fails when examined in detail: the required space distribution does not match limits on the electron radius: the required rotation speed exceeds the speed of light. In the Standard Model, the fundamental particles are all considered "point-like": they have their effects through the field that surrounds them.[5] Any model for spin based on mass rotation would need to be consistent with that model.

The classical analog for quantum spin is a circulation of energy or momentum-density in the particle wave field: "spin is essentially a wave property".[3] This same concept of spin can be applied to gravity waves in water: "spin is generated by subwavelength circular motion of water particles".[6]

Photon spin is the quantum-mechanical description of light polarization, where spin +1 and spin −1 represent two opposite directions of circular polarization. Thus, light of a defined circular polarization consists of photons with the same spin, either all +1 or all −1. Spin represents polarization for other vector bosons as well.

Newton's laws of motion are three basic laws of classical mechanics that describe the relationship between the motion of an object and the forces acting on it. These laws can be paraphrased as follows:

A body remains at rest, or in motion at a constant speed in a straight line, unless acted upon by a force.
When a body is acted upon by a net force, the body's acceleration multiplied by its mass is equal to the net force.
If two bodies exert forces on each other, these forces have the same magnitude but opposite directions.[2]
The three laws of motion were first stated by Isaac Newton in his Philosophiæ Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy), originally published in 1687.[3] Newton used them to investigate and explain the motion of many physical objects and systems, which laid the foundation for classical mechanics. In the time since Newton, the conceptual content of classical physics has been reformulated in alternative ways, involving different mathematical approaches that have yielded insights which were obscured in the original, Newtonian formulation. Limitations to Newton's laws have also been discovered; new theories are necessary when objects move at very high speeds (special relativity), are very massive (general relativity), or are very small (quantum mechanics).
$
9
Question 2: What is the primary conclusion drawn from the Stern–Gerlach experiment about silver atoms?
A: Silver atoms have multiple possible angular momenta.
B: Silver atoms have no orbital angular momentum.
C: Silver atoms are found to possess two possible discrete angular momenta.
D: Silver atoms are found to rotate when placed in a magnetic field.
E: Silver atoms can only possess a single angular momentum.

Answer: C

Question 3: How is spin mathematically described for photons?
A: As spinors.
B: As bispinors.
C: As vectors.
D: As tensors.
E: As scalars.

Answer: C

Question 4: Which statement is true about elementary particles of a given kind?
A: They have varying magnitudes of spin angular momentum.
B: They can be described using the mass rotation model.
C: Their spin can always be related to a physical rotation.
D: They have the same magnitude of spin angular momentum.
E: Their spin direction remains constant under all conditions.

Answer: D

Question 5: What is the relation between photon spin and light polarization?
A: Photon spin determines the amplitude of light.
B: Photon spin represents two opposite directions of linear polarization.
C: Photon spin is the quantum-mechanical description of light polarization.
D: Photon spin has no relation to light polarization.
E: Photon spin describes the frequency of light polarization.

Answer: C

Question 6: How does quantum spin differ from the early models of electron spin that imagined a rotating charged mass?
A: Quantum spin aligns with the rotation model.
B: Quantum spin matches the limits on the electron radius.
C: Quantum spin is related to the speed of light.
D: Quantum spin requires the rotation speed to exceed the speed of light.
E: Quantum spin does not match the required space distribution and rotation speed exceeds the speed of light.

Answer: E

Question 7: What is the SI unit of spin?
A: J
B: ħ
C: N·m·s
D: m/s
E: kg·m

Answer: C

Question 8: What does the "spin quantum number" commonly refer to?
A: The ratio of the spin angular momentum to the speed of light.
B: The ratio of the spin angular momentum to the mass of the particle.
C: The spin angular momentum multiplied by the reduced Planck constant ħ.
D: The spin angular momentum divided by the reduced Planck constant ħ.
E: The spin angular momentum as a factor of the electron radius.

Answer: D

Question 9: In the context of waves, how can spin be described?
A: Spin is related to the amplitude of the wave.
B: Spin is the frequency with which waves oscillate.
C: Spin is generated by subwavelength circular motion of water particles.
D: Spin is the velocity of the waves.
E: Spin is the reflection of waves on a surface.

Answer: C

Question 10: According to Newton's first law of motion, what happens to a body in motion at a constant speed in a straight line?
A: It accelerates due to inherent forces.
B: It changes direction randomly.
C: It remains in that state unless acted upon by a force.
D: It stops after some time due to natural resistance.
E: It will always experience a force due to its motion.

Answer: C
@
Subject:
In chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.

The term is closely associated with the work of mathematician and meteorologist Edward Norton Lorenz. He noted that the butterfly effect is derived from the metaphorical example of the details of a tornado (the exact time of formation, the exact path taken) being influenced by minor perturbations such as a distant butterfly flapping its wings several weeks earlier. Lorenz originally used a seagull causing a storm but was persuaded to make it more poetic with the use of a butterfly and tornado by 1972.[1][2] He discovered the effect when he observed runs of his weather model with initial condition data that were rounded in a seemingly inconsequential manner. He noted that the weather model would fail to reproduce the results of runs with the unrounded initial condition data. A very small change in initial conditions had created a significantly different outcome.[3]

The idea that small causes may have large effects in weather was earlier acknowledged by French mathematician and engineer Henri Poincaré. American mathematician and philosopher Norbert Wiener also contributed to this theory. Lorenz's work placed the concept of instability of the Earth's atmosphere onto a quantitative base and linked the concept of instability to the properties of large classes of dynamic systems which are undergoing nonlinear dynamics and deterministic chaos.[4]

The butterfly effect concept has since been used outside the context of weather science as a broad term for any situation where a small change is supposed to be the cause of larger consequences.
$
9
Question 2: Who is primarily credited for introducing the concept of the butterfly effect in relation to weather modeling?
A: Henri Poincaré
B: Norbert Wiener
C: Edward Norton Lorenz
D: Albert Einstein
E: Isaac Newton

Answer: C

Question 3: How did Lorenz discover the butterfly effect in his weather model?
A: By observing a butterfly directly affecting weather patterns.
B: By realizing a large change in the weather model always led to a small outcome.
C: By noticing that his weather model would produce different results with slightly rounded initial condition data.
D: By comparing his weather model to the theories proposed by Einstein.
E: By observing the long-term effects of actual tornadoes.

Answer: C

Question 4: What metaphor did Lorenz originally use before switching to the butterfly effect?
A: A bee causing a rainstorm.
B: An ant causing a flood.
C: A bird causing a hurricane.
D: A seagull causing a storm.
E: A fish causing a tsunami.

Answer: D

Question 5: Which mathematician earlier acknowledged the idea that small causes may have large effects in weather?
A: Albert Einstein
B: Sir Isaac Newton
C: Henri Poincaré
D: Edward Norton Lorenz
E: Norbert Wiener

Answer: C

Question 6: In which field has the butterfly effect concept been broadly applied outside its original context?
A: Only in weather science.
B: In mathematical equations.
C: As a term for any situation where a small change results in larger consequences.
D: Exclusively in the study of tornadoes and storms.
E: Only in Lorenz's subsequent publications.

Answer: C

Question 7: The butterfly effect emphasizes the significance of which type of dynamics?
A: Linear dynamics
B: Monotonic dynamics
C: Periodic dynamics
D: Nonlinear dynamics
E: Static dynamics

Answer: D

Question 8: What was the primary outcome of Lorenz's work related to the butterfly effect?
A: He provided an artistic representation of weather patterns.
B: He proved the significance of butterflies in weather systems.
C: He established the concept of instability of the Earth's atmosphere on a quantitative base and linked it to certain dynamic systems.
D: He demonstrated that weather systems are completely predictable.
E: He debunked earlier theories of weather prediction.

Answer: C

Question 9: What year did Lorenz make the poetic switch to use a butterfly in his metaphor instead of a seagull?
A: 1965
B: 1970
C: 1972
D: 1980
E: 1990

Answer: C

Question 10: Why is the butterfly effect considered to be part of chaos theory?
A: Because it represents the unpredictable nature of initial conditions.
B: Because butterflies are chaotic creatures.
C: Because Lorenz was an advocate for chaos in science.
D: Because it shows that all systems are ultimately predictable.
E: Because it focuses on the large changes in systems.

Answer: A
@
Subject:
Symmetry in biology refers to the symmetry observed in organisms, including plants, animals, fungi, and bacteria. External symmetry can be easily seen by just looking at an organism. For example, the face of a human being has a plane of symmetry down its centre, or a pine cone displays a clear symmetrical spiral pattern. Internal features can also show symmetry, for example the tubes in the human body (responsible for transporting gases, nutrients, and waste products) which are cylindrical and have several planes of symmetry.

Biological symmetry can be thought of as a balanced distribution of duplicate body parts or shapes within the body of an organism. Importantly, unlike in mathematics, symmetry in biology is always approximate. For example, plant leaves – while considered symmetrical – rarely match up exactly when folded in half. Symmetry is one class of patterns in nature whereby there is near-repetition of the pattern element, either by reflection or rotation.

While sponges and placozoans represent two groups of animals which do not show any symmetry (i.e. are asymmetrical), the body plans of most multicellular organisms exhibit, and are defined by, some form of symmetry. There are only a few types of symmetry which are possible in body plans. These are radial (cylindrical), bilateral, biradial and spherical symmetry.[1] While the classification of viruses as an "organism" remains controversial, viruses also contain icosahedral symmetry.

The importance of symmetry is illustrated by the fact that groups of animals have traditionally been defined by this feature in taxonomic groupings. The Radiata, animals with radial symmetry, formed one of the four branches of Georges Cuvier's classification of the animal kingdom.[2][3][4] Meanwhile, Bilateria is a taxonomic grouping still used today to represent organisms with embryonic bilateral symmetry.

Evolution of symmetry
Like all the traits of organisms, symmetry (or indeed asymmetry) evolves due to an advantage to the organism – a process of natural selection. This involves changes in the frequency of symmetry-related genes throughout time.

Evolution of symmetry in plants
Early flowering plants had radially symmetric flowers but since then many plants have evolved bilaterally symmetrical flowers. The evolution of bilateral symmetry is due to the expression of CYCLOIDEA genes. Evidence for the role of the CYCLOIDEA gene family comes from mutations in these genes which cause a reversion to radial symmetry. The CYCLOIDEA genes encode transcription factors, proteins which control the expression of other genes. This allows their expression to influence developmental pathways relating to symmetry.[22][23] For example, in Antirrhinum majus, CYCLOIDEA is expressed during early development in the dorsal domain of the flower meristem and continues to be expressed later on in the dorsal petals to control their size and shape. It is believed that the evolution of specialized pollinators may play a part in the transition of radially symmetrical flowers to bilaterally symmetrical flowers.[24]
$
9
Question 2: Which of the following best describes symmetry in biology?
A: Perfect duplication of body parts or shapes within an organism.
B: A balanced distribution of duplicate body parts or shapes within the body of an organism.
C: An exact match of internal and external features of an organism.
D: A mathematical pattern observed only in higher animals.
E: A balanced distribution of non-duplicate body parts within the body of an organism.

Answer: B

Question 3: Which animals do not exhibit any form of symmetry?
A: Bilateria
B: Radiata
C: Sponges and Placozoans
D: Viruses
E: Fungi

Answer: C

Question 4: Which classification of the animal kingdom by Georges Cuvier was based on radial symmetry?
A: Bilateria
B: CYCLOIDEA
C: Asymmetrical
D: Radiata
E: Spherical

Answer: D

Question 5: In the context of biological symmetry, what is considered when referring to "radial" symmetry?
A: A pattern based on a center point and extending outwards in all directions.
B: A bilateral mirror-like reflection across a single plane.
C: A spherical pattern observed in the internal structures of an organism.
D: A non-symmetrical pattern observed in primitive organisms.
E: A pattern based on two distinct and equal halves of an organism.

Answer: A

Question 6: Why is symmetry considered to be only approximate in biology?
A: Because biological organisms can never achieve mathematical perfection.
B: Because symmetry in biology is always changing due to evolutionary pressures.
C: Because plant leaves, while considered symmetrical, rarely match up exactly when folded.
D: Because only a few organisms exhibit symmetry.
E: Because symmetry in biology is an outdated concept.

Answer: C

Question 7: Which statement best describes the symmetry in viruses?
A: Viruses have a clear bilateral symmetry.
B: Viruses are considered asymmetrical organisms.
C: Viruses exhibit radial symmetry similar to many multicellular organisms.
D: Viruses possess icosahedral symmetry.
E: Viruses display biradial symmetry.

Answer: D

Question 8: How has the importance of symmetry traditionally been illustrated in the study of animals?
A: Through the study of genes related to symmetry.
B: By defining groups of animals based on this feature in taxonomic groupings.
C: By studying the effects of environmental factors on symmetry.
D: By emphasizing the role of symmetry in the classification of plants.
E: By focusing on the role of symmetry in disease resistance.

Answer: B

Question 9: In the evolution of plants, what is believed to play a part in the transition from radially symmetrical flowers to bilaterally symmetrical flowers?
A: The external environment and temperature changes.
B: The growth of the plant's roots.
C: The process of photosynthesis.
D: The evolution of specialized pollinators.
E: The competition for resources among plants.

Answer: D

Question 10: What do CYCLOIDEA genes encode?
A: Enzymes responsible for plant growth.
B: Proteins for cell wall synthesis.
C: Transcription factors which control the expression of other genes.
D: Nutrients essential for the plant's survival.
E: Genetic material for plant reproduction.

Answer: C
@
Subject:
A quark (/kwɔːrk, kwɑːrk/) is a type of elementary particle and a fundamental constituent of matter. Quarks combine to form composite particles called hadrons, the most stable of which are protons and neutrons, the components of atomic nuclei.[1] All commonly observable matter is composed of up quarks, down quarks and electrons. Owing to a phenomenon known as color confinement, quarks are never found in isolation; they can be found only within hadrons, which include baryons (such as protons and neutrons) and mesons, or in quark–gluon plasmas.[2][3][nb 1] For this reason, much of what is known about quarks has been drawn from observations of hadrons.

Quarks have various intrinsic properties, including electric charge, mass, color charge, and spin. They are the only elementary particles in the Standard Model of particle physics to experience all four fundamental interactions, also known as fundamental forces (electromagnetism, gravitation, strong interaction, and weak interaction), as well as the only known particles whose electric charges are not integer multiples of the elementary charge.

There are six types, known as flavors, of quarks: up, down, charm, strange, top, and bottom.[4] Up and down quarks have the lowest masses of all quarks. The heavier quarks rapidly change into up and down quarks through a process of particle decay: the transformation from a higher mass state to a lower mass state. Because of this, up and down quarks are generally stable and the most common in the universe, whereas strange, charm, bottom, and top quarks can only be produced in high energy collisions (such as those involving cosmic rays and in particle accelerators). For every quark flavor there is a corresponding type of antiparticle, known as an antiquark, that differs from the quark only in that some of its properties (such as the electric charge) have equal magnitude but opposite sign.

The quark model was independently proposed by physicists Murray Gell-Mann and George Zweig in 1964.[5] Quarks were introduced as parts of an ordering scheme for hadrons, and there was little evidence for their physical existence until deep inelastic scattering experiments at the Stanford Linear Accelerator Center in 1968.[6][7] Accelerator program experiments have provided evidence for all six flavors. The top quark, first observed at Fermilab in 1995, was the last to be discovered.[5]

he majority of ordinary matter in the universe is found in atomic nuclei, which are made of neutrons and protons. These nucleons are made up of smaller particles called quarks, and antimatter equivalents for each are predicted to exist by the Dirac equation in 1928.[8] Since then, each kind of antiquark has been experimentally verified. Hypotheses investigating the first few instants of the universe predict a composition with an almost equal number of quarks and antiquarks.[9] Once the universe expanded and cooled to a critical temperature of approximately 2×1012 K,[3] quarks combined into normal matter and antimatter and proceeded to annihilate up to the small initial asymmetry of about one part in five billion, leaving the matter around us.[3] Free and separate individual quarks and antiquarks have never been observed in experiments—quarks and antiquarks are always found in groups of three (baryons), or bound in quark–antiquark pairs (mesons). Likewise, there is no experimental evidence that there are any significant concentrations of antimatter in the observable universe.

There are two main interpretations for this disparity: either the universe began with a small preference for matter (total baryonic number of the universe different from zero), or the universe was originally perfectly symmetric, but somehow a set of phenomena contributed to a small imbalance in favour of matter over time. The second point of view is preferred, although there is no clear experimental evidence indicating either of them to be the correct one.

The Standard Model can incorporate baryogenesis, though the amount of net baryons (and leptons) thus created may not be sufficient to account for the present baryon asymmetry. There is a required one excess quark per billion quark-antiquark pairs in the early universe in order to provide all the observed matter in the universe.[3] This insufficiency has not yet been explained, theoretically or otherwise.
$
9
Question 2: Which particles are the most stable hadrons and components of atomic nuclei?
A: Electrons and Positrons
B: Mesons and Baryons
C: Protons and Electrons
D: Neutrons and Protons
E: Up Quarks and Down Quarks

Answer: D

Question 3: Due to which phenomenon are quarks never found in isolation?
A: Weak interaction
B: Electromagnetic force
C: Color confinement
D: Spin confinement
E: Fundamental force

Answer: C

Question 4: How many types or flavors of quarks are there?
A: Four
B: Five
C: Six
D: Seven
E: Eight

Answer: C

Question 5: Which quarks have the lowest masses of all?
A: Charm and Strange
B: Top and Bottom
C: Up and Down
D: Electric and Color
E: Positive and Negative

Answer: C

Question 6: Who independently proposed the quark model in 1964?
A: Albert Einstein and Richard Feynman
B: Marie Curie and Niels Bohr
C: Werner Heisenberg and Paul Dirac
D: Murray Gell-Mann and George Zweig
E: James Clerk Maxwell and Michael Faraday

Answer: D

Question 7: Which quark was the last to be discovered?
A: Charm
B: Strange
C: Bottom
D: Top
E: Up

Answer: D

Question 8: In which year was the existence of the top quark first observed?
A: 1968
B: 1975
C: 1982
D: 1990
E: 1995

Answer: E

Question 9: In which groupings are quarks and antiquarks always found?
A: Singles or Doubles
B: Groups of two or four
C: Groups of four or five
D: Groups of three or bound in quark–antiquark pairs
E: Groups of five or six

Answer: D

Question 10: According to the main interpretations regarding the disparity in the observed matter in the universe, which point of view is preferred?
A: The universe began with a small preference for antimatter.
B: The universe began with a strong preference for matter.
C: The universe was originally symmetric, but a set of phenomena contributed to a small imbalance in favor of matter.
D: The universe always had a preference for matter, which was a constant.
E: The universe was always asymmetrical with no phenomena affecting it.

Answer: C
@
Subject:
A vacuum (pl: vacuums or vacua) is a space devoid of matter. The word is derived from the Latin adjective vacuus for "vacant" or "void". An approximation to such vacuum is a region with a gaseous pressure much less than atmospheric pressure.[1] Physicists often discuss ideal test results that would occur in a perfect vacuum, which they sometimes simply call "vacuum" or free space, and use the term partial vacuum to refer to an actual imperfect vacuum as one might have in a laboratory or in space. In engineering and applied physics on the other hand, vacuum refers to any space in which the pressure is considerably lower than atmospheric pressure.[2] The Latin term in vacuo is used to describe an object that is surrounded by a vacuum.

In physics, horror vacui, or plenism (/ˈpliːnɪzəm/), commonly stated as "nature abhors a vacuum", is a postulate attributed to Aristotle, who articulated a belief, later criticized by the atomism of Epicurus and Lucretius, that nature contains no vacuums because the denser surrounding material continuum would immediately fill the rarity of an incipient void.[1] He also argued against the void in a more abstract sense (as "separable"), for example, that by definition[citation needed] a void (equivocally?) itself, is nothing, and following Plato, nothing cannot rightly be said to exist. Furthermore, insofar as it would be featureless, it could neither be encountered by the senses, nor could its supposition lend additional explanatory power. Hero of Alexandria challenged the theory in the first century AD, but his attempts to create an artificial vacuum failed.[2] The theory was debated in the context of 17th-century fluid mechanics, by Thomas Hobbes and Robert Boyle,[3] among others, and through the early 18th century by Sir Isaac Newton and Gottfried Leibniz.
$
9
Question 2: In physics, what is an approximation to a vacuum?
A: A region with gaseous pressure much greater than atmospheric pressure.
B: A space filled with matter.
C: A region with gaseous pressure equal to atmospheric pressure.
D: A region with gaseous pressure much less than atmospheric pressure.
E: A space where only liquids exist.

Answer: D

Question 3: When physicists refer to a "perfect vacuum," they mean:
A: A space filled with dense gases.
B: A region that is an imperfect version of a vacuum.
C: A space where the atmospheric pressure is very high.
D: A space that is devoid of all matter.
E: A partial vacuum found in space.

Answer: D

Question 4: The term "in vacuo" is used to describe:
A: A vacuum that contains a small amount of matter.
B: A space that is entirely filled with air.
C: An object that is surrounded by a vacuum.
D: An object floating in air.
E: A space where matter is transitioning to a gas.

Answer: C

Question 5: Who attributed the postulate "nature abhors a vacuum"?
A: Hero of Alexandria
B: Sir Isaac Newton
C: Aristotle
D: Robert Boyle
E: Epicurus

Answer: C

Question 6: Which of the following philosophers or scientists challenged the theory of "horror vacui" in the first century AD?
A: Lucretius
B: Hero of Alexandria
C: Thomas Hobbes
D: Gottfried Leibniz
E: Robert Boyle

Answer: B

Question 7: Who debated the theory of "horror vacui" in the context of 17th-century fluid mechanics?
A: Epicurus and Lucretius
B: Hero of Alexandria and Plato
C: Sir Isaac Newton and Lucretius
D: Thomas Hobbes and Robert Boyle
E: Descartes and Sir Isaac Newton

Answer: D

Question 8: Aristotle argued that a void, in a more abstract sense, is equivalent to:
A: A space filled with matter.
B: The amount of atmospheric pressure in a space.
C: Something that can be sensed and encountered.
D: Nothing and cannot rightly be said to exist.
E: The continuous motion of matter.

Answer: D

Question 9: In engineering and applied physics, how is a vacuum typically defined?
A: Any space filled with gaseous matter.
B: Any space in which the pressure is considerably lower than atmospheric pressure.
C: Any space in which the pressure is equal to atmospheric pressure.
D: Any space that strictly contains no matter.
E: A region with a gaseous pressure much greater than atmospheric pressure.

Answer: B

Question 10: Aristotle's view on vacuums was later criticized by the atomism of which individuals?
A: Plato and Hero of Alexandria
B: Thomas Hobbes and Robert Boyle
C: Sir Isaac Newton and Gottfried Leibniz
D: Epicurus and Lucretius
E: Descartes and Sir Isaac Newton

Answer: D
@
Subject:
The Droste effect (Dutch pronunciation: [ˈdrɔstə]), known in art as an example of mise en abyme, is the effect of a picture recursively appearing within itself, in a place where a similar picture would realistically be expected to appear. This produces a loop which in theory could go on forever, but in practice only continues as far as the image's resolution allows.

The effect is named after Droste, a Dutch brand of cocoa, with an image designed by Jan Misset in 1904. The Droste effect has since been used in the packaging of a variety of products. Apart from advertising, the effect is also seen in the Dutch artist M. C. Escher's 1956 lithograph Print Gallery, which portrays a gallery that depicts itself. The effect has been widely used on the covers of comic books, mainly in the 1940s.

The appearance is recursive: the smaller version contains an even smaller version of the picture, and so on.[4] Only in theory could this go on forever, as fractals do; practically, it continues only as long as the resolution of the picture allows, which is relatively short, since each iteration geometrically reduces the picture's size.

Recursion occurs when the definition of a concept or process depends on a simpler version of itself.[1] Recursion is used in a variety of disciplines ranging from linguistics to logic. The most common application of recursion is in mathematics and computer science, where a function being defined is applied within its own definition. While this apparently defines an infinite number of instances (function values), it is often done in such a way that no infinite loop or infinite chain of references can occur.

A process that exhibits recursion is recursive.
$
9
Question 2: What does the Droste effect depict in art?
A: A single image that is mirrored onto itself.
B: An image that shifts colors from its outer to inner regions.
C: A picture recursively appearing within itself.
D: Multiple images juxtaposed next to each other.
E: The blurring of a picture's boundary.

Answer: C

Question 3: The Droste effect is named after which of the following?
A: A famous Dutch painter.
B: A Dutch brand of cocoa.
C: A type of Dutch architecture.
D: An art technique from the 19th century.
E: A Dutch lithography method.

Answer: B

Question 4: Who designed the image for Droste that became synonymous with the Droste effect?
A: M. C. Escher
B: Jan Misset
C: Droste himself
D: Print Gallery
E: None of the above

Answer: B

Question 5: In which of M. C. Escher's works can the Droste effect be observed?
A: Waterfall
B: Ascending and Descending
C: Relativity
D: Print Gallery
E: Drawing Hands

Answer: D

Question 6: Why doesn't the Droste effect continue indefinitely in practice?
A: Because it gets magnified with each iteration.
B: Due to the physical constraints of the medium.
C: Because of the image's resolution limitations.
D: It always continues indefinitely.
E: Because artists choose to stop it.

Answer: C

Question 7: What is recursion in the context of the definition provided?
A: A process that repeats indefinitely.
B: The definition of a concept depending on a simpler version of itself.
C: A mathematical sequence.
D: A way of mirroring images.
E: The repetition of a picture within a picture.

Answer: B

Question 8: In which of the following fields is recursion commonly applied?
A: Music and art
B: Medicine and biology
C: Mathematics and computer science
D: History and literature
E: Philosophy and theology

Answer: C

Question 9: How is recursion typically defined in terms of function values?
A: It defines a fixed number of instances.
B: It defines an infinite number of instances.
C: It is always done in a way that an infinite loop occurs.
D: It only works for specific functions.
E: It is not applicable to function values.

Answer: B

Question 10: Which of the following statements best describes a process that exhibits recursion?
A: The process is linear.
B: The process is recursive.
C: The process is cyclic.
D: The process is fixed.
E: The process is infinite.

Answer: B
@
Subject:
Hydraulic shock (colloquial: water hammer; fluid hammer) is a pressure surge or wave caused when a fluid in motion, usually a liquid but sometimes also a gas is forced to stop or change direction suddenly; a momentum change. This phenomenon commonly occurs when a valve closes suddenly at an end of a pipeline system, and a pressure wave propagates in the pipe.

This pressure wave can cause major problems, from noise and vibration to pipe rupture or collapse. It is possible to reduce the effects of the water hammer pulses with accumulators, expansion tanks, surge tanks, blowoff valves, and other features. The effects can be avoided by ensuring that no valves will close too quickly with significant flow, but there are many situations that can cause the effect.

Rough calculations can be made using the Zhukovsky (Joukowsky) equation,[1] or more accurate ones using the method of characteristics.

In mathematics, the method of characteristics is a technique for solving partial differential equations. Typically, it applies to first-order equations, although more generally the method of characteristics is valid for any hyperbolic partial differential equation. The method is to reduce a partial differential equation to a family of ordinary differential equations along which the solution can be integrated from some initial data given on a suitable hypersurface.
$
9
Question 2: What causes hydraulic shock in a pipeline system?
A: Gradual decrease in the fluid speed.
B: Slow opening of a valve.
C: Fluid being forced to stop or change direction suddenly.
D: Constant flow of fluid without any interruptions.
E: Increase in the pipeline's temperature.

Answer: C

Question 3: What can be the potential consequence of hydraulic shock in a pipeline?
A: Decrease in fluid speed.
B: Noise and vibration.
C: Increase in pipeline diameter.
D: Decrease in fluid pressure.
E: Increase in fluid viscosity.

Answer: B

Question 4: Which of the following can be used to reduce the effects of hydraulic shock pulses?
A: Increase fluid velocity.
B: Expansion tanks.
C: Close all valves.
D: Increase pipeline diameter.
E: Use only gases in the pipeline.

Answer: B

Question 5: What is the primary purpose of the Zhukovsky (Joukowsky) equation in relation to hydraulic shock?
A: To calculate the viscosity of the fluid.
B: To determine the temperature of the fluid.
C: To measure the diameter of the pipeline.
D: To make rough calculations of hydraulic shock.
E: To calculate the age of the pipeline.

Answer: D

Question 6: Which mathematical method is used to reduce a partial differential equation to a family of ordinary differential equations?
A: The method of calculus.
B: The method of expansion.
C: The method of differentiation.
D: The method of integration.
E: The method of characteristics.

Answer: E

Question 7: For which type of partial differential equations is the method of characteristics typically applied?
A: First-order equations.
B: Zeroth-order equations.
C: Second-order equations.
D: Complex differential equations.
E: Integration-based equations.

Answer: A

Question 8: What is the result of the momentum change in a fluid when a sudden stop or direction change occurs?
A: Decrease in pressure.
B: Constant pressure maintenance.
C: Pressure surge or wave.
D: Vacuum creation.
E: Fluid solidification.

Answer: C

Question 9: What is another term for hydraulic shock?
A: Fluid whiplash.
B: Water turbine.
C: Water slam.
D: Fluid bang.
E: Water hammer.

Answer: E

Question 10: What is the primary reason for the negative effects of hydraulic shock?
A: Slow valve opening.
B: Steady flow in pipelines.
C: Sudden valve closure at the end of a pipeline system.
D: Regular fluid speed.
E: Use of soft pipes.

Answer: C
@
Subject:
Resistive random-access memory (ReRAM or RRAM) is a type of non-volatile (NV) random-access (RAM) computer memory that works by changing the resistance across a dielectric solid-state material, often referred to as a memristor.

ReRAM bears some similarities to conductive-bridging RAM (CBRAM) and phase-change memory (PCM). CBRAM involves one electrode providing ions that dissolve readily in an electrolyte material, while PCM involves generating sufficient Joule heating to effect amorphous-to-crystalline or crystalline-to-amorphous phase changes. By contrast, ReRAM involves generating defects in a thin oxide layer, known as oxygen vacancies (oxide bond locations where the oxygen has been removed), which can subsequently charge and drift under an electric field. The motion of oxygen ions and vacancies in the oxide would be analogous to the motion of electrons and holes in a semiconductor.

Although ReRAM was initially seen as a replacement technology for flash memory, the cost and performance benefits of ReRAM have not been enough for companies to proceed with the replacement. Apparently, a broad range of materials can be used for ReRAM. However, the discovery[1] that the popular high-κ gate dielectric HfO2 can be used as a low-voltage ReRAM has encouraged researchers to investigate more possibilities.

RRAM is the registered trademark name of Sharp Corporation, a Japanese electronic components manufacturer, in some countries, including members of the European Union.[2]

An energy-efficient chip called NeuRRAM fixes an old design flaw to run large-scale AI algorithms on smaller devices, reaching the same accuracy as wasteful digital computers, at least for applications needing only a few million bits of neural state. As NeuRRAM is an analog technology, it suffers from the same analog noise problems that plague other analog semiconductors. While this is a handicap, many neural processors do need bit-perfect state storage to do useful work.[3]

When one wants to go beyond mere curve fitting and aims at a real physical modeling of non-volatile memory elements, e.g., resistive random-access memory devices, one has to keep an eye on the aforementioned physical correlations. To check the adequacy of the proposed model and its resulting state equations, the input signal u(t) can be superposed with a stochastic term ξ(t), which takes into account the existence of inevitable thermal fluctuations.
A "resistance switching" event can simply be enforced by setting the external bias to a value above a certain threshold value. This is the trivial case, i.e., the free-energy barrier for the transition {i} → {j} is reduced to zero. In case one applies biases below the threshold value, there is still a finite probability that the device will switch in course of time (triggered by a random thermal fluctuation), but – as one is dealing with probabilistic processes – it is impossible to predict when the switching event will occur. That is the basic reason for the stochastic nature of all observed resistance-switching (ReRAM) processes. If the free-energy barriers are not high enough, the memory device can even switch without having to do anything.
$
9
Question 2: How does Resistive random-access memory (ReRAM or RRAM) store data?
A: By increasing voltage across a dielectric solid-state material.
B: By changing the capacitance across a dielectric solid-state material.
C: By altering the inductance across a memristor.
D: By changing the resistance across a dielectric solid-state material.
E: By rotating a magnetic field in a solid-state material.

Answer: D

Question 3: Which among the following best describes the operation of ReRAM?
A: Generating defects in a thin oxide layer to create oxygen vacancies.
B: Providing ions that dissolve readily in an electrolyte material.
C: Generating sufficient Joule heating to effect phase changes.
D: Applying an external magnetic field to change resistance.
E: Altering the polarization of a dielectric material.

Answer: A

Question 4: Which other memory technology does ReRAM closely resemble?
A: Flash Memory.
B: Dynamic RAM (DRAM).
C: Phase-change memory (PCM).
D: Read-Only Memory (ROM).
E: Magnetoresistive RAM (MRAM).

Answer: C

Question 5: What initially discouraged the replacement of flash memory with ReRAM?
A: ReRAM's high power consumption.
B: The complex manufacturing process of ReRAM.
C: ReRAM's inability to hold data.
D: The cost and performance benefits of ReRAM were not sufficient.
E: Flash memory's better adaptability to modern devices.

Answer: D

Question 6: Which company holds the registered trademark name of RRAM in the European Union?
A: Intel.
B: Samsung.
C: Micron.
D: Sharp Corporation.
E: Toshiba.

Answer: D

Question 7: Which characteristic is a challenge for NeuRRAM technology?
A: Its digital nature.
B: Analog noise problems.
C: Inability to run AI algorithms.
D: Excessive energy consumption.
E: Its inability to handle large datasets.

Answer: B

Question 8: What encourages researchers to investigate more possibilities with ReRAM?
A: Discovery of its compatibility with modern devices.
B: Its increased cost and performance benefits.
C: The use of the popular high-κ gate dielectric HfO2 as a low-voltage ReRAM.
D: Its similarity to flash memory.
E: Its inherent resistance to thermal fluctuations.

Answer: C

Question 9: What is the result when the external bias is set to a value above a certain threshold in ReRAM?
A: There is a decrease in the memory device's resistance.
B: There is an increase in the memory device's capacitance.
C: The free-energy barrier for the transition {i} → {j} is reduced to zero.
D: The memory device enters a standby mode.
E: The free-energy barrier for the transition {i} → {j} increases.

Answer: C

Question 10: In the context of non-volatile memory elements, why might one superpose the input signal u(t) with a stochastic term ξ(t)?
A: To enhance the memory device's performance.
B: To counteract external noise.
C: To increase the memory device's storage capacity.
D: To account for the existence of inevitable thermal fluctuations.
E: To reduce the energy consumption of the memory device.

Answer: D
@
Subject:
Einstein@Home is a volunteer computing project that searches for signals from spinning neutron stars in data from gravitational-wave detectors, from large radio telescopes, and from a gamma-ray telescope. Neutron stars are detected by their pulsed radio and gamma-ray emission as radio and/or gamma-ray pulsars. They also might be observable as continuous gravitational wave sources if they are rapidly spinning and non-axisymmetrically deformed. The project was officially launched on 19 February 2005 as part of the American Physical Society's contribution to the World Year of Physics 2005 event.[3]

Einstein@Home searches data from the LIGO gravitational-wave detectors. The project conducts the most sensitive all-sky searches for continuous gravitational waves. While no such signal has yet been detected, the upper limits set by Einstein@Home analyses provide astrophysical constraints on the Galactic population of spinning neutron stars.

Einstein@Home also searches radio telescope data from the Arecibo Observatory, and has in the past analyzed data from Parkes Observatory. On 12 August 2010, the first discovery by Einstein@Home of a previously undetected radio pulsar J2007+2722, found in data from the Arecibo Observatory, was published in Science.[4][5] This was the first data-based discovery by a volunteer computing project. As of July 2022 Einstein@Home had discovered 55 radio pulsars.[6][7][8]

The project also analyses data from the Fermi Gamma-ray Space Telescope to discover gamma-ray pulsars. On 26 November 2013, the first Einstein@Home results of the Fermi data analysis was published: the discovery of four young gamma-ray pulsars in LAT data.[9] As of July 2022, Einstein@Home has discovered 39 previously unknown gamma-ray pulsars[10][11][8] in data from the Large Area Telescope on board the Fermi Gamma-ray Space Telescope. The Einstein@Home search makes use of novel and more efficient data-analysis methods and discovered pulsars missed in other analyses of the same data.[12][13]

The project runs on the Berkeley Open Infrastructure for Network Computing (BOINC) software platform and uses free software released under the GNU General Public License, version 2.[1] Einstein@Home is hosted by the Max Planck Institute for Gravitational Physics (Albert Einstein Institute, Hannover, Germany) and the University of Wisconsin–Milwaukee. The project is supported by the Max Planck Society (MPG), the American Physical Society (APS), and the US National Science Foundation (NSF). The Einstein@Home project director is Bruce Allen.

Einstein@Home uses the power of volunteer computing in solving the computationally intensive problem of analyzing a large volume of data. Such an approach was pioneered by the SETI@home project, which is designed to look for signs of extraterrestrial life by analyzing radio wave data. Einstein@Home runs through the same software platform as SETI@home, the Berkeley Open Infrastructure for Network Computing (BOINC). As of July 2022, more than 487,000 volunteers in 226 countries had participated in the project, making it the third-most-popular active BOINC application.[14][15] Users regularly contribute about 12.7 petaFLOPS of computational power,[14] which would rank Einstein@Home among the top 45 on the TOP500 list of supercomputers.[16]
$
9
Question 2: Which celestial objects does Einstein@Home primarily search for signals from?
A: Black holes and Supernovae.
B: Spinning neutron stars.
C: Distant galaxies.
D: Alien life forms.
E: Planetary transits.

Answer: B

Question 3: In which year was Einstein@Home officially launched?
A: 1995
B: 2000
C: 2005
D: 2010
E: 2015

Answer: C

Question 4: From which observatory did Einstein@Home discover its first undetected radio pulsar?
A: LIGO Observatory
B: Fermi Gamma-ray Space Telescope
C: Parkes Observatory
D: Arecibo Observatory
E: Large Area Telescope

Answer: D

Question 5: How does Einstein@Home primarily analyze its vast amounts of data?
A: Through a dedicated supercomputer.
B: Using the computational power of a centralized data center.
C: Through volunteer computing by distributing data in small parts.
D: Using cloud computing platforms like AWS and Azure.
E: By manually reviewing each data segment.

Answer: C

Question 6: Which telescope data does Einstein@Home analyze to discover gamma-ray pulsars?
A: Hubble Space Telescope
B: James Webb Space Telescope
C: Fermi Gamma-ray Space Telescope
D: Very Large Telescope (VLT)
E: Chandra X-ray Observatory

Answer: C

Question 7: Which software platform is Einstein@Home based on?
A: Folding@home
B: World Community Grid
C: Rosetta@home
D: SETI@home
E: Berkeley Open Infrastructure for Network Computing (BOINC)

Answer: E

Question 8: As of July 2022, how many gamma-ray pulsars were discovered by Einstein@Home?
A: 4
B: 15
C: 39
D: 55
E: 100

Answer: C

Question 9: Who is the project director of Einstein@Home?
A: Carl Sagan
B: Stephen Hawking
C: Bruce Allen
D: Kip Thorne
E: Alan Guth

Answer: C

Question 10: Which other volunteer computing project shares the same purpose of analyzing radio wave data for signs of extraterrestrial life?
A: PrimeGrid
B: MilkyWay@home
C: LHC@home
D: SETI@home
E: Climateprediction.net

Answer: D
@
Subject:
A thermodynamic operation is an externally imposed manipulation that affects a thermodynamic system. The change can be either in the connection or wall between a thermodynamic system and its surroundings, or in the value of some variable in the surroundings that is in contact with a wall of the system that allows transfer of the extensive quantity belonging that variable.[1][2][3][4] It is assumed in thermodynamics that the operation is conducted in ignorance of any pertinent microscopic information.

A thermodynamic operation requires a contribution from an independent external agency, that does not come from the passive properties of the systems. Perhaps the first expression of the distinction between a thermodynamic operation and a thermodynamic process is in Kelvin's statement of the second law of thermodynamics: "It is impossible, by means of inanimate material agency, to derive mechanical effect from any portion of matter by cooling it below the temperature of the surrounding objects." A sequence of events that occurred other than "by means of inanimate material agency" would entail an action by an animate agency, or at least an independent external agency. Such an agency could impose some thermodynamic operations. For example, those operations might create a heat pump, which of course would comply with the second law. A Maxwell's demon conducts an extremely idealized and naturally unrealizable kind of thermodynamic operation.[5]

Another commonly used term that indicates a thermodynamic operation is 'change of constraint', for example referring to the removal of a wall between two otherwise isolated compartments.

An ordinary language expression for a thermodynamic operation is used by Edward A. Guggenheim: "tampering" with the bodies.[6]
$
7
Question 2: Which external agency is responsible for the contribution during a thermodynamic operation?
A: The thermodynamic system itself.
B: The internal microscopic processes of the system.
C: Passive properties of the systems.
D: An independent external agency.
E: The surroundings of the system.

Answer: D

Question 3: Which of the following describes Kelvin's statement of the second law of thermodynamics?
A: It is possible to cool matter below the temperature of the surrounding objects using passive agents.
B: Thermodynamic operations always require a human intervention.
C: Thermodynamics doesn't differentiate between animate and inanimate agency.
D: It is impossible to derive mechanical effect from matter by cooling it below the temperature of surrounding objects using inanimate material agency.
E: Thermodynamic operations always violate the second law of thermodynamics.

Answer: D

Question 4: What is an alternate term indicating a thermodynamic operation, especially when a barrier is removed?
A: Change of temperature.
B: Change of energy.
C: Change of entropy.
D: Change of volume.
E: Change of constraint.

Answer: E

Question 5: A Maxwell's demon is an example of:
A: A real-world thermodynamic operation.
B: An animate agency affecting thermodynamics.
C: A naturally realizable thermodynamic operation.
D: An extremely idealized and naturally unrealizable kind of thermodynamic operation.
E: A demon actively tampering with thermodynamic processes.

Answer: D

Question 6: According to Edward A. Guggenheim, what term can be used to describe the act of conducting a thermodynamic operation?
A: Restricting.
B: Isolating.
C: Changing.
D: Tampering.
E: Violating.

Answer: D

Question 7: In thermodynamics, what assumption is made regarding the conduct of the operation?
A: It is conducted with complete knowledge of all microscopic information.
B: It is conducted only if the system has reached equilibrium.
C: It is conducted in ignorance of any pertinent microscopic information.
D: It is conducted with the help of an inanimate material agency.
E: It is always conducted by an animate agency.

Answer: C

Question 8: Which operation can be regarded as a thermodynamic operation?
A: Increasing the internal temperature of a system using an external heat source.
B: Manually stirring a liquid within a system.
C: Observing the system without causing any changes.
D: Changing the value of a variable in the surroundings that contacts a system wall.
E: Predicting the future state of a system.

Answer: D
@
Subject:
In physics, the relativity of simultaneity is the concept that distant simultaneity – whether two spatially separated events occur at the same time – is not absolute, but depends on the observer's reference frame. This possibility was raised by mathematician Henri Poincaré in 1900, and thereafter became a central idea in the special theory of relativity.

According to the special theory of relativity introduced by Albert Einstein, it is impossible to say in an absolute sense that two distinct events occur at the same time if those events are separated in space. If one reference frame assigns precisely the same time to two events that are at different points in space, a reference frame that is moving relative to the first will generally assign different times to the two events (the only exception being when motion is exactly perpendicular to the line connecting the locations of both events).

For example, a car crash in London and another in New York appearing to happen at the same time to an observer on Earth, will appear to have occurred at slightly different times to an observer on an airplane flying between London and New York. Furthermore, if the two events cannot be causally connected, depending on the state of motion, the crash in London may appear to occur first in a given frame, and the New York crash may appear to occur first in another. However, if the events are causally connected, precedence order is preserved in all frames of reference.[1]

Einstein's version of the experiment[15] presumed that one observer was sitting midway inside a speeding traincar and another was standing on a platform as the train moved past. As measured by the standing observer, the train is struck by two bolts of lightning simultaneously, but at different positions along the axis of train movement (back and front of the train car). In the inertial frame of the standing observer, there are three events which are spatially dislocated, but simultaneous: standing observer facing the moving observer (i.e., the center of the train), lightning striking the front of the train car, and lightning striking the back of the car.

Thought Experiments
Einstein's train
Since the events are placed along the axis of train movement, their time coordinates become projected to different time coordinates in the moving train's inertial frame. Events which occurred at space coordinates in the direction of train movement happen earlier than events at coordinates opposite to the direction of train movement. In the moving train's inertial frame, this means that lightning will strike the front of the train car before the two observers align (face each other).
$
7
Question 2: Which mathematician first raised the possibility of the relativity of simultaneity in 1900?
A: Isaac Newton.
B: Albert Einstein.
C: Henri Poincaré.
D: Richard Feynman.
E: Max Planck.

Answer: C

Question 3: According to the special theory of relativity, it is impossible to absolutely state that two distinct events occur simultaneously if:
A: Those events occur in the same location.
B: Those events have a causal connection.
C: Those events are widely publicized.
D: Those events are separated in space.
E: Those events are moving in the same reference frame.

Answer: D

Question 4: If a car crash in London and another in New York appear to happen at the same time to an observer on Earth, how will they appear to an observer on an airplane flying between the two cities?
A: They will appear to happen at exactly the same time.
B: They will appear to have occurred at slightly different times.
C: The observer on the airplane will not see either crash.
D: The crash in London will always appear to occur first.
E: The crash in New York will always appear to occur first.

Answer: B

Question 5: In Einstein's train experiment, where does the standing observer see the two lightning strikes?
A: Only at the front of the train.
B: Only at the back of the train.
C: Simultaneously, at the front and back of the train car.
D: Sequentially, first at the back and then at the front.
E: Neither, the lightning misses the train entirely.

Answer: C

Question 6: According to the relativity of simultaneity, how do events along the axis of train movement appear in the moving train's inertial frame?
A: They appear to happen simultaneously.
B: Events in the direction of train movement happen after events opposite to it.
C: Events in the direction of train movement happen before events opposite to it.
D: The sequence of events remains consistent regardless of the train's movement.
E: Events inside the train always happen before those outside of it.

Answer: C

Question 7: If two events are causally connected, how does their precedence order appear across different frames of reference?
A: Their order is interchangeable based on the observer's frame.
B: Their order is always preserved.
C: Their order is dependent on the spatial separation.
D: Their order can be altered by traveling at the speed of light.
E: Their order is determined by the initial event's energy.

Answer: B

Question 8: For an observer on a moving train, when does the lightning strike in relation to the two observers aligning with each other?
A: After the two observers align.
B: Before the two observers align.
C: Exactly when the observers align.
D: It depends on the speed of the train.
E: It is impossible to determine from the given information.

Answer: B
@
Subject:
In physics, the Josephson effect is a phenomenon that occurs when two superconductors are placed in proximity, with some barrier or restriction between them. It is an example of a macroscopic quantum phenomenon, where the effects of quantum mechanics are observable at ordinary, rather than atomic, scale. The Josephson effect has many practical applications because it exhibits a precise relationship between different physical measures, such as voltage and frequency, facilitating highly accurate measurements.

The Josephson effect produces a current, known as a supercurrent, that flows continuously without any voltage applied, across a device known as a Josephson junction (JJ). These consist of two or more superconductors coupled by a weak link. The weak link can be a thin insulating barrier (known as a superconductor–insulator–superconductor junction, or S-I-S), a short section of non-superconducting metal (S-N-S), or a physical constriction that weakens the superconductivity at the point of contact (S-c-S).

Josephson junctions have important applications in quantum-mechanical circuits, such as SQUIDs, superconducting qubits, and RSFQ digital electronics. The NIST standard for one volt is achieved by an array of 20,208 Josephson junctions in series.
$
7
Question 2: What phenomenon occurs when two superconductors are placed in proximity with a barrier between them?
A: The Hall effect.
B: The Faraday effect.
C: The Josephson effect.
D: The Zeeman effect.
E: The Casimir effect.

Answer: C

Question 3: Why is the Josephson effect considered a macroscopic quantum phenomenon?
A: Because it shows the effects of quantum mechanics on a cosmic scale.
B: Because the effects of quantum mechanics are observable at a human-visible scale, not just atomic.
C: Because it only occurs within quantum computers.
D: Because it relates to macroscopic objects moving in superposition.
E: Because it is associated with large-scale electric charges.

Answer: B

Question 4: In a Josephson junction, what type of current flows without any applied voltage?
A: Normal current.
B: Alternating current.
C: Magnetic current.
D: Supercurrent.
E: Direct current.

Answer: D

Question 5: What can serve as a weak link in a Josephson junction?
A: A thick superconducting barrier.
B: A high-resistance metal section.
C: A thin insulating barrier.
D: A long stretch of superconducting metal.
E: An area of high thermal activity.

Answer: C

Question 6: The NIST standard for one volt is achieved by:
A: An array of 20,208 capacitors in series.
B: A single, large Josephson junction.
C: An array of 20,208 Josephson junctions in series.
D: A single, large superconducting wire.
E: An array of 20,208 resistors in series.

Answer: C

Question 7: What type of devices are Josephson junctions crucial for?
A: Battery circuits.
B: Traditional transistors.
C: Quantum-mechanical circuits, such as SQUIDs.
D: Classical logic gates.
E: High-voltage transformers.

Answer: C

Question 8: If a Josephson junction has a non-superconducting metal serving as the weak link, it is categorized as:
A: S-I-S.
B: S-c-S.
C: S-N-S.
D: N-S-N.
E: I-S-I.

Answer: C
@
Subject:
The International System of Units, internationally known by the abbreviation SI (for Système International),[a][1]: 125 [2]: iii [3] [b] is the modern form[1]: 117 [6][7] of the metric system[g] and the world's most widely used system of measurement.[1]: 123 [9][10] Established and maintained[11] by the General Conference on Weights and Measures[j] (CGPM[k]), it is the only system of measurement with an official status[m] in nearly every country in the world,[n] employed in science, technology, industry, and everyday commerce.

SI base units
Symbol	Name	Quantity
s	second	time
m	metre	length
kg	kilogram	mass
A	ampere	electric current
K	kelvin	thermodynamic temperature
mol	mole	amount of substance
cd	candela	luminous intensity
The SI comprises a coherent[o] system of units of measurement starting with seven base units, which are the second (symbol s, the unit of time), metre (m, length), kilogram (kg, mass), ampere (A, electric current), kelvin (K, thermodynamic temperature), mole (mol, amount of substance), and candela (cd, luminous intensity). The system can accommodate coherent units for an unlimited number of additional quantities. These are called coherent derived units, which can always be represented as products of powers of the base units.[p] Twenty-two coherent derived units have been provided with special names and symbols.[q]

The system allows for an unlimited number of additional units, called derived units, which can always be represented as products of powers of the base units, possibly with a nontrivial numeric multiplier. When that multiplier is one, the unit is called a coherent derived unit.[ar] The base and coherent derived units of the SI together form a coherent system of units (the set of coherent SI units).[as] Twenty-two coherent derived units have been provided with special names and symbols.[q] The seven base units and the 22 derived units with special names and symbols may be used in combination to express other derived units,[r] which are adopted to facilitate measurement of diverse quantities.

Prior to its redefinition in 2019, the SI was defined through the seven base units from which the derived units were constructed as products of powers of the base units. After the redefinition, the SI is defined by fixing the numerical values of seven defining constants. This has the effect that the distinction between the base units and derived units is, in principle, not needed, since all units, base as well as derived, may be constructed directly from the defining constants. Nevertheless, the distinction is retained because "it is useful and historically well established", and also because the ISO/IEC 80000 series of standards[at] specifies base and derived quantities that necessarily have the corresponding SI units.[1]: 129 

The derived units in the SI are formed by powers, products, or quotients of the base units and are potentially unlimited in number.[25]: 103 [2]: 14, 16  Derived units are associated with derived quantities; for example, velocity is a quantity that is derived from the base quantities of time and length, and thus the SI derived unit is metre per second (symbol m/s). The dimensions of derived units can be expressed in terms of the dimensions of the base units.

Combinations of base and derived units may be used to express other derived units. For example, the SI unit of force is the newton (N), the SI unit of pressure is the pascal (Pa)—and the pascal can be defined as one newton per square metre (N/m2).[30]

SI derived units with special names and symbols[2]: 15 
Name	Symbol	Quantity	In SI base units	In other SI units
radian[N 1]	rad	plane angle	m/m	1
steradian[N 1]	sr	solid angle	m2/m2	1
hertz	Hz	frequency	s−1	
newton	N	force, weight	kg⋅m⋅s−2	
pascal	Pa	pressure, stress	kg⋅m−1⋅s−2	N/m2 = J/m3
joule	J	energy, work, heat	kg⋅m2⋅s−2	N⋅m = Pa⋅m3
watt	W	power, radiant flux	kg⋅m2⋅s−3	J/s
coulomb	C	electric charge	s⋅A	
volt	V	electric potential, voltage, emf	kg⋅m2⋅s−3⋅A−1	W/A = J/C
farad	F	capacitance	kg−1⋅m−2⋅s4⋅A2	C/V = C2/J
ohm	Ω	resistance, impedance, reactance	kg⋅m2⋅s−3⋅A−2	V/A = J⋅s/C2
siemens	S	electrical conductance	kg−1⋅m−2⋅s3⋅A2	Ω−1
weber	Wb	magnetic flux	kg⋅m2⋅s−2⋅A−1	V⋅s
tesla	T	magnetic flux density	kg⋅s−2⋅A−1	Wb/m2
henry	H	inductance	kg⋅m2⋅s−2⋅A−2	Wb/A
degree Celsius	°C	temperature relative to 273.15 K	K	
lumen	lm	luminous flux	cd⋅m2/m2	cd⋅sr
lux	lx	illuminance	cd⋅m2/m4	lm/m2 = cd⋅sr⋅m−2
becquerel	Bq	activity referred to a radionuclide (decays per unit time)	s−1	
gray	Gy	absorbed dose (of ionising radiation)	m2⋅s−2	J/kg
sievert	Sv	equivalent dose (of ionising radiation)	m2⋅s−2	J/kg
katal	kat	catalytic activity	mol⋅s−1
$
9
Question 2: The International System of Units is abbreviated as:
A: ISO
B: IU
C: ISU
D: SI
E: IMS

Answer: D

Question 3: Which organization is responsible for establishing and maintaining the International System of Units?
A: United Nations
B: International Standards Organization
C: General Conference on Weights and Measures
D: World Trade Organization
E: International Physics Union

Answer: C

Question 4: Which of the following is NOT a base unit in the SI system?
A: second
B: newton
C: kilogram
D: mole
E: candela

Answer: B

Question 5: How is the SI unit of force represented in terms of base units?
A: kg⋅m⋅s−1
B: m/s
C: kg⋅m⋅s−2
D: s−1
E: kg⋅m2⋅s−2

Answer: C

Question 6: How can the SI unit of pressure, pascal (Pa), be defined?
A: joule per meter
B: newton per meter squared
C: kilogram per second squared
D: watt per meter
E: ampere per second

Answer: B

Question 7: Which SI derived unit corresponds to the quantity of electric charge?
A: watt
B: joule
C: farad
D: coulomb
E: hertz

Answer: D

Question 8: In the redefinition of SI in 2019, the SI was defined by:
A: fixing the numerical values of seven defining base units.
B: using the seven base units for all derived units.
C: fixing the numerical values of seven defining constants.
D: abandoning the distinction between base units and derived units.
E: relying solely on base quantities and their direct measurements.

Answer: C

Question 9: The dimensions of derived units in the SI can be expressed in terms of:
A: the dimensions of the defining constants.
B: the dimensions of the base units.
C: the historical units of measurement.
D: any random combinations of units.
E: the dimensions of other derived units.

Answer: B

Question 10: What is the SI derived unit for the quantity "resistance"?
A: coulomb
B: tesla
C: pascal
D: ohm
E: watt

Answer: D
@
Subject:
In crystallography, a crystallographic point group is a set of symmetry operations, corresponding to one of the point groups in three dimensions, such that each operation (perhaps followed by a translation) would leave the structure of a crystal unchanged i.e. the same kinds of atoms would be placed in similar positions as before the transformation. For example, in many crystals in the cubic crystal system, a rotation of the unit cell by 90 degrees around an axis that is perpendicular to one of the faces of the cube is a symmetry operation that moves each atom to the location of another atom of the same kind, leaving the overall structure of the crystal unaffected.

In the classification of crystals, each point group defines a so-called (geometric) crystal class. There are infinitely many three-dimensional point groups. However, the crystallographic restriction on the general point groups results in there being only 32 crystallographic point groups. These 32 point groups are one-and-the-same as the 32 types of morphological (external) crystalline symmetries derived in 1830 by Johann Friedrich Christian Hessel from a consideration of observed crystal forms.

The point group of a crystal determines, among other things, the directional variation of physical properties that arise from its structure, including optical properties such as birefringency, or electro-optical features such as the Pockels effect. For a periodic crystal (as opposed to a quasicrystal), the group must maintain the three-dimensional translational symmetry that defines crystallinity.
$
9
Question 2: What does a crystallographic point group in crystallography represent?
A: A set of symmetry operations that leave the structure of a crystal unchanged.
B: A method to define the density of a crystal.
C: A set of mathematical formulas to calculate the size of the crystal.
D: A method for arranging atoms in a crystal in a sequential manner.
E: A group of crystals with the same atomic composition.

Answer: A

Question 3: What is the result of a symmetry operation in the cubic crystal system?
A: A change in the atomic composition of the crystal.
B: A movement of each atom to a location of a different kind of atom.
C: A rotation of the unit cell by 90 degrees around an axis perpendicular to one of the faces of the cube.
D: A change in the molecular bond strength of the crystal.
E: A change in the translational symmetry of the crystal.

Answer: C

Question 4: Who derived the 32 types of morphological crystalline symmetries from observed crystal forms?
A: Isaac Newton
B: Albert Einstein
C: James Clerk Maxwell
D: Johann Friedrich Christian Hessel
E: Marie Curie

Answer: D

Question 5: What is determined by the point group of a crystal?
A: The color of the crystal.
B: The hardness of the crystal.
C: The melting point of the crystal.
D: The directional variation of physical properties arising from its structure.
E: The weight of the crystal.

Answer: D

Question 6: For a periodic crystal, what must the group maintain?
A: The two-dimensional rotational symmetry.
B: The three-dimensional translational symmetry.
C: The three-dimensional rotational symmetry.
D: The point-to-point linear symmetry.
E: The overall mass distribution symmetry.

Answer: B

Question 7: What does the classification of each point group in a crystal define?
A: Chemical class.
B: Molecular class.
C: Geometric crystal class.
D: Atomic structure class.
E: Electro-optical class.

Answer: C

Question 8: Which property might be influenced by the point group of a crystal?
A: Acidity or alkalinity of the crystal.
B: The number of atoms in the crystal.
C: Birefringency.
D: Solubility of the crystal in water.
E: Thermal conductivity of the crystal.

Answer: C

Question 9: How many general point groups are there in three-dimensional space?
A: There are only 32.
B: They are equal to the crystallographic point groups.
C: They are infinite in number.
D: They are limited to 14 point groups.
E: There is no specific count for general point groups.

Answer: C

Question 10: What remains unchanged after applying a symmetry operation on a crystallographic point group?
A: The size of the crystal.
B: The overall structure of the crystal.
C: The type of atoms present in the crystal.
D: The direction of atomic bonds.
E: The atomic temperature of the crystal.

Answer: B
@
Subject:
In physics, statistical mechanics is a mathematical framework that applies statistical methods and probability theory to large assemblies of microscopic entities. It does not assume or postulate any natural laws, but explains the macroscopic behavior of nature from the behavior of such ensembles.

Sometimes called statistical physics or statistical thermodynamics, its applications include many problems in the fields of physics, biology, chemistry, and neuroscience. Its main purpose is to clarify the properties of matter in aggregate, in terms of physical laws governing atomic motion.[1][2]

Statistical mechanics arose out of the development of classical thermodynamics, a field for which it was successful in explaining macroscopic physical properties—such as temperature, pressure, and heat capacity—in terms of microscopic parameters that fluctuate about average values and are characterized by probability distributions.

The founding of the field of statistical mechanics is generally credited to three physicists:

Ludwig Boltzmann, who developed the fundamental interpretation of entropy in terms of a collection of microstates
James Clerk Maxwell, who developed models of probability distribution of such states
Josiah Willard Gibbs, who coined the name of the field in 1884
While classical thermodynamics is primarily concerned with thermodynamic equilibrium, statistical mechanics has been applied in non-equilibrium statistical mechanics to the issues of microscopically modeling the speed of irreversible processes that are driven by imbalances. Examples of such processes include chemical reactions and flows of particles and heat. The fluctuation–dissipation theorem is the basic knowledge obtained from applying non-equilibrium statistical mechanics to study the simplest non-equilibrium situation of a steady state current flow in a system of many particles.

In physics, Lagrangian mechanics is a formulation of classical mechanics founded on the stationary-action principle (also known as the principle of least action). It was introduced by the Italian-French mathematician and astronomer Joseph-Louis Lagrange in his 1788 work, Mécanique analytique.[1]

Hamiltonian mechanics emerged in 1833 as a reformulation of Lagrangian mechanics. Introduced by Sir William Rowan Hamilton,[1] Hamiltonian mechanics replaces (generalized) velocities 
{\displaystyle {\dot {q}}^{i}} used in Lagrangian mechanics with (generalized) momenta. Both theories provide interpretations of classical mechanics and describe the same physical phenomena.

Hamiltonian mechanics has a close relationship with geometry (notably, symplectic geometry and Poisson structures) and serves as a link between classical and quantum mechanics.
$
9
Question 2: Statistical mechanics provides explanations for macroscopic physical properties in terms of what?
A: Classical laws of nature.
B: Microscopic parameters that fluctuate about average values.
C: The behavior of individual molecules.
D: The average speed of particles in a system.
E: Macroscopic equilibrium states.

Answer: B

Question 3: Which physicist is credited for the fundamental interpretation of entropy in terms of microstates?
A: Sir William Rowan Hamilton
B: James Clerk Maxwell
C: Joseph-Louis Lagrange
D: Josiah Willard Gibbs
E: Ludwig Boltzmann

Answer: E

Question 4: What did Hamiltonian mechanics replace from Lagrangian mechanics?
A: (Generalized) forces
B: (Generalized) potentials
C: (Generalized) accelerations
D: (Generalized) momenta
E: (Generalized) velocities

Answer: E

Question 5: What is the main purpose of statistical mechanics?
A: To describe the motion of individual atoms.
B: To quantify the behavior of large assemblies of microscopic entities.
C: To clarify the properties of matter in aggregate in terms of atomic motion.
D: To develop new laws of physics.
E: To analyze quantum mechanical systems.

Answer: C

Question 6: Who introduced Hamiltonian mechanics as a reformulation of Lagrangian mechanics?
A: James Clerk Maxwell
B: Ludwig Boltzmann
C: Sir William Rowan Hamilton
D: Josiah Willard Gibbs
E: Joseph-Louis Lagrange

Answer: C

Question 7: In which year was the field of statistical mechanics named by Josiah Willard Gibbs?
A: 1788
B: 1833
C: 1884
D: 1920
E: 1952

Answer: C

Question 8: The fluctuation–dissipation theorem is derived from which branch of statistical mechanics?
A: Equilibrium statistical mechanics
B: Non-equilibrium statistical mechanics
C: Quantum statistical mechanics
D: Classical statistical mechanics
E: Relativistic statistical mechanics

Answer: B

Question 9: Which theorem connects classical and quantum mechanics using Hamiltonian mechanics?
A: Uncertainty principle
B: Hamilton's equations
C: Fluctuation–dissipation theorem
D: Conservation of energy theorem
E: Schroedinger equation

Answer: B

Question 10: In his 1788 work, "Mécanique analytique", who introduced the concept of Lagrangian mechanics?
A: Ludwig Boltzmann
B: Sir William Rowan Hamilton
C: James Clerk Maxwell
D: Josiah Willard Gibbs
E: Joseph-Louis Lagrange

Answer: E
@
Subject:
Crystallinity refers to the degree of structural order in a solid. In a crystal, the atoms or molecules are arranged in a regular, periodic manner. The degree of crystallinity has a big influence on hardness, density, transparency and diffusion. In an ideal gas, the relative positions of the atoms or molecules are completely random. Amorphous materials, such as liquids and glasses, represent an intermediate case, having order over short distances (a few atomic or molecular spacings) but not over longer distances.

Many materials, such as glass-ceramics and some polymers, can be prepared in such a way as to produce a mixture of crystalline and amorphous regions. In such cases, crystallinity is usually specified as a percentage of the volume of the material that is crystalline. Even within materials that are completely crystalline, however, the degree of structural perfection can vary. For instance, most metallic alloys are crystalline, but they usually comprise many independent crystalline regions (grains or crystallites) in various orientations separated by grain boundaries; furthermore, they contain other crystallographic defects (notably dislocations) that reduce the degree of structural perfection. The most highly perfect crystals are silicon boules produced for semiconductor electronics; these are large single crystals (so they have no grain boundaries), are nearly free of dislocations, and have precisely controlled concentrations of defect atoms.

Crystallinity can be measured using x-ray crystallography, but calorimetric techniques are also commonly used.

Geologists describe four qualitative levels of crystallinity:

holocrystalline rocks are completely crystalline;
hypocrystalline rocks are partially crystalline, with crystals embedded in an amorphous or glassy matrix;
hypohyaline rocks are partially glassy;
holohyaline rocks (such as obsidian) are completely glassy.
$
9
Question 2: Which technique is NOT mentioned as a method to measure crystallinity?
A: Spectroscopy
B: Calorimetric techniques
C: X-ray crystallography
D: Electron microscopy
E: Magnetic resonance imaging

Answer: E

Question 3: In an ideal gas, how are the positions of atoms or molecules described?
A: Periodically arranged.
B: Ordered over short distances but not over longer distances.
C: Completely random.
D: Crystalline over long distances.
E: Arranged in a regular, repeating manner.

Answer: C

Question 4: What represents an intermediate case between a crystal and an ideal gas in terms of structural order?
A: Metallic alloys
B: Semiconductors
C: Amorphous materials
D: Polymers
E: Silicon boules

Answer: C

Question 5: Which type of rocks are described as completely glassy by geologists?
A: Holocrystalline
B: Hypocrystalline
C: Hypohyaline
D: Holohyaline
E: Hypercrystalline

Answer: D

Question 6: In materials that can be both crystalline and amorphous, how is crystallinity usually specified?
A: Percentage of weight that is crystalline.
B: Density of crystalline regions.
C: Percentage of the volume that is crystalline.
D: Hardness of crystalline regions.
E: Percentage of molecular order.

Answer: C

Question 7: What type of defects can reduce the degree of structural perfection in completely crystalline materials?
A: Concentrations of defect atoms
B: Non-uniform density
C: Inconsistent volume
D: Varying transparency levels
E: Random atomic or molecular spacings

Answer: A

Question 8: Which of the following best describes a hypocrystalline rock?
A: Completely crystalline.
B: Completely glassy.
C: Partially glassy.
D: Partially crystalline with crystals in an amorphous matrix.
E: Contains both crystalline and non-crystalline regions.

Answer: D

Question 9: Which type of crystals are described as having the highest degree of structural perfection?
A: Glass-ceramics
B: Amorphous polymers
C: Silicon boules produced for semiconductor electronics
D: Holohyaline rocks
E: Most metallic alloys

Answer: C

Question 10: The degree of crystallinity in a material influences all of the following EXCEPT:
A: Hardness.
B: Density.
C: Diffusion.
D: Transparency.
E: Conductivity.

Answer: E
@
Subject:
In chemistry, thermodynamics, and other related fields, a phase transition (or phase change) is the physical process of transition between one state of a medium and another. Commonly the term is used to refer to changes among the basic states of matter: solid, liquid, and gas, and in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change as a result of the change of external conditions, such as temperature or pressure. This can be a discontinuous change; for example, a liquid may become gas upon heating to its boiling point, resulting in an abrupt change in volume. The identification of the external conditions at which a transformation occurs defines the phase transition point.

Order parameters
An order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other.[30] At the critical point, the order parameter susceptibility will usually diverge.

An example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.

From a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions.

Some phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.[citation needed]

There also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.
$
9
Question 2: In which phase of a thermodynamic system is the order parameter usually zero?
A: Solid
B: Liquid
C: Gas
D: Above the critical point
E: Plasma

Answer: D

Question 3: What event defines the phase transition point of a medium?
A: A change in molecular composition.
B: A change in density.
C: An abrupt change in volume.
D: The identification of external conditions where a transformation occurs.
E: A change in net magnetization.

Answer: D

Question 4: For which type of transition is the order parameter the difference in densities?
A: Solid to liquid
B: Gas to plasma
C: Liquid to gas
D: Ferromagnetic to paramagnetic
E: Solid to gas

Answer: C

Question 5: When talking about a ferromagnetic system undergoing a phase transition, what can serve as an example of an order parameter?
A: Temperature variation
B: Density difference
C: Net magnetization
D: Critical point fluctuation
E: Volume change

Answer: C

Question 6: Which of the following arises from symmetry breaking?
A: The need to provide the net magnetization.
B: The change in temperature.
C: The identification of the critical point.
D: The abrupt change in volume.
E: The transformation of a liquid to a gas.

Answer: A

Question 7: Phase transitions like superconducting and ferromagnetic can have order parameters for how many degrees of freedom?
A: Only one
B: Maximum of two
C: Always zero
D: None
E: More than one

Answer: E

Question 8: In the context of phase transitions, what do disorder parameters indicate?
A: The presence of abrupt volume changes.
B: The presence of symmetry breaking.
C: The presence of line-like excitations.
D: The change in density of a system.
E: The identification of the critical point.

Answer: C

Question 9: What common states of matter undergo phase transitions?
A: Solid, plasma, and gas
B: Solid, liquid, and gas
C: Liquid, plasma, and gas
D: Solid and plasma
E: Liquid and gas

Answer: B

Question 10: Which of the following is NOT mentioned as a type of phase in the context of the topic?
A: Solid
B: Liquid
C: Gas
D: Vapor
E: Plasma

Answer: D
@
Subject:
The Crab Pulsar (PSR B0531+21) is a relatively young neutron star. The star is the central star in the Crab Nebula, a remnant of the supernova SN 1054, which was widely observed on Earth in the year 1054.[6][7][8] Discovered in 1968, the pulsar was the first to be connected with a supernova remnant.[9]


The sky seen in gamma-rays as seen by the Fermi Gamma-ray Space Telescope, reveals the Crab Pulsar as one of the brightest gamma-ray sources in the sky.
The Crab Pulsar is one of very few pulsars to be identified optically. The optical pulsar is roughly 20 kilometres (12 mi) in diameter and has a rotational period of about 33 milliseconds, that is, the pulsar "beams" perform about 30 revolutions per second.[4] The outflowing relativistic wind from the neutron star generates synchrotron emission, which produces the bulk of the emission from the nebula, seen from radio waves through to gamma rays. The most dynamic feature in the inner part of the nebula is the point where the pulsar's equatorial wind slams into the surrounding nebula, forming a termination shock. The shape and position of this feature shifts rapidly, with the equatorial wind appearing as a series of wisp-like features that steepen, brighten, then fade as they move away from the pulsar into the main body of the nebula. The period of the pulsar's rotation is increasing by 38 nanoseconds per day due to the large amounts of energy carried away in the pulsar wind.[10]

The Crab Nebula is often used as a calibration source in X-ray astronomy. It is very bright in X-rays, and the flux density and spectrum are known to be constant, with the exception of the pulsar itself. The pulsar provides a strong periodic signal that is used to check the timing of the X-ray detectors. In X-ray astronomy, "crab" and "millicrab" are sometimes used as units of flux density. A millicrab corresponds to a flux density of about 2.4×10−11 erg s−1 cm−2 (2.4×10−14 W/m2) in the 2–10 keV X-ray band, for a "crab-like" X-ray spectrum, which is roughly power-law in photon energy: I ~ E−1.1.[citation needed] Very few X-ray sources ever exceed one crab in brightness.

The Crab Pulsar was the first pulsar for which the spin-down limit was broken using several months of data of the LIGO observatory. Most pulsars do not rotate at constant rotation frequency, but can be observed to slow down at a very slow rate (3.7×10−10 Hz/s in case of the Crab). This spin-down can be explained as a loss of rotation energy due to various mechanisms. The spin-down limit is a theoretical upper limit of the amplitude of gravitational waves that a pulsar can emit, assuming that all the losses in energy are converted to gravitational waves. No gravitational waves observed at the expected amplitude and frequency (after correcting for the expected Doppler shift) proves that other mechanisms must be responsible for the loss in energy. The non-observation so far is not totally unexpected, since physical models of the rotational symmetry of pulsars puts a more realistic upper limit on the amplitude of gravitational waves several orders of magnitude below the spin-down limit. It is hoped that with the improvement of the sensitivity of gravitational wave instruments and the use of longer stretches of data, gravitational waves emitted by pulsars will be observed in future.[22] The only other pulsar for which the spin-down limit was broken so far is the Vela Pulsar.
$
9
Question 2: In what year was the Crab Pulsar discovered?
A: 1054
B: 1540
C: 1968
D: 2000
E: 2010

Answer: C

Question 3: How is the Crab Nebula used in X-ray astronomy?
A: As a cooling source for X-ray equipment.
B: To study the origins of X-rays.
C: As a calibration source.
D: To produce X-rays for space imaging.
E: As a model for X-ray formation.

Answer: C

Question 4: Approximately how many revolutions per second does the Crab Pulsar perform?
A: 3 revolutions
B: 10 revolutions
C: 15 revolutions
D: 20 revolutions
E: 30 revolutions

Answer: E

Question 5: The Crab Pulsar is identified optically. What is its approximate diameter?
A: 5 kilometres
B: 10 kilometres
C: 20 kilometres
D: 40 kilometres
E: 60 kilometres

Answer: C

Question 6: What dynamic feature exists in the inner part of the nebula where the pulsar's equatorial wind hits the surrounding nebula?
A: Black hole
B: Termination shock
C: Gamma ray burst
D: Neutrino pulse
E: Dark matter

Answer: B

Question 7: What is the rotational period of the Crab Pulsar increasing by daily?
A: 3 nanoseconds
B: 15 nanoseconds
C: 25 nanoseconds
D: 38 nanoseconds
E: 50 nanoseconds

Answer: D

Question 8: In X-ray astronomy, what does one "crab" unit represent in terms of flux density?
A: About 2.4×10−9 erg s−1 cm−2
B: About 2.4×10−11 erg s−1 cm−2
C: About 2.4×10−13 erg s−1 cm−2
D: About 2.4×10−15 erg s−1 cm−2
E: About 2.4×10−17 erg s−1 cm−2

Answer: B

Question 9: The Crab Pulsar's spin-down can be attributed to what?
A: An increase in rotation energy due to various mechanisms.
B: A constant rotation frequency.
C: A loss of rotation energy due to various mechanisms.
D: The gravitational pull from nearby stars.
E: The magnetic interference from nearby galaxies.

Answer: C

Question 10: Which other pulsar, apart from the Crab Pulsar, broke the spin-down limit?
A: Alpha Centauri Pulsar
B: Pleiades Pulsar
C: Orion Pulsar
D: Andromeda Pulsar
E: Vela Pulsar

Answer: E
@
Subject:
The De Haas–Van Alphen effect, often abbreviated to DHVA, is a quantum mechanical effect in which the magnetic susceptibility of a pure metal crystal oscillates as the intensity of the magnetic field B is increased. It can be used to determine the Fermi surface of a material. Other quantities also oscillate, such as the electrical resistivity (Shubnikov–de Haas effect), specific heat, and sound attenuation and speed.[1][2][3] It is named after Wander Johannes de Haas and his student Pieter M. van Alphen.[4] The DHVA effect comes from the orbital motion of itinerant electrons in the material. An equivalent phenomenon at low magnetic fields is known as Landau diamagnetism.

In electromagnetism, the magnetic susceptibility (from Latin susceptibilis 'receptive'; denoted χ, chi) is a measure of how much a material will become magnetized in an applied magnetic field. It is the ratio of magnetization M (magnetic moment per unit volume) to the applied magnetizing field intensity H. This allows a simple classification, into two categories, of most materials' responses to an applied magnetic field: an alignment with the magnetic field, χ > 0, called paramagnetism, or an alignment against the field, χ < 0, called diamagnetism.

In condensed matter physics, the Fermi surface is the surface in reciprocal space which separates occupied from unoccupied electron states at zero temperature.[1] The shape of the Fermi surface is derived from the periodicity and symmetry of the crystalline lattice and from the occupation of electronic energy bands. The existence of a Fermi surface is a direct consequence of the Pauli exclusion principle, which allows a maximum of one electron per quantum state.[2][3][4][5] The study of the Fermi surfaces of materials is called fermiology.
$
9
Question 2: Who are the scientists after whom the De Haas–Van Alphen effect is named?
A: Albert Einstein and Niels Bohr
B: James Clerk Maxwell and Richard Feynman
C: Wander Johannes de Haas and Pieter M. van Alphen
D: Erwin Schrödinger and Werner Heisenberg
E: Paul Dirac and Louis de Broglie

Answer: C

Question 3: What is the magnetic susceptibility (χ) a measure of?
A: The capacity of a metal to conduct electricity
B: The amount a material will become magnetized in an applied magnetic field
C: The temperature at which a material will lose its magnetism
D: The ability of a material to resist magnetization
E: The alignment of a material's magnetic field with Earth's magnetic field

Answer: B

Question 4: How can one classify materials based on their magnetic susceptibility?
A: Conductor or Insulator
B: Solid or Liquid
C: Paramagnetism (χ > 0) or Diamagnetism (χ < 0)
D: Ferromagnetism or Antiferromagnetism
E: Metal or Non-metal

Answer: C

Question 5: What does the Fermi surface in condensed matter physics separate?
A: Heavy metals from light metals
B: Solid materials from liquid materials
C: Protons from neutrons
D: Occupied from unoccupied electron states at zero temperature
E: Paramagnetic materials from diamagnetic materials

Answer: D

Question 6: Which principle allows a maximum of one electron per quantum state?
A: Heisenberg Uncertainty Principle
B: Bohr's Atomic Theory
C: Pauli Exclusion Principle
D: Einstein's Theory of Relativity
E: Dalton's Atomic Theory

Answer: C

Question 7: In what field of study is the study of the Fermi surfaces of materials called fermiology?
A: Astronomy
B: Biochemistry
C: Geology
D: Condensed Matter Physics
E: Thermodynamics

Answer: D

Question 8: What is the cause of the De Haas–Van Alphen effect?
A: Motion of protons within the material
B: Orbital motion of itinerant electrons in the material
C: Spin of the neutrons within the nucleus
D: Thermal vibrations of the crystal lattice
E: Rotation of atoms around a fixed point in the material

Answer: B

Question 9: An equivalent phenomenon to the De Haas–Van Alphen effect at low magnetic fields is known as what?
A: Bohr Magnetism
B: Fermi Magnetism
C: Maxwell Magnetism
D: Einstein Magnetism
E: Landau Diamagnetism

Answer: E

Question 10: What does the oscillation of the magnetic susceptibility in the De Haas–Van Alphen effect allow one to determine about a material?
A: Its melting point
B: Its total number of electrons
C: The Fermi surface
D: The total number of protons and neutrons
E: Its weight in an applied magnetic field

Answer: C
@
Subject:
In physics, a "coffee ring" is a pattern left by a puddle of particle-laden liquid after it evaporates. The phenomenon is named for the characteristic ring-like deposit along the perimeter of a spill of coffee. It is also commonly seen after spilling red wine. The mechanism behind the formation of these and similar rings is known as the coffee ring effect or in some instances, the coffee stain effect, or simply ring stain.

Flow mechanism
The coffee-ring pattern originates from the capillary flow induced by the evaporation of the drop: liquid evaporating from the edge is replenished by liquid from the interior.[1] The resulting current can carry nearly all the dispersed material to the edge. As a function of time, this process exhibits a "rush-hour" effect, that is, a rapid acceleration of the flow towards the edge at the final stage of the drying process.[2]

Evaporation induces a Marangoni flow inside a droplet. The flow, if strong, redistributes particles back to the center of the droplet. Thus, for particles to accumulate at the edges, the liquid must have a weak Marangoni flow, or something must occur to disrupt the flow.[3] For example, surfactants can be added to reduce the liquid's surface tension gradient, disrupting the induced flow. Water has a weak Marangoni flow to begin with, which is then reduced significantly by natural surfactants.[4]

Interaction of the particles suspended in a droplet with the free surface of the droplet is important in creating a coffee ring.[5] "When the drop evaporates, the free surface collapses and traps the suspended particles ... eventually all the particles are captured by the free surface and stay there for the rest of their trip towards the edge of the drop."[6] This result means that surfactants can be used to manipulate the motion of the solute particles by changing the surface tension of the drop, rather than trying to control the bulk flow inside the drop. A number of interesting morphologies of the deposited particles can result. For example, an enantiopure poly (isocyanate) derivative has been shown to form ordered arrays of squashed donut structures. [7]
$
9
Question 2: What causes the coffee-ring pattern?
A: The heating process while making coffee.
B: The capillary flow induced by the evaporation of the drop, with liquid evaporating from the edge being replenished by liquid from the interior.
C: The type of coffee bean used.
D: The temperature at which coffee is brewed.
E: The roasting process of coffee beans.

Answer: B

Question 3: What does a strong Marangoni flow inside a droplet do to particles?
A: It accelerates the evaporation process.
B: It drives the particles to the bottom of the droplet.
C: It redistributes particles back to the center of the droplet.
D: It pushes particles to the surface of the liquid.
E: It has no significant effect on the particles.

Answer: C

Question 4: Why do particles accumulate at the edges of a droplet?
A: Because of the strong Marangoni flow inside the droplet.
B: Because water naturally pushes particles to its edges.
C: Due to the gravitational pull on the particles.
D: The liquid must have a weak Marangoni flow, or an interference must occur to disrupt the flow.
E: The particles are naturally attracted to the edges.

Answer: D

Question 5: What can be added to a liquid to reduce its surface tension gradient and disrupt the induced Marangoni flow?
A: Alcohol
B: Sugar
C: Surfactants
D: Acid
E: Salt

Answer: C

Question 6: What is important for the creation of a coffee ring in terms of particle interaction?
A: Particles colliding with each other in the center.
B: Interaction of the particles suspended in a droplet with the free surface of the droplet.
C: Particles sinking to the bottom of the droplet.
D: Particles getting dissolved in the droplet.
E: Particles evaporating along with the liquid.

Answer: B

Question 7: As the drop evaporates, what happens to the suspended particles?
A: They dissolve into the liquid.
B: They float above the droplet.
C: The free surface collapses and traps them, making them stay for the rest of their journey towards the edge of the drop.
D: They evaporate along with the droplet.
E: They split into smaller particles.

Answer: C

Question 8: What is the "rush-hour" effect in the context of the coffee-ring pattern?
A: The time of day when most people drink coffee.
B: A sudden decrease in the speed of evaporation.
C: A rapid acceleration of the flow towards the edge at the final stage of the drying process.
D: A rapid movement of particles towards the center of the droplet.
E: The peak time for brewing coffee.

Answer: C

Question 9: How can surfactants influence the formation of coffee rings?
A: By strengthening the Marangoni flow.
B: By increasing the surface tension of the droplet.
C: By making the particles heavier.
D: By manipulating the motion of the solute particles by changing the surface tension of the drop.
E: By speeding up the evaporation process.

Answer: D

Question 10: What unique morphology can result from the deposited particles in the coffee-ring effect?
A: Ordered arrays of square structures.
B: Random clusters of particles.
C: Ordered arrays of squashed donut structures.
D: Lattices of hexagonal arrangements.
E: Spiraled patterns in the center of the droplet.

Answer: C
@
Subject:
In quantum mechanics, a probability amplitude is a complex number used for describing the behaviour of systems. The modulus squared of this quantity represents a probability density.

Probability amplitudes provide a relationship between the quantum state vector of a system and the results of observations of that system, a link was first proposed by Max Born, in 1926. Interpretation of values of a wave function as the probability amplitude is a pillar of the Copenhagen interpretation of quantum mechanics. In fact, the properties of the space of wave functions were being used to make physical predictions (such as emissions from atoms being at certain discrete energies) before any physical interpretation of a particular function was offered. Born was awarded half of the 1954 Nobel Prize in Physics for this understanding, and the probability thus calculated is sometimes called the "Born probability". These probabilistic concepts, namely the probability density and quantum measurements, were vigorously contested at the time by the original physicists working on the theory, such as Schrödinger and Einstein. It is the source of the mysterious consequences and philosophical difficulties in the interpretations of quantum mechanics—topics that continue to be debated even today.

Physical
Neglecting some technical complexities, the problem of quantum measurement is the behaviour of a quantum state, for which the value of the observable Q to be measured is uncertain. Such a state is thought to be a coherent superposition of the observable's eigenstates, states on which the value of the observable is uniquely defined, for different possible values of the observable.

When a measurement of Q is made, the system (under the Copenhagen interpretation) jumps to one of the eigenstates, returning the eigenvalue belonging to that eigenstate. The system may always be described by a linear combination or superposition of these eigenstates with unequal "weights". Intuitively it is clear that eigenstates with heavier "weights" are more "likely" to be produced. Indeed, which of the above eigenstates the system jumps to is given by a probabilistic law: the probability of the system jumping to the state is proportional to the absolute value of the corresponding numerical weight squared. These numerical weights are called probability amplitudes, and this relationship used to calculate probabilities from given pure quantum states (such as wave functions) is called the Born rule.
$
8
Question 3: What does the modulus squared of the probability amplitude represent in quantum mechanics?
A: Energy of the system
B: Speed of the particle
C: Probability density
D: Mass of the system
E: Velocity of the system

Answer: C

Question 4: Which interpretation of quantum mechanics considers the wave function as the probability amplitude?
A: Pilot wave theory
B: Many-worlds interpretation
C: Relational quantum mechanics
D: Copenhagen interpretation
E: Ensemble interpretation

Answer: D

Question 5: For what understanding was Max Born awarded half of the 1954 Nobel Prize in Physics?
A: For his work on the uncertainty principle.
B: For his contribution to the theory of relativity.
C: For interpreting the values of a wave function as the probability amplitude.
D: For his work on the double-slit experiment.
E: For his development of quantum field theory.

Answer: C

Question 6: What is the philosophical challenge of quantum measurements and probability amplitudes?
A: They can be accurately measured using classical tools.
B: They bring no mysterious consequences and philosophical difficulties.
C: They introduce a deterministic view of the universe.
D: They lead to mysterious consequences and philosophical difficulties in the interpretations of quantum mechanics.
E: They can be fully understood using Newtonian physics.

Answer: D

Question 7: In quantum mechanics, when a measurement is made, the system jumps to what kind of state?
A: A probabilistic state
B: An uncertain state
C: An eigenstate
D: A wave state
E: A coherent state

Answer: C

Question 8: How is the probability of the system jumping to a state related to probability amplitudes?
A: It's inversely proportional to the absolute value of the corresponding numerical weight squared.
B: It's directly proportional to the absolute value of the corresponding numerical weight.
C: It's proportional to the absolute value of the corresponding numerical weight squared.
D: It's independent of the numerical weight.
E: It's directly proportional to the modulus of the numerical weight.

Answer: C

Question 9: What does the Born rule describe in quantum mechanics?
A: It describes the relationship used to calculate energy from given pure quantum states.
B: It explains the uncertainty principle.
C: It describes the relationship used to calculate velocities from given pure quantum states.
D: It describes the relationship used to calculate probabilities from given pure quantum states.
E: It explains the duality of light.

Answer: D

Question 10: How can the behaviour of a quantum state for which the value of the observable is uncertain be described?
A: It remains in a fixed state regardless of measurements.
B: It is described by a linear combination or superposition of the observable's eigenstates.
C: It changes to a different quantum state after every observation.
D: It can only exist in one state at a given time.
E: It is always in a coherent state.

Answer: B
@
Subject:
In physics, sound is a vibration that propagates as an acoustic wave, through a transmission medium such as a gas, liquid or solid. In human physiology and psychology, sound is the reception of such waves and their perception by the brain.[1] Only acoustic waves that have frequencies lying between about 20 Hz and 20 kHz, the audio frequency range, elicit an auditory percept in humans. In air at atmospheric pressure, these represent sound waves with wavelengths of 17 meters (56 ft) to 1.7 centimeters (0.67 in). Sound waves above 20 kHz are known as ultrasound and are not audible to humans. Sound waves below 20 Hz are known as infrasound. Different animal species have varying hearing ranges.

The amplitude of sound waves and audio signals (which relates to the volume) conventionally refers to the amplitude of the air pressure in the wave, but sometimes the amplitude of the displacement (movements of the air or the diaphragm of a speaker) is described.[8][9] The logarithm of the amplitude squared is usually quoted in dB, so a null amplitude corresponds to −∞ dB. Loudness is related to amplitude and intensity and is one of the most salient qualities of a sound, although in general sounds it can be recognized independently of amplitude. The square of the amplitude is proportional to the intensity of the wave.

Loudness
Loudness is perceived as how "loud" or "soft" a sound is and relates to the totalled number of auditory nerve stimulations over short cyclic time periods, most likely over the duration of theta wave cycles.[26][27][28] This means that at short durations, a very short sound can sound softer than a longer sound even though they are presented at the same intensity level. Past around 200 ms this is no longer the case and the duration of the sound no longer affects the apparent loudness of the sound. Figure 3 gives an impression of how loudness information is summed over a period of about 200 ms before being sent to the auditory cortex. Louder signals create a greater 'push' on the Basilar membrane and thus stimulate more nerves, creating a stronger loudness signal. A more complex signal also creates more nerve firings and so sounds louder (for the same wave amplitude) than a simpler sound, such as a sine wave.
$
9
Question 2: Which of the following ranges of sound is not audible to humans?
A: Infrasound
B: Audible sound
C: Ultrasound
D: Theta wave
E: Loudness wave

Answer: C

Question 3: What is the frequency range of sound waves that elicit an auditory percept in humans?
A: 1 Hz to 10 Hz
B: 5 kHz to 25 kHz
C: 20 Hz to 20 kHz
D: 15 Hz to 25 kHz
E: 10 Hz to 30 kHz

Answer: C

Question 4: For sound waves in the air at atmospheric pressure, which wavelength corresponds to a frequency of about 20 Hz?
A: 1.7 centimeters
B: 5 meters
C: 17 meters
D: 2 meters
E: 56 centimeters

Answer: C

Question 5: What is sound in terms of physics?
A: The color perceived by humans.
B: The touch sensation by humans.
C: A vibration that propagates as an acoustic wave.
D: The temperature perceived by touch.
E: The perception of light by the eye.

Answer: C

Question 6: How do different animal species vary in terms of sound?
A: They perceive sound in the same way as humans.
B: They have different favorite sounds.
C: They produce sound at different frequencies.
D: They have varying hearing ranges.
E: They interpret music differently.

Answer: D

Question 7: How is amplitude squared usually quoted in terms of sound waves and audio signals?
A: In Hertz
B: In dB
C: In cm
D: In kHz
E: In meters

Answer: B

Question 8: How does the loudness of a sound get perceived in terms of duration?
A: Longer sounds are always louder.
B: Duration does not affect the loudness of sound.
C: At short durations, a shorter sound can sound softer than a longer sound at the same intensity level.
D: A sound's loudness increases exponentially with its duration.
E: The loudness of a sound remains constant regardless of its duration.

Answer: C

Question 9: What is the relationship between the intensity of a wave and the square of its amplitude?
A: They are inversely proportional.
B: They have no relationship.
C: The intensity of the wave is doubled when the amplitude is squared.
D: The square of the amplitude is proportional to the intensity of the wave.
E: The intensity of the wave decreases as the amplitude is squared.

Answer: D

Question 10: What determines the perceived loudness in terms of Basilar membrane stimulation?
A: The color of the sound wave.
B: The texture of the sound wave.
C: The duration of the sound wave.
D: The pitch of the sound wave.
E: Louder signals create a greater 'push' on the Basilar membrane, stimulating more nerves.

Answer: E
@
Subject:
Turbulent flows are complex multi-scale and chaotic motions that need to be classified into more elementary components, referred to coherent turbulent structures. Such a structure must have temporal coherence, i.e. it must persist in its form for long enough periods that the methods of time-averaged statistics can be applied. Coherent structures are typically studied on very large scales, but can be broken down into more elementary structures with coherent properties of their own, such examples include hairpin vortices. Hairpins and coherent structures have been studied and noticed in data since the 1930s, and have been since cited in thousands of scientific papers and reviews.[1]

Flow visualization experiments, using smoke and dye as tracers, have been historically used to simulate coherent structures and verify theories, but computer models are now the dominant tools widely used in the field to verify and understand the formation, evolution, and other properties of such structures. The kinematic properties of these motions include size, scale, shape, vorticity, energy, and the dynamic properties govern the way coherent structures grow, evolve, and decay. Most coherent structures are studied only within the confined forms of simple wall turbulence, which approximates the coherence to be steady, fully developed, incompressible, and with a zero pressure gradient in the boundary layer. Although such approximations depart from reality, they contain sufficient parameters needed to understand turbulent coherent structures in a highly conceptual degree.[2]

Definition
A turbulent flow is a flow regime in fluid dynamics where fluid velocity varies significantly and irregularly in both position and time.[3] Furthermore, a coherent structure is defined as a turbulent flow whose vorticity expression, which is usually stochastic, contains orderly components that can be described as being instantaneously coherent over the spatial extent of the flow structure. In other words, underlying the three-dimensional chaotic vorticity expressions typical of turbulent flows, there is an organized component of that vorticity which is phase-correlated over the entire space of the structure. The instantaneously space and phase correlated vorticity found within the coherent structure expressions can be defined as coherent vorticity, hence making coherent vorticity the main characteristic identifier for coherent structures. Another characteristic inherent in turbulent flows is their intermittency, but intermittency is a very poor identifier of the boundaries of a coherent structure, hence it is generally accepted that the best way to characterize the boundary of a structure is by identifying and defining the boundary of the coherent vorticity.[2]

By defining and identifying coherent structure in this manner, turbulent flows can be decomposed into coherent structures and incoherent structures depending on their coherence, particularly their correlations with their vorticity. Hence, similarly organized events in an ensemble average of organized events can be defined as a coherent structure, and whatever events not identified as similar or phase and space aligned in the ensemble average is an incoherent turbulent structure.

Other attempts at defining a coherent structure can be done through examining the correlation between their momenta or pressure and their turbulent flows. However, it often leads to false indications of turbulence, since pressure and velocity fluctuations over a fluid could be well correlated in the absence of any turbulence or vorticity. Some coherent structures, such as vortex rings, etc. can be large-scale motions comparable to the extent of the shear flow. There are also coherent motions at much smaller scales such as hairpin vortices and typical eddies, which are typically known as coherent substructures, as in coherent structures which can be broken up into smaller more elementary substructures.
$
9
Question 2: How have coherent structures traditionally been visualized?
A: Using computer models only.
B: Using smoke and dye as tracers.
C: Using sound waves and light.
D: Using lasers and mirrors.
E: Using mathematical equations without visualization.

Answer: B

Question 3: In the study of coherent structures, what properties are primarily used to understand the structures at a conceptual level?
A: Steady, fully developed, incompressible, and with a zero pressure gradient in the boundary layer.
B: Steady, partly developed, compressible, and with a high pressure gradient in the boundary layer.
C: Irregular, not developed, compressible, and with a zero pressure gradient outside the boundary layer.
D: Fully developed, steady, with high pressure and compressible nature.
E: Irregular, steady, with zero pressure and incompressible nature.

Answer: A

Question 4: Which statement best defines a turbulent flow?
A: A flow regime where fluid velocity is consistent and steady in both position and time.
B: A flow regime in fluid dynamics where fluid velocity varies slightly and regularly in both position and time.
C: A flow regime in fluid dynamics where fluid velocity varies significantly and irregularly in both position and time.
D: A flow regime where fluid motion is strictly one-dimensional.
E: A flow regime where fluid velocity remains constant over time.

Answer: C

Question 5: What is the main characteristic identifier for coherent structures?
A: Their intermittency.
B: Their steady velocity.
C: Coherent vorticity.
D: Incoherent vorticity.
E: Their correlation with incoherent structures.

Answer: C

Question 6: How is the boundary of a coherent structure best characterized?
A: By identifying and defining the boundary of the intermittency.
B: By identifying and defining the boundary of the coherent vorticity.
C: By examining the correlation between their momenta.
D: By examining the correlation between their pressure.
E: By measuring the volume of the structure.

Answer: B

Question 7: What is a typical example of a coherent substructure?
A: Large-scale vortex rings.
B: High-frequency sound waves.
C: Smoke tracers.
D: Hairpin vortices.
E: Pressure bubbles.

Answer: D

Question 8: In terms of coherence, how can turbulent flows be decomposed?
A: Into fast and slow structures.
B: Into dense and sparse structures.
C: Into coherent structures and incoherent structures.
D: Into regular and irregular structures.
E: Into simple and complex structures.

Answer: C

Question 9: What kind of correlation may lead to false indications of turbulence when defining a coherent structure?
A: Correlation between momentum and turbulent flows.
B: Correlation between their vorticity.
C: Correlation between their intermittency.
D: Correlation between their momenta or pressure and their turbulent flows.
E: Correlation between their density.

Answer: D

Question 10: What are coherent structures typically studied within?
A: Highly complex turbulent scenarios.
B: Open air environments without any constraints.
C: The confined forms of simple wall turbulence.
D: Highly pressurized chambers.
E: Vacuum chambers with zero gravity.

Answer: C
@
Subject:
A supernova (pl: supernovae or supernovas) is a powerful and luminous explosion of a star. A supernova occurs during the last evolutionary stages of a massive star or when a white dwarf is triggered into runaway nuclear fusion. The original object, called the progenitor, either collapses to a neutron star or black hole, or is completely destroyed to form a diffuse nebula. The peak optical luminosity of a supernova can be comparable to that of an entire galaxy before fading over several weeks or months.

The last supernova directly observed in the Milky Way was Kepler's Supernova in 1604, appearing not long after Tycho's Supernova in 1572, both of which were visible to the naked eye. The remnants of more recent supernovae have been found, and observations of supernovae in other galaxies suggest they occur in the Milky Way on average about three times every century. A supernova in the Milky Way would almost certainly be observable through modern astronomical telescopes. The most recent naked-eye supernova was SN 1987A, which was the explosion of a blue supergiant star in the Large Magellanic Cloud, a satellite of the Milky Way.

Theoretical studies indicate that most supernovae are triggered by one of two basic mechanisms: the sudden re-ignition of nuclear fusion in a white dwarf, or the sudden gravitational collapse of a massive star's core.

In the re-ignition of a white dwarf, the object's temperature is raised enough to trigger runaway nuclear fusion, completely disrupting the star. Possible causes are an accumulation of material from a binary companion through accretion, or by a stellar merger.
In the case of a massive star's sudden implosion, the core of a massive star will undergo sudden collapse once it is unable to produce sufficient energy from fusion to counteract the star's own gravity, which must happen once the star begins fusing iron, but may happen during an earlier stage of metal fusion.
Supernovae can expel several solar masses of material at velocities up to several percent of the speed of light. This drives an expanding shock wave into the surrounding interstellar medium, sweeping up an expanding shell of gas and dust observed as a supernova remnant. Supernovae are a major source of elements in the interstellar medium from oxygen to rubidium. The expanding shock waves of supernovae can trigger the formation of new stars. Supernovae are a major source of cosmic rays. They might also produce gravitational waves, though thus far gravitational waves have been detected only from the mergers of black holes and neutron stars.

Astronomers classify supernovae according to their light curves and the absorption lines of different chemical elements that appear in their spectra. If a supernova's spectrum contains lines of hydrogen (known as the Balmer series in the visual portion of the spectrum) it is classified Type II; otherwise it is Type I. In each of these two types there are subdivisions according to the presence of lines from other elements or the shape of the light curve (a graph of the supernova's apparent magnitude as a function of time).[60][61]

Supernova taxonomy[60][61]
Type I
No hydrogen	Type Ia
Presents a singly ionised silicon (Si II) line at 615.0 nm (nanometers), near peak light	Thermal runaway
Type Ib/c
Weak or no silicon absorption feature	Type Ib
Shows a non-ionised helium (He I) line at 587.6 nm	Core collapse
Type Ic
Weak or no helium
Type II
Shows hydrogen	Type II-P/-L/n
Type II spectrum throughout	Type II-P/L
No narrow lines	Type II-P
Reaches a "plateau" in its light curve
Type II-L
Displays a "linear" decrease in its light curve (linear in magnitude versus time)[62]
Type IIn
Some narrow lines
Type IIb
Spectrum changes to become like Type Ib
$
9
Question 2: What triggers the occurrence of a supernova?
A: The merging of two planets.
B: The last evolutionary stages of a massive star or when a white dwarf undergoes runaway nuclear fusion.
C: The appearance of a new star in the galaxy.
D: The star consuming all its helium reserves.
E: The gravitational pull from nearby asteroids.

Answer: B

Question 3: What is a significant consequence of supernovae in terms of element distribution?
A: They remove all elements in their vicinity.
B: They transform all elements into helium.
C: They primarily produce helium and hydrogen.
D: They are a major source of elements in the interstellar medium from oxygen to rubidium.
E: They create a vacuum in space, devoid of any elements.

Answer: D

Question 4: How can the shock waves from supernovae influence the surrounding medium?
A: They prevent the formation of new stars.
B: They cause nearby planets to disintegrate.
C: They trigger the formation of new stars.
D: They cause the immediate freezing of the interstellar medium.
E: They push all elements to the edge of the galaxy.

Answer: C

Question 5: What was the last supernova observed in the Milky Way with the naked eye?
A: SN 1987A
B: Tycho's Supernova in 1572
C: Kepler's Supernova in 1604
D: Type IIb Supernova in 1900
E: Type Ia Supernova in 1950

Answer: C

Question 6: If a supernova's spectrum contains lines of hydrogen, how is it classified?
A: Type I
B: Type Ib
C: Type II
D: Type Ia
E: Type Ic

Answer: C

Question 7: What happens in the re-ignition of a white dwarf leading to a supernova?
A: Its temperature drops drastically, causing it to implode.
B: It merges with another white dwarf.
C: Its temperature is raised enough to trigger runaway nuclear fusion, disrupting the star.
D: It spins rapidly, causing an imbalance in its gravitational pull.
E: It absorbs cosmic rays, leading to an explosion.

Answer: C

Question 8: What kind of waves might supernovae produce, although they have mainly been detected from other events?
A: Electromagnetic waves
B: Sound waves
C: Gravitational waves
D: Light waves
E: Radio waves

Answer: C

Question 9: How is a Type Ia supernova identified?
A: By its linear decrease in its light curve.
B: By showing a non-ionised helium line.
C: By presenting a singly ionised silicon line near peak light.
D: By its spectrum changing to become like Type Ib.
E: By its spectrum containing lines of hydrogen.

Answer: C

Question 10: Why does the core of a massive star undergo sudden collapse leading to a supernova?
A: Because of external gravitational forces acting on it.
B: Due to rapid cooling of its surface.
C: Because of its inability to produce sufficient energy from fusion to counteract its gravity.
D: Due to a sudden increase in its luminosity.
E: Because of external cosmic rays bombarding it.

Answer: C
@
Subject:
In mathematics, the Erlangen program is a method of characterizing geometries based on group theory and projective geometry. It was published by Felix Klein in 1872 as Vergleichende Betrachtungen über neuere geometrische Forschungen. It is named after the University Erlangen-Nürnberg, where Klein worked.

By 1872, non-Euclidean geometries had emerged, but without a way to determine their hierarchy and relationships. Klein's method was fundamentally innovative in three ways:

Projective geometry was emphasized as the unifying frame for all other geometries considered by him. In particular, Euclidean geometry was more restrictive than affine geometry, which in turn is more restrictive than projective geometry.
Klein proposed that group theory, a branch of mathematics that uses algebraic methods to abstract the idea of symmetry, was the most useful way of organizing geometrical knowledge; at the time it had already been introduced into the theory of equations in the form of Galois theory.
Klein made much more explicit the idea that each geometrical language had its own, appropriate concepts, thus for example projective geometry rightly talked about conic sections, but not about circles or angles because those notions were not invariant under projective transformations (something familiar in geometrical perspective). The way the multiple languages of geometry then came back together could be explained by the way subgroups of a symmetry group related to each other.
Later, Élie Cartan generalized Klein's homogeneous model spaces to Cartan connections on certain principal bundles, which generalized Riemannian geometry.
$
9
Question 2: Where does the name "Erlangen program" come from?
A: From Felix Klein's hometown.
B: From the equation used in the theory.
C: From the University Erlangen-Nürnberg, where Klein worked.
D: From a prominent mathematician named Erlangen.
E: From the German word for "enlightenment."

Answer: C

Question 3: What primary concern did the Erlangen program address in 1872?
A: The definition of calculus.
B: The hierarchy and relationships of non-Euclidean geometries.
C: The principles of basic arithmetic.
D: The foundational axioms of Euclidean geometry.
E: The abstraction of modern algebra.

Answer: B

Question 4: Which branch of mathematics did Klein emphasize for organizing geometrical knowledge?
A: Topology.
B: Calculus.
C: Galois theory.
D: Group theory.
E: Trigonometry.

Answer: D

Question 5: In terms of restrictiveness, which geometry was more restrictive than affine geometry according to Klein?
A: Non-Euclidean geometry.
B: Projective geometry.
C: Riemannian geometry.
D: Euclidean geometry.
E: Hyperbolic geometry.

Answer: D

Question 6: Which theory had already introduced group theory into the theory of equations by the time of Klein's proposal?
A: Newton's laws.
B: Pythagoras theorem.
C: Pascal's triangle.
D: Euler's identity.
E: Galois theory.

Answer: E

Question 7: In the context of projective geometry, which concept does not remain invariant under projective transformations?
A: Circles.
B: Straight lines.
C: Points.
D: Intersections.
E: Parallel lines.

Answer: A

Question 8: Who later generalized Klein's ideas to Cartan connections on principal bundles?
A: Isaac Newton.
B: Pythagoras.
C: Carl Friedrich Gauss.
D: Élie Cartan.
E: Leonhard Euler.

Answer: D

Question 9: The Erlangen program emphasizes the role of which geometry as a unifying frame for other geometries?
A: Euclidean geometry.
B: Non-Euclidean geometry.
C: Affine geometry.
D: Hyperbolic geometry.
E: Projective geometry.

Answer: E

Question 10: Which geometrical language is particularly suitable for discussing conic sections according to the Erlangen program?
A: Euclidean geometry.
B: Non-Euclidean geometry.
C: Projective geometry.
D: Topological geometry.
E: Affine geometry.

Answer: C
@
Subject:
The emissivity of the surface of a material is its effectiveness in emitting energy as thermal radiation. Thermal radiation is electromagnetic radiation that most commonly includes both visible radiation (light) and infrared radiation, which is not visible to human eyes. A portion of the thermal radiation from very hot objects (see photograph) is easily visible to the eye.

The emissivity of a surface depends on its chemical composition and geometrical structure. Quantitatively, it is the ratio of the thermal radiation from a surface to the radiation from an ideal black surface at the same temperature as given by the Stefan–Boltzmann law. (A comparison with Planck's law is used if one is concerned with particular wavelengths of thermal radiation.) The ratio varies from 0 to 1.

The surface of a perfect black body (with an emissivity of 1) emits thermal radiation at the rate of approximately 448 watts per square metre (W/m2) at a room temperature of 25 °C (298 K; 77 °F).

Objects generally have emissivities less than 1.0, and emit radiation at correspondingly lower rates.[1]

However, wavelength- and subwavelength-scale particles,[2] metamaterials,[3] and other nanostructures[4] may have an emissivity greater than 1.
$
9
Question 2: Which type of radiation is commonly included in thermal radiation?
A: Radio waves.
B: Microwaves.
C: Ultraviolet rays.
D: Visible light and infrared radiation.
E: X-rays.

Answer: D

Question 3: What determines the emissivity of a material's surface?
A: Its temperature and density.
B: Its chemical composition and geometrical structure.
C: Its conductivity and malleability.
D: Its atomic weight and mass.
E: Its reflectivity and transparency.

Answer: B

Question 4: The Stefan–Boltzmann law relates the emissivity of a surface to the radiation from which ideal surface?
A: White surface.
B: Reflective surface.
C: Transparent surface.
D: Grey surface.
E: Black surface.

Answer: E

Question 5: On a scale from 0 to 1, what is the emissivity of a perfect black body?
A: 0
B: 0.5
C: 1
D: 0.75
E: 1.5

Answer: C

Question 6: How much thermal radiation does a perfect black body emit at room temperature (25 °C)?
A: 448 W/m^2
B: 298 W/m^2
C: 25 W/m^2
D: 77 W/m^2
E: 500 W/m^2

Answer: A

Question 7: Which of the following statements about objects' emissivities is true?
A: Objects generally have emissivities greater than 1.0.
B: All objects have the same emissivity.
C: Objects generally have emissivities less than 1.0.
D: Emissivity is not a factor in determining how much radiation an object emits.
E: Objects always have emissivities of exactly 1.0.

Answer: C

Question 8: Which structures can potentially have an emissivity greater than 1?
A: Macroscopic objects
B: Single atoms
C: Metamaterials
D: Traditional building materials
E: Pure metals

Answer: C

Question 9: Which law would be used in comparison if one is concerned about specific wavelengths of thermal radiation?
A: Newton's law
B: Boyle's law
C: Planck's law
D: Faraday's law
E: Ohm's law

Answer: C

Question 10: What is thermal radiation mainly composed of?
A: Electromagnetic radiation
B: Mechanical waves
C: Radio waves
D: Sound waves
E: Ultrasonic waves

Answer: A
@
Subject:
The pulmonary circulation is a division of the circulatory system in all vertebrates. The circuit begins with deoxygenated blood returned from the body to the right atrium of the heart where it is pumped out from the right ventricle to the lungs. In the lungs the blood is oxygenated and returned to the left atrium to complete the circuit.[1]

The other division of the circulatory system is the systemic circulation that begins with receiving the oxygenated blood from the pulmonary circulation into the left atrium. From the atrium the oxygenated blood enters the left ventricle where it is pumped out to the rest of the body, returning as deoxygenated blood back to the pulmonary circulation.

The blood vessels of the pulmonary circulation are the pulmonary arteries and the pulmonary veins.

A separate circulatory circuit known as the bronchial circulation supplies oxygenated blood to the tissue of the larger airways of the lung.

De-oxygenated blood leaves the heart, goes to the lungs, and then enters back into the heart.[2] De-oxygenated blood leaves through the right ventricle through the pulmonary artery.[2] From the right atrium, the blood is pumped through the tricuspid valve (or right atrioventricular valve) into the right ventricle. Blood is then pumped from the right ventricle through the pulmonary valve and into the pulmonary artery.[2]

Lungs
The pulmonary arteries carry deoxygenated blood to the lungs, where carbon dioxide is released and oxygen is picked up during respiration.[3] Arteries are further divided into very fine capillaries which are extremely thin-walled.[4] The pulmonary veins return oxygenated blood to the left atrium of the heart.[3]

Veins
Main article: Pulmonary vein
Oxygenated blood leaves the lungs through pulmonary veins, which return it to the left part of the heart, completing the pulmonary cycle.[3][5] This blood then enters the left atrium, which pumps it through the mitral valve into the left ventricle.[3][5] From the left ventricle, the blood passes through the aortic valve to the aorta.[3][5] The blood is then distributed to the body through the systemic circulation before returning again to the pulmonary circulation.[3][5]

Arteries
Main article: Pulmonary artery
From the right ventricle, blood is pumped through the semilunar pulmonary valve into the left and right main pulmonary artery (one for each lung), which branch into smaller pulmonary arteries that spread throughout the lungs.[3][5]

The pulmonary circulation is archaically known as the "lesser circulation" which is still used in non-English literature.[13][14]

The discovery of the pulmonary circulation has been attributed to many scientists with credit distributed in varying ratios by varying sources. In much of modern medical literature, the discovery is credited to English physician William Harvey (1578 – 1657 CE) based on the comprehensive completeness and correctness of his model, despite its relative recency.[15][16] Other sources credit Greek philosopher Hippocrates (460 – 370 BCE), Spanish physician Michael Servetus (c. 1509 – 1553 CE), Arab physician Ibn al-Nafis (1213 – 1288 CE), and Syrian physician Qusta ibn Luqa.[17][18][19][20] Several figures such as Hippocrates and al-Nafis receive credit for accurately predicting or developing specific elements of the modern model of pulmonary circulation: Hippocrates[19] for being the first to describe pulmonary circulation as a discrete system separable from systemic circulation as a whole and al-Nafis[21] for making great strides over the understanding of those before him and towards a rigorous model. There is a great deal of subjectivity involved in deciding at which point a complex system is "discovered", as it is typically elucidated in piecemeal form so that the very first description, most complete or accurate description, and the most significant forward leaps in understanding are all considered acts of discovery of varying significance.[19]
$
9
Question 2: What does the pulmonary circulation involve primarily?
A: Carrying oxygenated blood to the body's tissues.
B: Carrying deoxygenated blood from the body to the lungs and back.
C: Distributing nutrients to the body's cells.
D: Removing waste products from body's tissues.
E: Regulating body temperature.

Answer: B

Question 3: What role does the right atrium play in the pulmonary circulation?
A: It pumps oxygenated blood to the rest of the body.
B: It receives deoxygenated blood returning from the body.
C: It carries oxygenated blood from the lungs.
D: It pumps blood directly to the body's tissues.
E: It receives blood directly from the pulmonary vein.

Answer: B

Question 4: The bronchial circulation provides oxygenated blood to which part of the body?
A: Heart
B: Liver
C: Large airways of the lung
D: Brain
E: Kidneys

Answer: C

Question 5: Through which valve does the blood flow from the right atrium to the right ventricle?
A: Aortic valve
B: Mitral valve
C: Pulmonary valve
D: Tricuspid valve
E: Semilunar valve

Answer: D

Question 6: What is the primary function of the pulmonary arteries?
A: Carrying oxygenated blood from the lungs to the heart.
B: Carrying deoxygenated blood from the heart to the lungs.
C: Distributing nutrients throughout the body.
D: Carrying oxygenated blood from the heart to the body.
E: Returning deoxygenated blood from the body to the heart.

Answer: B

Question 7: Which of the following veins returns oxygenated blood to the left atrium of the heart?
A: Aorta
B: Vena cava
C: Pulmonary vein
D: Pulmonary artery
E: Coronary vein

Answer: C

Question 8: The pulmonary circulation was traditionally referred to as the:
A: Greater circulation
B: Minor circulation
C: Lesser circulation
D: Major circulation
E: Systemic circulation

Answer: C

Question 9: Whose comprehensive model of pulmonary circulation is widely recognized in modern medical literature?
A: Hippocrates
B: Michael Servetus
C: William Harvey
D: Ibn al-Nafis
E: Qusta ibn Luqa

Answer: C

Question 10: Which of the following describes the blood's journey in the systemic circulation?
A: Deoxygenated blood is carried from the body to the lungs and back.
B: Oxygenated blood is carried from the lungs to the rest of the body and back.
C: Oxygenated blood is carried from the heart to the lungs.
D: Deoxygenated blood is pumped to the body's tissues.
E: Oxygenated blood is distributed to the body, then returns as deoxygenated blood to the pulmonary circulation.

Answer: E
@
Subject:
The Environmental Science Center is a research center at Qatar University[1] and was established in 1980[2][3][4] to promote environmental studies across the state of Qatar with main focus on marine science, atmospheric and biological sciences.[5] For the past 18 years,[6] ESC monitored and studied Hawksbill turtle nesting sites in Qatar.[7][8]

History
in 1980 it was named Scientific and Applied Research Center (SARC).[4]
in 2005 it was restructured and renamed Environmental Studies Center (ESC).
in 2015, the business name was changed to Environmental Science Center (ESC) to better reflect the research-driven objectives.[2]
Research clusters
The ESC has 3 major research clusters[9] that cover areas of strategic importance to Qatar. The clusters are:

Atmospheric sciences cluster
Earth sciences cluster
Marine sciences cluster with 2 majors:[10]
Terrestrial Ecology
Physical and Chemical Oceanography
UNESCO Chair in marine sciences
The first of its kind in the Arabian Gulf region, United Nations Educational, Scientific and Cultural Organization (UNESCO) have announced the establishment of the UNESCO Chair in marine sciences at QU's Environmental Science Center.[11][12][13][14] The chair is aiming to providing sustainable marine environment in the Arabian Gulf and protection of marine ecosystems.[15]

Inventions
Marine clutch technology.[16][17][18][19][20][21]
Mushroom artificial reef technology[22][23] (mushroom forest).[24]

Facilities
JANAN Research Vessel used by ESC since 2011 - Qatar University[28]
ESC is the home of wide range of facilities. The most notable one is the mobile labs on board the JANAN Research Vessel.[29][30][31]

JANAN is a 42.80 m. multipurpose Research Vessel[28] and was named after the island located in the western coast of the Qatari peninsula. It was donated to Qatar University by H.H. Sheikh Tamim bin Hamad Al Thani[30] the Amir of Qatar.

JANAN is used extensively in studying the state of marine environment in the Exclusive Economic Zone (EEZ) of the State of Qatar[32][33][34][35] and to advance critical marine environmental studies and research in Qatar and the wider Gulf.

The center also has 12 labs equipped with state-of-arts instruments.[36]
$
9
Question 2: In which year was the center first established?
A: 1975
B: 1980
C: 2005
D: 2015
E: 1990

Answer: B

Question 3: What was the original name of the Environmental Science Center when it was established in 1980?
A: Earth Sciences Center
B: Qatar Marine Research Center
C: Environmental Studies Center
D: Scientific and Applied Research Center
E: Qatar Environmental Laboratory

Answer: D

Question 4: Which organization established a Chair in marine sciences at QU's Environmental Science Center?
A: World Health Organization
B: United Nations
C: United Nations Educational, Scientific and Cultural Organization
D: World Environmental Organization
E: Qatar Science Foundation

Answer: C

Question 5: What is the purpose of the UNESCO Chair in marine sciences?
A: Promote atmospheric research in the Arabian Gulf.
B: Focus on the terrestrial ecology of Qatar.
C: Provide sustainable marine environment in the Arabian Gulf and protection of marine ecosystems.
D: Establish a marine museum in the Gulf region.
E: Monitor the migration of marine animals in the Arabian Gulf.

Answer: C

Question 6: Which research vessel is associated with the Environmental Science Center?
A: Qatari Explorer
B: Sheikh Tamim
C: JANAN
D: Arabian Voyager
E: Qatar Marine

Answer: C

Question 7: How was the JANAN Research Vessel acquired by Qatar University?
A: Purchased by the university's research funds.
B: Built by the students of Qatar University.
C: Donated by H.H. Sheikh Tamim bin Hamad Al Thani, the Amir of Qatar.
D: Loaned by a private marine research organization.
E: Won in an international marine research competition.

Answer: C

Question 8: For how many years has the ESC monitored and studied the Hawksbill turtle nesting sites in Qatar?
A: 5 years
B: 10 years
C: 15 years
D: 18 years
E: 25 years

Answer: D

Question 9: One of the inventions of the ESC is the:
A: Marine fuel technology
B: Fish tagging system
C: Coral reef simulation
D: Mushroom artificial reef technology
E: Oceanic wave prediction system

Answer: D

Question 10: How many major research clusters does the ESC have?
A: 1
B: 2
C: 3
D: 4
E: 5

Answer: C
@
Subject:
Types of surgery
Surgical procedures are commonly categorized by urgency, type of procedure, body system involved, the degree of invasiveness, and special instrumentation.

Based on timing:[citation needed]
Elective surgery is done to correct a non-life-threatening condition, and is carried out at the person's convenience, or to the surgeon's and the surgical facility's availability.
Semi-elective surgery is one that is better done early to avoid complications or potential deterioration of the patient's condition, but such risk are sufficiently low that the procedure can be postponed for a short period time.
Emergency surgery is surgery which must be done without any delay to prevent death or serious disabilities and/or loss of limbs and functions.
Based on purpose:[citation needed]
Exploratory surgery is performed to establish or aid a diagnosis.
Therapeutic surgery is performed to treat a previously diagnosed condition.
Curative surgery is a therapeutic procedure done to permanently remove a pathology.
Cosmetic surgery is done to subjectively improve the appearance of an otherwise normal structure.
Bariatric surgery is done to assist weight loss when dietary and pharmaceutical methods alone have failed.

By type of procedure:
Amputation involves removing an entire body part, usually a limb or digit; castration is the amputation of testes; circumcision is the removal of foreskin from the penis or clitoris (see female circumcision). Replantation involves reattaching a severed body part.
Resection is the removal of all or part of an internal organ and/or connective tissue. A segmental resection specifically removes an independent vascular region of an organ such as a hepatic segment, a bronchopulmonary segment or a renal lobe.[2] Excision is the resection of only part of an organ, tissue or other body part (e.g. skin) without discriminating specific vascular territories. Exenteration is the complete removal of all organs and soft tissue content (especially lymphoid tissues) within a body cavity.
Extirpation is the complete excision or surgical destruction of a body part.[3]
Ablation is destruction of tissue through the use of energy-transmitting devices such as electrocautery/fulguration, laser, focused ultrasound or freezing.
Repair involves the direct closure or restoration of an injured, mutilated or deformed organ or body part, usually by suturing or internal fixation. Reconstruction is an extensive repair of a complex body part (such as joints), often with some degrees of structural/functional replacement and commonly involves grafting and/or use of implants.
Grafting is the relocation and establishment of a tissue from one part of the body to another. A flap is the relocation of a tissue without complete separation of its original attachment, and a free flap is a completely detached flap that carries an intact neurovascular structure ready for grafting onto a new location.
Bypass involves the relocation/grafting of a tubular structure onto another in order to reroute the content flow of that target structure from a specific segment directly to a more distal ("downstream") segment.
Implantation is insertion of artificial medical devices to replace or augment existing tissue.
Transplantation is the replacement of an organ or body part by insertion of another from a different human (or animal) into the person undergoing surgery. Harvesting is the resection of an organ or body part from a live human or animal (known as the donor) for transplantation into another patient (known as the recipient).

Segmental resection (or segmentectomy) is a surgical procedure to remove part of an organ or gland, as a sub-type of a resection, which might involve removing the whole body part. It may also be used to remove a tumor and normal tissue around it. In lung cancer surgery, segmental resection refers to removing a section of a lobe of the lung. The resection margin is the edge of the removed tissue; it is important that this shows free of cancerous cells on examination by a pathologist.
$
9
Question 2: Which of the following surgeries is performed to subjectively improve the appearance of an otherwise normal structure?
A: Exploratory surgery
B: Therapeutic surgery
C: Curative surgery
D: Cosmetic surgery
E: Bariatric surgery

Answer: D

Question 3: What type of surgery is performed immediately to prevent death or serious disabilities?
A: Elective surgery
B: Semi-elective surgery
C: Emergency surgery
D: Therapeutic surgery
E: Exploratory surgery

Answer: C

Question 4: Which procedure involves the removal of a section of a lobe of the lung in the context of lung cancer surgery?
A: Amputation
B: Exenteration
C: Grafting
D: Implantation
E: Segmental resection

Answer: E

Question 5: When a severed body part is reattached, what is the surgical procedure called?
A: Resection
B: Excision
C: Extirpation
D: Replantation
E: Repair

Answer: D

Question 6: Which surgical procedure involves the complete removal of all organs and soft tissue content within a body cavity?
A: Resection
B: Exenteration
C: Ablation
D: Repair
E: Bypass

Answer: B

Question 7: In which type of surgery is tissue destroyed using energy-transmitting devices like lasers?
A: Grafting
B: Bypass
C: Implantation
D: Ablation
E: Transplantation

Answer: D

Question 8: What is the objective of a bypass surgery?
A: To completely remove a body part.
B: To reroute the content flow of a tubular structure from a specific segment to a more distal segment.
C: To restore an injured organ by suturing.
D: To destroy tissue through freezing.
E: To relocate a tissue without separating its original attachment.

Answer: B

Question 9: Which procedure involves the insertion of artificial medical devices to replace or augment existing tissue?
A: Transplantation
B: Harvesting
C: Bypass
D: Implantation
E: Grafting

Answer: D

Question 10: What is the purpose of exploratory surgery?
A: To treat a previously diagnosed condition.
B: To subjectively improve the appearance.
C: To assist weight loss.
D: To establish or aid a diagnosis.
E: To permanently remove a pathology.

Answer: D
@
Subject:
A mammary gland is an exocrine gland in humans and other mammals that produces milk to feed young offspring. Mammals get their name from the Latin word mamma, "breast". The mammary glands are arranged in organs such as the breasts in primates (for example, humans and chimpanzees), the udder in ruminants (for example, cows, goats, sheep, and deer), and the dugs of other animals (for example, dogs and cats). Lactorrhea, the occasional production of milk by the glands, can occur in any mammal, but in most mammals, lactation, the production of enough milk for nursing, occurs only in phenotypic females who have gestated in recent months or years. It is directed by hormonal guidance from sex steroids. In a few mammalian species, male lactation can occur. With humans, male lactation can occur only under specific circumstances.

Mammals are divided into 3 groups: prototherians, metatherians, and eutherians. In the case of prototherians, both males and females have functional mammary glands, but their mammary glands are without nipples. These mammary glands are modified sebaceous glands. Concerning metatherians and eutherians, only females have functional mammary glands. Their mammary glands can be termed as breasts or udders. In the case of breasts, each mammary gland has its own nipple (e.g., human mammary glands). In the case of udders, pairs of mammary glands comprise a single mass, with more than one nipple (or teat) hanging from it. For instance, cows and buffalo each have one udder with four teats, whereas sheep and goats each have two teats protruding from the udder. These mammary glands are modified sweat glands.

The basic components of a mature mammary gland are the alveoli (hollow cavities, a few millimeters large), which are lined with milk-secreting cuboidal cells and surrounded by myoepithelial cells. These alveoli join to form groups known as lobules. Each lobule has a lactiferous duct that drains into openings in the nipple. The myoepithelial cells contract under the stimulation of oxytocin, excreting the milk secreted by alveolar units into the lobule lumen toward the nipple. As the infant begins to suck, the oxytocin-mediated "let down reflex" ensues, and the mother's milk is secreted — not sucked — from the gland into the baby's mouth.[4]

All the milk-secreting tissue leading to a single lactiferous duct is collectively called a "simple mammary gland"; in a "complex mammary gland", all the simple mammary glands serve one nipple. Humans normally have two complex mammary glands, one in each breast, and each complex mammary gland consists of 10–20 simple glands. The opening of each simple gland on the surface of the nipple is called a "pore."[5] The presence of more than two nipples is known as polythelia and the presence of more than two complex mammary glands as polymastia.

Maintaining the correct polarized morphology of the lactiferous duct tree requires another essential component – mammary epithelial cells extracellular matrix (ECM) which, together with adipocytes, fibroblast, inflammatory cells, and others, constitute mammary stroma.[6] Mammary epithelial ECM mainly contains myoepithelial basement membrane and the connective tissue. They not only help to support mammary basic structure, but also serve as a communicating bridge between mammary epithelia and their local and global environment throughout this organ's development.

A mammary gland is a specific type of apocrine gland specialized for manufacture of colostrum when giving birth. Mammary glands can be identified as apocrine because they exhibit striking "decapitation" secretion. Many sources assert that mammary glands are modified sweat glands.[9][10][11] Some authors dispute that and argue instead that they are sebaceous glands.
$
9
Question 2: From which Latin word do mammals derive their name, and what does it mean?
A: Mammalia, "animal"
B: Mammo, "large"
C: Mammalium, "feeding"
D: Mamma, "breast"
E: Mammut, "mammoth"

Answer: D

Question 3: Which type of mammals have mammary glands without nipples?
A: Metatherians
B: Eutherians
C: Prototherians
D: Apocrine
E: Polythelia

Answer: C

Question 4: In which animals would you find an udder as the organ containing the mammary glands?
A: Humans and chimpanzees
B: Dogs and cats
C: Cows, goats, sheep, and deer
D: Prototherians
E: Primates

Answer: C

Question 5: What stimulates the myoepithelial cells to contract and excrete milk?
A: Colostrum
B: Adipocytes
C: Oxytocin
D: Mammary epithelial cells
E: Mammo

Answer: C

Question 6: Which term refers to the presence of more than two nipples?
A: Polymastia
B: Apocrine
C: Prototherian
D: Polythelia
E: Mammut

Answer: D

Question 7: How many complex mammary glands do humans typically have?
A: One
B: Two
C: Three
D: Four
E: Ten to twenty

Answer: B

Question 8: What is the primary component of mammary epithelial ECM?
A: Alveoli
B: Myoepithelial basement membrane and connective tissue
C: Lactiferous duct
D: Colostrum
E: Decapitation secretion

Answer: B

Question 9: Mammary glands can be identified as which type of gland due to their "decapitation" secretion?
A: Sebaceous
B: Apocrine
C: Sweat
D: Proocrine
E: Metatherian

Answer: B

Question 10: Which cell type, among the following, is NOT a key part of mammary stroma?
A: Adipocytes
B: Fibroblast
C: Myoepithelial cells
D: Inflammatory cells
E: Lobules

Answer: E
@
Subject:
Astrochemistry is the study of the abundance and reactions of molecules in the universe, and their interaction with radiation.[1] The discipline is an overlap of astronomy and chemistry. The word "astrochemistry" may be applied to both the Solar System and the interstellar medium. The study of the abundance of elements and isotope ratios in Solar System objects, such as meteorites, is also called cosmochemistry, while the study of interstellar atoms and molecules and their interaction with radiation is sometimes called molecular astrophysics. The formation, atomic and chemical composition, evolution and fate of molecular gas clouds is of special interest, because it is from these clouds that solar systems form.

Cosmochemistry (from Ancient Greek κόσμος (kósmos) 'universe', and χημεία (khēmeía) 'chemistry') or chemical cosmology is the study of the chemical composition of matter in the universe and the processes that led to those compositions.[1] This is done primarily through the study of the chemical composition of meteorites and other physical samples. Given that the asteroid parent bodies of meteorites were some of the first solid material to condense from the early solar nebula, cosmochemists are generally, but not exclusively, concerned with the objects contained within the Solar System.
$
9
Question 2: What is astrochemistry primarily concerned with?
A: The reactions of planets and stars.
B: The abundance and reactions of molecules in the universe and their interaction with radiation.
C: The isotopic ratios in Solar System objects.
D: The chemical reactions in the Earth's atmosphere.
E: The study of meteorites only.

Answer: B

Question 3: Which term refers to the study of the abundance of elements and isotope ratios in objects like meteorites within the Solar System?
A: Molecular astrophysics
B: Astrophysics
C: Astrobiology
D: Cosmochemistry
E: Astrogeology

Answer: D

Question 4: From what do solar systems primarily form?
A: Meteorites
B: Interstellar radiation
C: Molecular gas clouds
D: Solar nebula
E: Interstellar ices

Answer: C

Question 5: Which field focuses on the chemical composition of matter in the universe and the processes leading to those compositions?
A: Astrophysics
B: Cosmology
C: Astrogeology
D: Cosmochemistry
E: Astronomy

Answer: D

Question 6: What do the asteroid parent bodies of meteorites signify?
A: They are the remnants of ancient stars.
B: They are the youngest materials in the Solar System.
C: They were some of the first solid material to condense from the early solar nebula.
D: They are the primary source of interstellar radiation.
E: They were formed outside the Solar System.

Answer: C

Question 7: What is the primary overlap that constitutes astrochemistry?
A: Physics and biology
B: Cosmology and biology
C: Astronomy and chemistry
D: Geology and astronomy
E: Physics and chemistry

Answer: C

Question 8: What is the main difference between cosmochemistry and molecular astrophysics?
A: Cosmochemistry focuses on interstellar atoms and molecules, while molecular astrophysics is concerned with the Solar System's objects.
B: Cosmochemistry is related to the study of meteorites and objects within the Solar System, while molecular astrophysics deals with interstellar atoms and molecules.
C: Cosmochemistry and molecular astrophysics are essentially the same fields.
D: Molecular astrophysics is a subset of cosmochemistry.
E: Cosmochemistry focuses only on comets, while molecular astrophysics deals with asteroids.

Answer: B

Question 9: Which term is derived from the Ancient Greek words meaning "universe" and "chemistry"?
A: Astrophysics
B: Astrogeology
C: Cosmology
D: Cosmochemistry
E: Astrochemistry

Answer: D

Question 10: Why are cosmochemists generally concerned with objects within the Solar System?
A: Because they are the oldest objects in the universe.
B: Because the asteroid parent bodies of meteorites were among the earliest solid material to form.
C: Because these objects contain the only life forms in the universe.
D: Because these objects have the most diverse chemical compositions.
E: Because the Solar System is the center of the universe.

Answer: B
@
Subject:
The rare-earth elements (REE), also called the rare-earth metals or rare earths or, in context, rare-earth oxides, and sometimes the lanthanides (although yttrium and scandium, which do not belong to this series, are usually included as rare earths),[1] are a set of 17 nearly indistinguishable lustrous silvery-white soft heavy metals. Compounds containing rare earths have diverse applications in electrical and electronic components, lasers, glass, magnetic materials, and industrial processes.

Scandium and yttrium are considered rare-earth elements because they tend to occur in the same ore deposits as the lanthanides and exhibit similar chemical properties, but have different electronic and magnetic properties.[2][3] The term 'rare-earth' is a misnomer because they are not actually scarce, although historically it took a long time to isolate these elements.[4][5]

These metals tarnish slowly in air at room temperature and react slowly with cold water to form hydroxides, liberating hydrogen. They react with steam to form oxides and ignite spontaneously at a temperature of 400 °C (752 °F). These elements and their compounds have no biological function other than in several specialized enzymes, such as in lanthanide-dependent methanol dehydrogenases in bacteria.[6] The water-soluble compounds are mildly to moderately toxic, but the insoluble ones are not.[7] All isotopes of promethium are radioactive, and it does not occur naturally in the earth's crust, except for a trace amount generated by spontaneous fission of uranium-238. They are often found in minerals with thorium, and less commonly uranium.

Though rare-earth elements are technically relatively plentiful in the entire Earth's crust (cerium being the 25th-most-abundant element at 68 parts per million, more abundant than copper), in practice this is spread thin across trace impurities, so to obtain rare earths at usable purity requires processing enormous amounts of raw ore at great expense, thus the name "rare" earths.

Because of their geochemical properties, rare-earth elements are typically dispersed and not often found concentrated in rare-earth minerals. Consequently, economically exploitable ore deposits are sparse.[8] The first rare-earth mineral discovered (1787) was gadolinite, a black mineral composed of cerium, yttrium, iron, silicon, and other elements. This mineral was extracted from a mine in the village of Ytterby in Sweden; four of the rare-earth elements bear names derived from this single location.

Until 1948, most of the world's rare earths were sourced from placer sand deposits in India and Brazil. Through the 1950s, South Africa was the world's rare earth source, from a monazite-rich reef at the Steenkampskraal mine in Western Cape province.[39] Through the 1960s until the 1980s, the Mountain Pass rare earth mine in California made the United States the leading producer. Today, the Indian and South African deposits still produce some rare-earth concentrates, but they were dwarfed by the scale of Chinese production. In 2017, China produced 81% of the world's rare-earth supply, mostly in Inner Mongolia,[8][40] although it had only 36.7% of reserves. Australia was the second and only other major producer with 15% of world production.[41] All of the world's heavy rare earths (such as dysprosium) come from Chinese rare-earth sources such as the polymetallic Bayan Obo deposit.[40][42] The Browns Range mine, located 160 km south east of Halls Creek in northern Western Australia, is currently under development and is positioned to become the first significant dysprosium producer outside of China.[43]

Increased demand has strained supply, and there is growing concern that the world may soon face a shortage of the rare earths.[44] In several years from 2009 worldwide demand for rare-earth elements is expected to exceed supply by 40,000 tonnes annually unless major new sources are developed.[45] In 2013, it was stated that the demand for REEs would increase due to the dependence of the EU on these elements, the fact that rare-earth elements cannot be substituted by other elements and that REEs have a low recycling rate. Furthermore, due to the increased demand and low supply, future prices are expected to increase and there is a chance that countries other than China will open REE mines.[46] REE is increasing in demand due to the fact that they are essential for new and innovative technology that is being created. These new products that need REEs to be produced are high-technology equipment such as smart phones, digital cameras, computer parts, semiconductors, etc. In addition, these elements are more prevalent in the following industries: renewable energy technology, military equipment, glass making, and metallurgy.[47]
$
9
Question 2: Which elements are typically included as rare-earth elements even though they do not belong to the lanthanide series?
A: Yttrium and plutonium
B: Scandium and neptunium
C: Scandium and yttrium
D: Cerium and yttrium
E: Thorium and scandium

Answer: C

Question 3: Why was the term 'rare-earth' originally used for these elements?
A: Because they were believed to be rare in the Earth's crust.
B: Because they are often found in minerals with thorium.
C: Because they react slowly with cold water.
D: Due to the high cost and difficulty of isolating these elements historically.
E: Because they are only found in specific regions of the Earth.

Answer: D

Question 4: At what temperature do rare-earth metals ignite spontaneously?
A: 300 °C (572 °F)
B: 350 °C (662 °F)
C: 400 °C (752 °F)
D: 450 °C (842 °F)
E: 500 °C (932 °F)

Answer: C

Question 5: Where was the first rare-earth mineral, gadolinite, discovered?
A: In the village of Gadolite in Brazil.
B: At the Mountain Pass mine in California.
C: In a reef at the Steenkampskraal mine in South Africa.
D: In the village of Ytterby in Sweden.
E: In the Browns Range mine in Australia.

Answer: D

Question 6: Which country was the leading producer of rare-earth elements in 2017?
A: United States
B: India
C: South Africa
D: Australia
E: China

Answer: E

Question 7: Why is there a growing concern about the supply of rare-earth elements?
A: Due to decreased demand for rare-earth elements in technology.
B: Because demand for rare-earth elements is expected to fall below supply by 40,000 tonnes annually.
C: Because many countries have banned the mining of rare-earth elements.
D: Due to increased demand and limited supply, potentially causing a shortage.
E: Because rare-earth elements are being substituted by other more abundant elements.

Answer: D

Question 8: Which mine is positioned to become the first significant dysprosium producer outside of China?
A: Gadolinite Mine
B: Mountain Pass mine
C: Steenkampskraal mine
D: Browns Range mine
E: Bayan Obo deposit

Answer: D

Question 9: Which of the following industries is NOT mentioned as being more prevalent in the use of rare-earth elements?
A: Glass making
B: Renewable energy technology
C: Textile industry
D: Military equipment
E: Metallurgy

Answer: C

Question 10: What makes obtaining usable purity rare earths expensive?
A: They only exist in specific regions of the Earth.
B: The extraction process is highly complicated.
C: They are spread thin across trace impurities, requiring processing vast amounts of raw ore.
D: They are mixed with other more expensive metals.
E: There are only a few mines that can produce these elements.

Answer: C
@
Subject:
Radiometric dating, radioactive dating or radioisotope dating is a technique which is used to date materials such as rocks or carbon, in which trace radioactive impurities were selectively incorporated when they were formed. The method compares the abundance of a naturally occurring radioactive isotope within the material to the abundance of its decay products, which form at a known constant rate of decay.[1] The use of radiometric dating was first published in 1907 by Bertram Boltwood[2] and is now the principal source of information about the absolute age of rocks and other geological features, including the age of fossilized life forms or the age of Earth itself, and can also be used to date a wide range of natural and man-made materials.

Together with stratigraphic principles, radiometric dating methods are used in geochronology to establish the geologic time scale.[3] Among the best-known techniques are radiocarbon dating, potassium–argon dating and uranium–lead dating. By allowing the establishment of geological timescales, it provides a significant source of information about the ages of fossils and the deduced rates of evolutionary change. Radiometric dating is also used to date archaeological materials, including ancient artifacts.

Different methods of radiometric dating vary in the timescale over which they are accurate and the materials to which they can be applied.

All ordinary matter is made up of combinations of chemical elements, each with its own atomic number, indicating the number of protons in the atomic nucleus. Additionally, elements may exist in different isotopes, with each isotope of an element differing in the number of neutrons in the nucleus. A particular isotope of a particular element is called a nuclide. Some nuclides are inherently unstable. That is, at some point in time, an atom of such a nuclide will undergo radioactive decay and spontaneously transform into a different nuclide. This transformation may be accomplished in a number of different ways, including alpha decay (emission of alpha particles) and beta decay (electron emission, positron emission, or electron capture). Another possibility is spontaneous fission into two or more nuclides.[citation needed]

While the moment in time at which a particular nucleus decays is unpredictable, a collection of atoms of a radioactive nuclide decays exponentially at a rate described by a parameter known as the half-life, usually given in units of years when discussing dating techniques. After one half-life has elapsed, one half of the atoms of the nuclide in question will have decayed into a "daughter" nuclide or decay product. In many cases, the daughter nuclide itself is radioactive, resulting in a decay chain, eventually ending with the formation of a stable (nonradioactive) daughter nuclide; each step in such a chain is characterized by a distinct half-life. In these cases, usually the half-life of interest in radiometric dating is the longest one in the chain, which is the rate-limiting factor in the ultimate transformation of the radioactive nuclide into its stable daughter. Isotopic systems that have been exploited for radiometric dating have half-lives ranging from only about 10 years (e.g., tritium) to over 100 billion years (e.g., samarium-147).[4]

For most radioactive nuclides, the half-life depends solely on nuclear properties and is essentially constant.[5] This is known because decay constants measured by different techniques give consistent values within analytical errors and the ages of the same materials are consistent from one method to another. It is not affected by external factors such as temperature, pressure, chemical environment, or presence of a magnetic or electric field.[6][7][8] The only exceptions are nuclides that decay by the process of electron capture, such as beryllium-7, strontium-85, and zirconium-89, whose decay rate may be affected by local electron density. For all other nuclides, the proportion of the original nuclide to its decay products changes in a predictable way as the original nuclide decays over time.[citation needed] This predictability allows the relative abundances of related nuclides to be used as a clock to measure the time from the incorporation of the original nuclides into a material to the present.
$
7
Which scientist first published the use of radiometric dating?
A: Albert Einstein
B: Isaac Newton
C: Bertram Boltwood
D: Charles Darwin
E: Marie Curie
Answer: C

What does radiometric dating compare to determine the age of materials?
A: The abundance of a naturally occurring radioactive isotope with the abundance of its decay products.
B: The weight of the material with its density.
C: The size of the rock with its location.
D: The temperature of the rock with its state of matter.
E: The color of the material with its chemical composition.
Answer: A

Which of the following is NOT a method of radiometric dating mentioned?
A: Radiocarbon dating
B: Potassium–argon dating
C: Magnetic field dating
D: Uranium–lead dating
E: Tritium dating
Answer: C

A nuclide that is inherently unstable and will, at some point, undergo radioactive decay is known as a:
A: Stable nuclide
B: Daughter nuclide
C: Radioactive nuclide
D: Parent nuclide
E: Half-life nuclide
Answer: C

The half-life of a radioactive nuclide is defined as the time taken for:
A: All of its atoms to decay.
B: One-fourth of its atoms to decay.
C: Half of its atoms to decay.
D: Three-fourths of its atoms to decay.
E: None of its atoms to decay.
Answer: C

Which factor DOES NOT affect the half-life of most radioactive nuclides?
A: Temperature
B: Pressure
C: Chemical environment
D: Local electron density for certain nuclides
E: Presence of a magnetic field
Answer: D (This is a tricky question. While the half-life of most nuclides is not affected by factors such as temperature, pressure, and chemical environment, there are exceptions such as beryllium-7, strontium-85, and zirconium-89, whose decay rate may be affected by local electron density.)

Which term best describes the transformation process in which an unstable nuclide changes into a different nuclide?
A: Fusion
B: Radioactive decay
C: Combustion
D: Evaporation
E: Sublimation
Answer: B
@
Subject:
The Fischer–Tropsch process is a collection of chemical reactions that converts a mixture of carbon monoxide and hydrogen, known as syngas, into liquid hydrocarbons. These reactions occur in the presence of metal catalysts, typically at temperatures of 150–300 °C (302–572 °F) and pressures of one to several tens of atmospheres. The Fischer–Tropsch process is an important reaction in both coal liquefaction and gas to liquids technology for producing liquid hydrocarbons.[1]

In the usual implementation, carbon monoxide and hydrogen, the feedstocks for FT, are produced from coal, natural gas, or biomass in a process known as gasification. The process then converts these gases into synthetic lubrication oil and synthetic fuel.[2] This process has received intermittent attention as a source of low-sulfur diesel fuel and to address the supply or cost of petroleum-derived hydrocarbons. Fischer-Tropsch process is discussed as a step of producing carbon-neutral liquid hydrocarbon fuels from CO2 and hydrogen.[3][4]

The process was first developed by Franz Fischer and Hans Tropsch at the Kaiser Wilhelm Institute for Coal Research in Mülheim an der Ruhr, Germany, in 1925.[5]

Reaction mechanism
The Fischer–Tropsch process involves a series of chemical reactions that produce a variety of hydrocarbons, ideally having the formula (CnH2n+2). The more useful reactions produce alkanes as follows:[6]

(2n + 1) H2 + n CO → CnH2n+2 + n H2O
where n is typically 10–20. The formation of methane (n = 1) is unwanted. Most of the alkanes produced tend to be straight-chain, suitable as diesel fuel. In addition to alkane formation, competing reactions give small amounts of alkenes, as well as alcohols and other oxygenated hydrocarbons.[7]

The reaction is a highly exothermic reaction due to a standard reaction enthalpy (ΔH) of −165 kJ/mol CO combined.[8]

Methylidyne­tricobalt­nonacarbonyl is a molecule that illustrates the kind of reduced carbon species speculated to occur in the Fischer–Tropsch process.

Fischer–Tropsch intermediates and elemental reactions
Converting a mixture of H2 and CO into aliphatic products is a multi-step reaction with several intermediate compounds. The growth of the hydrocarbon chain may be visualized as involving a repeated sequence in which hydrogen atoms are added to carbon and oxygen, the C–O bond is split and a new C–C bond is formed. For one –CH2– group produced by CO + 2 H2 → (CH2) + H2O, several reactions are necessary:

Associative adsorption of CO
Splitting of the C–O bond
Dissociative adsorption of 2 H2
Transfer of 2 H to the oxygen to yield H2O
Desorption of H2O
Transfer of 2 H to the carbon to yield CH2
The conversion of CO to alkanes involves hydrogenation of CO, the hydrogenolysis (cleavage with H2) of C–O bonds, and the formation of C–C bonds. Such reactions are assumed to proceed via initial formation of surface-bound metal carbonyls. The CO ligand is speculated to undergo dissociation, possibly into oxide and carbide ligands.[9] Other potential intermediates are various C1 fragments including formyl (CHO), hydroxycarbene (HCOH), hydroxymethyl (CH2OH), methyl (CH3), methylene (CH2), methylidyne (CH), and hydroxymethylidyne (COH). Furthermore, and critical to the production of liquid fuels, are reactions that form C–C bonds, such as migratory insertion. Many related stoichiometric reactions have been simulated on discrete metal clusters, but homogeneous Fischer–Tropsch catalysts are of no commercial importance.

Addition of isotopically labelled alcohol to the feed stream results in incorporation of alcohols into product. This observation establishes the facility of C–O bond scission. Using 14C-labelled ethylene and propene over cobalt catalysts results in incorporation of these olefins into the growing chain. Chain growth reaction thus appears to involve both ‘olefin insertion’ as well as ‘CO-insertion’.[10]

8 CO + 17 H2 → C8H18 + 8 H2O
$
7
Which gases form the mixture known as syngas?
A: Carbon dioxide and hydrogen
B: Carbon monoxide and helium
C: Carbon monoxide and hydrogen
D: Oxygen and hydrogen
E: Nitrogen and carbon monoxide
Answer: C

Which of the following statements describes the Fischer–Tropsch process?
A: A process of converting solid coal into liquid hydrocarbons.
B: A method to convert carbon monoxide and hydrogen into liquid hydrocarbons using catalysts.
C: A mechanism for producing hydrogen gas from water and carbon monoxide.
D: A procedure to extract natural gas from deep under the earth's crust.
E: A technique for splitting water molecules into hydrogen and oxygen.
Answer: B

Where was the Fischer–Tropsch process first developed?
A: University of Berlin, Germany
B: Max Planck Institute, Germany
C: Kaiser Wilhelm Institute for Coal Research, Germany
D: Oxford University, UK
E: Massachusetts Institute of Technology, USA
Answer: C

Which chemical reaction correctly represents the conversion of carbon monoxide and hydrogen into hydrocarbons using the Fischer–Tropsch process?
A: n CO + (2n) H2 → CnH2n + n H2O
B: n CO + n H2 → CnH2n + n H2O
C: (2n) CO + n H2 → CnH2n + n H2O
D: (2n + 1) CO + n H2 → CnH2n+2 + n H2O
E: (2n + 1) H2 + n CO → CnH2n+2 + n H2O
Answer: E

Which molecule is speculated to occur in the Fischer–Tropsch process as a kind of reduced carbon species?
A: Methane
B: Hydroxycarbene
C: Methylidyne­tricobalt­nonacarbonyl
D: Carbon dioxide
E: Oxygenated hydrocarbon
Answer: C

The Fischer–Tropsch process has been considered as a way to produce which type of fuel?
A: High-sulfur gasoline
B: Natural gas
C: Low-sulfur diesel fuel
D: Alcohol-based fuels
E: High-octane aviation fuel
Answer: C

What happens during the Fischer–Tropsch process?
A: Carbon monoxide and hydrogen are combined to form hydrocarbons and water.
B: Carbon monoxide and hydrogen are split to produce oxygen and nitrogen.
C: Hydrogen and oxygen combine to form water.
D: Carbon monoxide is split to form carbon and oxygen.
E: Hydrocarbons are broken down to produce carbon monoxide and hydrogen.
Answer: A
@
Subject:
 phageome is a community of bacteriophages and their metagenomes localized in a particular environment, similar to a microbiome.[1][2] The term was first used in an article by Modi et al in 2013[3] and has continued to be used in scientific articles that relate to bacteriophages and their metagenomes. A bacteriophage, or phage for short, is a virus that has the ability to infect bacteria and archaea, and can replicate inside of them. Phageome is a subcategory of virome, which is all of the viruses that are associated with a host or environment.[4] Phages make up the majority of most viromes and are currently understood as being the most abundant organism.[5] Oftentimes scientists will look only at a phageome instead of a virome while conducting research.

In humans
See also: Human virome
Although bacteriophages do not have the capability to infect human cells, they are found in abundance in the human virome.[6]

The human gut phageome has recently become a topic of interest in the scientific community. The makeup of the gut phageome can be responsible for different gut related diseases such as IBD. The composition of phages that make up a healthy human gut phageome is currently debated, since different methods of research can lead to different results.[7]
$
7
Who first used the term "phageome"?
A: Human Genome Project researchers in 2000.
B: Watson and Crick in 1953.
C: Modi et al in 2013.
D: Carl Woese in 1977.
E: Richard Dawkins in 1982.
Answer: C

What does a bacteriophage infect?
A: Only human cells.
B: Only plant cells.
C: Bacteria and archaea.
D: Fungi and protozoa.
E: Only viruses.
Answer: C

Phageome is a subset of which larger category?
A: Genome.
B: Transcriptome.
C: Proteome.
D: Virome.
E: Microbiome.
Answer: D

Why do scientists sometimes focus on phageome instead of virome in research?
A: Because phages are less common than other viruses.
B: Because phages are easier to study than other organisms.
C: Because phages make up the majority of most viromes.
D: Because phages are the only type of viruses that infect humans.
E: Because phageomes do not include any significant information.
Answer: C

Which statement is true regarding bacteriophages in the human virome?
A: They can infect human cells.
B: They are found in very low numbers in the human virome.
C: They are found in abundance in the human virome.
D: They are responsible for all human diseases.
E: They are synthetic viruses created in labs.
Answer: C

What is a topic of interest related to the human gut phageome?
A: Its relationship to brain function.
B: Its role in determining eye color.
C: Its contribution to gut-related diseases like IBD.
D: Its role in determining blood type.
E: Its link to height and physical stature.
Answer: C

Why is the composition of a healthy human gut phageome currently debated?
A: Because it is considered insignificant in research.
B: Because different methods of research can lead to different results.
C: Because the human gut phageome remains static throughout a person's life.
D: Because all scientists have reached a unanimous decision on its composition.
E: Because there are no known methods to study the human gut phageome.
Answer: B
@
Subject:
Organography (from Greek όργανο, organo, "organ"; and -γραφή, -graphy) is the scientific description of the structure and function of the organs of living things.

History
Organography as a scientific study starts with Aristotle, who considered the parts of plants as "organs" and began to consider the relationship between different organs and different functions. In the 17th century Joachim Jung,[1] clearly articulated that plants are composed of different organ types such as root, stem and leaf, and he went on to define these organ types on the basis of form and position.

In the following century Caspar Friedrich Wolff [2] was able to follow the development of organs from the "growing points" or apical meristems. He noted the commonality of development between foliage leaves and floral leaves (e.g. petals) and wrote: "In the whole plant, whose parts we wonder at as being, at the first glance, so extraordinarily diverse, I finally perceive and recognize nothing beyond leaves and stem (for the root may be regarded as a stem). Consequently all parts of the plant, except the stem, are modified leaves."

Similar views were propounded at by Goethe in his well-known treatise.[3] He wrote: "The underlying relationship between the various external parts of the plant, such as the leaves, the calyx, the corolla, the stamens, which develop one after the other and, as it were, out of one another, has long been generally recognized by investigators, and has in fact been specially studied; and the operation by which one and the same organ presents itself to us in various forms has been termed Metamorphosis of Plants."
$
7
Who initiated organography as a scientific study by considering parts of plants as "organs"?
A: Goethe
B: Joachim Jung
C: Aristotle
D: Caspar Friedrich Wolff
E: Isaac Newton
Answer: C

In the 17th century, who articulated that plants have distinct organ types such as root, stem, and leaf?
A: Caspar Friedrich Wolff
B: Aristotle
C: Goethe
D: Joachim Jung
E: Charles Darwin
Answer: D

Who observed the common development between foliage leaves and floral leaves and emphasized that most parts of the plant are modified leaves?
A: Aristotle
B: Goethe
C: Joachim Jung
D: Isaac Newton
E: Caspar Friedrich Wolff
Answer: E

The concept that one organ can present in various forms in plants is called:
A: Organ transformation
B: Organ change
C: Metamorphosis of Plants
D: Organ adaptation
E: Organ evolution
Answer: C

Which of the following organs is regarded as a stem, according to Caspar Friedrich Wolff?
A: Foliage leaves
B: Floral leaves
C: Root
D: Petals
E: Corolla
Answer: C

What was the focus of Goethe's treatise regarding plants?
A: The distinctiveness of each organ in a plant
B: The economic importance of plants
C: The underlying relationship and metamorphosis among various parts of a plant
D: The medicinal properties of plants
E: The genetic modifications of plants
Answer: C

Organography primarily deals with which aspect of biology?
A: Genetic relationships
B: Evolutionary trends
C: Physiological mechanisms
D: Structural and functional description
E: Ecological adaptations
Answer: D
@
Subject:
Anatomy (from Ancient Greek ἀνατομή (anatomḗ) 'dissection') is the branch of biology concerned with the study of the structure of organisms and their parts.[1] Anatomy is a branch of natural science that deals with the structural organization of living things. It is an old science, having its beginnings in prehistoric times.[2] Anatomy is inherently tied to developmental biology, embryology, comparative anatomy, evolutionary biology, and phylogeny,[3] as these are the processes by which anatomy is generated, both over immediate and long-term timescales. Anatomy and physiology, which study the structure and function of organisms and their parts respectively, make a natural pair of related disciplines, and are often studied together. Human anatomy is one of the essential basic sciences that are applied in medicine.[4]

Anatomy is a complex and dynamic field that is constantly evolving as new discoveries are made. In recent years, there has been a significant increase in the use of advanced imaging techniques, such as MRI and CT scans, which allow for more detailed and accurate visualizations of the body's structures.

The discipline of anatomy is divided into macroscopic and microscopic parts. Macroscopic anatomy, or gross anatomy, is the examination of an animal's body parts using unaided eyesight. Gross anatomy also includes the branch of superficial anatomy. Microscopic anatomy involves the use of optical instruments in the study of the tissues of various structures, known as histology, and also in the study of cells.

The history of anatomy is characterized by a progressive understanding of the functions of the organs and structures of the human body. Methods have also improved dramatically, advancing from the examination of animals by dissection of carcasses and cadavers (corpses)[5] to 20th-century medical imaging techniques, including X-ray, ultrasound, and magnetic resonance imaging.[6]
$
Which pair of related disciplines often gets studied together?
A: Anatomy and Botany
B: Anatomy and Geology
C: Anatomy and Physiology
D: Anatomy and Zoology
E: Anatomy and Chemistry
Answer: C

Which imaging techniques have seen a significant increase in their use for a detailed visualization of the body's structures in recent years?
A: Ultrasound and X-ray
B: CT scans and MRI
C: Endoscopy and Biopsy
D: Thermography and Polarimetry
E: Positron Emission Tomography and Scintigraphy
Answer: B

What does microscopic anatomy primarily focus on?
A: Study of whole organs using unaided eyesight
B: Study of body parts using basic magnifying glasses
C: Study of tissues using optical instruments
D: Study of the external body using imaging techniques
E: Study of the body's systems using tactile senses
Answer: C

What does macroscopic (or gross) anatomy involve?
A: Examination of an animal's genetic composition
B: Examination of an animal's body parts using unaided eyesight
C: Examination of an animal's cellular structures under a microscope
D: Examination of an animal's physiological responses
E: Examination of an animal's ecological interactions
Answer: B

Which branch of anatomy involves the examination of surface or external features?
A: Comparative anatomy
B: Developmental anatomy
C: Superficial anatomy
D: Phylogenetic anatomy
E: Embryonic anatomy
Answer: C

Anatomy has its roots dating back to:
A: Modern times with the invention of imaging techniques
B: Renaissance period with the studies of Leonardo da Vinci
C: Prehistoric times
D: The 19th century with the advent of cellular studies
E: The 20th century with the advent of genetics
Answer: C

What advanced significantly from dissection of carcasses and cadavers to modern-day imaging techniques in the history of anatomy?
A: Ethical considerations
B: Preservation techniques
C: Methods of study and examination
D: Surgical techniques
E: Genetic understanding
Answer: C
@
Subject:
The trophic level of an organism is the position it occupies in a food web. A food chain is a succession of organisms that eat other organisms and may, in turn, be eaten themselves. The trophic level of an organism is the number of steps it is from the start of the chain. A food web starts at trophic level 1 with primary producers such as plants, can move to herbivores at level 2, carnivores at level 3 or higher, and typically finish with apex predators at level 4 or 5. The path along the chain can form either a one-way flow or a food "web". Ecological communities with higher biodiversity form more complex trophic paths.

The word trophic derives from the Greek τροφή (trophē) referring to food or nourishment.[1]

The three basic ways in which organisms get food are as producers, consumers, and decomposers.

Producers (autotrophs) are typically plants or algae. Plants and algae do not usually eat other organisms, but pull nutrients from the soil or the ocean and manufacture their own food using photosynthesis. For this reason, they are called primary producers. In this way, it is energy from the sun that usually powers the base of the food chain.[4] An exception occurs in deep-sea hydrothermal ecosystems, where there is no sunlight. Here primary producers manufacture food through a process called chemosynthesis.[5]
Consumers (heterotrophs) are species that cannot manufacture their own food and need to consume other organisms. Animals that eat primary producers (like plants) are called herbivores. Animals that eat other animals are called carnivores, and animals that eat both plants and other animals are called omnivores.
Decomposers (detritivores) break down dead plant and animal material and wastes and release it again as energy and nutrients into the ecosystem for recycling. Decomposers, such as bacteria and fungi (mushrooms), feed on waste and dead matter, converting it into inorganic chemicals that can be recycled as mineral nutrients for plants to use again.
Trophic levels can be represented by numbers, starting at level 1 with plants. Further trophic levels are numbered subsequently according to how far the organism is along the food chain.

Level 1
Plants and algae make their own food and are called producers.
Level 2
Herbivores eat plants and are called primary consumers.
Level 3
Carnivores that eat herbivores are called secondary consumers.
Level 4
Carnivores that eat other carnivores are called tertiary consumers.
Apex predator
By definition, healthy adult apex predators have no predators (with members of their own species a possible exception) and are at the highest numbered level of their food web.
$
8
Which organisms are typically found at trophic level 1 in a food web?
A: Carnivores
B: Omnivores
C: Herbivores
D: Producers
E: Decomposers
Answer: D

Which of the following organisms primarily rely on chemosynthesis to manufacture their own food?
A: Grasses in a meadow
B: Trees in a rainforest
C: Algae in the sunlight zone of the ocean
D: Bacteria in deep-sea hydrothermal vents
E: Carnivorous animals in the tundra
Answer: D

What is the primary role of decomposers in an ecosystem?
A: To consume herbivores
B: To produce their own food using sunlight
C: To eat plants and algae
D: To break down dead matter and waste, recycling nutrients back into the ecosystem
E: To be apex predators in food webs
Answer: D

Animals that eat both plants and other animals are termed as:
A: Herbivores
B: Producers
C: Carnivores
D: Decomposers
E: Omnivores
Answer: E

In a food web, where would you typically find tertiary consumers?
A: Trophic level 1
B: Trophic level 2
C: Trophic level 3
D: Trophic level 4
E: Trophic level 5
Answer: D

Apex predators are characterized by which of the following traits?
A: They are always at trophic level 2
B: They primarily consume producers
C: They are typically eaten by many other species
D: They have no predators, and are at the highest trophic level in their ecosystem
E: They are essential primary consumers in their ecosystem
Answer: D

Which term best describes organisms that cannot manufacture their own food and need to consume other organisms?
A: Producers
B: Autotrophs
C: Consumers
D: Phototrophs
E: Primary producers
Answer: C

What is the primary source of energy that usually powers the base of the food chain?
A: Wind energy
B: Chemical energy
C: Mechanical energy
D: Solar energy
E: Kinetic energy
Answer: D
@
Subject:
In chemistry, a crossover experiment is a method used to study the mechanism of a chemical reaction. In a crossover experiment, two similar but distinguishable reactants simultaneously undergo a reaction as part of the same reaction mixture. The products formed will either correspond directly to one of the two reactants (non-crossover products) or will include components of both reactants (crossover products). The aim of a crossover experiment is to determine whether or not a reaction process involves a stage where the components of each reactant have an opportunity to exchange with each other.

The results of crossover experiments are often straightforward to analyze, making them one of the most useful and most frequently applied methods of mechanistic study. In organic chemistry, crossover experiments are most often used to distinguish between intramolecular and intermolecular reactions.[1][2][3] Inorganic and organometallic chemists rely heavily on crossover experiments, and in particular isotopic labeling experiments, for support or contradiction of proposed mechanisms.[4] When the mechanism being investigated is more complicated than an intra- or intermolecular substitution or rearrangement, crossover experiment design can itself become a challenging question.[5] A well-designed crossover experiment can lead to conclusions about a mechanism that would otherwise be impossible to make. Many mechanistic studies include both crossover experiments and measurements of rate and kinetic isotope effects.

Purpose
Crossover experiments allow for experimental study of a reaction mechanism. Mechanistic studies are of interest to theoretical and experimental chemists for a variety of reasons including prediction of stereochemical outcomes, optimization of reaction conditions for rate and selectivity, and design of improved catalysts for better turnover number, robustness, etc.[6][7] Since a mechanism cannot be directly observed or determined solely based on the reactants or products, mechanisms are challenging to study experimentally. Only a handful of experimental methods are capable of providing information about the mechanism of a reaction, including crossover experiments, studies of the kinetic isotope effect, and rate variations by substituent. The crossover experiment has the advantage of being conceptually straightforward and relatively easy to design, carry out, and interpret. In modern mechanistic studies, crossover experiments and KIE studies are commonly used in conjunction with computational methods.[8]

Theory
The concept underlying the crossover experiment is a basic one: provided that the labeling method chosen does not affect the way a reaction proceeds, a shift in the labeling as observed in the products can be attributed to the reaction mechanism. The most important limitation in crossover experiment design is therefore that the labeling not affect the reaction mechanism itself.[1]

It can be difficult to know whether or not the changes made to reactants for a crossover experiment will affect the mechanism by which the reaction proceeds. This is particularly true since the aim of the crossover experiment is to provide insight into the mechanism that would allow these types of predictions. There is always the possibility that a label will alter the course of the reaction.[1]

In practice, crossover experiments aim to use the least change possible between the usual conditions of the reaction being studied and the conditions of the crossover experiment. This principle favors isotopic labeling, since changing the isotope of one atom in a molecule is the smallest change that can be both easily enacted and traced in the reaction. If the isotope is placed in the molecule at a position directly involved in the mechanism of the reaction, a kinetic isotope effect is expected. This can be used to study aspects of the mechanism independently or alongside a crossover experiment.[1][2][8] The kinetic isotope effect is a change in the rate of reaction based on the change in isotope, not a change in the mechanism of the reaction itself, so isotopic labeling generally satisfies the requirements for a valid crossover experiment. In crossover experiments that do not use isotopic labeling, addition or subtraction of a methyl substituent at a position not involved in any proposed mechanism for the reaction is typically expected to give a valid crossover experiment.[1]
$
8
What is the primary goal of a crossover experiment in chemistry?
A: To combine two reactants and analyze the resulting mixture.
B: To create a new substance by mixing two similar chemicals.
C: To determine if components of each reactant can exchange with each other during a reaction.
D: To observe the kinetic isotope effect in different reactions.
E: To change the isotope of an atom in a molecule.
Answer: C

Why are crossover experiments often considered straightforward to analyze?
A: Because the reactants are easily distinguishable.
B: Because they always involve isotopic labeling.
C: Because the products either correspond directly to a reactant or include components of both reactants.
D: Because they always result in a new substance.
E: Because the reactions involved are always intermolecular.
Answer: C

What could be a potential limitation when designing a crossover experiment?
A: That all reactants will yield the same product.
B: That the labeling method might change the reaction mechanism itself.
C: That the reactants are not similar.
D: That a kinetic isotope effect is always expected.
E: That the products will always be inorganic.
Answer: B

In the context of the crossover experiment, what is the kinetic isotope effect?
A: A change in the type of reaction based on the change in isotope.
B: A change in the mechanism of the reaction based on the change in isotope.
C: A change in the rate of reaction based on the change in isotope.
D: A change in the outcome of the reaction based on the change in isotope.
E: A change in the labeling method based on the change in isotope.
Answer: C

Why is isotopic labeling favored in crossover experiments?
A: Because isotopes yield a higher rate of reaction.
B: Because changing the isotope of one atom in a molecule is a minimal change that can be easily tracked.
C: Because isotopes always change the mechanism of the reaction.
D: Because isotopes ensure the reaction always remains intermolecular.
E: Because isotopes always yield a crossover product.
Answer: B

Which is NOT an advantage of the crossover experiment as mentioned?
A: Conceptually straightforward.
B: Relatively easy to design.
C: Always involves inorganic reactants.
D: Commonly used alongside computational methods.
E: Relatively easy to interpret.
Answer: C

For mechanistic studies, why are crossover experiments of interest to chemists?
A: To always get a crossover product.
B: To determine the concentration of reactants.
C: To understand and predict the stereochemical outcomes and design improved catalysts.
D: To always use isotopic labeling.
E: To study the physical properties of reactants.
Answer: C

Which statement is NOT true about a crossover experiment?
A: It aims to study the mechanism of a chemical reaction.
B: Crossover experiments distinguish between intramolecular and intermolecular reactions.
C: It is always conducted at high temperatures.
D: Crossover experiments can lead to conclusions about a mechanism that would otherwise be challenging.
E: Crossover experiments are often combined with measurements of rate and kinetic isotope effects.
Answer: C
@
Subject:
Tolerogenic dendritic cells (a. k. a. tol-DCs, tDCs, or DCregs) are heterogenous pool of dendritic cells with immuno-suppressive properties, priming immune system into tolerogenic state against various antigens. These tolerogenic effects are mostly mediated through regulation of T cells such as inducing T cell anergy, T cell apoptosis and induction of Tregs.[1] Tol-DCs also affect local micro-environment toward tolerogenic state by producing anti-inflammatory cytokines.

Tol-DCs are not lineage specific and their immune-suppressive functions is due to their state of activation and/or differentiation. Generally, properties of all types of dendritic cells can be highly affected by local micro-environment such as presence of pro or anti-inflammatory cytokines, therefore tolerogenic properties of tol-DCs are often context dependant and can be even eventually overridden into pro-inflammatory phenotype.[2][3][4]

Tolerogenic DCs present a potential strategy for treatment of autoimmune diseases, allergic diseases and transplant rejections. Moreover, Ag-specific tolerance in humans can be induced in vivo via vaccination with Ag-pulsed ex vivo generated tolerogenic DCs.[5] For that reason, tolerogenic DCs are an important promising therapeutic tool.[6]

Dendritic cells
Dendritic cells (DCs) were first discovered and described in 1973 by Ralph M.  Steinman. They represent a bridge between innate and adaptive immunity and play a key role in the regulation of initiation of immune responses. DCs populate almost all body surfaces and they do not kill the pathogens directly, they utilize and subsequently degrade antigens to peptides by their proteolytic activity. After that, they present these peptides in complexes together with their MHC molecules on their cell surface. DCs are also the only cell type which can activate naïve T cells and induce antigen-specific immune responses.[6][7]

Therefore, their role is crucially important in balance between tolerance and immune response.

Tolerogenic dendritic cells
Tolerogenic DCs are essential in maintenance of central and peripheral tolerance through induction of T cell clonal deletion, T cell anergy and generation and activation of regulatory T (Treg) cells. For that reason, tolerogenic DCs are possible candidates for specific cellular therapy for treatment of allergic diseases, autoimmune diseases (e.g. type 1 diabetes, multiple sclerosis, rheumatoid arthritis) or transplant rejections.[8][9][6]

Tolerogenic DCs often display an immature or semi-mature phenotype with characteristically low expression of costimulatory (e.g. CD80, CD86) and MHC molecules on their surface. Tolerogenic DCs also produce different cytokines as mature DCs (e.g. anti-inflammatory cytokines interleukin (IL)-10, transforming growth factor-β (TGF-β)). Moreover, tolerogenic DCs may also express various inhibitory surface molecules (e.g. programmed cell death ligand (PDL)-1, PDL-2) or can modulate metabolic parameters and change T cell response. For example, tolerogenic DCs can release or induce enzymes such as indoleamine 2,3-dioxygenase (IDO) or heme oxygenase-1 (HO-1). IDO promotes the degradation of tryptophan to N-formylkynurenin leading to reduced T cell proliferation, whereas HO- 1 catalyzes degradation of hemoglobin resulting in production of monoxide and lower DC immunogenicity. Besides that, tolerogenic DCs also may produce retinoic acid (RA), which induces Treg differentiation.[10][11]

Human tolerogenic DCs may be induced by various immunosuppressive drugs or biomediators. Immunosuppressive drugs, e.g. corticosteroid dexamethasone, rapamycin, cyclosporine or acetylsalicylic acid, cause low expression of costimulatory molecules, reduced expression of MHC, higher expression of inhibitory molecules (e.g. PDL-1) or higher secretion of IL-10 or IDO. In addition, incubation with inhibitory cytokines IL-10 or TGF-β leads to generation of tolerogenic phenotype. Other mediators also affect generation of tolerogenic DC, e.g. vitamin D3, vitamin D2,[12] hepatocyte growth factor or vasoactive intestinal peptide. The oldest and mostly used cytokine cocktail for in vitro DC generation is GM-CSF/IL-4.[10][5]

Tolerogenic DCs may be a potential candidate for specific immunotherapy and are studied for using them for treatment of inflammatory, autoimmune and allergic diseases and also in transplant medicine. Important and interesting feature of tolerogenic DCs is also the migratory capacity toward secondary lymph organs, leading to T-cell mediated immunosuppression. The first trial to transfer tolerogenic DCs to humans was undertaken by Ralph Steinman's group in 2001. Relating to the DC administration, various application have been used in humans in last years. Tolerogenic DCs have been injected e.g. intraperitoneally in patients with Crohn's disease, intradermally in diabetes and rheumatoid arthritis patients, subcutaneously in rheumatoid arthritis patients and via arthroscopic injections in joints of patient with rheumatoid and inflammatory arthritis.[13]

Therefore, it is necessary to test tolerogenic DCs for a stable phenotype to exclude a loss of the regulatory function and a switch to an immunostimulatory activity.

Interleukin 10 (IL-10), also known as human cytokine synthesis inhibitory factor (CSIF), is an anti-inflammatory cytokine. In humans, interleukin 10 is encoded by the IL10 gene.[5] IL-10 signals through a receptor complex consisting of two IL-10 receptor-1 and two IL-10 receptor-2 proteins.[6] Consequently, the functional receptor consists of four IL-10 receptor molecules. IL-10 binding induces STAT3 signalling via the phosphorylation of the cytoplasmic tails of IL-10 receptor 1 + IL-10 receptor 2 by JAK1 and Tyk2 respectively.[6]
$
10
Who first discovered and described dendritic cells (DCs)?
A: Charles Darwin
B: Albert Einstein
C: Ralph M. Steinman
D: Marie Curie
E: James Watson
Answer: C

What do tolerogenic dendritic cells (tol-DCs) primarily regulate?
A: Digestive enzymes
B: T cell activities
C: Red blood cell production
D: Hormone secretion
E: Neural connections
Answer: B

Which of the following is a key feature of tolerogenic DCs in comparison to mature DCs in terms of cytokine production?
A: They produce pro-inflammatory cytokines like IL-6.
B: They produce high levels of transforming growth factor-β (TGF-β).
C: They release pro-inflammatory cytokines like TNF-α.
D: They produce similar cytokines as mature DCs.
E: They focus on producing insulin.
Answer: B

Which enzyme released or induced by tolerogenic DCs promotes the degradation of tryptophan, leading to reduced T cell proliferation?
A: Amylase
B: Catalase
C: Ribonuclease
D: IDO (indoleamine 2,3-dioxygenase)
E: Lysozyme
Answer: D

Dendritic cells (DCs) are essential in bridging which two types of immunity?
A: Humoral and Cell-mediated
B: Passive and Active
C: Innate and Adaptive
D: Congenital and Acquired
E: Natural and Artificial
Answer: C

For what medical applications are tolerogenic DCs considered potential candidates?
A: Enhancement of muscle growth
B: Treatment of viral infections
C: Treatment of autoimmune diseases, transplant rejections, and allergic diseases
D: Prevention of common cold
E: Stimulating hair growth
Answer: C

Which of the following cytokines has an anti-inflammatory property?
A: IL-2
B: IL-6
C: IL-10
D: TNF-α
E: IL-12
Answer: C

Which is a characteristic phenotype of tolerogenic DCs in terms of costimulatory molecule expression?
A: High expression of CD80 and CD86
B: Absence of MHC molecules
C: Low expression of CD80 and CD86
D: Presence of only CD80, but not CD86
E: High expression of CD45
Answer: C

What kind of factor can potentially switch tolerogenic DCs into a pro-inflammatory phenotype?
A: Presence of anti-inflammatory cytokines
B: Lack of exposure to antigens
C: Presence of pro-inflammatory cytokines
D: Isolation from T cells
E: Absence of MHC molecules
Answer: C

Which compound released by tolerogenic DCs is known to induce Treg differentiation?
A: Glucose
B: Cholesterol
C: Retinoic acid (RA)
D: Sodium chloride
E: ATP
Answer: C
@
Subject:
Cool red and brown dwarf classes
Main articles: Brown dwarf and Red dwarf
The new spectral types L, T, and Y were created to classify infrared spectra of cool stars. This includes both red dwarfs and brown dwarfs that are very faint in the visible spectrum.[90]

Brown dwarfs, stars that do not undergo hydrogen fusion, cool as they age and so progress to later spectral types. Brown dwarfs start their lives with M-type spectra and will cool through the L, T, and Y spectral classes, faster the less massive they are; the highest-mass brown dwarfs cannot have cooled to Y or even T dwarfs within the age of the universe. Because this leads to an unresolvable overlap between spectral types' effective temperature and luminosity for some masses and ages of different L-T-Y types, no distinct temperature or luminosity values can be given.[9]

Class L
Class L dwarfs get their designation because they are cooler than M stars and L is the remaining letter alphabetically closest to M. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs. They are a very dark red in color and brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra.[91][92][93]

Due to low surface gravity in giant stars, TiO- and VO-bearing condensates never form. Thus, L-type stars larger than dwarfs can never form in an isolated environment. However, it may be possible for these L-type supergiants to form through stellar collisions, an example of which is V838 Monocerotis while in the height of its luminous red nova eruption.

Brown dwarfs (also called failed stars) are substellar objects that are not massive enough to sustain nuclear fusion of ordinary hydrogen (1H) into helium in their cores, unlike a main-sequence star. Instead, they have a mass between the most massive gas giant planets and the least massive stars, approximately 13 to 80 times that of Jupiter (MJ).[2][3] However, they can fuse deuterium (2H) and the most massive ones (> 65 MJ) can fuse lithium (7Li).[3]

Astronomers classify self-luminous objects by spectral type, a distinction intimately tied to the surface temperature, and brown dwarfs occupy types M, L, T, and Y.[4][5] As brown dwarfs do not undergo stable hydrogen fusion, they cool down over time, progressively passing through later spectral types as they age.

Despite their name, to the naked eye, brown dwarfs would appear in different colors depending on their temperature.[4] The warmest ones are possibly orange or red,[6] while cooler brown dwarfs would likely appear magenta or black to the human eye.[4][7] Brown dwarfs may be fully convective, with no layers or chemical differentiation by depth.[8]

Though their existence was initially theorized in the 1960s, it was not until the mid-1990s that the first unambiguous brown dwarfs were discovered. As brown dwarfs have relatively low surface temperatures, they are not very bright at visible wavelengths, emitting most of their light in the infrared. However, with the advent of more capable infrared detecting devices, thousands of brown dwarfs have been identified. The nearest known brown dwarfs are located in the Luhman 16 system, a binary of L- and T-type brown dwarfs about 6.5 light-years (2.0 parsecs) from the Sun. Luhman 16 is the third closest system to the Sun after Alpha Centauri and Barnard's Star.
$
6
What are brown dwarfs commonly referred to as?
A: Failed stars.
B: Bright stars.
C: Gas giants.
D: Helium stars.
E: Red giants.
Answer: A

Which fusion processes can the most massive brown dwarfs undergo?
A: Helium and Deuterium.
B: Hydrogen and Helium.
C: Only Hydrogen.
D: Deuterium and Lithium.
E: Only Lithium.
Answer: D

What spectral types do brown dwarfs occupy?
A: A, B, C, and D.
B: K, L, M, and N.
C: M, L, T, and Y.
D: X, Y, Z, and W.
E: H, I, J, and K.
Answer: C

How would the warmest brown dwarfs likely appear to the naked human eye?
A: Green.
B: Black.
C: Magenta.
D: Orange or red.
E: White.
Answer: D

Why was it challenging to discover brown dwarfs before the mid-1990s?
A: Because they were too far away.
B: Because they are very bright in the visible spectrum.
C: Due to their high surface temperatures.
D: Due to their relatively low surface temperatures, making them not very bright at visible wavelengths.
E: Because they do not exist in our galaxy.
Answer: D

Which system is the third closest to the Sun and consists of L- and T-type brown dwarfs?
A: Barnard's Star system.
B: Luhman 16 system.
C: Alpha Centauri system.
D: Proxima Centauri system.
E: Kepler-16 system.
Answer: B
@
Subject:
Rectilinear propagation describes the tendency of electromagnetic waves (light) to travel in a straight line. Light does not deviate when travelling through a homogeneous medium, which has the same refractive index throughout; otherwise, light suffers refraction. Even though a wave front may be bent, (e.g. the waves created by a rock hitting a pond) the individual rays are moving in straight lines. Rectilinear propagation was discovered by Pierre de Fermat

Proof
Take three cardboard A, B and C, of the same size. Make a pin hole at the centre of each of three cardboard. Place the cardboard in the upright position, such that the holes in A, B and C are in the same straight line, in the order. Place a luminous source like a candle near the cardboard A and look through the hole in the cardboard C. We can see the candle flame. This implies that light rays travel along a straight line ABC, and hence, candle flame is visible. When one of the cardboard is slightly displaced, candle light would not be visible. It means that the light emitted by the candle is unable to bend and reach observers eye. This proves that light travels along a straight path. This proves the rectilinear propagation of light.

Optics

A replica of the reflecting telescope Newton presented to the Royal Society in 1672 (the first one he made in 1668 was loaned to an instrument maker but there is no further record of what happened to it).[51]
In 1666, Newton observed that the spectrum of colours exiting a prism in the position of minimum deviation is oblong, even when the light ray entering the prism is circular, which is to say, the prism refracts different colours by different angles.[52][53] This led him to conclude that colour is a property intrinsic to light – a point which had, until then, been a matter of debate.

From 1670 to 1672, Newton lectured on optics.[54] During this period he investigated the refraction of light, demonstrating that the multicoloured image produced by a prism, which he named a spectrum, could be recomposed into white light by a lens and a second prism.[55] Modern scholarship has revealed that Newton's analysis and resynthesis of white light owes a debt to corpuscular alchemy.[56]

He showed that coloured light does not change its properties by separating out a coloured beam and shining it on various objects, and that regardless of whether reflected, scattered, or transmitted, the light remains the same colour. Thus, he observed that colour is the result of objects interacting with already-coloured light rather than objects generating the colour themselves. This is known as Newton's theory of colour.[57]


Illustration of a dispersive prism separating white light into the colours of the spectrum, as discovered by Newton
From this work, he concluded that the lens of any refracting telescope would suffer from the dispersion of light into colours (chromatic aberration). As a proof of the concept, he constructed a telescope using reflective mirrors instead of lenses as the objective to bypass that problem.[58][59] Building the design, the first known functional reflecting telescope, today known as a Newtonian telescope,[59] involved solving the problem of a suitable mirror material and shaping technique. Newton ground his own mirrors out of a custom composition of highly reflective speculum metal, using Newton's rings to judge the quality of the optics for his telescopes. In late 1668,[60] he was able to produce this first reflecting telescope. It was about eight inches long and it gave a clearer and larger image. In 1671, the Royal Society asked for a demonstration of his reflecting telescope.[61] Their interest encouraged him to publish his notes, Of Colours,[62] which he later expanded into the work Opticks. When Robert Hooke criticised some of Newton's ideas, Newton was so offended that he withdrew from public debate. Newton and Hooke had brief exchanges in 1679–80, when Hooke, appointed to manage the Royal Society's correspondence, opened up a correspondence intended to elicit contributions from Newton to Royal Society transactions,[63] which had the effect of stimulating Newton to work out a proof that the elliptical form of planetary orbits would result from a centripetal force inversely proportional to the square of the radius vector. But the two men remained generally on poor terms until Hooke's death.[64]


Facsimile of a 1682 letter from Newton to William Briggs, commenting on Briggs' A New Theory of Vision
Newton argued that light is composed of particles or corpuscles, which were refracted by accelerating into a denser medium. He verged on soundlike waves to explain the repeated pattern of reflection and transmission by thin films (Opticks Bk. II, Props. 12), but still retained his theory of 'fits' that disposed corpuscles to be reflected or transmitted (Props.13). However, later physicists favoured a purely wavelike explanation of light to account for the interference patterns and the general phenomenon of diffraction. Today's quantum mechanics, photons, and the idea of wave–particle duality bear only a minor resemblance to Newton's understanding of light.

In his Hypothesis of Light of 1675, Newton posited the existence of the ether to transmit forces between particles. The contact with the Cambridge Platonist philosopher Henry More revived his interest in alchemy.[65] He replaced the ether with occult forces based on Hermetic ideas of attraction and repulsion between particles. John Maynard Keynes, who acquired many of Newton's writings on alchemy, stated that "Newton was not the first of the age of reason: He was the last of the magicians."[66] Newton's contributions to science cannot be isolated from his interest in alchemy.[65] This was at a time when there was no clear distinction between alchemy and science, and had he not relied on the occult idea of action at a distance, across a vacuum, he might not have developed his theory of gravity.

In 1704, Newton published Opticks, in which he expounded his corpuscular theory of light. He considered light to be made up of extremely subtle corpuscles, that ordinary matter was made of grosser corpuscles and speculated that through a kind of alchemical transmutation "Are not gross Bodies and Light convertible into one another, ... and may not Bodies receive much of their Activity from the Particles of Light which enter their Composition?"[67] Newton also constructed a primitive form of a frictional electrostatic generator, using a glass globe.[68]

In his book Opticks, Newton was the first to show a diagram using a prism as a beam expander, and also the use of multiple-prism arrays.[69] Some 278 years after Newton's discussion, multiple-prism beam expanders became central to the development of narrow-linewidth tunable lasers. Also, the use of these prismatic beam expanders led to the multiple-prism dispersion theory.[69]

Subsequent to Newton, much has been amended. Young and Fresnel discarded Newton's particle theory in favour of Huygens' wave theory to show that colour is the visible manifestation of light's wavelength. Science also slowly came to realise the difference between perception of colour and mathematisable optics. The German poet and scientist, Goethe, could not shake the Newtonian foundation but "one hole Goethe did find in Newton's armour, ... Newton had committed himself to the doctrine that refraction without colour was impossible. He, therefore, thought that the object-glasses of telescopes must forever remain imperfect, achromatism and refraction being incompatible. This inference was proved by Dollond to be wrong."[70]
$
8
Which of the following describes rectilinear propagation?
A. Electromagnetic waves bend while passing through a medium.
B. Electromagnetic waves always deviate when traveling.
C. Electromagnetic waves, like light, tend to travel in a straight line.
D. Electromagnetic waves always refract when hitting a medium.
E. Electromagnetic waves are always circular.
Answer: C

Who discovered the concept of rectilinear propagation?
A. Isaac Newton
B. Robert Hooke
C. Pierre de Fermat
D. Albert Einstein
E. Johann Goethe
Answer: C

How did Newton view the nature of light?
A. As waves that refract by accelerating into a denser medium.
B. As corpuscles that refract by accelerating into a denser medium.
C. As waves that reflect and do not refract.
D. As particles that produce colors themselves.
E. As continuous beams that do not have a specific property.
Answer: B

In Newton's Opticks, how did he visualize light?
A. As a result of objects generating colors.
B. As made up of extremely subtle corpuscles.
C. As a manifestation of light's frequency.
D. As waves interacting with other waves.
E. As colored light that changes its properties upon reflection.
Answer: B

Which of the following was NOT a belief of Newton regarding light?
A. Light consists of particles or corpuscles.
B. Light is composed of longitudinal waves.
C. Color is a result of objects interacting with already-colored light.
D. Light can be dispersed into colors using a prism.
E. Light's dispersion is due to the refraction of different colors at different angles.
Answer: B

Which telescope design was introduced by Newton to address the issue of chromatic aberration?
A. Refracting telescope
B. Radio telescope
C. Galilean telescope
D. Reflecting telescope
E. Hubble space telescope
Answer: D

What did Newton use to judge the quality of optics for his telescopes?
A. Hooke's rings
B. Planetary orbits
C. Prismatic dispersion theory
D. Newton's rings
E. Fresnel equations
Answer: D

Which of the following statements best summarizes the view of Young and Fresnel about light after Newton?
A. They supported Newton's particle theory and expanded upon it.
B. They favored a purely wavelike explanation of light, dismissing Newton's corpuscle theory.
C. They combined Newton's and Huygens' theories to form a unified theory of light.
D. They introduced the electromagnetic wave theory, considering both wave and particle nature.
E. They concluded that light does not have any intrinsic property and is a mere perception.
Answer: B
@
Subject:
In thermodynamics, the chemical potential of a species is the energy that can be absorbed or released due to a change of the particle number of the given species, e.g. in a chemical reaction or phase transition. The chemical potential of a species in a mixture is defined as the rate of change of free energy of a thermodynamic system with respect to the change in the number of atoms or molecules of the species that are added to the system. Thus, it is the partial derivative of the free energy with respect to the amount of the species, all other species' concentrations in the mixture remaining constant. When both temperature and pressure are held constant, and the number of particles is expressed in moles, the chemical potential is the partial molar Gibbs free energy.[1][2] At chemical equilibrium or in phase equilibrium, the total sum of the product of chemical potentials and stoichiometric coefficients is zero, as the free energy is at a minimum.[3][4][5] In a system in diffusion equilibrium, the chemical potential of any chemical species is uniformly the same everywhere throughout the system.[6]

In semiconductor physics, the chemical potential of a system of electrons at zero absolute temperature is known as the Fermi level.[7]

Sub-nuclear particles
In recent years,[when?] thermal physics has applied the definition of chemical potential to systems in particle physics and its associated processes. For example, in a quark–gluon plasma or other QCD matter, at every point in space there is a chemical potential for photons, a chemical potential for electrons, a chemical potential for baryon number, electric charge, and so forth.

In the case of photons, photons are bosons and can very easily and rapidly appear or disappear. Therefore, at thermodynamic equilibrium, the chemical potential of photons is always and everywhere zero. The reason is, if the chemical potential somewhere was higher than zero, photons would spontaneously disappear from that area until the chemical potential went back to zero; likewise, if the chemical potential somewhere was less than zero, photons would spontaneously appear until the chemical potential went back to zero. Since this process occurs extremely rapidly (at least, it occurs rapidly in the presence of dense charged matter), it is safe to assume that the photon chemical potential is never different from zero.

Electric charge is different because it is conserved, i.e. it can be neither created nor destroyed. It can, however, diffuse. The "chemical potential of electric charge" controls this diffusion: Electric charge, like anything else, will tend to diffuse from areas of higher chemical potential to areas of lower chemical potential.[22] Other conserved quantities like baryon number are the same. In fact, each conserved quantity is associated with a chemical potential and a corresponding tendency to diffuse to equalize it out.[23]

In the case of electrons, the behaviour depends on temperature and context. At low temperatures, with no positrons present, electrons cannot be created or destroyed. Therefore, there is an electron chemical potential that might vary in space, causing diffusion. At very high temperatures, however, electrons and positrons can spontaneously appear out of the vacuum (pair production), so the chemical potential of electrons by themselves becomes a less useful quantity than the chemical potential of the conserved quantities like (electrons minus positrons).

The chemical potentials of bosons and fermions is related to the number of particles and the temperature by Bose–Einstein statistics and Fermi–Dirac statistics respectively.
$
7
1. In thermodynamics, the chemical potential of a species is related to which of the following?
A: The energy required to change the particle number of the species.
B: The rate of chemical reactions in the system.
C: The overall heat capacity of the system.
D: The conductivity of the species.
E: The amount of work done on the system.
Answer: A

2. How is the chemical potential of a species in a mixture defined?
A: As the rate of change of kinetic energy with respect to the number of atoms or molecules of the species.
B: As the total change in free energy of a thermodynamic system when a mole of the species is added.
C: As the partial derivative of the entropy with respect to the amount of the species.
D: As the rate of change of free energy of a system with respect to the change in the number of atoms or molecules of the species.
E: As the change in pressure when the number of particles of the species changes.
Answer: D

3. At chemical or phase equilibrium, the product of chemical potentials and stoichiometric coefficients will:
A: Always be zero.
B: Always be equal to the Gibbs free energy.
C: Vary depending on the species in the system.
D: Increase proportionally with temperature.
E: Decrease proportionally with temperature.
Answer: A

4. In semiconductor physics, what is the chemical potential of a system of electrons at zero absolute temperature called?
A: Bose level
B: Thermodynamic potential
C: Gibbs level
D: Electron potential
E: Fermi level
Answer: E

5. Which of the following statements is true regarding the chemical potential of photons?
A: At thermodynamic equilibrium, it always has a positive value.
B: At thermodynamic equilibrium, it is always and everywhere zero.
C: Photons have different chemical potentials depending on their wavelength.
D: Photons spontaneously appear when their chemical potential is high.
E: Photons' chemical potential increases proportionally with temperature.
Answer: B

6. Why is the chemical potential of electric charge different from that of photons?
A: Because electric charge is not conserved.
B: Because electric charge cannot be created or destroyed.
C: Because electric charge can spontaneously appear from the vacuum.
D: Because electric charge tends to equalize rapidly.
E: Because electric charge can change its form.
Answer: B

7. How is the chemical potential of bosons and fermions related to the number of particles and temperature?
A: Using the principles of quantum mechanics.
B: Using Newton's laws.
C: Using Maxwell's equations.
D: Using Bose–Einstein statistics and Fermi–Dirac statistics respectively.
E: Using the first law of thermodynamics.
Answer: D
@
Subject:
The American Petroleum Institute gravity, or API gravity, is a measure of how heavy or light a petroleum liquid is compared to water: if its API gravity is greater than 10, it is lighter and floats on water; if less than 10, it is heavier and sinks.

API gravity is thus an inverse measure of a petroleum liquid's density relative to that of water (also known as specific gravity). It is used to compare densities of petroleum liquids. For example, if one petroleum liquid is less dense than another, it has a greater API gravity. Although API gravity is mathematically a dimensionless quantity (see the formula below), it is referred to as being in 'degrees'. API gravity is graduated in degrees on a hydrometer instrument. API gravity values of most petroleum liquids fall between 10 and 70 degrees.

In 1916, the U.S. National Bureau of Standards accepted the Baumé scale, which had been developed in France in 1768, as the U.S. standard for measuring the specific gravity of liquids less dense than water. Investigation by the U.S. National Academy of Sciences found major errors in salinity and temperature controls that had caused serious variations in published values. Hydrometers in the U.S. had been manufactured and distributed widely with a modulus of 141.5 instead of the Baumé scale modulus of 140. The scale was so firmly established that, by 1921, the remedy implemented by the American Petroleum Institute was to create the API gravity scale, recognizing the scale that was actually being used.[1]
$
1. If the API gravity of a petroleum liquid is 15, which of the following is true?
A: The liquid is lighter than water and floats on it.
B: The liquid is heavier than water and sinks.
C: The liquid is as dense as water.
D: The liquid is acidic.
E: The liquid has the same viscosity as water.
Answer: A

2. API gravity can be considered as:
A: A direct measure of a petroleum liquid's viscosity.
B: An inverse measure of the freezing point of a petroleum liquid.
C: A direct measure of the acidity or alkalinity of a petroleum liquid.
D: An inverse measure of a petroleum liquid's density relative to water.
E: A direct measure of the electrical conductivity of a petroleum liquid.
Answer: D

3. For most petroleum liquids, where do the API gravity values typically range?
A: Between 0 and 10 degrees.
B: Between 10 and 70 degrees.
C: Between 100 and 150 degrees.
D: Between -10 and 10 degrees.
E: Above 100 degrees.
Answer: B

4. Which of the following statements about the Baumé scale is accurate?
A: It was developed in the U.S. in 1916 for measuring the viscosity of liquids.
B: The Baumé scale was accepted by the American Petroleum Institute as the U.S. standard in 1921.
C: The Baumé scale was developed in France in 1768 for measuring the specific gravity of liquids less dense than water.
D: The Baumé scale modulus is 141.5.
E: All hydrometers in the U.S. accurately followed the Baumé scale modulus.
Answer: C

5. What action did the American Petroleum Institute take in 1921 due to the inaccuracies found with the Baumé scale?
A: They abandoned the Baumé scale entirely.
B: They introduced the API gravity scale.
C: They corrected the modulus of the Baumé scale to 140.
D: They started measuring viscosity instead of density.
E: They replaced the Baumé scale with a new international standard.
Answer: B

6. If two petroleum liquids have API gravities of 20 and 30 respectively, which one is denser?
A: The one with an API gravity of 20.
B: The one with an API gravity of 30.
C: They have the same density.
D: It depends on the temperature of the liquids.
E: It depends on the viscosity of the liquids.
Answer: A
@
Subject:
The electrical resistance of an object is a measure of its opposition to the flow of electric current. Its reciprocal quantity is electrical conductance, measuring the ease with which an electric current passes. Electrical resistance shares some conceptual parallels with mechanical friction. The SI unit of electrical resistance is the ohm (Ω), while electrical conductance is measured in siemens (S) (formerly called the 'mho' and then represented by ℧).

The resistance of an object depends in large part on the material it is made of. Objects made of electrical insulators like rubber tend to have very high resistance and low conductance, while objects made of electrical conductors like metals tend to have very low resistance and high conductance. This relationship is quantified by resistivity or conductivity. The nature of a material is not the only factor in resistance and conductance, however; it also depends on the size and shape of an object because these properties are extensive rather than intensive. For example, a wire's resistance is higher if it is long and thin, and lower if it is short and thick. All objects resist electrical current, except for superconductors, which have a resistance of zero.

Introduction
In the hydraulic analogy, current flowing through a wire (or resistor) is like water flowing through a pipe, and the voltage drop across the wire is like the pressure drop that pushes water through the pipe. Conductance is proportional to how much flow occurs for a given pressure, and resistance is proportional to how much pressure is required to achieve a given flow.

The voltage drop (i.e., difference between voltages on one side of the resistor and the other), not the voltage itself, provides the driving force pushing current through a resistor. In hydraulics, it is similar: the pressure difference between two sides of a pipe, not the pressure itself, determines the flow through it. For example, there may be a large water pressure above the pipe, which tries to push water down through the pipe. But there may be an equally large water pressure below the pipe, which tries to push water back up through the pipe. If these pressures are equal, no water flows. (In the image at right, the water pressure below the pipe is zero.)

The resistance and conductance of a wire, resistor, or other element is mostly determined by two properties:

geometry (shape), and
material
Geometry is important because it is more difficult to push water through a long, narrow pipe than a wide, short pipe. In the same way, a long, thin copper wire has higher resistance (lower conductance) than a short, thick copper wire.

Materials are important as well. A pipe filled with hair restricts the flow of water more than a clean pipe of the same shape and size. Similarly, electrons can flow freely and easily through a copper wire, but cannot flow as easily through a steel wire of the same shape and size, and they essentially cannot flow at all through an insulator like rubber, regardless of its shape. The difference between copper, steel, and rubber is related to their microscopic structure and electron configuration, and is quantified by a property called resistivity.

In addition to geometry and material, there are various other factors that influence resistance and conductance, such as temperature; see below.

Near room temperature, the resistivity of metals typically increases as temperature is increased, while the resistivity of semiconductors typically decreases as temperature is increased. The resistivity of insulators and electrolytes may increase or decrease depending on the system. For the detailed behavior and explanation, see Electrical resistivity and conductivity.
$
10
1. Which unit is used to measure electrical resistance?
A: Siemens
B: Newton
C: Ohm
D: Joule
E: Mho
Answer: C

2. What does the electrical conductance measure?
A: The opposition to the flow of electric current.
B: The ease with which an electric current passes.
C: The purity of an electric current.
D: The voltage drop across a material.
E: The electrical pressure applied to a material.
Answer: B

3. Which of the following objects is most likely to have a very high electrical resistance?
A: Copper wire
B: Steel rod
C: Rubber ball
D: Aluminium foil
E: Superconductor
Answer: C

4. In the hydraulic analogy, what is current flowing through a wire comparable to?
A: The pressure drop that pushes water through a pipe.
B: The flow of water through a pipe.
C: The width of a pipe.
D: The purity of water in a pipe.
E: The temperature of the water in a pipe.
Answer: B

5. A wire's resistance is lower if:
A: It is long and thick.
B: It is long and thin.
C: It is short and thin.
D: It is short and thick.
E: It has a positive temperature.
Answer: D

6. What is the relationship between resistance and temperature for most metals near room temperature?
A: Resistance decreases as temperature increases.
B: Resistance remains constant regardless of temperature.
C: Resistance increases as temperature decreases.
D: Resistance increases as temperature increases.
E: There is no general relationship between resistance and temperature for metals.
Answer: D

7. The reciprocal of electrical resistance is:
A: Voltage.
B: Conductance.
C: Current.
D: Resistivity.
E: Pressure.
Answer: B

8. If you were to describe electrical resistance, which of the following would be a suitable analogy?
A: The speed at which a car moves.
B: The volume of water in a tank.
C: The friction between two sliding surfaces.
D: The brightness of a light bulb.
E: The height of a building.
Answer: C

9. Why does a material like rubber have high resistance?
A: Because it is a good conductor of electricity.
B: Because of its high electron configuration.
C: Because it is an electrical insulator.
D: Because it has high purity.
E: Because it has a very high temperature.
Answer: C

10. What would likely happen to the resistance of a copper wire if its length were doubled while keeping its thickness constant?
A: Resistance would remain the same.
B: Resistance would be halved.
C: Resistance would be doubled.
D: Resistance would be quartered.
E: There would be no predictable change in resistance.
Answer: C
@
Subject:
Hubble's law, also known as the Hubble–Lemaître law,[1] is the observation in physical cosmology that galaxies are moving away from Earth at speeds proportional to their distance. In other words, the farther they are, the faster they are moving away from Earth. The velocity of the galaxies has been determined by their redshift, a shift of the light they emit toward the red end of the visible spectrum.

Hubble's law is considered the first observational basis for the expansion of the universe, and today it serves as one of the pieces of evidence most often cited in support of the Big Bang model.[2][3] The motion of astronomical objects due solely to this expansion is known as the Hubble flow.[4] It is described by the equation v = H0D, with H0 the constant of proportionality—the Hubble constant—between the "proper distance" D to a galaxy, which can change over time, unlike the comoving distance, and its speed of separation v, i.e. the derivative of proper distance with respect to the cosmological time coordinate. (See Comoving and proper distances § Uses of the proper distance for some discussion of the subtleties of this definition of "velocity".)

The Hubble constant is most frequently quoted in (km/s)/Mpc, thus giving the speed in km/s of a galaxy 1 megaparsec (3.09×1019 km) away, and its value is about 70 (km/s)/Mpc. However, crossing out units reveals that H0 is a unit of frequency (SI unit: s−1) and the reciprocal of H0 is known as the Hubble time. The Hubble constant can also be interpreted as the relative rate of expansion. In this form H0 = 7%/Gyr, meaning that at the current rate of expansion it takes a billion years for an unbound structure to grow by 7%.

Although widely attributed to Edwin Hubble,[5][6][7] the notion of the universe expanding at a calculable rate was first derived from general relativity equations in 1922 by Alexander Friedmann. Friedmann published a set of equations, now known as the Friedmann equations, showing that the universe might be expanding, and presenting the expansion speed if that were the case.[8] Then Georges Lemaître, in a 1927 article, independently derived that the universe might be expanding, observed the proportionality between recessional velocity of, and distance to, distant bodies, and suggested an estimated value for the proportionality constant; this constant, when Edwin Hubble confirmed the existence of cosmic expansion and determined a more accurate value for it two years later, came to be known by his name as the Hubble constant.[2][9][10][11][12] Hubble inferred the recession velocity of the objects from their redshifts, many of which were earlier measured and related to velocity by Vesto Slipher in 1917.[13][14][15] Though the Hubble constant H0 is constant at any given moment in time, the Hubble parameter H, of which the Hubble constant is the current value, varies with time, so the term constant is sometimes thought of as somewhat of a misnomer.[16][17]
$
10
1. What does Hubble's law state about the relationship between the velocity of a galaxy and its distance from Earth?
A: The closer a galaxy is to Earth, the faster it is moving towards Earth.
B: The farther a galaxy is from Earth, the slower it is moving away.
C: The farther a galaxy is from Earth, the faster it is moving away.
D: The velocity of a galaxy is not related to its distance from Earth.
E: The closer a galaxy is to Earth, the slower it is moving away.
Answer: C

2. Which shift of light is used to determine the velocity of galaxies?
A: Blueshift
B: Ultraviolet shift
C: Infrared shift
D: Redshift
E: Green shift
Answer: D

3. What is the significance of the Hubble constant in Hubble's law?
A: It represents the age of the universe.
B: It is the constant rate at which galaxies are created.
C: It is the constant of proportionality between the distance to a galaxy and its speed of separation.
D: It determines the light intensity emitted by distant galaxies.
E: It defines the shape of the universe.
Answer: C

4. The reciprocal of the Hubble constant is referred to as:
A: Hubble's ratio.
B: Hubble's factor.
C: Hubble's speed.
D: Hubble's time.
E: Hubble's distance.
Answer: D

5. Who first derived from general relativity equations that the universe might be expanding?
A: Edwin Hubble
B: Vesto Slipher
C: Alexander Friedmann
D: Georges Lemaître
E: Albert Einstein
Answer: C

6. In what unit is the Hubble constant most frequently quoted?
A: (km/s)/Mpc
B: (m/s)/Mpc
C: km/s
D: Gyr
E: Hz
Answer: A

7. What aspect of galaxies did Hubble infer the recession velocity from?
A: Their luminosity
B: Their mass
C: Their spin
D: Their size
E: Their redshifts
Answer: E

8. Why might the term "Hubble constant" be considered a misnomer?
A: Because the value of H0 can change over time.
B: Because the value of H0 is not related to the expansion of the universe.
C: Because H0 varies from galaxy to galaxy.
D: Because the Hubble parameter H varies with time, even though H0 is constant at any given moment.
E: Because Edwin Hubble did not actually discover it.
Answer: D

9. Who independently derived that the universe might be expanding and suggested an estimated value for the proportionality constant before Edwin Hubble?
A: Vesto Slipher
B: Georges Lemaître
C: Albert Einstein
D: Alexander Friedmann
E: Neil deGrasse Tyson
Answer: B

10. Which of the following describes the Hubble flow?
A: The flow of energy between galaxies.
B: The rotational flow of galaxies around a central point.
C: The motion of astronomical objects due to the expansion of the universe.
D: The flow of matter into black holes.
E: The gravitational flow between galaxies.
Answer: C
@
Subject:
An Evans balance, also known as a Johnson's balance (after a commercial producer of the Evans balance), is a device for measuring magnetic susceptibility. Magnetic susceptibility is related to the force experienced by a substance in a magnetic field. Various practical devices are available for the measurement of susceptibility, which differ in the shape of the magnetic field and the way the force is measured.[1]

The Evans balance employs a similar sample configuration but measures the force on the magnet.[2]

Mechanism
The suspension trip has two pairs of magnets placed back-to-back, making a balanced system with a magnetic field at each end. When a sample, which is fixed in a glass tube holder, is introduced into the field of one magnet, it experiences a force that deflects the beam. The deflection is detected by an optical transducer. A magnetic field is generated at the second magnet which, by negative feedback, restores the beam to its original position. The magnetic field required to do it is generated by passing a current through a coil of wire. One end of this wire is between the poles of the second magnet. The current required to do this is proportional to the force exerted on the first magnet. There is a second coil of wire that creates an electrical zero and a copper sheet that can critically dampen the system.[3]

The original Evans balance was described by the English scientist Dennis F. Evans in 1973, based on a torsional balance developed in 1937 by Alexander Rankine. Evans used Ticonal bars with cadmium-plated mild steel yokes as the magnets, a Johnson Matthey gold alloy (hence the other name of the balance) for the suspension strip, all glued together with epoxy resin onto a phosphor brown spacer. The tubes were made from NMR tubes and the current came from CdS photocells.[3] This original was modified with help from the Johnson Matthey company. Two pairs of magnets were glued between the arms of an H-frame. The sample was placed into the gap between one pair of magnets and a small coil in the gap between the second pair of magnets. This entire construction pivoted horizontally around a torsion strip. When a sample tube was placed between the first pair of magnets, the torsional force was restored by the current passed through the coil between the second pair of magnets, giving a reading on a display instead of a Helipot (as was used in the original).[4]

Advantages vs alternative magnetic balances
The main advantage of this system is that it is cheap to construct as it does not require a precision weighing device. Moreover, using a Evans balance is less time-consuming than using a Gouy or Faraday balances, although it is not sensitive and accurate in comparison to these last two systems.[3] One reason that they were time-consuming is that the sample had to be suspended between the two poles of a very powerful magnet. The tube had to be suspended in the same place every time for the apparatus constant to be accurate. In the case of the Gouy balance, the static charge on the glass tube often caused the tube to stick to magnets. With the Evans balance, a reading could be taken in a matter of seconds with only small sacrifices in sensitivity and accuracy. A Johnson-Matthey balance has a range from 0.001 x 10−7 to 1.99 x 10−7 c.g.s. volume susceptibility units.[5] The original Evans balance had an accuracy within 1% of literature values for diamagnetic solutions and within 2% of literature values of paramagnetic solids.[3]

The system allows for measurements of solid, liquid, and gaseous forms of a wide range of paramagnetic and diamagnetic materials. For each measurement, only around 250 mg of sample is required (50 mg can be used for a thin-bore sample tube).[6]
$
10
1. What was the name of the English scientist who originally described the Evans balance?
A: Albert Einstein
B: Dennis F. Evans
C: Michael Faraday
D: Alexander Rankine
E: James Maxwell
Answer: B

2. In the original Evans balance, what was used for the suspension strip?
A: Stainless steel
B: Copper alloy
C: Johnson Matthey gold alloy
D: Ticonal bars
E: Aluminum strips
Answer: C

3. Which of the following is NOT a part of the Evans balance's mechanism?
A: A copper sheet to dampen the system.
B: A glass tube holder for the sample.
C: An optical transducer to detect deflection.
D: A Helipot to display the reading.
E: Two pairs of magnets placed back-to-back.
Answer: D

4. The current that helps restore the beam to its original position in the Evans balance is proportional to:
A: The magnetic field strength.
B: The weight of the sample.
C: The distance of the sample from the magnet.
D: The force exerted on the first magnet.
E: The rotation of the second magnet.
Answer: D

5. One of the main advantages of the Evans balance compared to alternative magnetic balances is:
A: Its unparalleled sensitivity.
B: Its speed and ease of use.
C: The use of a very powerful magnet.
D: Its ability to handle large samples.
E: Its precision weighing capability.
Answer: B

6. Why was using the Gouy balance time-consuming?
A: The sample needed a specific temperature.
B: The tube had to be suspended between two poles of a powerful magnet in a consistent manner.
C: It required electricity to operate.
D: The samples had to be in a solid form.
E: The balance needed calibration after every measurement.
Answer: B

7. How much sample is typically required for each measurement using the Evans balance?
A: Around 1 g
B: Around 500 mg
C: Around 100 mg
D: Around 250 mg
E: Less than 10 mg
Answer: D

8. What was a common issue with the Gouy balance regarding the glass tube?
A: The tube broke easily.
B: The tube had to be cleaned after every use.
C: The static charge on the glass tube caused it to stick to magnets.
D: The tube refracted the magnetic field.
E: The tube absorbed some of the magnetic field.
Answer: C

9. The Evans balance allows for measurements of:
A: Only solid forms of paramagnetic materials.
B: Only liquid forms of diamagnetic materials.
C: Only gaseous forms of paramagnetic materials.
D: Solid, liquid, and gaseous forms of both paramagnetic and diamagnetic materials.
E: Only solids and liquids without consideration of their magnetic properties.
Answer: D

10. Which component in the Evans balance detects the deflection caused by the force experienced by the sample?
A: The torsion strip
B: The coil of wire
C: The copper sheet
D: The optical transducer
E: The H-frame
Answer: D
@
Subject:
In physics and mathematics, the dimension of a mathematical space (or object) is informally defined as the minimum number of coordinates needed to specify any point within it.[1][2] Thus, a line has a dimension of one (1D) because only one coordinate is needed to specify a point on it – for example, the point at 5 on a number line. A surface, such as the boundary of a cylinder or sphere, has a dimension of two (2D) because two coordinates are needed to specify a point on it – for example, both a latitude and longitude are required to locate a point on the surface of a sphere. A two-dimensional Euclidean space is a two-dimensional space on the plane. The inside of a cube, a cylinder or a sphere is three-dimensional (3D) because three coordinates are needed to locate a point within these spaces.

In classical mechanics, space and time are different categories and refer to absolute space and time. That conception of the world is a four-dimensional space but not the one that was found necessary to describe electromagnetism. The four dimensions (4D) of spacetime consist of events that are not absolutely defined spatially and temporally, but rather are known relative to the motion of an observer. Minkowski space first approximates the universe without gravity; the pseudo-Riemannian manifolds of general relativity describe spacetime with matter and gravity. 10 dimensions are used to describe superstring theory (6D hyperspace + 4D), 11 dimensions can describe supergravity and M-theory (7D hyperspace + 4D), and the state-space of quantum mechanics is an infinite-dimensional function space.

The concept of dimension is not restricted to physical objects. High-dimensional spaces frequently occur in mathematics and the sciences. They may be Euclidean spaces or more general parameter spaces or configuration spaces such as in Lagrangian or Hamiltonian mechanics; these are abstract spaces, independent of the physical space.
$
10
1. How many coordinates are needed to specify a point on a line?
A: One
B: Two
C: Three
D: Four
E: Infinite
Answer: A

2. To locate a point on the surface of a sphere, how many coordinates are required?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: B

3. What dimension does the inside of a cube represent?
A: 1D
B: 2D
C: 3D
D: 4D
E: 5D
Answer: C

4. In classical mechanics, space and time are categorized as:
A: The same and refer to relative space and time.
B: The same and refer to absolute space and time.
C: Different and refer to absolute space and time.
D: Different and refer to relative space and time.
E: Nonexistent.
Answer: C

5. In the context of spacetime related to electromagnetism, how many dimensions are necessary?
A: 1D
B: 2D
C: 3D
D: 4D
E: 5D
Answer: D

6. The pseudo-Riemannian manifolds of general relativity are used to describe:
A: Spacetime without gravity.
B: Spacetime with only matter.
C: Spacetime with both matter and gravity.
D: The universe without electromagnetism.
E: The universe in 10 dimensions.
Answer: C

7. How many dimensions are used in superstring theory?
A: 4D
B: 6D
C: 7D
D: 10D
E: 11D
Answer: D

8. The state-space of quantum mechanics is:
A: A one-dimensional function space.
B: A three-dimensional function space.
C: A finite-dimensional function space.
D: An infinite-dimensional function space.
E: A two-dimensional Euclidean space.
Answer: D

9. High-dimensional spaces that occur in mathematics and the sciences are often:
A: Restricted to physical objects only.
B: Only Euclidean spaces.
C: Only abstract spaces.
D: More general parameter spaces or configuration spaces.
E: Only related to classical mechanics.
Answer: D

10. In the concept of dimension, what is not a determining factor for an object's dimension?
A: The number of independent parameters needed.
B: The number of coordinates required.
C: The size of the object.
D: The position of a point on the object.
E: The movement constraints on the object.
Answer: C
@
Subject:
Nuclear fusion is a reaction in which two or more atomic nuclei, usually deuterium and tritium (hydrogen variants), combine to form one or more different atomic nuclei and subatomic particles (neutrons or protons). The difference in mass between the reactants and products is manifested as either the release or absorption of energy. This difference in mass arises due to the difference in nuclear binding energy between the atomic nuclei before and after the reaction. Nuclear fusion is the process that powers active or main-sequence stars and other high-magnitude stars, where large amounts of energy are released.

A nuclear fusion process that produces atomic nuclei lighter than iron-56 or nickel-62 will generally release energy. These elements have a relatively small mass and a relatively large binding energy per nucleon. Fusion of nuclei lighter than these releases energy (an exothermic process), while the fusion of heavier nuclei results in energy retained by the product nucleons, and the resulting reaction is endothermic. The opposite is true for the reverse process, called nuclear fission. Nuclear fusion uses lighter elements, such as hydrogen and helium, which are in general more fusible; while the heavier elements, such as uranium, thorium and plutonium, are more fissionable. The extreme astrophysical event of a supernova can produce enough energy to fuse nuclei into elements heavier than iron.

Beam–beam or beam–target fusion
Main article: Colliding beam fusion
Accelerator-based light-ion fusion is a technique using particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions.[23]

Accelerating light ions is relatively easy, and can be done in an efficient manner—requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer; fusion can be observed with as little as 10 kV between the electrodes.[citation needed] The system can be arranged to accelerate ions into a static fuel-infused target, known as beam–target fusion, or by accelerating two streams of ions towards each other, beam–beam fusion.[citation needed] The key problem with accelerator-based fusion (and with cold targets in general) is that fusion cross sections are many orders of magnitude lower than Coulomb interaction cross-sections. Therefore, the vast majority of ions expend their energy emitting bremsstrahlung radiation and the ionization of atoms of the target. Devices referred to as sealed-tube neutron generators are particularly relevant to this discussion. These small devices are miniature particle accelerators filled with deuterium and tritium gas in an arrangement that allows ions of those nuclei to be accelerated against hydride targets, also containing deuterium and tritium, where fusion takes place, releasing a flux of neutrons. Hundreds of neutron generators are produced annually for use in the petroleum industry where they are used in measurement equipment for locating and mapping oil reserves.[citation needed]

A number of attempts to recirculate the ions that "miss" collisions have been made over the years. One of the better-known attempts in the 1970s was Migma, which used a unique particle storage ring to capture ions into circular orbits and return them to the reaction area. Theoretical calculations made during funding reviews pointed out that the system would have significant difficulty scaling up to contain enough fusion fuel to be relevant as a power source. In the 1990s, a new arrangement using a field-reverse configuration (FRC) as the storage system was proposed by Norman Rostoker and continues to be studied by TAE Technologies as of 2021. A closely related approach is to merge two FRC's rotating in opposite directions,[24] which is being actively studied by Helion Energy. Because these approaches all have ion energies well beyond the Coulomb barrier, they often suggest the use of alternative fuel cycles like p-11B that are too difficult to attempt using conventional approaches.[25]
$
10
1. Nuclear fusion in active or main-sequence stars results in:
A: Absorption of large amounts of energy.
B: Release of a small amount of energy.
C: Release of large amounts of energy.
D: Conversion of all matter into energy.
E: Absorption of a small amount of energy.
Answer: C

2. The fusion of atomic nuclei lighter than iron-56 or nickel-62:
A: Is always endothermic.
B: Requires energy to be added.
C: Is always exothermic.
D: Does not produce any energy.
E: Produces elements heavier than uranium.
Answer: C

3. In nuclear fission, which elements are generally more fissionable?
A: Hydrogen and Helium
B: Iron and Nickel
C: Uranium, Thorium, and Plutonium
D: Carbon and Oxygen
E: Sodium and Chlorine
Answer: C

4. Which astrophysical event can produce enough energy to fuse nuclei into elements heavier than iron?
A: Neutron star collision
B: Black hole formation
C: Supernova
D: Solar flares
E: Gamma-ray bursts
Answer: C

5. In the context of accelerator-based fusion, what is a significant problem?
A: Fusion cross sections are many times larger than Coulomb interaction cross-sections.
B: The vast majority of ions produce energy.
C: Fusion cross sections are many orders of magnitude lower than Coulomb interaction cross-sections.
D: It requires extremely high temperatures.
E: It only works with heavy-ion fusion.
Answer: C

6. What do sealed-tube neutron generators produce?
A: Protons
B: Photons
C: Electrons
D: Neutrons
E: Positrons
Answer: D

7. What was the primary issue with Migma as a fusion technique in the 1970s?
A: It had too much fusion fuel.
B: It had difficulty scaling up to contain enough fusion fuel.
C: It produced too much energy.
D: It was too efficient.
E: It could not accelerate ions.
Answer: B

8. Who proposed a new arrangement using a field-reverse configuration (FRC) for fusion in the 1990s?
A: Albert Einstein
B: Michael Faraday
C: Isaac Newton
D: Norman Rostoker
E: Richard Feynman
Answer: D

9. Helion Energy is actively studying which approach related to fusion?
A: Using only neutron generators.
B: Merging two FRC's rotating in the same direction.
C: Using conventional fuel cycles.
D: Merging two FRC's rotating in opposite directions.
E: Relying solely on heavy-ion fusion.
Answer: D

10. Why do some approaches in accelerator-based fusion suggest the use of alternative fuel cycles like p-11B?
A: They have ion energies below the Coulomb barrier.
B: They can fuse easily at room temperature.
C: They are the most abundant fuel on Earth.
D: They have ion energies well beyond the Coulomb barrier.
E: They are the cheapest fuel available.
Answer: D
@
Subject:
In astronomy, the interstellar medium (ISM) is the matter and radiation that exist in the space between the star systems in a galaxy. This matter includes gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills interstellar space and blends smoothly into the surrounding intergalactic space. The energy that occupies the same volume, in the form of electromagnetic radiation, is the interstellar radiation field. Although the density of atoms in the ISM is usually far below that in the best laboratory vacuums, the mean free path between collisions is short compared to typical interstellar lengths, so on these scales the ISM behaves as a gas (more precisely, as a plasma: it is everywhere at least slightly ionized), responding to pressure forces, and not as a collection of non-interacting particles.

The interstellar medium is composed of multiple phases distinguished by whether matter is ionic, atomic, or molecular, and the temperature and density of the matter. The interstellar medium is composed primarily of hydrogen, followed by helium with trace amounts of carbon, oxygen, and nitrogen.[1] The thermal pressures of these phases are in rough equilibrium with one another. Magnetic fields and turbulent motions also provide pressure in the ISM, and are typically more important, dynamically, than the thermal pressure. In the interstellar medium, matter is primarily in molecular form and reaches number densities of 1012 molecules per m3 (1 trillion molecules per m3). In hot, diffuse regions, gas is highly ionized, and the density may be as low as 100 ions per m3. Compare this with a number density of roughly 1025 molecules per m3 for air at sea level, and 1016 molecules per m3 (10 quadrillion molecules per m3) for a laboratory high-vacuum chamber. By mass, 99% of the ISM is gas in any form, and 1% is dust.[2] Of the gas in the ISM, by number 91% of atoms are hydrogen and 8.9% are helium, with 0.1% being atoms of elements heavier than hydrogen or helium,[3] known as "metals" in astronomical parlance. By mass this amounts to 70% hydrogen, 28% helium, and 1.5% heavier elements. The hydrogen and helium are primarily a result of primordial nucleosynthesis, while the heavier elements in the ISM are mostly a result of enrichment (due to stellar nucleosynthesis) in the process of stellar evolution.

The ISM plays a crucial role in astrophysics precisely because of its intermediate role between stellar and galactic scales. Stars form within the densest regions of the ISM, which ultimately contributes to molecular clouds and replenishes the ISM with matter and energy through planetary nebulae, stellar winds, and supernovae. This interplay between stars and the ISM helps determine the rate at which a galaxy depletes its gaseous content, and therefore its lifespan of active star formation.

Voyager 1 reached the ISM on August 25, 2012, making it the first artificial object from Earth to do so. Interstellar plasma and dust will be studied until the estimated mission end date of 2025. Its twin Voyager 2 entered the ISM on November 5, 2018.[4]
$
10
1. Which of the following best describes the behavior of the interstellar medium (ISM) on typical interstellar scales?
A: It behaves as a collection of non-interacting particles.
B: It behaves as a solid due to high densities.
C: It behaves as a liquid due to gravitational forces.
D: It behaves as a gas or plasma due to short mean free paths between collisions.
E: It behaves as a perfect vacuum.
Answer: D

2. What is the primary element found in the interstellar medium?
A: Helium
B: Oxygen
C: Nitrogen
D: Hydrogen
E: Carbon
Answer: D

3. Which of the following provides pressure in the ISM and is typically more dynamically significant than thermal pressure?
A: Radiative forces
B: Gravitational forces
C: Tidal forces
D: Magnetic fields and turbulent motions
E: Cosmic winds
Answer: D

4. By mass, how much of the ISM is gas?
A: 50%
B: 70%
C: 91%
D: 99%
E: 100%
Answer: D

5. In astronomical parlance, elements heavier than hydrogen or helium are referred to as:
A: Isotopes
B: Radionuclides
C: Alkalines
D: Metals
E: Heavy gases
Answer: D

6. How does the ISM contribute to the lifecycle of a galaxy?
A: It blocks all stellar radiation, preventing further star formation.
B: It helps determine the rate at which a galaxy depletes its gaseous content and its lifespan of active star formation.
C: It remains static and doesn't interact with stars or galaxies.
D: It accelerates the movement of galaxies away from each other.
E: It causes stars to collide frequently, leading to galactic disruption.
Answer: B

7. Which of the following best describes the relationship between stars and the ISM?
A: Stars repel the ISM, preventing any interaction.
B: Stars absorb the ISM to gain energy.
C: Stars form within the densest regions of the ISM and later replenish it with matter and energy.
D: Stars and the ISM do not have any interaction.
E: The ISM deteriorates stars over time, reducing their lifespan.
Answer: C

8. When did Voyager 1 reach the interstellar medium?
A: August 25, 2008
B: November 5, 2018
C: January 1, 2000
D: August 25, 2012
E: December 31, 1999
Answer: D

9. By what year is it estimated that Voyager 1 will end its mission to study the interstellar plasma and dust?
A: 2012
B: 2018
C: 2020
D: 2025
E: 2030
Answer: D

10. Which of the following is NOT a phase of matter in the interstellar medium?
A: Ionic
B: Molecular
C: Atomic
D: Liquid
E: Dust
Answer: D
@
Subject:
Resistive random-access memory (ReRAM or RRAM) is a type of non-volatile (NV) random-access (RAM) computer memory that works by changing the resistance across a dielectric solid-state material, often referred to as a memristor.

Experimental tests
Chua suggested experimental tests to determine if a device may properly be categorized as a memristor:[2]

The Lissajous curve in the voltage-current plane is a pinched hysteresis loop when driven by any bipolar periodic voltage or current without respect to initial conditions.
The area of each lobe of the pinched hysteresis loop shrinks as the frequency of the forcing signal increases.
As the frequency tends to infinity, the hysteresis loop degenerates to a straight line through the origin, whose slope depends on the amplitude and shape of the forcing signal.
According to Chua[45][46] all resistive switching memories including ReRAM, MRAM and phase-change memory meet these criteria and are memristors. However, the lack of data for the Lissajous curves over a range of initial conditions or over a range of frequencies complicates assessments of this claim.

Experimental evidence shows that redox-based resistance memory (ReRAM) includes a nanobattery effect that is contrary to Chua's memristor model. This indicates that the memristor theory needs to be extended or corrected to enable accurate ReRAM modeling.[25]

Pinched hysteresis
One of the resulting properties of memristors and memristive systems is the existence of a pinched hysteresis effect.[47] For a current-controlled memristive system, the input u(t) is the current i(t), the output y(t) is the voltage v(t), and the slope of the curve represents the electrical resistance. The change in slope of the pinched hysteresis curves demonstrates switching between different resistance states which is a phenomenon central to ReRAM and other forms of two-terminal resistance memory. At high frequencies, memristive theory predicts the pinched hysteresis effect will degenerate, resulting in a straight line representative of a linear resistor. It has been proven that some types of non-crossing pinched hysteresis curves (denoted Type-II) cannot be described by memristors.[48]
$
10
1. What is the primary function of Resistive random-access memory (ReRAM)?
A: It changes resistance across a memristor to store data.
B: It improves the speed of processing data in a CPU.
C: It uses magnetic fields to read and write data.
D: It reduces the power consumption of computers by switching between resistance states.
E: It uses quantum principles to perform computation tasks.
Answer: A

2. As the frequency of the forcing signal in a memristor increases, what happens to the area of each lobe of the pinched hysteresis loop?
A: It remains constant.
B: It enlarges.
C: It becomes more circular.
D: It shrinks.
E: It oscillates.
Answer: D

3. At very high frequencies, the pinched hysteresis effect in a memristive system will:
A: Become more pinched.
B: Oscillate uncontrollably.
C: Demonstrate switching between various resistance states.
D: Show a curve that spans multiple quadrants.
E: Degenerate to a straight line, resembling a linear resistor.
Answer: E

4. Which memory type(s) are considered to be memristors, according to Chua?
A: Only ReRAM
B: Only MRAM
C: Only phase-change memory
D: All of ReRAM, MRAM, and phase-change memory
E: None of the above
Answer: D

5. Experimental evidence indicates that redox-based resistance memory (ReRAM) has what effect that is contrary to Chua's memristor model?
A: Magnetic resistance
B: Photovoltaic effect
C: Nanobattery effect
D: Superconducting effect
E: Quantum tunneling effect
Answer: C

6. For a current-controlled memristive system, what is the input and what is the output?
A: Input: voltage v(t), Output: current i(t)
B: Input: current i(t), Output: voltage v(t)
C: Input: resistance r(t), Output: current i(t)
D: Input: voltage v(t), Output: resistance r(t)
E: Input: current i(t), Output: resistance r(t)
Answer: B

7. Which statement best describes Type-II non-crossing pinched hysteresis curves?
A: They can be easily described by memristors.
B: They are the most common type of pinched hysteresis curves.
C: They cannot be described by memristors.
D: They represent the nanobattery effect in ReRAM.
E: They are primarily used in MRAM devices.
Answer: C

8. What characteristic shape is seen in the voltage-current plane when a device is driven by any bipolar periodic voltage or current?
A: A linear path
B: An oscillating wave
C: A pinched hysteresis loop
D: A circular loop
E: A square wave
Answer: C

9. As the frequency tends to infinity in the context of a memristor, what happens to the hysteresis loop?
A: It expands outward indefinitely.
B: It rotates in a clockwise direction.
C: It becomes a straight line through the origin.
D: It starts oscillating in a chaotic pattern.
E: It turns into a circular loop.
Answer: C

10. What happens to ReRAM when there's a change in the slope of the pinched hysteresis curves?
A: It indicates a malfunction in the ReRAM.
B: It demonstrates the presence of a nanobattery effect.
C: It shows a switching between different resistance states.
D: It indicates the transition to a Type-II non-crossing curve.
E: It shows that the device is now a linear resistor.
Answer: C
@
Subject:
In mathematical physics, geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization, for which there is in general no exact recipe, in such a way that certain analogies between the classical theory and the quantum theory remain manifest. For example, the similarity between the Heisenberg equation in the Heisenberg picture of quantum mechanics and the Hamilton equation in classical physics should be built in.

Origins
One of the earliest attempts at a natural quantization was Weyl quantization, proposed by Hermann Weyl in 1927. Here, an attempt is made to associate a quantum-mechanical observable (a self-adjoint operator on a Hilbert space) with a real-valued function on classical phase space. The position and momentum in this phase space are mapped to the generators of the Heisenberg group, and the Hilbert space appears as a group representation of the Heisenberg group. In 1946, H. J. Groenewold considered the product of a pair of such observables and asked what the corresponding function would be on the classical phase space.[1] This led him to discover the phase-space star-product of a pair of functions.

The modern theory of geometric quantization was developed by Bertram Kostant and Jean-Marie Souriau in the 1970s. One of the motivations of the theory was to understand and generalize Kirillov's orbit method in representation theory.

Types
The geometric quantization procedure falls into the following three steps: prequantization, polarization, and metaplectic correction. Prequantization produces a natural Hilbert space together with a quantization procedure for observables that exactly transforms Poisson brackets on the classical side into commutators on the quantum side. Nevertheless, the prequantum Hilbert space is generally understood to be "too big".[2] The idea is that one should then select a Poisson-commuting set of n variables on the 2n-dimensional phase space and consider functions (or, more properly, sections) that depend only on these n variables. The n variables can be either real-valued, resulting in a position-style Hilbert space, or complex analytic, producing something like the Segal–Bargmann space.[a] A polarization is a coordinate-independent description of such a choice of n Poisson-commuting functions. The metaplectic correction (also known as the half-form correction) is a technical modification of the above procedure that is necessary in the case of real polarizations and often convenient for complex polarizations.
$
10
1. Who proposed Weyl quantization, one of the earliest attempts at natural quantization?
A: Bertram Kostant
B: H. J. Groenewold
C: Hermann Weyl
D: Jean-Marie Souriau
E: Heisenberg
Answer: C

2. What is the primary purpose of geometric quantization?
A: To define classical mechanics based on a given quantum theory.
B: To find the best representation of the Heisenberg group.
C: To determine the relationship between position and momentum in quantum mechanics.
D: To define a quantum theory based on a given classical theory.
E: To rewrite classical physics equations in terms of quantum physics.
Answer: D

3. In Weyl quantization, the position and momentum in phase space are mapped to:
A: The Hamilton equation.
B: The generators of the Heisenberg group.
C: The Hilbert space of quantum mechanics.
D: The metaplectic correction.
E: The Segal–Bargmann space.
Answer: B

4. What did H. J. Groenewold discover when he considered the product of a pair of quantum-mechanical observables?
A: The Heisenberg equation
B: Kirillov's orbit method
C: The phase-space star-product of a pair of functions
D: The polarization procedure in geometric quantization
E: Weyl quantization process
Answer: C

5. Geometric quantization was modernly developed by which two individuals in the 1970s?
A: Hermann Weyl and H. J. Groenewold
B: Heisenberg and Hamilton
C: Bertram Kostant and Jean-Marie Souriau
D: Segal and Bargmann
E: Kirillov and Groenewold
Answer: C

6. Which of the following is NOT one of the three steps in the geometric quantization procedure?
A: Metaplectic multiplication
B: Prequantization
C: Polarization
D: Weyl transformation
E: Metaplectic correction
Answer: D

7. The prequantum Hilbert space in geometric quantization is often seen as:
A: Nonexistent
B: Perfectly sized
C: Too small
D: Too big
E: Balanced
Answer: D

8. What is a polarization in the context of geometric quantization?
A: It is the process of aligning quantum states.
B: It describes a set of n variables that depend only on a chosen set of n Poisson-commuting functions.
C: It is a technique to shift from classical to quantum physics.
D: It represents the space of all possible quantum states.
E: It is a correction method applied after quantization.
Answer: B

9. The metaplectic correction in geometric quantization is necessary in the case of:
A: Complex polarizations
B: Kirillov's orbit method
C: Quantum polarizations
D: Real polarizations
E: Heisenberg group representation
Answer: D

10. Which analogy between classical and quantum theory does geometric quantization attempt to maintain?
A: The equivalence of quantum and classical energy levels.
B: The relationship between the Heisenberg equation in quantum mechanics and the Hamilton equation in classical physics.
C: The transform of Kirillov's orbit method.
D: The relationship between phase space and Hilbert space.
E: The polarization of quantum states in a given system.
Answer: B
@
Subject:
In geometry, an improper rotation[1] (also called rotation-reflection,[2] rotoreflection,[1] rotary reflection,[3] or rotoinversion[4]) is an isometry in Euclidean space that is a combination of a rotation about an axis and a reflection in a plane perpendicular to that axis. Reflection and inversion are each special case of improper rotation. Any improper rotation is an affine transformation and, in cases that keep the coordinate origin fixed, a linear transformation.[5] It is used as a symmetry operation in the context of geometric symmetry, molecular symmetry and crystallography, where an object that is unchanged by a combination of rotation and reflection is said to have improper rotation symmetry.

In 3 dimensions, improper rotation is equivalently defined as a combination of rotation about an axis and inversion in a point on the axis.[1] For this reason it is also called a rotoinversion or rotary inversion. The two definitions are equivalent because rotation by an angle θ followed by reflection is the same transformation as rotation by θ + 180° followed by inversion (taking the point of inversion to be in the plane of reflection). In both definitions, the operations commute.

A three-dimensional symmetry that has only one fixed point is necessarily an improper rotation.[3]

An improper rotation of an object thus produces a rotation of its mirror image. The axis is called the rotation-reflection axis.[6] This is called an n-fold improper rotation if the angle of rotation, before or after reflexion, is 360°/n (where n must be even).[6]
$
10
1. In geometry, which of the following is NOT another term for improper rotation?
A: Rotoreflection
B: Rotary inversion
C: Rotoscaling
D: Rotoinversion
E: Rotation-reflection
Answer: C

2. What operations are equivalent in the 3-dimensional definition of improper rotation?
A: Rotation by θ and inversion
B: Rotation by θ + 180° and inversion
C: Reflection and rotation by θ
D: Reflection and rotation by θ + 90°
E: Rotation by θ and reflection
Answer: B

3. In the context of geometric symmetry, an object unchanged by a combination of rotation and reflection has:
A: Rotation symmetry
B: Proper rotation symmetry
C: Improper rotation symmetry
D: Inverse rotation symmetry
E: Mirror symmetry
Answer: C

4. A three-dimensional symmetry that has only one fixed point is:
A: Always a proper rotation
B: Never an improper rotation
C: Always an improper rotation
D: Sometimes a reflection
E: Sometimes an inversion
Answer: C

5. Which statement best describes the result of an improper rotation on an object?
A: It produces a reflection of its original position.
B: It rotates its mirror image without any reflection.
C: It inverts the object without any rotation.
D: It produces a rotation of its mirror image.
E: It produces a pure rotation without changing any symmetry.
Answer: D

6. If an object undergoes an n-fold improper rotation, what is the angle of rotation, before or after reflection?
A: 180°/n
B: θ + 180°
C: 360°/n
D: 360°/(n+1)
E: 180°/(n+1)
Answer: C

7. The axis in an improper rotation, which produces a rotation of the object's mirror image, is referred to as:
A: Reflexion axis
B: Mirror axis
C: Rotoinversion axis
D: Rotary inversion axis
E: Rotation-reflection axis
Answer: E

8. In improper rotation, the operations of rotation and reflection:
A: Always conflict and produce unpredictable results.
B: Never produce a net change in the object's position.
C: Are dependent on the angle of rotation.
D: Commute.
E: Produce two distinct objects.
Answer: D

9. Which of the following is a special case of improper rotation?
A: Translation
B: Scaling
C: Inversion
D: Twisting
E: Bending
Answer: C

10. Reflection and rotation by an angle θ followed by inversion produces a transformation equivalent to:
A: Reflection followed by rotation by θ
B: Rotation by θ followed by reflection
C: Rotation by θ + 90° followed by inversion
D: Rotation by θ + 180° followed by inversion
E: Rotation by 2θ followed by reflection
Answer: D
@
Subject:
Surface power density
In physics and engineering, surface power density is power per unit area.

Applications
The intensity of electromagnetic radiation can be expressed in W/m2. An example of such a quantity is the solar constant.
Wind turbines are often compared using a specific power measuring watts per square meter of turbine disk area, which is 
2
\pi r^{2}, where r is the length of a blade. This measure is also commonly used for solar panels, at least for typical applications.
Radiance is surface power density per unit of solid angle (steradians) in a specific direction. Spectral radiance is radiance per unit of frequency (Hertz) at a specific frequency.
Surface power densities of energy sources
Surface power density is an important factor in comparison of industrial energy sources.[1] The concept was popularised by geographer Vaclav Smil. The term is usually shortened to "power density" in the relevant literature, which can lead to confusion with homonymous or related terms.

Measured in W/m2 it describes the amount of power obtained per unit of Earth surface area used by a specific energy system, including all supporting infrastructure, manufacturing, mining of fuel (if applicable) and decommissioning.[2],[3] Fossil fuels and nuclear power are characterized by high power density which means large power can be drawn from power plants occupying relatively small area. Renewable energy sources have power density at least three orders of magnitude smaller and for the same energy output they need to occupy accordingly larger area, which has been already highlighted as a limiting factor of renewable energy in German Energiewende.[4]

The following table shows median surface power density of renewable and non-renewable energy sources.[5]

Energy source	Median PD
[W/m2]

Fossil gas	482.10
Nuclear power	240.81
Oil	194.61
Coal	135.10
Solar power	6.63
Geothermal	2.24
Wind power	1.84
Hydropower	0.14
Biomass	0.08
$
10
1. Surface power density can be used to express:
A: The intensity of sound waves.
B: The speed of electromagnetic waves.
C: The density of a substance.
D: The intensity of electromagnetic radiation.
E: The energy content of a material.
Answer: D

2. Which measurement is commonly used for both wind turbines and solar panels?
A: Specific energy in J/m2
B: Spectral radiance in Hertz
C: Specific power in watts per square meter of turbine disk area
D: Solar constant in W/m2
E: Radiance in steradians
Answer: C

3. Radiance differs from surface power density as it accounts for:
A: Unit of solid angle (steradians) in a specific direction.
B: Unit of frequency (Hertz) in a specific frequency.
C: Watts per square meter of turbine disk area.
D: Intensity of electromagnetic radiation.
E: Solar constant.
Answer: A

4. Vaclav Smil popularized the concept of surface power density for comparing:
A: Sound intensities.
B: Energy efficiencies.
C: Industrial energy sources.
D: Electromagnetic waves.
E: Light brightness.
Answer: C

5. Which energy source from the given table has the highest median power density?
A: Biomass
B: Solar power
C: Wind power
D: Fossil gas
E: Hydropower
Answer: D

6. The main reason why renewable energy sources need to occupy larger areas for the same energy output compared to fossil fuels and nuclear power is because of their:
A: High infrastructure costs.
B: Low power density.
C: High energy losses.
D: Low energy content.
E: Complex manufacturing processes.
Answer: B

7. How is spectral radiance different from radiance?
A: Spectral radiance is radiance per unit of power.
B: Spectral radiance is radiance per unit of frequency at a specific frequency.
C: Spectral radiance is the total energy radiance.
D: Spectral radiance is radiance per unit of solid angle.
E: Spectral radiance is radiance per square meter.
Answer: B

8. Which of the following energy sources has a median power density closest to solar power?
A: Coal
B: Geothermal
C: Nuclear power
D: Wind power
E: Oil
Answer: B

9. An important factor when comparing industrial energy sources in terms of their effect on land usage is:
A: Radiance.
B: Spectral radiance.
C: Surface power density.
D: Solar constant.
E: Specific power.
Answer: C

10. Which of the following energy sources requires the largest surface area for a given amount of energy output?
A: Fossil gas
B: Solar power
C: Nuclear power
D: Coal
E: Biomass
Answer: E
@
Subject:
Frame-dragging is an effect on spacetime, predicted by Albert Einstein's general theory of relativity, that is due to non-static stationary distributions of mass–energy. A stationary field is one that is in a steady state, but the masses causing that field may be non-static ⁠— rotating, for instance. More generally, the subject that deals with the effects caused by mass–energy currents is known as gravitoelectromagnetism, which is analogous to the magnetism of classical electromagnetism.

The first frame-dragging effect was derived in 1918, in the framework of general relativity, by the Austrian physicists Josef Lense and Hans Thirring, and is also known as the Lense–Thirring effect.[1][2][3] They predicted that the rotation of a massive object would distort the spacetime metric, making the orbit of a nearby test particle precess. This does not happen in Newtonian mechanics for which the gravitational field of a body depends only on its mass, not on its rotation. The Lense–Thirring effect is very small – about one part in a few trillion. To detect it, it is necessary to examine a very massive object, or build an instrument that is very sensitive.

In 2015, new general-relativistic extensions of Newtonian rotation laws were formulated to describe geometric dragging of frames which incorporates a newly discovered antidragging effect.[4]

Effects
Rotational frame-dragging (the Lense–Thirring effect) appears in the general principle of relativity and similar theories in the vicinity of rotating massive objects. Under the Lense–Thirring effect, the frame of reference in which a clock ticks the fastest is one which is revolving around the object as viewed by a distant observer. This also means that light traveling in the direction of rotation of the object will move past the massive object faster than light moving against the rotation, as seen by a distant observer. It is now the best known frame-dragging effect, partly thanks to the Gravity Probe B experiment. Qualitatively, frame-dragging can be viewed as the gravitational analog of electromagnetic induction.

Also, an inner region is dragged more than an outer region. This produces interesting locally rotating frames. For example, imagine that a north–south-oriented ice skater, in orbit over the equator of a rotating black hole and rotationally at rest with respect to the stars, extends her arms. The arm extended toward the black hole will be "torqued" spinward due to gravitomagnetic induction ("torqued" is in quotes because gravitational effects are not considered "forces" under GR). Likewise the arm extended away from the black hole will be torqued anti-spinward. She will therefore be rotationally sped up, in a counter-rotating sense to the black hole. This is the opposite of what happens in everyday experience. There exists a particular rotation rate that, should she be initially rotating at that rate when she extends her arms, inertial effects and frame-dragging effects will balance and her rate of rotation will not change. Due to the equivalence principle, gravitational effects are locally indistinguishable from inertial effects, so this rotation rate, at which when she extends her arms nothing happens, is her local reference for non-rotation. This frame is rotating with respect to the fixed stars and counter-rotating with respect to the black hole. This effect is analogous to the hyperfine structure in atomic spectra due to nuclear spin. A useful metaphor is a planetary gear system with the black hole being the sun gear, the ice skater being a planetary gear and the outside universe being the ring gear. See Mach's principle.

Another interesting consequence is that, for an object constrained in an equatorial orbit, but not in freefall, it weighs more if orbiting anti-spinward, and less if orbiting spinward. For example, in a suspended equatorial bowling alley, a bowling ball rolled anti-spinward would weigh more than the same ball rolled in a spinward direction. Note, frame dragging will neither accelerate nor slow down the bowling ball in either direction. It is not a "viscosity". Similarly, a stationary plumb-bob suspended over the rotating object will not list. It will hang vertically. If it starts to fall, induction will push it in the spinward direction.

Linear frame dragging is the similarly inevitable result of the general principle of relativity, applied to linear momentum. Although it arguably has equal theoretical legitimacy to the "rotational" effect, the difficulty of obtaining an experimental verification of the effect means that it receives much less discussion and is often omitted from articles on frame-dragging (but see Einstein, 1921).[5]

Static mass increase is a third effect noted by Einstein in the same paper.[6] The effect is an increase in inertia of a body when other masses are placed nearby. While not strictly a frame dragging effect (the term frame dragging is not used by Einstein), it is demonstrated by Einstein that it derives from the same equation of general relativity. It is also a tiny effect that is difficult to confirm experimentally.
$
6
Which effect predicts the distortion of spacetime metric due to the rotation of a massive object, causing the orbit of a nearby test particle to precess?

A: Einstein's effect
B: Newton's effect
C: Mach's principle
D: Lense–Thirring effect
E: Gravitoelectromagnetism effect
Answer: D
In the context of general relativity, what is gravitoelectromagnetism analogous to in classical electromagnetism?

A: Capacitance
B: Resistance
C: Magnetism
D: Current
E: Voltage
Answer: C
If an object was rotationally at rest with respect to the stars while in orbit over the equator of a rotating black hole, and extends her arms, what will happen due to gravitomagnetic induction?

A: The arms will remain stationary.
B: The arm extended towards the black hole will be "torqued" spinward.
C: The arm extended away from the black hole will be "torqued" anti-spinward.
D: Both B and C.
E: The object will start orbiting the black hole faster.
Answer: D
What is the frame of reference in which a clock ticks the fastest as per the Lense–Thirring effect?

A: One which is stationary to a massive rotating object.
B: One which is moving in the direction of rotation of the object.
C: One which is revolving around the object as viewed by a distant observer.
D: One which is moving against the direction of rotation of the object.
E: One which is distant from any massive object.
Answer: C
How does light behave in the vicinity of a rotating massive object due to the Lense–Thirring effect, as observed by a distant observer?

A: Light traveling in the direction of rotation of the object will move slower than light moving against the rotation.
B: Light traveling in the direction of rotation of the object will move at the same speed as light moving against the rotation.
C: Light traveling in the direction of rotation of the object will move faster than light moving against the rotation.
D: The speed of light is not affected by the rotation of the object.
E: Light cannot travel in the vicinity of a rotating massive object.
Answer: C
What analogy is used to describe frame-dragging effects due to the rotation of a black hole on an ice skater in orbit?

A: Centrifuge experiment
B: Planetary gear system
C: Foucault pendulum
D: Atomic spectra due to nuclear spin
E: Both B and D
Answer: E
@
Subject:
In theoretical physics, explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion (most typically, to the Lagrangian or the Hamiltonian) that do not respect the symmetry. Usually this term is used in situations where these symmetry-breaking terms are small, so that the symmetry is approximately respected by the theory. An example is the spectral line splitting in the Zeeman effect, due to a magnetic interaction perturbation in the Hamiltonian of the atoms involved.

Explicit symmetry breaking differs from spontaneous symmetry breaking. In the latter, the defining equations respect the symmetry but the ground state (vacuum) of the theory breaks it.[1]

Explicit symmetry breaking is also associated with electromagnetic radiation. A system of accelerated charges results in electromagnetic radiation when the geometric symmetry of the electric field in free space is explicitly broken by the associated electrodynamic structure under time varying excitation of the given system. This is quite evident in an antenna where the electric lines of field curl around or have rotational geometry around the radiating terminals in contrast to linear geometric orientation within a pair of transmission lines which does not radiate even under time varying excitation.[2]
$
5
How does explicit symmetry breaking differentiate from spontaneous symmetry breaking?

A: In explicit symmetry breaking, the ground state breaks the symmetry while the defining equations respect it.
B: In explicit symmetry breaking, the defining equations and the ground state both break the symmetry.
C: In explicit symmetry breaking, the ground state respects the symmetry while the defining equations break it.
D: In explicit symmetry breaking, the defining equations break the symmetry while the ground state respects it.
E: In explicit symmetry breaking, neither the defining equations nor the ground state affect the symmetry.
Answer: D
Which effect provides an example of explicit symmetry breaking due to a magnetic interaction perturbation in the Hamiltonian of the atoms?

A: The Compton effect
B: The Doppler effect
C: The Zeeman effect
D: The Stark effect
E: The Pauli exclusion principle
Answer: C
Explicit symmetry breaking is associated with what type of radiation?

A: Gravitational radiation
B: Ultraviolet radiation
C: Gamma radiation
D: Infrared radiation
E: Electromagnetic radiation
Answer: E
In what situation does electromagnetic radiation result from a system of accelerated charges in relation to explicit symmetry breaking?

A: When the geometric symmetry of the electric field in free space respects the associated electrodynamic structure.
B: When the electric lines of field have a linear geometric orientation within a pair of transmission lines.
C: When the geometric symmetry of the electric field in free space is explicitly broken by the associated electrodynamic structure under time varying excitation.
D: When the geometric symmetry of the electric field remains unaltered in the presence of an external electric field.
E: When the electric lines of field are stationary and do not exhibit any geometry.
Answer: C
Why do transmission lines not radiate even under time varying excitation?

A: Because the electric lines of field curl around the radiating terminals.
B: Because the electric lines of field have rotational geometry around the radiating terminals.
C: Because the electric lines of field have a linear geometric orientation within them.
D: Because the electric lines of field are always stationary within transmission lines.
E: Because the transmission lines do not allow for time varying excitations.
Answer: C
@
Subject:
The Higgs boson, sometimes called the Higgs particle,[9][10] is an elementary particle in the Standard Model of particle physics produced by the quantum excitation of the Higgs field,[11][12] one of the fields in particle physics theory.[12] In the Standard Model, the Higgs particle is a massive scalar boson with zero spin, even (positive) parity, no electric charge, and no colour charge that couples to (interacts with) mass.[13] It is also very unstable, decaying into other particles almost immediately upon generation.

The Higgs field is a scalar field with two neutral and two electrically charged components that form a complex doublet of the weak isospin SU(2) symmetry. Its "Mexican hat-shaped" potential leads it to take a nonzero value everywhere (including otherwise empty space), which breaks the weak isospin symmetry of the electroweak interaction and, via the Higgs mechanism, gives mass to many particles.

Both the field and the boson are named after physicist Peter Higgs, who in 1964, along with five other scientists in three teams, proposed the Higgs mechanism, a way for some particles to acquire mass. (All fundamental particles known at the time[c] should be massless at very high energies, but fully explaining how some particles gain mass at lower energies had been extremely difficult.) If these ideas were correct, a particle known as a scalar boson should also exist (with certain properties). This particle was called the Higgs boson and could be used to test whether the Higgs field was the correct explanation.

After a 40 year search, a subatomic particle with the expected properties was discovered in 2012 by the ATLAS and CMS experiments at the Large Hadron Collider (LHC) at CERN near Geneva, Switzerland. The new particle was subsequently confirmed to match the expected properties of a Higgs boson. Physicists from two of the three teams, Peter Higgs and François Englert, were awarded the Nobel Prize in Physics in 2013 for their theoretical predictions. Although Higgs's name has come to be associated with this theory, several researchers between about 1960 and 1972 independently developed different parts of it.

In the mainstream media, the Higgs boson is sometimes called the "God particle" after the 1993 book The God Particle by Nobel Laureate Leon Lederman,[14] although the nickname has been criticised by many physicists.[15][16]
$
5
Which field is the Higgs boson an excitation of?

A: Electromagnetic field
B: Gravitational field
C: Higgs field
D: Quantum field
E: Strong nuclear field
Answer: C
What are the primary characteristics of the Higgs boson in the Standard Model?

A: Zero spin, negative parity, electric charge, and colour charge.
B: Half spin, even (positive) parity, no electric charge, and colour charge.
C: Zero spin, even (positive) parity, no electric charge, and no colour charge.
D: Half spin, no parity, electric charge, and no colour charge.
E: Zero spin, odd parity, electric charge, and colour charge.
Answer: C
How is the weak isospin symmetry of the electroweak interaction affected by the Higgs field?

A: The Higgs field enhances the weak isospin symmetry.
B: The Higgs field preserves the weak isospin symmetry.
C: The Higgs field breaks the weak isospin symmetry.
D: The Higgs field has no effect on the weak isospin symmetry.
E: The Higgs field reverses the weak isospin symmetry.
Answer: C
For what theoretical predictions regarding the Higgs mechanism were Peter Higgs and François Englert awarded the Nobel Prize in Physics in 2013?

A: The discovery of the Higgs field.
B: The proposal of the Higgs mechanism.
C: The experimental detection of the Higgs boson.
D: The development of the Large Hadron Collider.
E: The mathematical foundation of quantum mechanics.
Answer: B
Why is the Higgs boson sometimes referred to as the "God particle"?

A: Because it is the fundamental particle of the universe.
B: Due to its association with the beginning of the universe.
C: After the book "The God Particle" by Nobel Laureate Leon Lederman.
D: Because it has omnipotent properties in particle physics.
E: Peter Higgs, its discoverer, coined the term.
Answer: C
@
Subject:
In relativistic physics, Lorentz symmetry or Lorentz invariance, named after the Dutch physicist Hendrik Lorentz, is an equivalence of observation or observational symmetry due to special relativity implying that the laws of physics stay the same for all observers that are moving with respect to one another within an inertial frame. It has also been described as "the feature of nature that says experimental results are independent of the orientation or the boost velocity of the laboratory through space".[1]

Lorentz covariance, a related concept, is a property of the underlying spacetime manifold. Lorentz covariance has two distinct, but closely related meanings:

A physical quantity is said to be Lorentz covariant if it transforms under a given representation of the Lorentz group. According to the representation theory of the Lorentz group, these quantities are built out of scalars, four-vectors, four-tensors, and spinors. In particular, a Lorentz covariant scalar (e.g., the space-time interval) remains the same under Lorentz transformations and is said to be a Lorentz invariant (i.e., they transform under the trivial representation).
An equation is said to be Lorentz covariant if it can be written in terms of Lorentz covariant quantities (confusingly, some use the term invariant here). The key property of such equations is that if they hold in one inertial frame, then they hold in any inertial frame; this follows from the result that if all the components of a tensor vanish in one frame, they vanish in every frame. This condition is a requirement according to the principle of relativity; i.e., all non-gravitational laws must make the same predictions for identical experiments taking place at the same spacetime event in two different inertial frames of reference.
On manifolds, the words covariant and contravariant refer to how objects transform under general coordinate transformations. Both covariant and contravariant four-vectors can be Lorentz covariant quantities.

Local Lorentz covariance, which follows from general relativity, refers to Lorentz covariance applying only locally in an infinitesimal region of spacetime at every point. There is a generalization of this concept to cover Poincaré covariance and Poincaré invariance.
$
5
Which physicist is Lorentz symmetry named after?

A: Albert Einstein
B: Isaac Newton
C: Niels Bohr
D: Hendrik Lorentz
E: Richard Feynman
Answer: D
How is a Lorentz covariant scalar, like the space-time interval, affected under Lorentz transformations?

A: It transforms under a special representation of the Lorentz group.
B: It changes according to the relative velocity of the observer.
C: It remains the same.
D: It reverses in direction.
E: It becomes zero.
Answer: C
What does it mean for an equation to be Lorentz covariant?

A: It is only true in certain inertial frames.
B: It holds true for one inertial frame but not necessarily for others.
C: It cannot be written in terms of Lorentz covariant quantities.
D: It holds in one inertial frame and, therefore, holds in any inertial frame.
E: It is always zero regardless of the frame.
Answer: D
In the context of manifolds, what do the terms covariant and contravariant refer to?

A: The curvature of spacetime.
B: How objects transform under general coordinate transformations.
C: The spacetime interval.
D: The boost velocity of a laboratory.
E: The representation of the Lorentz group.
Answer: B
What does local Lorentz covariance refer to in the context of general relativity?

A: Lorentz covariance applying to the entire universe.
B: Lorentz covariance applying only locally in an infinitesimal region of spacetime at every point.
C: Lorentz covariance being a global property of all spacetime.
D: Lorentz covariance applying to only massive objects.
E: Lorentz covariance being invariant under all coordinate transformations.
Answer: B
@
Subject:
In cosmology, baryon acoustic oscillations (BAO) are fluctuations in the density of the visible baryonic matter (normal matter) of the universe, caused by acoustic density waves in the primordial plasma of the early universe. In the same way that supernovae provide a "standard candle" for astronomical observations,[1] BAO matter clustering provides a "standard ruler" for length scale in cosmology.[2] The length of this standard ruler is given by the maximum distance the acoustic waves could travel in the primordial plasma before the plasma cooled to the point where it became neutral atoms (the epoch of recombination), which stopped the expansion of the plasma density waves, "freezing" them into place. The length of this standard ruler (≈490 million light years in today's universe[3]) can be measured by looking at the large scale structure of matter using astronomical surveys.[3] BAO measurements help cosmologists understand more about the nature of dark energy (which causes the accelerating expansion of the universe) by constraining cosmological parameters.[2]

BAO signal in the Sloan Digital Sky Survey
The Sloan Digital Sky Survey (SDSS) is a major multi-spectral imaging and spectroscopic redshift survey using the dedicated 2.5-metre wide-angle SDSS optical telescope at Apache Point Observatory in New Mexico. The goal of this five-year survey was to take images and spectra of millions of celestial objects. The result of compiling the SDSS data is a three-dimensional map of objects in the nearby universe: the SDSS catalog. The SDSS catalog provides a picture of the distribution of matter in a large enough portion of the universe that one can search for a BAO signal by noting whether there is a statistically significant overabundance of galaxies separated by the predicted sound horizon distance.

The SDSS team looked at a sample of 46,748 luminous red galaxies (LRGs), over 3,816 square-degrees of sky (approximately five billion light years in diameter) and out to a redshift of z = 0.47.[3] They analyzed the clustering of these galaxies by calculating a two-point correlation function on the data.[12] The correlation function (ξ) is a function of comoving galaxy separation distance (s) and describes the probability that one galaxy will be found within a given distance of another.[13] One would expect a high correlation of galaxies at small separation distances (due to the clumpy nature of galaxy formation) and a low correlation at large separation distances. The BAO signal would show up as a bump in the correlation function at a comoving separation equal to the sound horizon. This signal was detected by the SDSS team in 2005.[3][14] SDSS confirmed the WMAP results that the sound horizon is ~150 Mpc in today's universe.[2][3]
$
5
What causes baryon acoustic oscillations (BAOs) in the universe?

A: Gravitational waves from black holes.
B: The motion of galaxies in galaxy clusters.
C: Acoustic density waves in the primordial plasma of the early universe.
D: Magnetic field fluctuations in the current universe.
E: Cosmic microwave background radiation from the Big Bang.
Answer: C
Why do BAO matter clustering serve as a "standard ruler" in cosmology?

A: Because they provide a consistent way to measure time in the universe.
B: They provide a consistent brightness to measure distances to galaxies.
C: They help to understand the rotational speed of galaxies.
D: They provide a consistent length scale that can be used for measurements.
E: Because they provide a constant background noise.
Answer: D
At what point were the expansion of plasma density waves "frozen" into place?

A: During the Big Bang.
B: After the formation of the first galaxies.
C: During the epoch of recombination when plasma cooled to become neutral atoms.
D: During the inflationary period of the universe.
E: During the era of galaxy mergers.
Answer: C
What was a significant achievement of the Sloan Digital Sky Survey (SDSS) in relation to BAOs?

A: It provided a two-dimensional map of the universe's temperature.
B: It determined the nature of dark energy.
C: It detected the BAO signal by noting an overabundance of galaxies separated by the predicted sound horizon distance.
D: It established the average speed of galaxies in the universe.
E: It confirmed the nature of cosmic microwave background radiation.
Answer: C
What does the correlation function (ξ) in the context of BAOs describe?

A: The rate at which galaxies merge over time.
B: The probability that one galaxy will be found within a given distance of another.
C: The temperature variation between different galaxies.
D: The speed at which galaxies are moving away from each other.
E: The brightness variation among galaxies.
Answer: B
@
Subject:
Electronic entropy is the entropy of a system attributable to electrons' probabilistic occupation of states. This entropy can take a number of forms. The first form can be termed a density of states based entropy. The Fermi–Dirac distribution implies that each eigenstate of a system, i, is occupied with a certain probability, pi. As the entropy is given by a sum over the probabilities of occupation of those states, there is an entropy associated with the occupation of the various electronic states. In most molecular systems, the energy spacing between the highest occupied molecular orbital and the lowest unoccupied molecular orbital is usually large, and thus the probabilities associated with the occupation of the excited states are small. Therefore, the electronic entropy in molecular systems can safely be neglected. Electronic entropy is thus most relevant for the thermodynamics of condensed phases, where the density of states at the Fermi level can be quite large, and the electronic entropy can thus contribute substantially to thermodynamic behavior.[1][2] A second form of electronic entropy can be attributed to the configurational entropy associated with localized electrons and holes.[3] This entropy is similar in form to the configurational entropy associated with the mixing of atoms on a lattice.

Electronic entropy can substantially modify phase behavior, as in lithium ion battery electrodes,[3] high temperature superconductors,[4][5] and some perovskites.[6] It is also the driving force for the coupling of heat and charge transport in thermoelectric materials, via the Onsager reciprocal relations.[7]

Application to different materials classes
Insulators have zero density of states at the Fermi level due to their band gaps. Thus, the density of states-based electronic entropy is essentially zero in these systems.

Metals have non-zero density of states at the Fermi level. Metals with free-electron-like band structures (e.g. alkali metals, alkaline earth metals, Cu, and Al) generally exhibit relatively low density of states at the Fermi level, and therefore exhibit fairly low electronic entropies. Transition metals, wherein the flat d-bands lie close to the Fermi level, generally exhibit much larger electronic entropies than the free-electron like metals.

Oxides have particularly flat band structures and thus can exhibit large n(EF), if the Fermi level intersects these bands. As most oxides are insulators, this is generally not the case. However, when oxides are metallic (i.e. the Fermi level lies within an unfilled, flat set of bands), oxides exhibit some of the largest electronic entropies of any material.

Thermoelectric materials are specifically engineered to have large electronic entropies. The thermoelectric effect relies on charge carriers exhibiting large entropies, as the driving force to establish a gradient in electrical potential is driven by the entropy associated with the charge carriers. In the thermoelectric literature, the term band structure engineering refers to the manipulation of material structure and chemistry to achieve a high density of states near the Fermi level. More specifically, thermoelectric materials are intentionally doped to exhibit only partially filled bands at the Fermi level, resulting in high electronic entropies.[9] Instead of engineering band filling, one may also engineer the shape of the band structure itself via introduction of nanostructures or quantum wells to the materials.[10][11][12][13]
$
5
Question 1: How does electronic entropy impact the thermodynamics of condensed phases?
A: Electronic entropy contributes minimally as the energy spacing between the highest occupied molecular orbital and the lowest unoccupied molecular orbital is usually large.
B: Electronic entropy plays no role in the thermodynamics of condensed phases.
C: Electronic entropy is most relevant for the thermodynamics of condensed phases due to a potentially large density of states at the Fermi level.
D: Electronic entropy affects only the molecular systems and has no impact on condensed phases.
E: Electronic entropy is determined by the configurational entropy associated with the mixing of atoms on a lattice.

Answer: C

Question 2: Which materials are specifically designed to have large electronic entropies?
A: Transition metals
B: Insulators
C: Oxides
D: Metals with free-electron-like band structures
E: Thermoelectric materials

Answer: E

Question 3: What can be inferred about electronic entropy associated with the majority of oxides?
A: Most oxides exhibit high electronic entropies as they are primarily metallic.
B: Most oxides are insulators, so they generally do not have a Fermi level that intersects the flat bands, making their electronic entropy relatively low.
C: Oxides always have a Fermi level that intersects flat bands, resulting in high electronic entropy.
D: The electronic entropy of oxides is primarily determined by the configurational entropy associated with localized electrons and holes.
E: Oxides are specifically engineered to have zero density of states at the Fermi level.

Answer: B

Question 4: How is electronic entropy in thermoelectric materials achieved through band structure engineering?
A: By ensuring fully filled bands at the Fermi level.
B: By engineering band filling to ensure partially filled bands at the Fermi level.
C: By ensuring zero density of states at the Fermi level.
D: By removing all charge carriers in the material.
E: By ensuring flat band structures regardless of the Fermi level.

Answer: B

Question 5: Which group of metals typically has a larger electronic entropy due to their relationship with the Fermi level?
A: Alkali metals and alkaline earth metals
B: Metals with free-electron-like band structures
C: Transition metals with flat d-bands close to the Fermi level
D: Metals that are insulators
E: Metals that do not have a Fermi level

Answer: C
@
Subject:
In chemistry, molecular symmetry describes the symmetry present in molecules and the classification of these molecules according to their symmetry. Molecular symmetry is a fundamental concept in chemistry, as it can be used to predict or explain many of a molecule's chemical properties, such as whether or not it has a dipole moment, as well as its allowed spectroscopic transitions. To do this it is necessary to use group theory. This involves classifying the states of the molecule using the irreducible representations from the character table of the symmetry group of the molecule. Symmetry is useful in the study of molecular orbitals, with applications to the Hückel method, to ligand field theory, and to the Woodward-Hoffmann rules. Many university level textbooks on physical chemistry, quantum chemistry, spectroscopy and inorganic chemistry discuss symmetry.[1][2][3][4][5][6] Another framework on a larger scale is the use of crystal systems to describe crystallographic symmetry in bulk materials.

There are many techniques for determining the symmetry of a given molecule, including X-ray crystallography and various forms of spectroscopy. Spectroscopic notation is based on symmetry considerations.

Point groups and permutation-inversion groups
The successive application (or composition) of one or more symmetry operations of a molecule has an effect equivalent to that of some single symmetry operation of the molecule. For example, a C2 rotation followed by a σv reflection is seen to be a σv' symmetry operation: σv*C2 = σv'. ("Operation A followed by B to form C" is written BA = C).[9] Moreover, the set of all symmetry operations (including this composition operation) obeys all the properties of a group, given above. So (S,*) is a group, where S is the set of all symmetry operations of some molecule, and * denotes the composition (repeated application) of symmetry operations.

This group is called the point group of that molecule, because the set of symmetry operations leave at least one point fixed (though for some symmetries an entire axis or an entire plane remains fixed). In other words, a point group is a group that summarises all symmetry operations that all molecules in that category have.[9] The symmetry of a crystal, by contrast, is described by a space group of symmetry operations, which includes translations in space.

One can determine the symmetry operations of the point group for a particular molecule by considering the geometrical symmetry of its molecular model. However, when one uses a point group to classify molecular states, the operations in it are not to be interpreted in the same way. Instead the operations are interpreted as rotating and/or reflecting the vibronic (vibration-electronic) coordinates[10] and these operations commute with the vibronic Hamiltonian. They are "symmetry operations" for that vibronic Hamiltonian. The point group is used to classify by symmetry the vibronic eigenstates of a rigid molecule. The symmetry classification of the rotational levels, the eigenstates of the full (rotation-vibration-electronic) Hamiltonian, requires the use of the appropriate permutation-inversion group as introduced by Longuet-Higgins.[11] Point groups describe the geometrical symmetry of a molecule whereas permutation-inversion groups describe the energy-invariant symmetry.
$
10
Question 1: Which concept plays a crucial role in predicting many of a molecule's chemical properties, such as its dipole moment?
A: Crystallography
B: Spectroscopy
C: Molecular symmetry
D: Point group
E: Woodward-Hoffmann rules

Answer: C

Question 2: The use of group theory in molecular symmetry involves classifying the states of the molecule using what?
A: The molecular weight
B: The atomic structure
C: Irreducible representations from the character table of the symmetry group
D: Atomic orbitals of the symmetry group
E: Crystallographic symmetry

Answer: C

Question 3: Symmetry is particularly important for studying what in molecules?
A: Molecular weight
B: Atomic radius
C: Molecular orbitals
D: Nucleus symmetry
E: Atomic rotations

Answer: C

Question 4: What is the difference between point groups and space groups in terms of what they describe?
A: Point groups describe molecular symmetry, while space groups describe atomic symmetry.
B: Point groups describe the geometrical symmetry of a molecule, while space groups describe the symmetry of a crystal including translations in space.
C: Point groups describe the symmetry of a crystal, while space groups describe the geometrical symmetry of a molecule.
D: Point groups summarize all molecules in a category, while space groups classify atoms.
E: Point groups and space groups both describe the geometrical symmetry of a molecule.

Answer: B

Question 5: Which technique is NOT commonly used for determining the symmetry of a molecule?
A: X-ray crystallography
B: UV spectroscopy
C: MRI imaging
D: Infrared spectroscopy
E: Raman spectroscopy

Answer: C

Question 6: In the expression σv*C2 = σv', what does the operation BA = C represent?
A: Operation B is equivalent to Operation A and forms C.
B: Operation A preceded by B results in C.
C: Operation B subtracted by A gives C.
D: Operation A and B combined will neutralize to C.
E: Operation A divided by B equals C.

Answer: B

Question 7: The point group of a molecule gets its name because the set of symmetry operations does what?
A: Leaves at least one point fixed
B: Focuses on the center point of the molecule
C: Has a point of intersection with the molecular axis
D: Has a central point from which all symmetries are measured
E: Always has a singular focus point in the molecule

Answer: A

Question 8: For classifying the rotational levels and eigenstates of the full Hamiltonian, what is required instead of just the point group?
A: Inversion group
B: Permutation group
C: Rotation group
D: Permutation-inversion group
E: Vibration-electronic coordinates

Answer: D

Question 9: Point groups and permutation-inversion groups differ in what way?
A: Point groups describe atomic symmetry, whereas permutation-inversion groups describe molecular symmetry.
B: Point groups describe the symmetry of a crystal, whereas permutation-inversion groups describe the geometrical symmetry of a molecule.
C: Point groups describe the geometrical symmetry of a molecule, whereas permutation-inversion groups describe the energy-invariant symmetry.
D: Point groups describe the rotation-vibration-electronic Hamiltonian, whereas permutation-inversion groups describe vibronic Hamiltonian.
E: Point groups and permutation-inversion groups both describe the symmetry of a molecule in the same way.

Answer: C

Question 10: The symmetry classification of the rotational levels requires the interpretation of operations as doing what to the vibronic coordinates?
A: Enlarging
B: Dividing
C: Rotating and/or reflecting
D: Multiplying
E: Inverting or permuting

Answer: C
@
Subject:
Transparency in insulators
An object may be not transparent either because it reflects the incoming light or because it absorbs the incoming light. Almost all solids reflect a part and absorb a part of the incoming light.

When light falls onto a block of metal, it encounters atoms that are tightly packed in a regular lattice and a "sea of electrons" moving randomly between the atoms.[12] In metals, most of these are non-bonding electrons (or free electrons) as opposed to the bonding electrons typically found in covalently bonded or ionically bonded non-metallic (insulating) solids. In a metallic bond, any potential bonding electrons can easily be lost by the atoms in a crystalline structure. The effect of this delocalization is simply to exaggerate the effect of the "sea of electrons". As a result of these electrons, most of the incoming light in metals is reflected back, which is why we see a shiny metal surface.

Most insulators (or dielectric materials) are held together by ionic bonds. Thus, these materials do not have free conduction electrons, and the bonding electrons reflect only a small fraction of the incident wave. The remaining frequencies (or wavelengths) are free to propagate (or be transmitted). This class of materials includes all ceramics and glasses.

If a dielectric material does not include light-absorbent additive molecules (pigments, dyes, colorants), it is usually transparent to the spectrum of visible light. Color centers (or dye molecules, or "dopants") in a dielectric absorb a portion of the incoming light. The remaining frequencies (or wavelengths) are free to be reflected or transmitted. This is how colored glass is produced.

Most liquids and aqueous solutions are highly transparent. For example, water, cooking oil, rubbing alcohol, air, and natural gas are all clear. Absence of structural defects (voids, cracks, etc.) and molecular structure of most liquids are chiefly responsible for their excellent optical transmission. The ability of liquids to "heal" internal defects via viscous flow is one of the reasons why some fibrous materials (e.g., paper or fabric) increase their apparent transparency when wetted. The liquid fills up numerous voids making the material more structurally homogeneous.[citation needed]

Light scattering in an ideal defect-free crystalline (non-metallic) solid which provides no scattering centers for incoming light will be due primarily to any effects of anharmonicity within the ordered lattice. Light transmission will be highly directional due to the typical anisotropy of crystalline substances, which includes their symmetry group and Bravais lattice. For example, the seven different crystalline forms of quartz silica (silicon dioxide, SiO2) are all clear, transparent materials.[13]
$
10
Question 1: What causes metals to reflect most of the incoming light?
A: Absence of bonding electrons
B: Presence of the "sea of electrons"
C: Presence of ionic bonds
D: Propagation of all wavelengths
E: Dielectric loss

Answer: B

Question 2: Why do insulators or dielectric materials typically reflect only a small fraction of the incident wave?
A: They have a "sea of electrons".
B: They have free conduction electrons.
C: They are held together by ionic bonds and lack free conduction electrons.
D: They are primarily metallic.
E: They are mostly composed of non-bonding electrons.

Answer: C

Question 3: What makes colored glass produce its color?
A: Scattering of all wavelengths
B: Total internal reflection
C: Absorption of a portion of the incoming light by color centers or dye molecules
D: Refraction of light in all directions
E: Excessive dielectric loss

Answer: C

Question 4: What property makes most liquids highly transparent?
A: Presence of the "sea of electrons"
B: Absence of structural defects and their molecular structure
C: Propagation of all wavelengths
D: Anisotropy of the liquid
E: Absorption of all wavelengths

Answer: B

Question 5: Why do some fibrous materials like paper appear more transparent when wet?
A: Wetting reduces the dielectric loss in the material.
B: The liquid absorbs all the incoming light.
C: The wetting process breaks down the fibrous structure.
D: The liquid fills up numerous voids, making the material more structurally homogeneous.
E: The liquid increases the refraction of light.

Answer: D

Question 6: Light scattering in an ideal defect-free crystalline solid will primarily be due to what?
A: Free conduction electrons
B: Anharmonicity within the ordered lattice
C: The "sea of electrons"
D: Dielectric loss
E: Absence of bonding electrons

Answer: B

Question 7: Most insulators are held together by which type of bond?
A: Metallic bond
B: Covalent bond
C: Ionic bond
D: Van der Waals bond
E: Hydrogen bond

Answer: C

Question 8: Which property results in the shiny appearance of metal surfaces?
A: Propagation of all wavelengths
B: Refraction of light in all directions
C: Absorption of most of the incoming light
D: Reflection of most of the incoming light
E: Scattering of all wavelengths

Answer: D

Question 9: What determines the light transmission directionality in an ideal defect-free crystalline solid?
A: Dielectric loss of the material
B: The anisotropy of the crystalline substances, including their symmetry group and Bravais lattice
C: Presence of free conduction electrons
D: The "sea of electrons" in the lattice
E: The percentage of non-bonding electrons

Answer: B

Question 10: Most liquids become transparent because they can "heal" internal defects through what mechanism?
A: Refraction
B: Reflection
C: Scattering
D: Viscous flow
E: Absorption

Answer: D
@
Subject:
Magnetic resonance imaging (MRI) is a medical imaging technique used in radiology to form pictures of the anatomy and the physiological processes of the body. MRI scanners use strong magnetic fields, magnetic field gradients, and radio waves to generate images of the organs in the body. MRI does not involve X-rays or the use of ionizing radiation, which distinguishes it from computed tomography (CT) and positron emission tomography (PET) scans. MRI is a medical application of nuclear magnetic resonance (NMR) which can also be used for imaging in other NMR applications, such as NMR spectroscopy.

Construction and physics
In most medical applications, hydrogen nuclei, which consist solely of a proton, that are in tissues create a signal that is processed to form an image of the body in terms of the density of those nuclei in a specific region. Given that the protons are affected by fields from other atoms to which they are bonded, it is possible to separate responses from hydrogen in specific compounds. To perform a study, the person is positioned within an MRI scanner that forms a strong magnetic field around the area to be imaged. First, energy from an oscillating magnetic field is temporarily applied to the patient at the appropriate resonance frequency. Scanning with X and Y gradient coils causes a selected region of the patient to experience the exact magnetic field required for the energy to be absorbed. The atoms are excited by a RF pulse and the resultant signal is measured by a receiving coil. The RF signal may be processed to deduce position information by looking at the changes in RF level and phase caused by varying the local magnetic field using gradient coils. As these coils are rapidly switched during the excitation and response to perform a moving line scan, they create the characteristic repetitive noise of an MRI scan as the windings move slightly due to magnetostriction. The contrast between different tissues is determined by the rate at which excited atoms return to the equilibrium state. Exogenous contrast agents may be given to the person to make the image clearer.[5]

MRI requires a magnetic field that is both strong and uniform to a few parts per million across the scan volume. The field strength of the magnet is measured in teslas – and while the majority of systems operate at 1.5 T, commercial systems are available between 0.2 and 7 T. Whole-body MRI systems for research application operate in e.g. 9.4T,[6][7] 10.5T,[8] 11.7T.[9] Even higher field whole-body MRI systems e.g. 14 T and beyond are in conceptual proposal[10] or in engineering design.[11] Most clinical magnets are superconducting magnets, which require liquid helium to keep them at low temperatures. Lower field strengths can be achieved with permanent magnets, which are often used in "open" MRI scanners for claustrophobic patients.[12] Lower field strengths are also used in a portable MRI scanner approved by the FDA in 2020.[13] Recently, MRI has been demonstrated also at ultra-low fields, i.e., in the microtesla-to-millitesla range, where sufficient signal quality is made possible by prepolarization (on the order of 10–100 mT) and by measuring the Larmor precession fields at about 100 microtesla with highly sensitive superconducting quantum interference devices (SQUIDs).
$
10
Question 1: Which form of radiation is notably absent in MRI compared to CT and PET scans?
A: Radio waves
B: Magnetic fields
C: X-rays
D: Gradient fields
E: Gamma rays

Answer: C

Question 2: MRI is a medical application of what kind of resonance?
A: Electro-resonance
B: Gamma resonance
C: Ion resonance
D: Nuclear magnetic resonance
E: Atomic resonance

Answer: D

Question 3: Which nuclei, primarily found in tissues, are often used for creating MRI signals?
A: Neutron
B: Electron
C: Hydrogen
D: Carbon
E: Oxygen

Answer: C

Question 4: What determines the contrast between different tissues in an MRI image?
A: Intensity of the radio waves
B: Strength of the external magnetic field
C: Rate at which excited atoms return to the equilibrium state
D: Position of the patient within the scanner
E: Frequency of the X and Y gradient coils

Answer: C

Question 5: What is the role of exogenous contrast agents in MRI?
A: To generate radio waves
B: To create the magnetic field gradient
C: To make the image clearer
D: To reduce noise in the receiving coil
E: To change the resonance frequency of the protons

Answer: C

Question 6: The magnetic field strength of MRI systems is measured in which unit?
A: Coulombs
B: Hertz
C: Ohms
D: Watts
E: Teslas

Answer: E

Question 7: Most clinical MRI magnets are what type of magnets?
A: Electromagnets
B: Permanent magnets
C: Dipole magnets
D: Superconducting magnets
E: Ferromagnetic magnets

Answer: D

Question 8: Which MRI scanners are often used for claustrophobic patients?
A: High-field MRI scanners
B: Superconducting MRI scanners
C: "Open" MRI scanners
D: Ultra-low field MRI scanners
E: Portable MRI scanners

Answer: C

Question 9: As of the information provided, when was the portable MRI scanner approved by the FDA?
A: 2017
B: 2018
C: 2019
D: 2020
E: 2021

Answer: D

Question 10: What phenomenon produces the characteristic repetitive noise during an MRI scan?
A: Radiofrequency (RF) pulse
B: Switching of X and Y gradient coils
C: Larmor precession of hydrogen nuclei
D: Oscillating magnetic field
E: Absorption of energy by protons

Answer: B
@
Subject:
In photometry, illuminance is the total luminous flux incident on a surface, per unit area.[1] It is a measure of how much the incident light illuminates the surface, wavelength-weighted by the luminosity function to correlate with human brightness perception.[2] Similarly, luminous emittance is the luminous flux per unit area emitted from a surface. Luminous emittance is also known as luminous exitance.[3][4]

In SI units illuminance is measured in lux (lx), or equivalently in lumens per square metre (lm·m−2).[2] Luminous exitance is measured in lm·m−2 only, not lux.[4] In the CGS system, the unit of illuminance is the phot, which is equal to 10000 lux. The foot-candle is a non-metric unit of illuminance that is used in photography.[5]

Illuminance was formerly often called brightness, but this leads to confusion with other uses of the word, such as to mean luminance. "Brightness" should never be used for quantitative description, but only for nonquantitative references to physiological sensations and perceptions of light.

The human eye is capable of seeing somewhat more than a 2 trillion-fold range. The presence of white objects is somewhat discernible under starlight, at 5×10−5 lux, while at the bright end, it is possible to read large text at 108 lux, or about 1000 times that of direct sunlight, although this can be very uncomfortable and cause long-lasting afterimages.

Luminance is a photometric measure of the luminous intensity per unit area of light travelling in a given direction.[1] It describes the amount of light that passes through, is emitted from, or is reflected from a particular area, and falls within a given solid angle.
$
10
Question 1: What unit is used to measure illuminance in the SI system?
A: Foot-candle
B: Phot
C: Lux
D: Lumen
E: Candela

Answer: C

Question 2: Which term was formerly used to refer to illuminance but can lead to confusion?
A: Intensity
B: Luminance
C: Brightness
D: Exitance
E: Flux

Answer: C

Question 3: In which unit is luminous exitance exclusively measured?
A: Foot-candle
B: Phot
C: Lux
D: Lumen per square meter (lm·m−2)
E: Candela

Answer: D

Question 4: How does the CGS system unit of illuminance, the phot, relate to the lux?
A: 1 phot = 100 lux
B: 1 phot = 10 lux
C: 1 phot = 1000 lux
D: 1 phot = 10000 lux
E: 1 phot = 0.1 lux

Answer: D

Question 5: The human eye can discern the presence of white objects under which lighting condition?
A: Direct sunlight
B: Starlight at 5×10−5 lux
C: Luminance of 108 lux
D: Moonlight at 1 lux
E: Candlelight at 10 lux

Answer: B

Question 6: What non-metric unit of illuminance is commonly used in photography?
A: Lumen
B: Candela
C: Foot-candle
D: Phot
E: Nit

Answer: C

Question 7: What is luminous emittance also known as?
A: Luminance
B: Luminous efficiency
C: Luminous exitance
D: Illuminance
E: Luminous flux

Answer: C

Question 8: Which measure describes the amount of light that passes through, is emitted from, or is reflected from a particular area?
A: Luminous flux
B: Luminous intensity
C: Luminous emittance
D: Luminance
E: Illuminance

Answer: D

Question 9: Luminance is a measure of luminous intensity per unit area traveling in a specific:
A: Spectrum
B: Distance
C: Wavelength
D: Direction
E: Time span

Answer: D

Question 10: How is illuminance wavelength-weighted?
A: By the luminous function to correlate with human brightness perception
B: By the amount of light in the visible spectrum
C: By the angle of incidence of the light
D: By the distance between the light source and the illuminated surface
E: By the color temperature of the light source

Answer: A
@
Subject:
In particle physics, a magnetic monopole is a hypothetical elementary particle that is an isolated magnet with only one magnetic pole (a north pole without a south pole or vice versa).[1][2] A magnetic monopole would have a net north or south "magnetic charge". Modern interest in the concept stems from particle theories, notably the grand unified and superstring theories, which predict their existence.[3][4][full citation needed] The known elementary particles that have electric charge are electric monopoles.

Magnetism in bar magnets and electromagnets is not caused by magnetic monopoles, and indeed, there is no known experimental or observational evidence that magnetic monopoles exist.

Some condensed matter systems contain effective (non-isolated) magnetic monopole quasi-particles,[5] or contain phenomena that are mathematically analogous to magnetic monopoles.[6]

Early science and classical physics
Many early scientists attributed the magnetism of lodestones to two different "magnetic fluids" ("effluvia"), a north-pole fluid at one end and a south-pole fluid at the other, which attracted and repelled each other in analogy to positive and negative electric charge.[7][8] However, an improved understanding of electromagnetism in the nineteenth century showed that the magnetism of lodestones was properly explained not by magnetic monopole fluids, but rather by a combination of electric currents, the electron magnetic moment, and the magnetic moments of other particles. Gauss's law for magnetism, one of Maxwell's equations, is the mathematical statement that magnetic monopoles do not exist. Nevertheless, Pierre Curie pointed out in 1894[9] that magnetic monopoles could conceivably exist, despite not having been seen so far.

Quantum mechanics
The quantum theory of magnetic charge started with a paper by the physicist Paul Dirac in 1931.[10] In this paper, Dirac showed that if any magnetic monopoles exist in the universe, then all electric charge in the universe must be quantized (Dirac quantization condition).[11] The electric charge is, in fact, quantized, which is consistent with (but does not prove) the existence of monopoles.[11]

Since Dirac's paper, several systematic monopole searches have been performed. Experiments in 1975[12] and 1982[13] produced candidate events that were initially interpreted as monopoles, but are now regarded as inconclusive.[14] Therefore, it remains an open question whether monopoles exist. Further advances in theoretical particle physics, particularly developments in grand unified theories and quantum gravity, have led to more compelling arguments (detailed below) that monopoles do exist. Joseph Polchinski, a string theorist, described the existence of monopoles as "one of the safest bets that one can make about physics not yet seen".[15] These theories are not necessarily inconsistent with the experimental evidence. In some theoretical models, magnetic monopoles are unlikely to be observed, because they are too massive to create in particle accelerators (see § Searches for magnetic monopoles below), and also too rare in the Universe to enter a particle detector with much probability.[15]

Some condensed matter systems propose a structure superficially similar to a magnetic monopole, known as a flux tube. The ends of a flux tube form a magnetic dipole, but since they move independently, they can be treated for many purposes as independent magnetic monopole quasiparticles. Since 2009, numerous news reports from the popular media[16][17] have incorrectly described these systems as the long-awaited discovery of the magnetic monopoles, but the two phenomena are only superficially related to one another.[18][19] These condensed-matter systems remain an area of active research. (See § "Monopoles" in condensed-matter systems below.)
$
10
Question 1: Which theories notably predict the existence of magnetic monopoles?
A: Newton's law and Einstein's relativity
B: Quantum mechanics and wave theory
C: Grand unified and superstring theories
D: Thermodynamics and classical physics
E: Particle theory and fluid dynamics

Answer: C

Question 2: How is magnetism in bar magnets and electromagnets primarily caused?
A: By magnetic monopoles
B: By the electron magnetic moment
C: By quantum fluctuations
D: By Dirac's quantum theory
E: By superstring vibrations

Answer: B

Question 3: In the context of early science, what did many early scientists attribute the magnetism of lodestones to?
A: The movement of electrons
B: Two different "magnetic fluids" (north and south)
C: The presence of magnetic monopoles
D: Quantum interactions
E: Gravitational effects

Answer: B

Question 4: Gauss's law for magnetism is a statement that:
A: Confirms the existence of magnetic monopoles
B: Describes the properties of magnetic monopoles
C: Argues against the existence of magnetic monopoles
D: Shows magnetic monopoles do not exist
E: Proposes a new kind of magnetic monopole

Answer: D

Question 5: Paul Dirac's paper on magnetic charge suggested that if magnetic monopoles exist, then:
A: All magnetic charge must be quantized
B: All electric charge in the universe must be quantized
C: Magnetic and electric charges are interchangeable
D: Monopoles would have both north and south poles
E: Electromagnetism would be invalidated

Answer: B

Question 6: What did experiments in 1975 and 1982 initially interpret as evidence for?
A: The quantization of electric charge
B: The existence of electric monopoles
C: The non-existence of magnetic monopoles
D: The existence of magnetic monopoles
E: The presence of flux tubes in all magnets

Answer: D

Question 7: Joseph Polchinski, a string theorist, described the existence of monopoles as:
A: Highly unlikely
B: A fundamental flaw in modern physics
C: One of the safest bets about unseen physics
D: A myth in quantum mechanics
E: Irrelevant to the study of strings

Answer: C

Question 8: What is a structure in some condensed matter systems that is superficially similar to a magnetic monopole?
A: Quantum loop
B: Flux tube
C: Monopole web
D: Electron cloud
E: Quantum string

Answer: B

Question 9: Numerous news reports since 2009 have incorrectly described which phenomena as the discovery of magnetic monopoles?
A: Electron interactions in superconductors
B: Quantum entanglement in semiconductors
C: Magnetic dipoles in conductors
D: Flux tubes in condensed-matter systems
E: Quarks in high-energy collisions

Answer: D

Question 10: Which scientist pointed out the possibility of magnetic monopoles in 1894, despite their lack of observation?
A: Albert Einstein
B: Niels Bohr
C: Pierre Curie
D: Richard Feynman
E: Paul Dirac

Answer: C
@
Subject:
In physics, a redshift is an increase in the wavelength, and corresponding decrease in the frequency and photon energy, of electromagnetic radiation (such as light). The opposite change, a decrease in wavelength and simultaneous increase in frequency and energy, is known as a negative redshift, or blueshift. The terms derive from the colours red and blue which form the extremes of the visible light spectrum. The main causes of electromagnetic redshift in astronomy and cosmology are the relative motions of radiation sources, which give rise to the relativistic Doppler effect), and gravitational potentials, which gravitationally redshift escaping radiation. All sufficiently distant light sources show cosmological redshift corresponding to recession speeds proportional to their distances from Earth, a fact known as Hubble's law that implies the universe is expanding.

All redshifts can be understood under the umbrella of frame transformation laws. Gravitational waves, which also travel at the speed of light, are subject to the same redshift phenomena. The value of a redshift is often denoted by the letter z, corresponding to the fractional change in wavelength (positive for redshifts, negative for blueshifts), and by the wavelength ratio 1 + z (which is greater than 1 for redshifts and less than 1 for blueshifts).

Examples of strong redshifting are a gamma ray perceived as an X-ray, or initially visible light perceived as radio waves. Subtler redshifts are seen in the spectroscopic observations of astronomical objects, and are used in terrestrial technologies such as Doppler radar and radar guns.

Other physical processes exist that can lead to a shift in the frequency of electromagnetic radiation, including scattering and optical effects; however, the resulting changes are distinguishable from (astronomical) redshift and are not generally referred to as such (see section on physical optics and radiative transfer).
$
10
Question 1: What happens to the wavelength of electromagnetic radiation during a redshift?
A: It decreases
B: It remains the same
C: It increases
D: It fluctuates unpredictably
E: It oscillates between two fixed values

Answer: C

Question 2: Which of the following describes blueshift?
A: An increase in the wavelength of electromagnetic radiation
B: A decrease in the frequency of electromagnetic radiation
C: A decrease in the wavelength and an increase in frequency and energy
D: A uniformity in the wavelength across the visible spectrum
E: A conversion of visible light to gamma rays

Answer: C

Question 3: Why are redshift and blueshift named after the colors red and blue?
A: They represent the positive and negative ends of the electromagnetic spectrum
B: They represent the middle of the visible light spectrum
C: They represent the extremes of the visible light spectrum
D: They represent the thermal temperatures of objects
E: They represent the initial and final wavelengths

Answer: C

Question 4: Which of the following factors can cause electromagnetic redshift in astronomy and cosmology?
A: Color transformation
B: Relative motions of radiation sources
C: Speed of the observer
D: Temperature of the source
E: Vibration of atoms in a source

Answer: B

Question 5: Which law implies that the universe is expanding?
A: Newton's law
B: Einstein's theory of relativity
C: Hubble's law
D: Kepler's law
E: Doppler's law

Answer: C

Question 6: What symbol is commonly used to denote the value of a redshift?
A: y
B: x
C: z
D: r
E: s

Answer: C

Question 7: If an initially visible light is strongly redshifted, it can be perceived as:
A: Gamma rays
B: Ultraviolet light
C: Infrared radiation
D: Radio waves
E: Microwaves

Answer: D

Question 8: Which terrestrial technologies use the principle of redshift?
A: Solar panels and GPS
B: Transistors and capacitors
C: Doppler radar and radar guns
D: Nuclear reactors and steam turbines
E: Satellites and remote sensing instruments

Answer: C

Question 9: Which of the following processes can lead to a change in the frequency of electromagnetic radiation but is NOT referred to as an astronomical redshift?
A: Frame transformation laws
B: Gravitational waves
C: Scattering and optical effects
D: Relativistic Doppler effect
E: Cosmic background radiation

Answer: C

Question 10: An example of redshifting would be a:
A: Radio wave perceived as visible light
B: Ultraviolet light perceived as infrared radiation
C: X-ray perceived as a gamma ray
D: Gamma ray perceived as an X-ray
E: Visible light perceived as ultraviolet light

Answer: D
@
Subject:
Coordinated Universal Time or UTC is the primary time standard by which the world regulates clocks and time. It is within about one second of mean solar time (such as UT1) at 0° longitude (at the IERS Reference Meridian as the currently used prime meridian) and is not adjusted for daylight saving time. It is effectively a successor to Greenwich Mean Time (GMT).

The coordination of time and frequency transmissions around the world began on 1 January 1960. UTC was first officially adopted as CCIR Recommendation 374, Standard-Frequency and Time-Signal Emissions, in 1963, but the official abbreviation of UTC and the official English name of Coordinated Universal Time (along with the French equivalent) were not adopted until 1967.[1]

The system has been adjusted several times, including a brief period during which the time-coordination radio signals broadcast both UTC and "Stepped Atomic Time (SAT)" before a new UTC was adopted in 1970 and implemented in 1972. This change also adopted leap seconds to simplify future adjustments. This CCIR Recommendation 460 "stated that (a) carrier frequencies and time intervals should be maintained constant and should correspond to the definition of the SI second; (b) step adjustments, when necessary, should be exactly 1 s to maintain approximate agreement with Universal Time (UT); and (c) standard signals should contain information on the difference between UTC and UT."[2]

The General Conference on Weights and Measures adopted a resolution to alter UTC with a new system that would eliminate leap seconds by 2035.[3]

The current version of UTC is defined by International Telecommunication Union Recommendation (ITU-R TF.460-6), Standard-frequency and time-signal emissions,[4] and is based on International Atomic Time (TAI) with leap seconds added at irregular intervals to compensate for the accumulated difference between TAI and time measured by Earth's rotation.[5] Leap seconds are inserted as necessary to keep UTC within 0.9 seconds of the UT1 variant of universal time.[6] See the "Current number of leap seconds" section for the number of leap seconds inserted to date.
$
10
Question 1: What is the primary purpose of Coordinated Universal Time (UTC)?
A: To regulate the world's calendars
B: To define the astronomical meridian
C: To regulate daylight saving time
D: To regulate clocks and time worldwide
E: To maintain the position of the prime meridian

Answer: D

Question 2: How close is UTC to mean solar time at 0° longitude?
A: About 10 minutes
B: About one hour
C: About one day
D: About one second
E: Exactly the same

Answer: D

Question 3: Which time standard was effectively succeeded by UTC?
A: Universal Time (UT1)
B: Stepped Atomic Time (SAT)
C: International Atomic Time (TAI)
D: Greenwich Mean Time (GMT)
E: International Meridian Time (IMT)

Answer: D

Question 4: When did the coordination of time and frequency transmissions around the world begin?
A: 1 January 1900
B: 1 January 1960
C: 1 January 1970
D: 1 January 1980
E: 1 January 2000

Answer: B

Question 5: In which year was UTC officially adopted as CCIR Recommendation 374?
A: 1953
B: 1960
C: 1963
D: 1967
E: 1972

Answer: C

Question 6: What was broadcast alongside UTC for a brief period?
A: International Atomic Time (TAI)
B: Greenwich Mean Time (GMT)
C: Universal Time (UT1)
D: "Stepped Atomic Time (SAT)"
E: International Meridian Time (IMT)

Answer: D

Question 7: What is the primary difference that accumulates between International Atomic Time (TAI) and time measured by Earth's rotation?
A: Daylight saving time
B: Leap years
C: Leap seconds
D: Time dilation effects
E: Earth's orbital period

Answer: C

Question 8: By how much time is UTC intended to stay within UT1?
A: 0.9 minutes
B: 0.9 hours
C: 0.9 days
D: 0.9 seconds
E: 9 seconds

Answer: D

Question 9: What is the proposed change to UTC by the General Conference on Weights and Measures to be implemented by 2035?
A: To introduce leap minutes
B: To abolish daylight saving time
C: To eliminate leap seconds
D: To synchronize with International Atomic Time (TAI)
E: To redefine the second's duration

Answer: C

Question 10: On what is the current version of UTC based?
A: Solely on the Earth's rotation
B: Solely on International Atomic Time (TAI)
C: On a combination of Earth's rotation and the prime meridian
D: On International Atomic Time (TAI) with added leap seconds at irregular intervals
E: On a combination of Stepped Atomic Time (SAT) and Universal Time (UT1)

Answer: D
@
Subject:
Proper heat treating requires precise control over temperature, time held at a certain temperature and cooling rate.[12]

With the exception of stress-relieving, tempering, and aging, most heat treatments begin by heating an alloy beyond a certain transformation, or arrest (A), temperature. This temperature is referred to as an "arrest" because at the A temperature the metal experiences a period of hysteresis. At this point, all of the heat energy is used to cause the crystal change, so the temperature stops rising for a short time (arrests) and then continues climbing once the change is complete.[13] Therefore, the alloy must be heated above the critical temperature for a transformation to occur. The alloy will usually be held at this temperature long enough for the heat to completely penetrate the alloy, thereby bringing it into a complete solid solution. Iron, for example, has four critical-temperatures, depending on carbon content. Pure iron in its alpha (room temperature) state changes to nonmagnetic gamma-iron at its A2 temperature, and weldable delta-iron at its A4 temperature. However, as carbon is added, becoming steel, the A2 temperature splits into the A3 temperature, also called the austenizing temperature (all phases become austenite, a solution of gamma iron and carbon) and its A1 temperature (austenite changes into pearlite upon cooling). Between these upper and lower temperatures the pro eutectoid phase forms upon cooling.

Because a smaller grain size usually enhances mechanical properties, such as toughness, shear strength and tensile strength, these metals are often heated to a temperature that is just above the upper critical temperature, in order to prevent the grains of solution from growing too large. For instance, when steel is heated above the upper critical-temperature, small grains of austenite form. These grow larger as the temperature is increased. When cooled very quickly, during a martensite transformation, the austenite grain-size directly affects the martensitic grain-size. Larger grains have large grain-boundaries, which serve as weak spots in the structure. The grain size is usually controlled to reduce the probability of breakage.[14]

The diffusion transformation is very time-dependent. Cooling a metal will usually suppress the precipitation to a much lower temperature. Austenite, for example, usually only exists above the upper critical temperature. However, if the austenite is cooled quickly enough, the transformation may be suppressed for hundreds of degrees below the lower critical temperature. Such austenite is highly unstable and, if given enough time, will precipitate into various microstructures of ferrite and cementite. The cooling rate can be used to control the rate of grain growth or can even be used to produce partially martensitic microstructures.[15] However, the martensite transformation is time-independent. If the alloy is cooled to the martensite transformation (Ms) temperature before other microstructures can fully form, the transformation will usually occur at just under the speed of sound.[16]
$
10
Question 1: What is a key requirement for proper heat treating?
A: Exact melting point
B: Precise control over temperature, time, and cooling rate
C: Maximum heating temperature
D: Slow and steady heating rate
E: Frequent reheating

Answer: B

Question 2: Why is the transformation temperature referred to as an "arrest"?
A: Because it's the minimum temperature for transformation
B: Because the metal remains in a fixed state
C: Because the temperature stops rising momentarily during the phase change
D: Because it's the maximum temperature for transformation
E: Because the metal cools instantly

Answer: C

Question 3: Why must the alloy be heated above the critical temperature?
A: To make it melt faster
B: To reduce its mechanical properties
C: For a transformation to occur and achieve a complete solid solution
D: To prevent any hysteresis
E: To decrease the carbon content

Answer: C

Question 4: At what temperature does pure iron change to nonmagnetic gamma-iron?
A: A1 temperature
B: A2 temperature
C: A3 temperature
D: A4 temperature
E: Austenizing temperature

Answer: B

Question 5: What forms when steel is heated above the upper critical temperature?
A: Ferrite grains
B: Pearlite
C: Small grains of austenite
D: Cementite
E: Martensite

Answer: C

Question 6: How does a larger grain size in a metal structure affect its properties?
A: Increases tensile strength
B: Introduces weak spots due to large grain-boundaries
C: Increases ductility
D: Reduces brittleness
E: Prevents hysteresis

Answer: B

Question 7: What does the diffusion transformation in metals depend on?
A: Pressure
B: Grain size
C: Time
D: Hardness
E: Malleability

Answer: C

Question 8: What happens to austenite when cooled very quickly?
A: It instantly transforms into ferrite
B: It grows in grain size
C: It may be suppressed for hundreds of degrees below the lower critical temperature
D: It instantly melts
E: It immediately turns into pearlite

Answer: C

Question 9: What is the nature of the martensite transformation in terms of time?
A: Time-dependent
B: Oscillatory
C: Time-independent
D: Reversible
E: Periodic

Answer: C

Question 10: When is the martensite transformation likely to occur?
A: At very high temperatures
B: During a prolonged phase of hysteresis
C: When the alloy is cooled to the Ms temperature before other microstructures form fully
D: When the metal is heated slowly
E: At the A2 temperature

Answer: C
@
Subject:
The orbital period (also revolution period) is the amount of time a given astronomical object takes to complete one orbit around another object. In astronomy, it usually applies to planets or asteroids orbiting the Sun, moons orbiting planets, exoplanets orbiting other stars, or binary stars. It may also refer to the time it takes a satellite orbiting a planet or moon to complete one orbit.

For celestial objects in general, the orbital period is determined by a 360° revolution of one body around its primary, e.g. Earth around the Sun.

Periods in astronomy are expressed in units of time, usually hours, days, or years.

For celestial objects in general, the orbital period typically refers to the sidereal period, determined by a 360° revolution of one body around its primary relative to the fixed stars projected in the sky. For the case of the Earth orbiting around the Sun, this period is referred to as the sidereal year. This is the orbital period in an inertial (non-rotating) frame of reference.

Orbital periods can be defined in several ways. The tropical period is more particularly about the position of the parent star. It is the basis for the solar year, and respectively the calendar year.

The synodic period refers to not the orbital relation to the parent star, but to other celestial objects, making it not a mere different approach to the orbit of an object around its parent, but a period of orbital relations with other objects, normally Earth, and their orbits around the Sun. It applies to the elapsed time where planets return to the same kind of phenomenon or location, such as when any planet returns between its consecutive observed conjunctions with or oppositions to the Sun. For example, Jupiter has a synodic period of 398.8 days from Earth; thus, Jupiter's opposition occurs once roughly every 13 months.

There are many periods related to the orbits of objects, each of which are often used in the various fields of astronomy and astrophysics, particularly they must not be confused with other revolving periods like rotational periods. Examples of some of the common orbital ones include the following:

The synodic period is the amount of time that it takes for an object to reappear at the same point in relation to two or more other objects. In common usage, these two objects are typically Earth and the Sun. The time between two successive oppositions or two successive conjunctions is also equal to the synodic period. For celestial bodies in the solar system, the synodic period (with respect to Earth and the Sun) differs from the tropical period owing to Earth's motion around the Sun. For example, the synodic period of the Moon's orbit as seen from Earth, relative to the Sun, is 29.5 mean solar days, since the Moon's phase and position relative to the Sun and Earth repeats after this period. This is longer than the sidereal period of its orbit around Earth, which is 27.3 mean solar days, owing to the motion of Earth around the Sun.
The draconitic period (also draconic period or nodal period), is the time that elapses between two passages of the object through its ascending node, the point of its orbit where it crosses the ecliptic from the southern to the northern hemisphere. This period differs from the sidereal period because both the orbital plane of the object and the plane of the ecliptic precess with respect to the fixed stars, so their intersection, the line of nodes, also precesses with respect to the fixed stars. Although the plane of the ecliptic is often held fixed at the position it occupied at a specific epoch, the orbital plane of the object still precesses, causing the draconitic period to differ from the sidereal period.[5]
The anomalistic period is the time that elapses between two passages of an object at its periapsis (in the case of the planets in the Solar System, called the perihelion), the point of its closest approach to the attracting body. It differs from the sidereal period because the object's semi-major axis typically advances slowly.
Also, the tropical period of Earth (a tropical year) is the interval between two alignments of its rotational axis with the Sun, also viewed as two passages of the object at a right ascension of 0 hr. One Earth year is slightly shorter than the period for the Sun to complete one circuit along the ecliptic (a sidereal year) because the inclined axis and equatorial plane slowly precess (rotate with respect to reference stars), realigning with the Sun before the orbit completes. This cycle of axial precession for Earth, known as precession of the equinoxes, recurs roughly every 25,772 years.[6]
Periods can be also defined under different specific astronomical definitions that are mostly caused by the small complex external gravitational influences of other celestial objects. Such variations also include the true placement of the centre of gravity between two astronomical bodies (barycenter), perturbations by other planets or bodies, orbital resonance, general relativity, etc. Most are investigated by detailed complex astronomical theories using celestial mechanics using precise positional observations of celestial objects via astrometry.
$
10
Question 1: What does the orbital period refer to in astronomy?
A: The distance an object travels in space
B: The time an object takes to spin on its axis
C: The time an object takes to complete one orbit around another
D: The speed at which an object moves in space
E: The gravitational pull between two objects

Answer: C

Question 2: For celestial objects, the sidereal period refers to:
A: The time it takes to spin on its own axis
B: A 360° revolution around its primary relative to other planets
C: A 360° revolution around its primary relative to the fixed stars in the sky
D: The time taken for the object to rotate around the equator
E: The time between two solar eclipses

Answer: C

Question 3: Which period serves as the basis for the calendar year?
A: Sidereal period
B: Synodic period
C: Tropical period
D: Anomalistic period
E: Draconitic period

Answer: C

Question 4: The synodic period is essential for understanding:
A: The closest approach of an object to its attracting body
B: The time between two alignments of an object’s rotational axis with the Sun
C: The time an object takes to reappear at the same point in relation to two or more other objects
D: The time that elapses between two passages of the object through its ascending node
E: The rate of precession of equinoxes

Answer: C

Question 5: The time between two successive oppositions or conjunctions is equivalent to:
A: Anomalistic period
B: Draconitic period
C: Tropical period
D: Sidereal period
E: Synodic period

Answer: E

Question 6: What does the draconitic period refer to?
A: Time between two passages at the perihelion
B: Time between two alignments of an object’s rotational axis with the Sun
C: Time that elapses between two passages of the object through its ascending node
D: Time for an object to reappear at the same point in relation to other celestial objects
E: Time taken for a celestial body to spin on its own axis

Answer: C

Question 7: The anomalistic period concerns:
A: The period of rotational relations with other objects, normally Earth
B: The interval between two alignments of an object's rotational axis with the Sun
C: The time that elapses between two passages of an object at its closest approach to the attracting body
D: The time that elapses between two passages of the object through its ascending node
E: The time taken for an object to move a quarter of its orbit

Answer: C

Question 8: One significant difference between a tropical year and a sidereal year is:
A: Tropical year accounts for the moon's phases
B: Sidereal year accounts for an object’s rotational axis alignments with other stars
C: Tropical year is shorter due to the precession of the equinoxes
D: Sidereal year is the basis for the calendar year
E: Tropical year accounts for the precise position of the center of gravity between two bodies

Answer: C

Question 9: Which period is affected by the complex external gravitational influences of other celestial objects, barycenter, perturbations by other bodies, and general relativity?
A: Tropical period
B: Sidereal period
C: Synodic period
D: Specific astronomical periods
E: Anomalistic period

Answer: D

Question 10: What term refers to the point in a celestial object's orbit where it crosses from the southern to the northern hemisphere relative to the ecliptic?
A: Perihelion
B: Periapsis
C: Ascending node
D: Opposition
E: Conjunction

Answer: C
@
Subject:
Remnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.[206]

The Big Bang produced hydrogen, helium, and traces of lithium, while all heavier elements are synthesised in stars, supernovae, and collisions between neutron stars (thus being indirectly due to supernovae). Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as "metals".[207] These ejected elements ultimately enrich the molecular clouds that are the sites of star formation.[208] Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star's life,[207][209] and may influence the possibility of having planets orbiting it: more giant planets form around stars of higher metallicity.[210][211]

The kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space.[212] The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.[213]

Evidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system.[214]

Fast Radio Bursts (FRBs) are intense, transient pulses of radio waves that typically last no more than milliseconds. Many explanations for these events have been proposed; magnetars produced by core-collapse supernovae are leading candidates.[215][216][217][218]
$
10
Question 1: What phase follows the free expansion phase of the supernova remnants?
A: Condensation phase
B: Adiabatic expansion phase
C: Compression phase
D: Synthesis phase
E: Fusion phase

Answer: B

Question 2: What was primarily produced by the Big Bang?
A: All known elements in the periodic table
B: Hydrogen, helium, and traces of lithium
C: Only hydrogen
D: Hydrogen, helium, and traces of carbon
E: Only helium

Answer: B

Question 3: How do astronomers usually refer to elements other than hydrogen and helium that are ejected by supernovae?
A: Isotopes
B: Radionuclides
C: Metals
D: Neutrons
E: Gas clouds

Answer: C

Question 4: What role do supernovae play in the composition of each stellar generation?
A: They decrease the amount of hydrogen and helium in each generation.
B: They make each generation richer in hydrogen.
C: They make each generation more metal-rich compared to the previous one.
D: They maintain a consistent composition across generations.
E: They remove all metals from the subsequent stellar generation.

Answer: C

Question 5: What has evidence indicated about the influence of a nearby supernova on the Solar System?
A: It changed the orbit of the Earth.
B: It caused the Big Bang.
C: It may have played a role in the formation of the Solar System.
D: It was responsible for the creation of the Moon.
E: It led to the extinction of the dinosaurs.

Answer: C

Question 6: Which celestial phenomena are intense, transient pulses of radio waves that typically last milliseconds?
A: Gamma-ray bursts
B: Neutron star pulsations
C: Black hole emissions
D: Fast Radio Bursts (FRBs)
E: Solar flares

Answer: D

Question 7: What is a primary candidate for the cause of Fast Radio Bursts (FRBs)?
A: Black hole mergers
B: Cosmic microwave background radiation
C: Magnetic fields of rotating planets
D: Magnetars produced by core-collapse supernovae
E: Collisions between comets and asteroids

Answer: D

Question 8: How does the kinetic energy of an expanding supernova remnant influence nearby molecular clouds in space?
A: It dissolves the molecular clouds.
B: It converts them into new stars instantly.
C: It can trigger star formation by compressing them.
D: It repels the clouds, preventing star formation.
E: It ionizes the clouds, making them visible.

Answer: C

Question 9: Which of the following describes the process of elements synthesized in stars, supernovae, and collisions between neutron stars?
A: They are primarily formed through the fusion of gases.
B: They are directly produced during the Big Bang.
C: They are indirectly due to supernovae.
D: They come from external galaxies.
E: They are synthesized from cosmic microwave background radiation.

Answer: C

Question 10: Why do more giant planets form around stars of higher metallicity?
A: Because metal-rich stars have a stronger gravitational pull.
B: Because metals act as catalysts for planet formation.
C: Because metal-rich environments provide the necessary components for planet formation.
D: Because higher metallicity results in higher temperatures, promoting planet formation.
E: Because metals enhance the luminosity of the star, attracting more debris.

Answer: C
@
Subject:
In theoretical physics, supersymmetric quantum mechanics is an area of research where supersymmetry are applied to the simpler setting of plain quantum mechanics, rather than quantum field theory. Supersymmetric quantum mechanics has found applications outside of high-energy physics, such as providing new methods to solve quantum mechanical problems, providing useful extensions to the WKB approximation, and statistical mechanics.

Understanding the consequences of supersymmetry (SUSY) has proven mathematically daunting, and it has likewise been difficult to develop theories that could account for symmetry breaking, i.e., the lack of observed partner particles of equal mass. To make progress on these problems, physicists developed supersymmetric quantum mechanics, an application of the supersymmetry superalgebra to quantum mechanics as opposed to quantum field theory. It was hoped that studying SUSY's consequences in this simpler setting would lead to new understanding; remarkably, the effort created new areas of research in quantum mechanics itself.

For example, students are typically taught to "solve" the hydrogen atom by a laborious process which begins by inserting the Coulomb potential into the Schrödinger equation. After a considerable amount of work using many differential equations, the analysis produces a recursion relation for the Laguerre polynomials. The final outcome is the spectrum of hydrogen-atom energy states (labeled by quantum numbers n and l). Using ideas drawn from SUSY, the final result can be derived with significantly greater ease, in much the same way that operator methods are used to solve the harmonic oscillator.[1] A similar supersymmetric approach can also be used to more accurately find the hydrogen spectrum using the Dirac equation.[2] Oddly enough, this approach is analogous to the way Erwin Schrödinger first solved the hydrogen atom.[3][4] Of course, he did not call his solution supersymmetric, as SUSY was thirty years in the future.

The SUSY solution of the hydrogen atom is only one example of the very general class of solutions which SUSY provides to shape-invariant potentials, a category which includes most potentials taught in introductory quantum mechanics courses.

SUSY quantum mechanics involves pairs of Hamiltonians which share a particular mathematical relationship, which are called partner Hamiltonians. (The potential energy terms which occur in the Hamiltonians are then called partner potentials.) An introductory theorem shows that for every eigenstate of one Hamiltonian, its partner Hamiltonian has a corresponding eigenstate with the same energy (except possibly for zero energy eigenstates). This fact can be exploited to deduce many properties of the eigenstate spectrum. It is analogous to the original description of SUSY, which referred to bosons and fermions. We can imagine a "bosonic Hamiltonian", whose eigenstates are the various bosons of our theory. The SUSY partner of this Hamiltonian would be "fermionic", and its eigenstates would be the theory's fermions. Each boson would have a fermionic partner of equal energy—but, in the relativistic world, energy and mass are interchangeable, so we can just as easily say that the partner particles have equal mass.

SUSY concepts have provided useful extensions to the WKB approximation in the form of a modified version of the Bohr-Sommerfeld quantization condition. In addition, SUSY has been applied to non-quantum statistical mechanics through the Fokker–Planck equation, showing that even if the original inspiration in high-energy particle physics turns out to be a blind alley, its investigation has brought about many useful benefits.
$
10
Question 1: What is the primary setting of application for supersymmetric quantum mechanics, as opposed to the more complex realm?
A: Quantum field theory
B: Quantum thermodynamics
C: Plain quantum mechanics
D: Quantum relativity
E: Quantum electromagnetics

Answer: C

Question 2: What is the goal of studying the consequences of SUSY in the simpler setting of quantum mechanics?
A: To develop new theories of symmetry breaking
B: To lead to a new understanding and areas of research in quantum mechanics
C: To confirm the existence of partner particles
D: To contest the results derived from quantum field theory
E: To simplify the equations of quantum field theory

Answer: B

Question 3: How are students traditionally taught to solve the hydrogen atom in quantum mechanics?
A: By inserting the Coulomb potential into the Maxwell's equations
B: By utilizing the Bohr-Sommerfeld quantization condition
C: By inserting the Coulomb potential into the Schrödinger equation
D: By the application of SUSY directly on the atom
E: Through the Dirac equation without any potential

Answer: C

Question 4: What is the result of solving the hydrogen atom using traditional methods?
A: A recursion relation for the Dirac polynomials
B: A recursion relation for the Laguerre polynomials
C: The spectrum of helium-atom energy states
D: The operator methods for the harmonic oscillator
E: A list of all possible quantum numbers

Answer: B

Question 5: How can ideas from SUSY simplify the solution of the hydrogen atom?
A: By bypassing the need to use the Schrödinger equation
B: By utilizing operator methods similar to solving the harmonic oscillator
C: By directly solving the Dirac equation
D: By excluding the Coulomb potential from consideration
E: By applying the Bohr model of atomic structure

Answer: B

Question 6: What is the significance of shape-invariant potentials in SUSY quantum mechanics?
A: They are irrelevant to the topic.
B: They include most potentials taught in introductory quantum mechanics courses.
C: They are only important for high-energy physics.
D: They exclude the potential energy terms.
E: They are specific only to fermionic systems.

Answer: B

Question 7: What term describes the potential energy terms that occur in the Hamiltonians in SUSY quantum mechanics?
A: Laguerre potentials
B: Bosonic potentials
C: Fermionic potentials
D: Partner potentials
E: Eigenstate potentials

Answer: D

Question 8: In the context of SUSY, which Hamiltonian's eigenstates can be imagined as the various bosons of a theory?
A: Fermionic Hamiltonian
B: Coulombic Hamiltonian
C: Bosonic Hamiltonian
D: Relativistic Hamiltonian
E: Laguerre Hamiltonian

Answer: C

Question 9: How has SUSY benefited the field of non-quantum statistical mechanics?
A: By simplifying the Schrödinger equation
B: By redefining the Bohr-Sommerfeld quantization condition
C: Through the application to the Fokker–Planck equation
D: By providing a direct solution to the hydrogen atom
E: By proving the existence of partner particles

Answer: C

Question 10: In the context of SUSY, how can one relate energy and mass in the relativistic world?
A: Energy is a form of mass, and vice versa.
B: Energy and mass are unrelated in relativistic contexts.
C: Energy can be transformed into mass, but not the other way around.
D: Energy and mass are interchangeable.
E: Only energy with high frequencies can be converted to mass.

Answer: D
@
Subject:
The inflaton field is a hypothetical scalar field which is conjectured to have driven cosmic inflation in the very early universe.[1][2][3] The field, originally postulated by Alan Guth,[1] provides a mechanism by which a period of rapid expansion from 10−35 to 10−34 seconds after the initial expansion can be generated, forming a universe consistent with observed spatial isotropy and homogeneity.

Cosmological inflation
Main article: Inflation (cosmology)
The basic[clarification needed] model of inflation proceeds in three phases:[4]

Expanding vacuum state with high potential energy
Phase transition to true vacuum
Slow roll and reheating

Expanding vacuum state with high potential energy
In quantum field theory, a vacuum state or vacuum is a state of quantum fields which is at locally minimal potential energy. Quantum particles are excitations which deviate from this minimal potential energy state, therefore a vacuum state has no particles in it. Depending on the specifics of a quantum field theory, it can have more than one vacuum state. Different vacua, despite all "being empty" (having no particles), will generally have different vacuum energy. Quantum field theory stipulates that the pressure of the vacuum energy is always negative and equal in magnitude to its energy density.

Inflationary theory postulates that there is a vacuum state with very large vacuum energy, caused by a non-zero vacuum expectation value of the inflaton field. Any region of space in this state will rapidly expand. Even if initially it is not empty (contains some particles), very rapid exponential expansion dilutes particle density to essentially zero.

Phase transition to true vacuum
Inflationary theory further postulates that this "inflationary vacuum" state is not the state with globally lowest energy; rather, it is a "false vacuum", also known as a metastable state.

For each observer at any chosen point of space, the false vacuum eventually tunnels into a state with the same potential energy, but which is not a vacuum (it is not at a local minimum of the potential energy—it can "decay"). This state can be seen as a true vacuum, filled with a large number of inflaton particles. However, the rate of expansion of the true vacuum does not change at that moment: Only its exponential character changes to much slower expansion of the FLRW metric. This ensures that expansion rate precisely matches the energy density.

Slow roll and reheating
In the true vacuum, inflaton particles decay, eventually giving rise to the observed Standard Model particles.[citation needed] The shape of the potential energy function near "tunnel exit" from false vacuum state must have a shallow slope, otherwise particle production would be confined to the boundary of expanding true vacuum bubble, which contradicts observation (our Universe is not built of huge completely void bubbles). In other words, the quantum state should "roll to the bottom slowly".

When complete, the decay of inflaton particles fills the space with hot and dense Big Bang plasma.
$
10
Question 1: Who originally postulated the inflaton field?
A: Stephen Hawking
B: Roger Penrose
C: Richard Feynman
D: Alan Guth
E: Max Planck

Answer: D

Question 2: How long is the period of rapid expansion driven by the inflaton field after the initial expansion?
A: From 10−34 to 10−35 seconds
B: From 10−35 to 10−34 seconds
C: From 10−33 to 10−32 seconds
D: From 10−36 to 10−37 seconds
E: From 10−32 to 10−31 seconds

Answer: B

Question 3: In the basic model of inflation, which phase describes a vacuum state with high potential energy?
A: Slow roll and reheating
B: Phase transition to true vacuum
C: Expanding vacuum state with high potential energy
D: Reheating phase
E: Phase transition to false vacuum

Answer: C

Question 4: In quantum field theory, what is a vacuum state?
A: A state of quantum fields with the highest potential energy.
B: A state with multiple particles.
C: A state of quantum fields which is at locally minimal potential energy.
D: The state where quantum fields have reached maximum entropy.
E: The final state in the cosmic inflation.

Answer: C

Question 5: How is the "inflationary vacuum" state described in the inflationary theory?
A: True vacuum
B: False vacuum or metastable state
C: A local minimum of potential energy
D: A state filled with inflaton particles
E: The lowest energy state in the universe

Answer: B

Question 6: During which phase do inflaton particles decay to give rise to observed Standard Model particles?
A: Phase transition to false vacuum
B: Slow roll and reheating
C: Expanding vacuum state with high potential energy
D: Decay of inflaton particles
E: Quantum transition phase

Answer: B

Question 7: What is a significant factor that ensures the expansion rate precisely matches the energy density?
A: The true vacuum's constant expansion rate.
B: The exponential character change to slower expansion of the FLRW metric.
C: The decay of inflaton particles into Standard Model particles.
D: The shallow slope of potential energy function near "tunnel exit".
E: The phase transition from false to true vacuum.

Answer: B

Question 8: What happens when the decay of inflaton particles is complete?
A: Space collapses to a singularity.
B: A new inflationary period begins.
C: The universe undergoes a phase transition.
D: Space is filled with cold and dilute plasma.
E: The space is filled with hot and dense Big Bang plasma.

Answer: E

Question 9: What prevents particle production from being confined to the boundary of an expanding true vacuum bubble?
A: The potential energy function having a steep slope.
B: The potential energy function having a shallow slope.
C: The existence of a false vacuum.
D: The rapid expansion of the false vacuum.
E: The decay of the FLRW metric.

Answer: B

Question 10: Different vacua in quantum field theory, despite all being empty, can have:
A: The same vacuum energy.
B: Different vacuum energy.
C: Different inflaton fields.
D: The same potential energy.
E: Varying number of particles.

Answer: B
@
Subject:
Gravitational waves are constantly passing Earth; however, even the strongest have a minuscule effect and their sources are generally at a great distance. For example, the waves given off by the cataclysmic final merger of GW150914 reached Earth after travelling over a billion light-years, as a ripple in spacetime that changed the length of a 4 km LIGO arm by a thousandth of the width of a proton, proportionally equivalent to changing the distance to the nearest star outside the Solar System by one hair's width.[50] This tiny effect from even extreme gravitational waves makes them observable on Earth only with the most sophisticated detectors.

The effects of a passing gravitational wave, in an extremely exaggerated form, can be visualized by imagining a perfectly flat region of spacetime with a group of motionless test particles lying in a plane, e.g., the surface of a computer screen. As a gravitational wave passes through the particles along a line perpendicular to the plane of the particles, i.e., following the observer's line of vision into the screen, the particles will follow the distortion in spacetime, oscillating in a "cruciform" manner, as shown in the animations. The area enclosed by the test particles does not change and there is no motion along the direction of propagation.[citation needed]

The oscillations depicted in the animation are exaggerated for the purpose of discussion – in reality a gravitational wave has a very small amplitude (as formulated in linearized gravity). However, they help illustrate the kind of oscillations associated with gravitational waves as produced by a pair of masses in a circular orbit. In this case the amplitude of the gravitational wave is constant, but its plane of polarization changes or rotates at twice the orbital rate, so the time-varying gravitational wave size, or 'periodic spacetime strain', exhibits a variation as shown in the animation.[51] If the orbit of the masses is elliptical then the gravitational wave's amplitude also varies with time according to Einstein's quadrupole formula.[3]

As with other waves, there are a number of characteristics used to describe a gravitational wave:

Amplitude: Usually denoted h, this is the size of the wave – the fraction of stretching or squeezing in the animation. The amplitude shown here is roughly h = 0.5 (or 50%). Gravitational waves passing through the Earth are many sextillion times weaker than this – h ≈ 10−20.
Frequency: Usually denoted f, this is the frequency with which the wave oscillates (1 divided by the amount of time between two successive maximum stretches or squeezes)
Wavelength: Usually denoted λ, this is the distance along the wave between points of maximum stretch or squeeze.
Speed: This is the speed at which a point on the wave (for example, a point of maximum stretch or squeeze) travels. For gravitational waves with small amplitudes, this wave speed is equal to the speed of light (c).
The speed, wavelength, and frequency of a gravitational wave are related by the equation c = λf, just like the equation for a light wave. For example, the animations shown here oscillate roughly once every two seconds. This would correspond to a frequency of 0.5 Hz, and a wavelength of about 600 000 km, or 47 times the diameter of the Earth.

In the above example, it is assumed that the wave is linearly polarized with a "plus" polarization, written h+. Polarization of a gravitational wave is just like polarization of a light wave except that the polarizations of a gravitational wave are 45 degrees apart, as opposed to 90 degrees.[52] In particular, in a "cross"-polarized gravitational wave, h×, the effect on the test particles would be basically the same, but rotated by 45 degrees, as shown in the second animation. Just as with light polarization, the polarizations of gravitational waves may also be expressed in terms of circularly polarized waves. Gravitational waves are polarized because of the nature of their source.
$
10
Question 1: What causes gravitational waves to be observed on Earth only with the most advanced detectors?
A: Their extremely large amplitudes.
B: Their high frequency.
C: The minuscule effect even from the strongest waves.
D: The quick speed at which they dissipate.
E: Their short wavelength.

Answer: C

Question 2: Gravitational waves can be visualized by imagining a flat region of spacetime with test particles. As a gravitational wave passes through, how do these particles move?
A: They move in a circular manner.
B: They stay motionless.
C: They oscillate in a "cruciform" manner.
D: They scatter randomly.
E: They move linearly in the direction of the wave.

Answer: C

Question 3: How do the oscillations of gravitational waves produced by a pair of masses in a circular orbit behave?
A: They remain constant in amplitude but the plane of polarization changes.
B: They show random changes in amplitude.
C: Their amplitude gradually increases.
D: Their frequency changes but amplitude remains constant.
E: Both amplitude and frequency decrease with time.

Answer: A

Question 4: Which formula determines the variation of gravitational wave's amplitude in the case of elliptical orbits of the masses?
A: Newton's binomial formula
B: Gauss's quadrature formula
C: Pythagorean theorem
D: Einstein's quadrupole formula
E: Kepler's orbital formula

Answer: D

Question 5: If the amplitude 
ℎ
h of a gravitational wave is described as the size of the wave, what is a typical amplitude for gravitational waves passing through Earth?
A: 
ℎ
≈
1
0
20
h≈10 
20
 
B: 
ℎ
≈
1
0
−
20
h≈10 
−20
 
C: 
ℎ
≈
50
%
h≈50%
D: 
ℎ
≈
0.5
h≈0.5
E: 
ℎ
≈
1
h≈1

Answer: B

Question 6: How is the speed of gravitational waves with small amplitudes compared to the speed of light?
A: It is slower than the speed of light.
B: It is faster than the speed of light.
C: It is equal to the speed of light.
D: It is twice the speed of light.
E: It varies depending on the source.

Answer: C

Question 7: Which equation relates the speed, wavelength, and frequency of a gravitational wave?
A: 
ℎ
=
�
�
h=λf
B: 
�
=
�
+
�
c=λ+f
C: 
�
=
�
�
c=λf
D: 
ℎ
=
�
+
�
h=λ+f
E: 
�
=
�
/
�
f=λ/c

Answer: C

Question 8: Which type of polarization would cause the effect on test particles to be the same as "plus" polarization but rotated by 45 degrees?
A: "Minus" polarization
B: "Cross"-polarized
C: Linear polarization
D: Circularly polarized
E: Transverse polarization

Answer: B

Question 9: What distinguishes the polarizations of gravitational waves from that of light waves?
A: Gravitational wave polarizations are 180 degrees apart.
B: Gravitational wave polarizations are 90 degrees apart.
C: Gravitational wave polarizations are 60 degrees apart.
D: Gravitational wave polarizations are 45 degrees apart.
E: Gravitational wave polarizations are 30 degrees apart.

Answer: D

Question 10: Why are gravitational waves polarized?
A: Due to the interference from other cosmic waves.
B: Due to the Earth's gravitational field.
C: Because of the nature of their source.
D: Due to the quantum fluctuations in spacetime.
E: Because of the Doppler effect.

Answer: C
@
Subject:
Myrmecophytes (/mərˈmɛkəfaɪt/; literally "ant-plant") are plants that live in a mutualistic association with a colony of ants. There are over 100 different genera of myrmecophytes.[1] These plants possess structural adaptations that provide ants with food and/or shelter. These specialized structures include domatia, food bodies, and extrafloral nectaries.[1] In exchange for food and shelter, ants aid the myrmecophyte in pollination, seed dispersal, gathering of essential nutrients, and/or defense.[1] Specifically, domatia adapted to ants may be called myrmecodomatia.[2]

A mycorrhiza (from Greek μύκης mýkēs, "fungus", and ῥίζα rhiza, "root"; pl: mycorrhizae, mycorrhiza or mycorrhizas[1]) is a symbiotic association between a fungus and a plant.[2] The term mycorrhiza refers to the role of the fungus in the plant's rhizosphere, its root system. Mycorrhizae play important roles in plant nutrition, soil biology, and soil chemistry.

In a mycorrhizal association, the fungus colonizes the host plant's root tissues, either intracellularly as in arbuscular mycorrhizal fungi, or extracellularly as in ectomycorrhizal fungi.[3] The association is normally mutualistic. In particular species, or in particular circumstances, mycorrhizae may have a parasitic association with host plants.[4]
$
10
Question 1: Which term specifically refers to plants living in a mutualistic association with ants?
A: Mycorrhiza
B: Myrmecophytes
C: Mycetophytes
D: Myrmecology
E: Mycoflora

Answer: B

Question 2: What do myrmecophytes offer ants?
A: Only food
B: Only shelter
C: Both food and shelter
D: Neither food nor shelter
E: Only protection from predators

Answer: C

Question 3: The term mycorrhiza primarily refers to the association of which two organisms?
A: Ants and plants
B: Plants and birds
C: Plants and fungi
D: Plants and bacteria
E: Fungi and ants

Answer: C

Question 4: Myrmecodomatia are specifically adapted for what?
A: Birds
B: Fungi
C: Bees
D: Ants
E: Butterflies

Answer: D

Question 5: In a mycorrhizal association, if the fungus colonizes the host plant's root tissues extracellularly, it is known as?
A: Arbuscular mycorrhizal fungi
B: Ectomycorrhizal fungi
C: Intracellular fungi
D: Parasitic fungi
E: Commensal fungi

Answer: B

Question 6: Which of the following is NOT a benefit that ants offer to myrmecophytes?
A: Parasitism
B: Seed dispersal
C: Defense
D: Gathering of essential nutrients
E: Pollination

Answer: A

Question 7: In what situation might mycorrhizae not display a mutualistic relationship with host plants?
A: When they absorb sunlight
B: When they have a parasitic association
C: When they coevolve with ants
D: When they produce food bodies
E: When they defend plants from herbivores

Answer: B

Question 8: Which structure in myrmecophytes directly provides ants with food?
A: Myrmecodomatia
B: Extrafloral nectaries
C: Rhizosphere
D: Arbuscular fungi
E: Ectomycorrhiza

Answer: B

Question 9: What is the primary role of the fungus in the plant's rhizosphere?
A: Defense against herbivores
B: Seed dispersal
C: Mutualistic association with ants
D: Symbiotic association with the plant
E: Providing shelter to birds

Answer: D

Question 10: If the mycorrhizal fungus colonizes the host plant's root tissues within the cells, what type of mycorrhizal fungi is it?
A: Ectomycorrhizal fungi
B: Myrmecodomatia fungi
C: Parasitic fungi
D: Arbuscular mycorrhizal fungi
E: Extracellular fungi

Answer: D
@
Subject:
The Kelvin–Helmholtz instability (after Lord Kelvin and Hermann von Helmholtz) is a fluid instability that occurs when there is velocity shear in a single continuous fluid or a velocity difference across the interface between two fluids. Kelvin-Helmholtz instabilities are visible in the atmospheres of planets and moons, such as in cloud formations on Earth or the Red Spot on Jupiter, and the atmospheres of the Sun and other stars.[1]

Theory overview and mathematical concepts
Fluid dynamics predicts the onset of instability and transition to turbulent flow within fluids of different densities moving at different speeds.[3] If surface tension is ignored, two fluids in parallel motion with different velocities and densities yield an interface that is unstable to short-wavelength perturbations for all speeds. However, surface tension is able to stabilize the short wavelength instability up to a threshold velocity.

If the density and velocity vary continuously in space (with the lighter layers uppermost, so that the fluid is RT-stable), the dynamics of the Kelvin-Helmholtz instability is described by the Taylor–Goldstein equation:
{\displaystyle (U-c)^{2}\left({d^{2}{\tilde {\phi }} \over dz^{2}}-k^{2}{\tilde {\phi }}\right)+\left[N^{2}-(U-c){d^{2}U \over dz^{2}}\right]{\tilde {\phi }}=0,}
where 
�
=
�
/
�
�{\textstyle N={\sqrt {g/L_{\rho }}}} denotes the Brunt–Väisälä frequency, U is the horizontal parallel velocity, k is the wave number, c is the eigenvalue parameter of the problem, 
�
~{\tilde {\phi }} is complex amplitude of the stream function. Its onset is given by the Richardson number 
R
i
{\displaystyle \mathrm {Ri} }. Typically the layer is unstable for 
R
i
<
0.25
{\displaystyle \mathrm {Ri} <0.25}. These effects are common in cloud layers. The study of this instability is applicable in plasma physics, for example in inertial confinement fusion and the plasma–beryllium interface. In situations where there is a state of static stability, evident by heavier fluids found below than the lower fluid, the Rayleigh-Taylor instability can be ignored as the Kelvin–Helmholtz instability is sufficient given the conditions.

Numerically, the Kelvin–Helmholtz instability is simulated in a temporal or a spatial approach. In the temporal approach, the flow is considered in a periodic (cyclic) box "moving" at mean speed (absolute instability). In the spatial approach, simulations mimic a lab experiment with natural inlet and outlet conditions (convective instability).
$
10
Question 1: The Kelvin–Helmholtz instability can be observed in:
A: Only the atmospheres of stars
B: Only on Earth's cloud formations
C: Both the atmospheres of planets and in plasma physics
D: Only the Red Spot on Jupiter
E: The atmospheres of planets, moons, and stars.

Answer: E

Question 2: What factor can stabilize the short-wavelength instability up to a certain threshold?
A: Velocity difference
B: Density difference
C: Surface tension
D: Turbulent flow
E: Eigenvalue parameter

Answer: C

Question 3: In the Taylor–Goldstein equation describing the dynamics of the Kelvin-Helmholtz instability, what denotes the Brunt–Väisälä frequency?
A: U
B: c
C: k
D: N
E: Ri

Answer: D

Question 4: For what Richardson number value is a layer typically unstable?
A: Ri > 0.25
B: Ri < 0.25
C: Ri = 0.25
D: Ri > 1
E: Ri < 1

Answer: B

Question 5: The study of Kelvin–Helmholtz instability is significant in:
A: Seismic activity
B: Plasma physics
C: Meteorology
D: Quantum mechanics
E: Geology

Answer: B

Question 6: What other instability can be disregarded when there is a static stability state, as the Kelvin–Helmholtz instability is adequate?
A: Kelvin-Vortex instability
B: Rayleigh-Bénard instability
C: Rayleigh-Taylor instability
D: Taylor-Couette instability
E: Helmholtz resonance

Answer: C

Question 7: In which simulation approach for the Kelvin–Helmholtz instability is the flow regarded in a periodic box that "moves" at an average speed?
A: Spatial approach
B: Temporal approach
C: Convective approach
D: Absolute approach
E: Periodic approach

Answer: B

Question 8: Kelvin–Helmholtz instabilities result from which of the following:
A: A single fluid moving at constant velocity
B: Two fluids moving at the same velocity with different densities
C: Velocity shear in a continuous fluid or a velocity difference across two fluid interfaces
D: Two fluids moving at different velocities with the same density
E: An unchanging velocity profile across a single fluid

Answer: C

Question 9: When considering the spatial approach to simulating the Kelvin–Helmholtz instability, it aims to replicate:
A: A periodic motion of the flow
B: A lab experiment with natural inlet and outlet conditions
C: The movement of flow at an average speed
D: The variation of fluid density over time
E: The interplay of fluid densities in a confined space

Answer: B

Question 10: In which celestial body's feature is the Kelvin–Helmholtz instability observed?
A: Earth's North Pole
B: Mars' Ice Caps
C: Jupiter's Red Spot
D: Venus' Volcanic Surface
E: Saturn's Rings

Answer: C
@
Subject:
Thylakoids are membrane-bound compartments inside chloroplasts and cyanobacteria. They are the site of the light-dependent reactions of photosynthesis. Thylakoids consist of a thylakoid membrane surrounding a thylakoid lumen. Chloroplast thylakoids frequently form stacks of disks referred to as grana (singular: granum). Grana are connected by intergranal or stromal thylakoids, which join granum stacks together as a single functional compartment.

In thylakoid membranes, chlorophyll pigments are found in packets called quantasomes. Each quantasome contains 230 to 250 chlorophyll molecules.

Thylakoids are membrane-bound structures embedded in the chloroplast stroma. A stack of thylakoids is called a granum and resembles a stack of coins.

Membrane
The thylakoid membrane is the site of the light-dependent reactions of photosynthesis with the photosynthetic pigments embedded directly in the membrane. It is an alternating pattern of dark and light bands measuring each 1 nanometre.[3] The thylakoid lipid bilayer shares characteristic features with prokaryotic membranes and the inner chloroplast membrane. For example, acidic lipids can be found in thylakoid membranes, cyanobacteria and other photosynthetic bacteria and are involved in the functional integrity of the photosystems.[4] The thylakoid membranes of higher plants are composed primarily of phospholipids[5] and galactolipids that are asymmetrically arranged along and across the membranes.[6] Thylakoid membranes are richer in galactolipids rather than phospholipids; also they predominantly consist of hexagonal phase II forming monogalacotosyl diglyceride lipid. Despite this unique composition, plant thylakoid membranes have been shown to assume largely lipid-bilayer dynamic organization.[7] Lipids forming the thylakoid membranes, richest in high-fluidity linolenic acid[8] are synthesized in a complex pathway involving exchange of lipid precursors between the endoplasmic reticulum and inner membrane of the plastid envelope and transported from the inner membrane to the thylakoids via vesicles.[9]

Lumen
The thylakoid lumen is a continuous aqueous phase enclosed by the thylakoid membrane. It plays an important role for photophosphorylation during photosynthesis. During the light-dependent reaction, protons are pumped across the thylakoid membrane into the lumen making it acidic down to pH 4.

Granum and stroma lamellae
In higher plants thylakoids are organized into a granum-stroma membrane assembly. A granum (plural grana) is a stack of thylakoid discs. Chloroplasts can have from 10 to 100 grana. Grana are connected by stroma thylakoids, also called intergranal thylakoids or lamellae. Grana thylakoids and stroma thylakoids can be distinguished by their different protein composition. Grana contribute to chloroplasts' large surface area to volume ratio. A recent electron tomography study of the thylakoid membranes has shown that the stroma lamellae are organized in wide sheets perpendicular to the grana stack axis and form multiple right-handed helical surfaces at the granal interface.[2] Left-handed helical surfaces consolidate between the right-handed helices and sheets. This complex network of alternating helical membrane surfaces of different radii and pitch was shown to minimize the surface and bending energies of the membranes.[2] This new model, the most extensive one generated to date, revealed that features from two, seemingly contradictory, older models[10][11] coexist in the structure. Notably, similar arrangements of helical elements of alternating handedness, often referred to as "parking garage" structures, were proposed to be present in the endoplasmic reticulum[12] and in ultradense nuclear matter.[13][14][15] This structural organization may constitute a fundamental geometry for connecting between densely packed layers or sheets.[2]
$
10
Question 1: Which of the following is the site of the light-dependent reactions of photosynthesis?
A: Chloroplast stroma
B: Thylakoid lumen
C: Thylakoid membrane
D: Chloroplast envelope
E: Quanta

Answer: C

Question 2: What does a stack of thylakoids resemble?
A: A bundle of threads
B: A series of boxes
C: A stack of coins
D: A group of bubbles
E: A flow of water

Answer: C

Question 3: Where can chlorophyll pigments be found in thylakoid membranes?
A: In quantasomes
B: In the stroma lamellae
C: In the lumen
D: Dispersed freely in the thylakoid membrane
E: Bound directly to grana

Answer: A

Question 4: How many chlorophyll molecules does each quantasome typically contain?
A: 50 to 100
B: 120 to 150
C: 200 to 220
D: 230 to 250
E: 260 to 290

Answer: D

Question 5: What is the pH level the thylakoid lumen can drop to during the light-dependent reaction?
A: 2
B: 4
C: 6
D: 7
E: 9

Answer: B

Question 6: Which of the following structures connects grana?
A: Stroma thylakoids
B: Chlorophyll
C: Quantasomes
D: Lumen
E: Thylakoid membranes

Answer: A

Question 7: Thylakoid membranes in higher plants are primarily composed of:
A: Amino acids
B: Galactolipids
C: Nucleic acids
D: Proteins
E: Sugars

Answer: B

Question 8: In what kind of membrane assembly are thylakoids organized in higher plants?
A: Lumen-thylakoid assembly
B: Granum-stroma membrane assembly
C: Chlorophyll-quantasome assembly
D: Lumen-membrane assembly
E: Stroma-granal assembly

Answer: B

Question 9: Which of the following structures has the most extensive model generated, revealing features from two older models?
A: Thylakoid lumen
B: Stroma lamellae
C: Thylakoid membrane
D: Chloroplast envelope
E: Grana

Answer: B

Question 10: What is primarily responsible for the functional integrity of photosystems?
A: Acidic lipids in thylakoid membranes
B: The high number of chlorophyll molecules in quantasomes
C: The distinct pH in the thylakoid lumen
D: The presence of nucleic acids in thylakoid membranes
E: The helical structures in the stroma lamellae

Answer: A
@
Subject:
In a supersymmetric theory the equations for force and the equations for matter are identical. In theoretical and mathematical physics, any theory with this property has the principle of supersymmetry (SUSY). Dozens of supersymmetric theories exist.[1] Supersymmetry is a spacetime symmetry between two basic classes of particles: bosons, which have an integer-valued spin and follow Bose–Einstein statistics, and fermions, which have a half-integer-valued spin and follow Fermi–Dirac statistics.[2][3]

In supersymmetry, each particle from one class would have an associated particle in the other, known as its superpartner, the spin of which differs by a half-integer. For example, if the electron exists in a supersymmetric theory, then there would be a particle called a selectron (superpartner electron), a bosonic partner of the electron. In the simplest supersymmetry theories, with perfectly "unbroken" supersymmetry, each pair of superpartners would share the same mass and internal quantum numbers besides spin. More complex supersymmetry theories have a spontaneously broken symmetry, allowing superpartners to differ in mass.[4][5][6]

Supersymmetry has various applications to different areas of physics, such as quantum mechanics, statistical mechanics, quantum field theory, condensed matter physics, nuclear physics, optics, stochastic dynamics, astrophysics, quantum gravity, and cosmology. Supersymmetry has also been applied to high energy physics, where a supersymmetric extension of the Standard Model is a possible candidate for physics beyond the Standard Model. However, no supersymmetric extensions of the Standard Model have been experimentally verified.[7][8]

Supersymmetry in quantum field theory
In quantum field theory, supersymmetry is motivated by solutions to several theoretical problems, for generally providing many desirable mathematical properties, and for ensuring sensible behavior at high energies. Supersymmetric quantum field theory is often much easier to analyze, as many more problems become mathematically tractable. When supersymmetry is imposed as a local symmetry, Einstein's theory of general relativity is included automatically, and the result is said to be a theory of supergravity. Another theoretically appealing property of supersymmetry is that it offers the only "loophole" to the Coleman–Mandula theorem, which prohibits spacetime and internal symmetries from being combined in any nontrivial way, for quantum field theories with very general assumptions. The Haag–Łopuszański–Sohnius theorem demonstrates that supersymmetry is the only way spacetime and internal symmetries can be combined consistently.[25]

While supersymmetry has not yet been discovered at high energy, see Section Supersymmetry in particle physics, supersymmetry was found to be effectively realized at the intermediate energy of hadronic physics where baryons and mesons are superpartners. An exception is the pion that appears as a zero mode in the mass spectrum and thus protected by the supersymmetry: It has no baryonic partner.[26][27] The realization of this effective supersymmetry is readily explained in quark–diquark models: Because two different color charges close together (e.g., blue and red) appear under coarse resolution as the corresponding anti-color (e.g. anti-green), a diquark cluster viewed with coarse resolution (i.e., at the energy-momentum scale used to study hadron structure) effectively appears as an antiquark. Therefore, a baryon containing 3 valence quarks, of which two tend to cluster together as a diquark, behaves likes a meson.
$
10
Question 1: In a supersymmetric theory, the equations for which of the following are identical?
A: Force and gravity
B: Matter and energy
C: Force and matter
D: Energy and force
E: Gravity and energy

Answer: C

Question 2: What spacetime symmetry exists between which two classes of particles in supersymmetry?
A: Fermions and quarks
B: Bosons and leptons
C: Bosons and fermions
D: Gluons and bosons
E: Quarks and gluons

Answer: C

Question 3: If the electron exists in a supersymmetric theory, which particle would be its superpartner?
A: Bosontron
B: Neutron
C: Selectron
D: Fermiontron
E: Electron-neutrino

Answer: C

Question 4: In perfectly "unbroken" supersymmetry theories, which property is shared between each pair of superpartners?
A: Size
B: Electric charge
C: Spin
D: Mass and internal quantum numbers (besides spin)
E: Interaction range

Answer: D

Question 5: Supersymmetry has not been experimentally verified in which area?
A: Extensions of the Standard Model
B: Quantum mechanics
C: Quantum gravity
D: Nuclear physics
E: Statistical mechanics

Answer: A

Question 6: What does supersymmetric quantum field theory provide solutions to?
A: Only high-energy problems
B: Some statistical problems
C: Several theoretical problems in field theory
D: Classical mechanics problems
E: Thermodynamic problems

Answer: C

Question 7: When supersymmetry is imposed as a local symmetry, what theory is included automatically?
A: Quantum mechanics
B: Superelectricity
C: String theory
D: Supergravity
E: Superconductivity

Answer: D

Question 8: The Coleman–Mandula theorem prohibits the combination of spacetime and internal symmetries in any nontrivial way, with one exception. What is that exception?
A: Superconductivity
B: Superfluidity
C: Supergeometry
D: Supersymmetry
E: Superposition

Answer: D

Question 9: Which theorem demonstrates that supersymmetry is the only way spacetime and internal symmetries can be combined consistently?
A: Fermi-Dirac theorem
B: Haag–Łopuszański–Sohnius theorem
C: Bose-Einstein theorem
D: Heisenberg's uncertainty theorem
E: Schrödinger's wave equation theorem

Answer: B

Question 10: In the realization of effective supersymmetry, which particle appears as a zero mode in the mass spectrum and has no baryonic partner?
A: Quark
B: Diquark
C: Meson
D: Pion
E: Baryon

Answer: D
@
Subject:
In physics, a parity transformation (also called parity inversion) is the flip in the sign of one spatial coordinate.

It can also be thought of as a test for chirality of a physical phenomenon, in that a parity inversion transforms a phenomenon into its mirror image.

All fundamental interactions of elementary particles, with the exception of the weak interaction, are symmetric under parity. As established by the Wu experiment conducted at the US National Bureau of Standards by Chinese-American scientist Chien-Shiung Wu, the weak interaction is chiral and thus provides a means for probing chirality in physics. In her experiment, Wu took advantage of the controlling role of weak interactions in radioactive decay of atomic isotopes to establish the chirality of the weak force.

By contrast, in interactions that are symmetric under parity, such as electromagnetism in atomic and molecular physics, parity serves as a powerful controlling principle underlying quantum transitions.

A matrix representation of P (in any number of dimensions) has determinant equal to −1, and hence is distinct from a rotation, which has a determinant equal to 1. In a two-dimensional plane, a simultaneous flip of all coordinates in sign is not a parity transformation; it is the same as a 180° rotation.

In quantum mechanics, wave functions that are unchanged by a parity transformation are described as even functions, while those that change sign under a parity transformation are odd functions.

Consequences of parity symmetry
When parity generates the Abelian group ℤ2, one can always take linear combinations of quantum states such that they are either even or odd under parity (see the figure). Thus the parity of such states is ±1. The parity of a multiparticle state is the product of the parities of each state; in other words parity is a multiplicative quantum number.

In quantum mechanics, Hamiltonians are invariant (symmetric) under a parity transformation if 
�
^{\displaystyle {\hat {\mathcal {P}}}} commutes with the Hamiltonian. In non-relativistic quantum mechanics, this happens for any scalar potential, i.e., 
�
=
�
(
�
)
{\displaystyle V=V{\left(r\right)}}, hence the potential is spherically symmetric.
$
10
Question 1: A parity transformation in physics is equivalent to:
A: A rotation of 90°.
B: The flip in the sign of one spatial coordinate.
C: A translation in space.
D: A mirror reflection along the time axis.
E: A rotation of 180° in three-dimensional space.

Answer: B

Question 2: Which of the following interactions is not symmetric under parity?
A: Electromagnetic
B: Gravitational
C: Strong
D: Weak
E: Magnetic

Answer: D

Question 3: The Wu experiment aimed to establish the chirality of:
A: The strong force.
B: Electromagnetism.
C: The gravitational force.
D: The weak force.
E: The electromagnetic force in molecules.

Answer: D

Question 4: In quantum mechanics, wave functions that are unchanged by a parity transformation are described as:
A: Neutral functions.
B: Odd functions.
C: Mixed functions.
D: Even functions.
E: Complex functions.

Answer: D

Question 5: If the determinant of a matrix representation of P is equal to −1, it indicates:
A: A rotation.
B: A translation.
C: A parity transformation.
D: An inversion.
E: A reflection.

Answer: C

Question 6: In a multiparticle state, the parity is the:
A: Sum of the parities of each state.
B: Difference of the parities of each state.
C: Division of the parities of each state.
D: Logarithm of the parities of each state.
E: Product of the parities of each state.

Answer: E

Question 7: A Hamiltonian is invariant under a parity transformation if:
A: It does not commute with the Hamiltonian.
B: It commutes with the Hamiltonian.
C: It inverses the Hamiltonian.
D: It has a determinant of 1.
E: It is not related to the Hamiltonian.

Answer: B

Question 8: In non-relativistic quantum mechanics, the potential is spherically symmetric if:
A: V is a vector.
B: V is equal to r.
C: V depends only on the radius r.
D: V is independent of the radius r.
E: V is zero everywhere.

Answer: C

Question 9: In a two-dimensional plane, a simultaneous flip of all coordinates in sign can be viewed as:
A: A 90° rotation.
B: A parity transformation.
C: A translation.
D: A 180° rotation.
E: A reflection about the origin.

Answer: D

Question 10: Which of the following is a test for chirality in physics?
A: A rotation transformation.
B: A translation.
C: A parity inversion.
D: An isometry.
E: A scale transformation.

Answer: C
@
Subject:
In cellular biology, active transport is the movement of molecules or ions across a cell membrane from a region of lower concentration to a region of higher concentration—against the concentration gradient. Active transport requires cellular energy to achieve this movement. There are two types of active transport: primary active transport that uses adenosine triphosphate (ATP), and secondary active transport that uses an electrochemical gradient. This process is in contrast to passive transport, which allows molecules or ions to move down their concentration gradient, from an area of high concentration to an area of low concentration, without energy.

Active transport is essential for various physiological processes, such as nutrient uptake, hormone secretion, and nerve impulse transmission. For example, the sodium-potassium pump uses ATP to pump sodium ions out of the cell and potassium ions into the cell, maintaining a concentration gradient essential for cellular function. Active transport is highly selective and regulated, with different transporters specific to different molecules or ions. Dysregulation of active transport can lead to various disorders, including cystic fibrosis, caused by a malfunctioning chloride channel, and diabetes, resulting from defects in glucose transport into cells.
$
6
Question 1: Which type of active transport utilizes adenosine triphosphate (ATP)?
A: Primary active transport
B: Passive transport
C: Secondary active transport
D: Osmosis
E: Facilitated diffusion

Answer: A

Question 2: In what direction do molecules or ions move during active transport across a cell membrane?
A: From an area of low concentration to an area of high concentration.
B: From an area of high concentration to an area of low concentration.
C: They do not move across the membrane.
D: They move randomly without any specific direction.
E: They move from an area of neutral concentration to an area of high concentration.

Answer: A

Question 3: What role does the sodium-potassium pump play in active transport?
A: It uses ATP to pump sodium ions into the cell and potassium ions out of the cell.
B: It uses ATP to pump sodium ions out of the cell and potassium ions into the cell.
C: It facilitates the diffusion of sodium and potassium ions without using ATP.
D: It blocks the movement of sodium and potassium ions across the cell membrane.
E: It helps in the passive transport of sodium and potassium ions.

Answer: B

Question 4: Which of the following disorders is caused by a malfunctioning chloride channel related to active transport?
A: Hypertension
B: Cystic fibrosis
C: Asthma
D: Coronary artery disease
E: Sickle cell anemia

Answer: B

Question 5: Which of the following best describes secondary active transport?
A: It directly uses ATP to transport molecules.
B: It allows molecules to move down their concentration gradient.
C: It uses an electrochemical gradient for the transport of molecules.
D: It relies solely on the passive diffusion of molecules.
E: It prevents any form of transport across the cell membrane.

Answer: C

Question 6: Why is active transport considered highly selective?
A: Because it allows any molecule to pass through regardless of size and charge.
B: Because it uses the arithmetic sum of osmosis and an electric field.
C: Because it has different transporters specific to different molecules or ions.
D: Because it only operates in prokaryotic cells.
E: Because it relies solely on the presence of ATP for selectivity.

Answer: C
@
Subject:
In quantum mechanics, the uncertainty principle (also known as Heisenberg's uncertainty principle)[1] is any of a variety of mathematical inequalities asserting a fundamental limit to the product of the accuracy of certain related pairs of measurements on a quantum system, such as position, x, and momentum, p.[2] Such paired-variables are known as complementary variables or canonically conjugate variables.

In quantum mechanics, angular momentum (like other quantities) is expressed as an operator, and its one-dimensional projections have quantized eigenvalues. Angular momentum is subject to the Heisenberg uncertainty principle, implying that at any time, only one projection (also called "component") can be measured with definite precision; the other two then remain uncertain. Because of this, the axis of rotation of a quantum particle is undefined. Quantum particles do possess a type of non-orbital angular momentum called "spin", but this angular momentum does not correspond to a spinning motion.[32] In relativistic quantum mechanics the above relativistic definition becomes a tensorial operator.
$
6
Question 1: Which of the following variables are considered as complementary or canonically conjugate variables in the context of Heisenberg's uncertainty principle?
A: Energy and time
B: Position, 
�
x, and momentum, 
�
p
C: Angular momentum and linear momentum
D: Speed and acceleration
E: Force and torque

Answer: B

Question 2: What type of non-orbital angular momentum do quantum particles possess?
A: Linear momentum
B: Torque
C: Inertia
D: Spin
E: Radius

Answer: D

Question 3: Which of the following best describes the nature of "spin" in quantum particles?
A: It corresponds to a physical spinning motion of the particle.
B: It is related to the orbital motion of the particle.
C: It does not correspond to a spinning motion.
D: It represents the angular velocity of a quantum particle.
E: It signifies the trajectory of a quantum particle in motion.

Answer: C

Question 4: In the context of quantum mechanics, which of the following is a key characteristic of an operator for angular momentum?
A: It has continuous eigenvalues.
B: It represents the arithmetic sum of all momenta.
C: Its one-dimensional projections have quantized eigenvalues.
D: It is solely defined for non-relativistic systems.
E: It describes the total energy of the quantum system.

Answer: C

Question 5: What limitation does the Heisenberg uncertainty principle place on the measurement of a quantum particle's angular momentum?
A: It ensures that all projections of angular momentum can be precisely measured.
B: It allows for precise measurement of two projections simultaneously.
C: It guarantees that the axis of rotation is always defined.
D: It implies that only one projection can be measured precisely at a given time, leaving the other two uncertain.
E: It defines a direct relation between the quantum particle's energy and its angular momentum.

Answer: D

Question 6: How does relativistic quantum mechanics modify the definition of angular momentum in quantum mechanics?
A: It introduces the concept of spin.
B: It disregards the Heisenberg uncertainty principle.
C: It expresses angular momentum as a scalar value.
D: It expresses angular momentum as a tensorial operator.
E: It merges angular momentum with linear momentum.

Answer: D
@
Subject:
Convection is single or multiphase fluid flow that occurs spontaneously due to the combined effects of material property heterogeneity and body forces on a fluid, most commonly density and gravity (see buoyancy). When the cause of the convection is unspecified, convection due to the effects of thermal expansion and buoyancy can be assumed. Convection may also take place in soft solids or mixtures where particles can flow.

Convective flow may be transient (such as when a multiphase mixture of oil and water separates) or steady state (see Convection cell). The convection may be due to gravitational, electromagnetic or fictitious body forces. Heat transfer by natural convection plays a role in the structure of Earth's atmosphere, its oceans, and its mantle. Discrete convective cells in the atmosphere can be identified by clouds, with stronger convection resulting in thunderstorms. Natural convection also plays a role in stellar physics. Convection is often categorised or described by the main effect causing the convective flow, e.g. Thermal convection.

Convection cannot take place in most solids because neither bulk current flows nor significant diffusion of matter can take place. Granular convection is a similar phenomenon in granular material instead of fluids. Advection is fluid motion created by velocity instead of thermal gradients. Convective heat transfer is the intentional use of convection as a method for heat transfer. Convection is a process in which heat is carried from place to place by the bulk movement of a fluid and gases.

Natural convection is a type of flow, of motion of a liquid such as water or a gas such as air, in which the fluid motion is not generated by any external source (like a pump, fan, suction device, etc.) but by some parts of the fluid being heavier than other parts. In most cases this leads to natural circulation, the ability of a fluid in a system to circulate continuously, with gravity and possible changes in heat energy. The driving force for natural convection is gravity. For example if there is a layer of cold dense air on top of hotter less dense air, gravity pulls more strongly on the denser layer on top, so it falls while the hotter less dense air rises to take its place. This creates circulating flow: convection. As it relies on gravity, there is no convection in free-fall (inertial) environments, such as that of the orbiting International Space Station. Natural convection can occur when there are hot and cold regions of either air or water, because both water and air become less dense as they are heated. But, for example, in the world's oceans it also occurs due to salt water being heavier than fresh water, so a layer of salt water on top of a layer of fresher water will also cause convection.

Forced convection is a mechanism, or type of transport, in which fluid motion is generated by an external source (like a pump, fan, suction device, etc.). Alongside natural convection, thermal radiation, and thermal conduction it is one of the methods of heat transfer and allows significant amounts of heat energy to be transported very efficiently.
$
10
Question 1: Which force is the primary driving factor behind natural convection?
A: Electromagnetic
B: Tensional
C: Gravitational
D: Frictional
E: Centrifugal

Answer: C

Question 2: Why can't convection take place in most solids?
A: They are too dense.
B: They can't produce heat.
C: They don't have significant diffusion or bulk current flows.
D: They don't respond to gravitational forces.
E: They lack fluidity.

Answer: C

Question 3: Which phenomenon involves heat being carried from one place to another by the bulk movement of a fluid or gases?
A: Conduction
B: Radiation
C: Advection
D: Evaporation
E: Convection

Answer: E

Question 4: In which environment is convection NOT observed due to its reliance on gravity?
A: On the moon's surface
B: At the Earth's core
C: In the orbiting International Space Station
D: In the world's oceans
E: In Earth's atmosphere

Answer: C

Question 5: Which of the following can cause natural convection in the world's oceans?
A: Air pressure
B: Differences in water's electrical conductivity
C: Salt water being heavier than fresh water
D: The moon's gravitational pull
E: Ocean currents created by wind

Answer: C

Question 6: What causes convective heat transfer?
A: Direct contact between two solids
B: Emission of electromagnetic waves
C: Movement of molecules within a solid
D: Bulk movement of a fluid
E: Reflection of sunlight

Answer: D

Question 7: What type of convection is driven by external sources like pumps or fans?
A: Thermal convection
B: Granular convection
C: Natural convection
D: Forced convection
E: Inertial convection

Answer: D

Question 8: Discrete convective cells in the atmosphere can be identified by which of the following?
A: Wind patterns
B: Atmospheric pressure
C: Clouds
D: Ocean currents
E: Earth's magnetic field

Answer: C

Question 9: What is the main difference between advection and convection?
A: Advection relies on heat, while convection relies on velocity.
B: Advection involves bulk movement due to velocity gradients, while convection is due to thermal gradients.
C: Advection is a type of convection.
D: Advection and convection are terms that can be used interchangeably.
E: Advection is a phenomenon only observed in liquids, while convection is only observed in gases.

Answer: B

Question 10: In the context of convection, what does it mean when fluid flow is transient?
A: It means the flow is permanent and unchanging.
B: It means the flow changes direction periodically.
C: It means the flow is temporary or changing.
D: It means the flow is extremely slow.
E: It means the flow is driven by natural forces.

Answer: C
@
Subject:
In electromagnetism, the magnetic susceptibility (from Latin susceptibilis 'receptive'; denoted χ, chi) is a measure of how much a material will become magnetized in an applied magnetic field. It is the ratio of magnetization M (magnetic moment per unit volume) to the applied magnetizing field intensity H. This allows a simple classification, into two categories, of most materials' responses to an applied magnetic field: an alignment with the magnetic field, χ > 0, called paramagnetism, or an alignment against the field, χ < 0, called diamagnetism.

Magnetic susceptibility indicates whether a material is attracted into or repelled out of a magnetic field. Paramagnetic materials align with the applied field and are attracted to regions of greater magnetic field. Diamagnetic materials are anti-aligned and are pushed away, toward regions of lower magnetic fields. On top of the applied field, the magnetization of the material adds its own magnetic field, causing the field lines to concentrate in paramagnetism, or be excluded in diamagnetism.[1] Quantitative measures of the magnetic susceptibility also provide insights into the structure of materials, providing insight into bonding and energy levels. Furthermore, it is widely used in geology for paleomagnetic studies and structural geology.[2]

The magnetizability of materials comes from the atomic-level magnetic properties of the particles of which they are made. Usually, this is dominated by the magnetic moments of electrons. Electrons are present in all materials, but without any external magnetic field, the magnetic moments of the electrons are usually either paired up or random so that the overall magnetism is zero (the exception to this usual case is ferromagnetism). The fundamental reasons why the magnetic moments of the electrons line up or do not are very complex and cannot be explained by classical physics. However, a useful simplification is to measure the magnetic susceptibility of a material and apply the macroscopic form of Maxwell's equations. This allows classical physics to make useful predictions while avoiding the underlying quantum mechanical details.
$
6
Question 1: In the context of magnetic susceptibility, which category describes materials that align with an applied magnetic field?
A: Diamagnetism
B: Ferromagnetism
C: Antimagnetism
D: Paramagnetism
E: Supermagnetism

Answer: D

Question 2: What is the behavior of diamagnetic materials when exposed to a magnetic field?
A: They align with the applied field and attract towards regions of greater magnetic field.
B: They show no response to the applied field.
C: They are anti-aligned and move toward regions of lower magnetic fields.
D: Their magnetic moments are usually paired up.
E: They are indifferent to the direction of the magnetic field.

Answer: C

Question 3: Why do the magnetic moments of electrons usually result in overall zero magnetism in most materials without an external magnetic field?
A: Because the electrons' magnetic moments are either paired up or random.
B: Because electrons repel each other due to their negative charges.
C: Because electrons lose their magnetic properties over time.
D: Because of their high susceptibility to external magnetic fields.
E: Because electrons do not inherently have magnetic properties.

Answer: A

Question 4: Which of the following is NOT a use or application of measuring magnetic susceptibility?
A: Paleomagnetic studies
B: Structural geology
C: Temperature determination
D: Gaining insight into bonding and energy levels
E: Assessing material response to applied magnetic fields

Answer: C

Question 5: The fundamental reasons behind the alignment or non-alignment of electrons' magnetic moments can be best explained by:
A: Thermodynamics
B: Classical physics
C: Quantum mechanics
D: Newtonian physics
E: Electrodynamics

Answer: C

Question 6: Which statement about magnetic susceptibility is true?
A: Magnetic susceptibility describes how much a material can be magnetized by its own inherent properties.
B: Magnetic susceptibility provides the ratio of the magnetic field to magnetization in a material.
C: A positive magnetic susceptibility indicates a material's diamagnetic response.
D: Paramagnetic materials are pushed away towards regions of lower magnetic fields.
E: Diamagnetic materials add their own magnetic field which causes field lines to be excluded.

Answer: E
@
Subject:
A transient condensation cloud, also called a Wilson cloud, is observable surrounding large explosions in humid air.

When a nuclear weapon or a large amount of a conventional explosive is detonated in sufficiently humid air, the "negative phase" of the shock wave causes a rarefaction of the air surrounding the explosion, but not contained within it. This rarefaction results in a temporary cooling of that air, which causes a condensation of some of the water vapor contained in it. When the pressure and the temperature return to normal, the Wilson cloud dissipates.[1]

Mechanism
Since heat does not leave the affected air mass, this change of pressure is adiabatic, with an associated change of temperature. In humid air, the drop in temperature in the most rarefied portion of the shock wave can bring the air temperature below its dew point, at which moisture condenses to form a visible cloud of microscopic water droplets. Since the pressure effect of the wave is reduced by its expansion (the same pressure effect is spread over a larger radius), the vapor effect also has a limited radius. Such vapor can also be seen in low pressure regions during high–g subsonic maneuvers of aircraft in humid conditions.

Nuclear weapons testing
Scientists observing the Operation Crossroads nuclear tests in 1946 at Bikini Atoll named that transitory cloud a "Wilson cloud" because the same pressure effect is employed in a Wilson cloud chamber to let condensation mark the tracks of electrically-charged sub-atomic particles. Analysts of later nuclear bomb tests used the more general term condensation cloud.

The shape of the shock wave, influenced by different speed in different altitudes, and the temperature and humidity of different atmospheric layers determines the appearance of the Wilson clouds. During nuclear tests, condensation rings around or above the fireball are commonly observed. Rings around the fireball may become stable and form rings around the rising stem of the mushroom cloud.

The lifetime of the Wilson cloud during nuclear air bursts can be shortened by the thermal radiation from the fireball, which heats the cloud above to the dew point and evaporates the droplets.

Any sufficiently large explosion, such as one caused by a large quantity of conventional explosives or a volcanic eruption, can create a condensation cloud,[2][3] as seen in Operation Sailor Hat[4] or in the 2020 Beirut explosion, where a very large Wilson cloud expanded outwards from the blast.[2]
$
6
Question 1: Which phase of the shock wave in an explosion causes a rarefaction of the surrounding air leading to the formation of a Wilson cloud?
A: Positive phase
B: Neutral phase
C: Negative phase
D: Stationary phase
E: Oscillatory phase

Answer: C

Question 2: Why does the affected air mass experience a change in temperature during the formation of a Wilson cloud?
A: Because of the radiation emitted from the explosion.
B: Because of the heat leaving the affected air mass.
C: Due to the adiabatic change in pressure.
D: Because of external atmospheric conditions.
E: Due to heat generated by the burning materials in the explosion.

Answer: C

Question 3: In what device is the same pressure effect as in a Wilson cloud employed to mark the tracks of electrically-charged sub-atomic particles?
A: Nuclear reactor
B: Electromagnetic chamber
C: Wilson cloud chamber
D: Geiger counter
E: Atomic clock

Answer: C

Question 4: What determines the appearance of the Wilson clouds during nuclear tests?
A: Only the temperature and humidity of the atmospheric layers.
B: Only the shape of the shock wave.
C: The speed of the explosion and the amount of conventional explosives used.
D: The shape of the shock wave, temperature, and humidity of different atmospheric layers.
E: The altitude of the explosion and the presence of other gases.

Answer: D

Question 5: What effect can thermal radiation from a nuclear explosion's fireball have on a Wilson cloud?
A: It can increase the lifetime of the cloud.
B: It causes the cloud to change color.
C: It makes the cloud denser.
D: It can shorten the cloud's lifetime by heating it above the dew point and evaporating the droplets.
E: It causes the cloud to rise higher into the atmosphere.

Answer: D

Question 6: What event from 2020 exhibited a large Wilson cloud expanding outwards from the blast?
A: The Fukushima nuclear incident
B: The Chernobyl disaster anniversary
C: The 2020 Beirut explosion
D: The North Korean nuclear test
E: The eruption of Mount Vesuvius

Answer: C
@
Subject:
In mathematics, hyperbolic geometry (also called Lobachevskian geometry or Bolyai–Lobachevskian geometry) is a non-Euclidean geometry. The parallel postulate of Euclidean geometry is replaced with:

For any given line R and point P not on R, in the plane containing both line R and point P there are at least two distinct lines through P that do not intersect R.
(Compare the above with Playfair's axiom, the modern version of Euclid's parallel postulate.)

The hyperbolic plane is a plane where every point is a saddle point. Hyperbolic plane geometry is also the geometry of pseudospherical surfaces, surfaces with a constant negative Gaussian curvature. Saddle surfaces have negative Gaussian curvature in at least some regions, where they locally resemble the hyperbolic plane.

A modern use of hyperbolic geometry is in the theory of special relativity, particularly the Minkowski model.

When geometers first realised they were working with something other than the standard Euclidean geometry, they described their geometry under many different names; Felix Klein finally gave the subject the name hyperbolic geometry to include it in the now rarely used sequence elliptic geometry (spherical geometry), parabolic geometry (Euclidean geometry), and hyperbolic geometry. In the former Soviet Union, it is commonly called Lobachevskian geometry, named after one of its discoverers, the Russian geometer Nikolai Lobachevsky.

This page is mainly about the 2-dimensional (planar) hyperbolic geometry and the differences and similarities between Euclidean and hyperbolic geometry. See hyperbolic space for more information on hyperbolic geometry extended to three and more dimensions.

In hyperbolic geometry, a uniform hyperbolic tiling (or regular, quasiregular or semiregular hyperbolic tiling) is an edge-to-edge filling of the hyperbolic plane which has regular polygons as faces and is vertex-transitive (transitive on its vertices, isogonal, i.e. there is an isometry mapping any vertex onto any other). It follows that all vertices are congruent, and the tiling has a high degree of rotational and translational symmetry.

Uniform tilings can be identified by their vertex configuration, a sequence of numbers representing the number of sides of the polygons around each vertex. For example, 7.7.7 represents the heptagonal tiling which has 3 heptagons around each vertex. It is also regular since all the polygons are the same size, so it can also be given the Schläfli symbol {7,3}.

Uniform tilings may be regular (if also face- and edge-transitive), quasi-regular (if edge-transitive but not face-transitive) or semi-regular (if neither edge- nor face-transitive). For right triangles (p q 2), there are two regular tilings, represented by Schläfli symbol {p,q} and {q,p}.
$
10
Question 1: Which of the following is not a non-Euclidean geometry?
A: Elliptic geometry
B: Parabolic geometry
C: Lobachevskian geometry
D: Hyperbolic geometry
E: Cubic geometry

Answer: E

Question 2: In hyperbolic geometry, what is different compared to Euclidean geometry regarding lines and points?
A: There is only one line through a point parallel to a given line.
B: There are at least two distinct lines through a point that do not intersect a given line.
C: All lines through a point intersect a given line.
D: All lines are curved.
E: Points can be on multiple lines simultaneously.

Answer: B

Question 3: Which of the following surfaces resembles the hyperbolic plane due to its negative Gaussian curvature?
A: Sphere
B: Flat plane
C: Cone
D: Saddle surface
E: Cylinder

Answer: D

Question 4: Hyperbolic geometry plays a role in which modern theory?
A: Quantum Mechanics
B: General Relativity
C: Newtonian Physics
D: Special Relativity
E: Thermodynamics

Answer: D

Question 5: Which geometer from Russia is hyperbolic geometry also named after in the former Soviet Union?
A: Pierre de Fermat
B: Henri Poincaré
C: Nikolai Lobachevsky
D: Carl Friedrich Gauss
E: Felix Klein

Answer: C

Question 6: The vertex configuration in uniform tiling is represented by a sequence of numbers. What do these numbers represent?
A: The angles of the polygons.
B: The number of vertices in each polygon.
C: The distance between vertices.
D: The number of sides of the polygons around each vertex.
E: The area of the polygons.

Answer: D

Question 7: Which of the following is NOT a type of uniform tiling in hyperbolic geometry?
A: Regular
B: Quasi-regular
C: Semi-regular
D: Irregular
E: Non-uniform

Answer: D

Question 8: For right triangles in hyperbolic geometry, how many regular tilings are there?
A: One
B: Two
C: Three
D: Four
E: Infinite

Answer: B

Question 9: What does the Schläfli symbol {7,3} represent in hyperbolic tiling?
A: A tiling with 7 triangles around each vertex and each triangle has 3 sides.
B: A tiling with 3 heptagons around each vertex.
C: A tiling with 7 squares around each vertex.
D: A tiling with 3 septagons around each vertex.
E: A tiling with 7 heptagons around each vertex.

Answer: E

Question 10: In which of the following dimensions is this page mainly focused on for hyperbolic geometry?
A: 1-dimensional
B: 2-dimensional (planar)
C: 3-dimensional
D: 4-dimensional
E: Infinite-dimensional

Answer: B
@
Subject:
In solid mechanics, a bending moment is the reaction induced in a structural element when an external force or moment is applied to the element, causing the element to bend.[1][2] The most common or simplest structural element subjected to bending moments is the beam. The diagram shows a beam which is simply supported (free to rotate and therefore lacking bending moments) at both ends; the ends can only react to the shear loads. Other beams can have both ends fixed (known as encastre beam); therefore each end support has both bending moments and shear reaction loads. Beams can also have one end fixed and one end simply supported. The simplest type of beam is the cantilever, which is fixed at one end and is free at the other end (neither simple or fixed). In reality, beam supports are usually neither absolutely fixed nor absolutely rotating freely.

In civil engineering and structural analysis Clapeyron's theorem of three moments is a relationship among the bending moments at three consecutive supports of a horizontal beam.

Mohr's first theorem
The change in slope of a deflection curve between two points of a beam is equal to the area of the M/EI diagram between those two points.

Mohr's second theorem
Consider two points k1 and k2 on a beam. The deflection of k1 and k2 relative to the point of intersection between tangent at k1 and k2 and vertical through k1 is equal to the moment of M/EI diagram between k1 and k2 about k1.(Figure 03)

The three moment equation expresses the relation between bending moments at three successive supports of a continuous beam, subject to a loading on a two adjacent span with or without settlement of the supports.
$
10
Question 1: Which structural element is most commonly subjected to bending moments?
A: Column
B: Slab
C: Beam
D: Foundation
E: Pylon

Answer: C

Question 2: If a beam is simply supported at both ends, which type of reaction can the ends of the beam react to?
A: Compression
B: Tension
C: Shear loads
D: Torque
E: Axial loads

Answer: C

Question 3: What type of beam is fixed at one end and free at the other end?
A: Supported beam
B: Encastre beam
C: Cantilever
D: T-beam
E: Composite beam

Answer: C

Question 4: In real-life scenarios, how are beam supports usually characterized?
A: Absolutely fixed
B: Absolutely rotating freely
C: Neither absolutely fixed nor absolutely rotating freely
D: Perfectly elastic
E: Infinite strength

Answer: C

Question 5: In structural analysis, what is the significance of Clapeyron's theorem of three moments?
A: It calculates the weight distribution on the beam.
B: It describes the relationship between tension and compression in a beam.
C: It is a relationship among the bending moments at three consecutive supports of a horizontal beam.
D: It measures the shear stress at any point along the beam.
E: It calculates the elastic modulus of the beam material.

Answer: C

Question 6: According to Mohr's first theorem, what is equal to the change in slope of a deflection curve between two points of a beam?
A: The moment of M/EI diagram between the two points
B: The shear force between the two points
C: The bending moment at the midpoint between the two points
D: The area of the M/EI diagram between those two points
E: The axial load applied at the midpoint between the two points

Answer: D

Question 7: Mohr's second theorem relates to the deflection of two points on a beam with respect to what?
A: The length of the beam
B: The weight distribution of the beam
C: The point of intersection between the tangent at those two points and the vertical through the first point
D: The maximum bending moment of the beam
E: The compression force on the beam

Answer: C

Question 8: In the three moment equation, what can potentially influence the bending moments at the supports of a continuous beam?
A: The temperature of the beam
B: The elasticity of the beam
C: Loading on two adjacent spans
D: The thickness of the beam
E: The type of material used for the beam

Answer: C

Question 9: When is the three moment equation especially important in beam analysis?
A: When there is settlement of the supports
B: When the beam is in a vacuum
C: When only tension forces are present
D: When axial loads are dominant
E: When there is no external loading

Answer: A

Question 10: If the supports of a beam are neither absolutely fixed nor absolutely rotating freely, how would you best describe them?
A: Rigid supports
B: Elastically restrained supports
C: Intermediate supports
D: Dynamic supports
E: Flexible supports

Answer: B
@
Subject:
In thermodynamics, the Joule–Thomson effect (also known as the Joule–Kelvin effect or Kelvin–Joule effect) describes the temperature change of a real gas or liquid (as differentiated from an ideal gas) when it is forced through a valve or porous plug while keeping it insulated so that no heat is exchanged with the environment.[1][2][3] This procedure is called a throttling process or Joule–Thomson process.[4] At room temperature, all gases except hydrogen, helium, and neon cool upon expansion by the Joule–Thomson process when being throttled through an orifice; these three gases experience the same effect but only at lower temperatures.[5][6] Most liquids such as hydraulic oils will be warmed by the Joule–Thomson throttling process.

The gas-cooling throttling process is commonly exploited in refrigeration processes such as liquefiers in air separation industrial process.[7][8] In hydraulics, the warming effect from Joule–Thomson throttling can be used to find internally leaking valves as these will produce heat which can be detected by thermocouple or thermal-imaging camera. Throttling is a fundamentally irreversible process. The throttling due to the flow resistance in supply lines, heat exchangers, regenerators, and other components of (thermal) machines is a source of losses that limits their performance.

Since it is a constant-enthalpy process, it can be used to experimentally measure the lines of constant enthalpy (isenthalps) on the 
(
�
,
�
)
{\displaystyle (p,T)} diagram of a gas. Combined with the specific heat capacity at constant pressure 
�
�
=
(
∂
ℎ
/
∂
�
)
�
{\displaystyle c_{P}=(\partial h/\partial T)_{P}} it allows the complete measurement of the thermodynamic potential for the gas.[9]

The adiabatic (no heat exchanged) expansion of a gas may be carried out in a number of ways. The change in temperature experienced by the gas during expansion depends not only on the initial and final pressure, but also on the manner in which the expansion is carried out.

If the expansion process is reversible, meaning that the gas is in thermodynamic equilibrium at all times, it is called an isentropic expansion. In this scenario, the gas does positive work during the expansion, and its temperature decreases.
In a free expansion, on the other hand, the gas does no work and absorbs no heat, so the internal energy is conserved. Expanded in this manner, the temperature of an ideal gas would remain constant, but the temperature of a real gas decreases, except at very high temperature.[10]
The method of expansion discussed in this article, in which a gas or liquid at pressure P1 flows into a region of lower pressure P2 without significant change in kinetic energy, is called the Joule–Thomson expansion. The expansion is inherently irreversible. During this expansion, enthalpy remains unchanged (see proof below). Unlike a free expansion, work is done, causing a change in internal energy. Whether the internal energy increases or decreases is determined by whether work is done on or by the fluid; that is determined by the initial and final states of the expansion and the properties of the fluid.

The temperature change produced during a Joule–Thomson expansion is quantified by the Joule–Thomson coefficient, 
�
J
T
\mu _{{{\mathrm  {JT}}}}. This coefficient may be either positive (corresponding to cooling) or negative (heating); the regions where each occurs for molecular nitrogen, N2, are shown in the figure. Note that most conditions in the figure correspond to N2 being a supercritical fluid, where it has some properties of a gas and some of a liquid, but can not be really described as being either. The coefficient is negative at both very high and very low temperatures; at very high pressure it is negative at all temperatures. The maximum inversion temperature (621 K for N2[11]) occurs as zero pressure is approached. For N2 gas at low pressures, 
�
J
T
\mu _{{{\mathrm  {JT}}}} is negative at high temperatures and positive at low temperatures. At temperatures below the gas-liquid coexistence curve, N2 condenses to form a liquid and the coefficient again becomes negative. Thus, for N2 gas below 621 K, a Joule–Thomson expansion can be used to cool the gas until liquid N2 forms.
$
10
Based on the information provided on the Joule–Thomson effect, here are 10 practice questions:

Question 1: Which of the following gases does NOT cool upon expansion by the Joule–Thomson process at room temperature?
A: Oxygen
B: Hydrogen
C: Nitrogen
D: Carbon Dioxide
E: Argon

Answer: B

Question 2: Which effect best describes the temperature change of a real gas or liquid when forced through a valve while insulated?
A: Gibbs free energy change
B: Carnot cycle effect
C: Kelvin–Planck effect
D: Joule–Thomson effect
E: Rankine cycle effect

Answer: D

Question 3: How is the Joule–Thomson effect commonly exploited in industries?
A: In combustion processes
B: In hydraulic processes
C: In air separation processes
D: In steam turbine processes
E: In gas turbine processes

Answer: C

Question 4: In hydraulics, how can the Joule–Thomson throttling be utilized?
A: To find externally leaking valves
B: To measure gas pressures
C: To find internally leaking valves
D: To determine the volume flow rate
E: To analyze the chemical composition of fluids

Answer: C

Question 5: The throttling process is fundamentally:
A: Reversible
B: Isothermal
C: Irreversible
D: Isobaric
E: Adiabatic

Answer: C

Question 6: The Joule–Thomson expansion can be used to experimentally measure the lines of:
A: Constant volume
B: Constant entropy
C: Constant pressure
D: Constant enthalpy
E: Constant temperature

Answer: D

Question 7: Which type of expansion ensures that a gas is in thermodynamic equilibrium at all times?
A: Free expansion
B: Adiabatic expansion
C: Isothermal expansion
D: Isentropic expansion
E: Joule–Thomson expansion

Answer: D

Question 8: During the Joule–Thomson expansion, which of the following remains unchanged?
A: Temperature
B: Pressure
C: Volume
D: Entropy
E: Enthalpy

Answer: E

Question 9: What does a positive Joule–Thomson coefficient represent?
A: Heating
B: No change in temperature
C: Pressure increase
D: Pressure decrease
E: Cooling

Answer: E

Question 10: For Nitrogen gas below 621 K, what can a Joule–Thomson expansion be used for?
A: Heat the gas
B: Maintain the temperature of the gas
C: Condense the gas to form a solid
D: Cool the gas until liquid N2 forms
E: Increase the pressure of the gas

Answer: D
@
Subject:
The specific composition of an alloy system will usually have a great effect on the results of heat treating. If the percentage of each constituent is just right, the alloy will form a single, continuous microstructure upon cooling. Such a mixture is said to be eutectoid. However, If the percentage of the solutes varies from the eutectoid mixture, two or more different microstructures will usually form simultaneously. A hypo eutectoid solution contains less of the solute than the eutectoid mix, while a hypereutectoid solution contains more.[9]

Eutectoid alloys
A eutectoid (eutectic-like) alloy is similar in behavior to a eutectic alloy. A eutectic alloy is characterized by having a single melting point. This melting point is lower than that of any of the constituents, and no change in the mixture will lower the melting point any further. When a molten eutectic alloy is cooled, all of the constituents will crystallize into their respective phases at the same temperature.

A eutectoid alloy is similar, but the phase change occurs, not from a liquid, but from a solid solution. Upon cooling a eutectoid alloy from the solution temperature, the constituents will separate into different crystal phases, forming a single microstructure. A eutectoid steel, for example, contains 0.77% carbon. Upon cooling slowly, the solution of iron and carbon (a single phase called austenite) will separate into platelets of the phases ferrite and cementite. This forms a layered microstructure called pearlite.

Since pearlite is harder than iron, the degree of softness achievable is typically limited to that produced by the pearlite. Similarly, the hardenability is limited by the continuous martensitic microstructure formed when cooled very fast.[10]

Hypoeutectoid alloys
A hypoeutectic alloy has two separate melting points. Both are above the eutectic melting point for the system but are below the melting points of any constituent forming the system. Between these two melting points, the alloy will exist as part solid and part liquid. The constituent with the higher melting point will solidify first. When completely solidified, a hypoeutectic alloy will often be in a solid solution.

Similarly, a hypoeutectoid alloy has two critical temperatures, called "arrests". Between these two temperatures, the alloy will exist partly as the solution and partly as a separate crystallizing phase, called the "pro eutectoid phase". These two temperatures are called the upper (A3) and lower (A1) transformation temperatures. As the solution cools from the upper transformation temperature toward an insoluble state, the excess base metal will often be forced to "crystallize-out", becoming the pro eutectoid. This will occur until the remaining concentration of solutes reaches the eutectoid level, which will then crystallize as a separate microstructure.

For example, a hypoeutectoid steel contains less than 0.77% carbon. Upon cooling a hypoeutectoid steel from the austenite transformation temperature, small islands of proeutectoid-ferrite will form. These will continue to grow and the carbon will recede until the eutectoid concentration in the rest of the steel is reached. This eutectoid mixture will then crystallize as a microstructure of pearlite. Since ferrite is softer than pearlite, the two microstructures combine to increase the ductility of the alloy. Consequently, the hardenability of the alloy is lowered.[11]
$
10
Question 1: What is a eutectoid mixture known for?
A: Forming two separate microstructures upon cooling.
B: Forming a single, continuous microstructure upon cooling.
C: Forming multiple layered microstructures upon heating.
D: Having a higher melting point than any of its constituents.
E: Having multiple melting points.

Answer: B

Question 2: What distinguishes a eutectic alloy from a eutectoid alloy?
A: Eutectic alloys change phase from a solid solution, while eutectoid alloys change from liquid.
B: Eutectic alloys have a single melting point while eutectoid alloys have two.
C: Eutectic alloys form pearlite microstructures, while eutectoid alloys don't.
D: Eutectic alloys change phase from liquid, while eutectoid alloys change from a solid solution.
E: Eutectic alloys contain more solute than eutectoid alloys.

Answer: D

Question 3: A eutectoid steel with 0.77% carbon forms which microstructure upon cooling?
A: Austenite
B: Ferrite
C: Pearlite
D: Cementite
E: Martensite

Answer: C

Question 4: What does pearlite consist of in eutectoid steel?
A: Austenite and ferrite
B: Ferrite and cementite
C: Austenite and cementite
D: Ferrite and martensite
E: Austenite and martensite

Answer: B

Question 5: What characteristic does a hypoeutectic alloy display between its two melting points?
A: Exists only as a solid
B: Exists only as a liquid
C: Exists as both solid and liquid
D: Exists neither as a solid nor as a liquid
E: Exists in a supercritical state

Answer: C

Question 6: Between which two temperatures does a hypoeutectoid alloy exist partly as the solution and partly as a separate crystallizing phase?
A: E1 and E2 temperatures
B: A1 and A3 temperatures
C: A2 and A4 temperatures
D: M1 and M2 temperatures
E: C1 and C2 temperatures

Answer: B

Question 7: As a hypoeutectoid steel cools from the austenite transformation temperature, what forms first?
A: Pearlite
B: Ferrite
C: Austenite
D: Proeutectoid-ferrite
E: Cementite

Answer: D

Question 8: What is the effect of the combined microstructures of ferrite and pearlite in a hypoeutectoid alloy?
A: They decrease the alloy's ductility.
B: They increase the alloy's hardness.
C: They increase the alloy's ductility.
D: They decrease the alloy's strength.
E: They make the alloy brittle.

Answer: C

Question 9: Which of the following best describes a hypereutectoid solution?
A: Contains less of the solute than the eutectoid mix.
B: Contains more of the solute than the eutectoid mix.
C: Contains equal solute and solvent.
D: Does not contain any solute.
E: Has the same solute concentration as a eutectic solution.

Answer: B

Question 10: What happens when the eutectoid concentration in a hypoeutectoid steel is reached during cooling?
A: The alloy will crystallize as a microstructure of ferrite.
B: The alloy will crystallize as a microstructure of austenite.
C: The alloy will crystallize as a microstructure of pearlite.
D: The alloy will crystallize as a microstructure of cementite.
E: The alloy will not crystallize and remain in a liquid state.

Answer: C
@
Subject:
Newton's laws of motion are three basic laws of classical mechanics that describe the relationship between the motion of an object and the forces acting on it. These laws can be paraphrased as follows:

A body remains at rest, or in motion at a constant speed in a straight line, unless acted upon by a force.
When a body is acted upon by a net force, the body's acceleration multiplied by its mass is equal to the net force.
If two bodies exert forces on each other, these forces have the same magnitude but opposite directions.[2]
The three laws of motion were first stated by Isaac Newton in his Philosophiæ Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy), originally published in 1687.[3] Newton used them to investigate and explain the motion of many physical objects and systems, which laid the foundation for classical mechanics. In the time since Newton, the conceptual content of classical physics has been reformulated in alternative ways, involving different mathematical approaches that have yielded insights which were obscured in the original, Newtonian formulation. Limitations to Newton's laws have also been discovered; new theories are necessary when objects move at very high speeds (special relativity), are very massive (general relativity), or are very small (quantum mechanics).
$
10
Question 1: Which of Newton's laws of motion describes a body's inertia?
A: First law
B: Second law
C: Third law
D: Fourth law
E: None of the above

Answer: A

Question 2: In which publication did Sir Isaac Newton first state his three laws of motion?
A: The Art of Physics
B: Classical Physics Theorems
C: Philosophiæ Naturalis Principia Mathematica
D: Physics Today
E: The Nature of Physics

Answer: C

Question 3: If a body is moving in a straight line at a constant speed, and no forces act on it, what can be said about the body according to Newton's laws?
A: It will slow down over time.
B: It will eventually stop.
C: It will change direction randomly.
D: It will continue moving in the same manner indefinitely.
E: It will accelerate at a steady rate.

Answer: D

Question 4: When two bodies exert forces on each other, how are the magnitudes and directions of these forces related, according to Newton's third law?
A: Different magnitudes and opposite directions
B: Different magnitudes and same direction
C: Same magnitudes and same direction
D: Same magnitudes and opposite directions
E: Forces do not act on each other.

Answer: D

Question 5: Which theory is necessary to describe the motion of objects that move at very high speeds?
A: Classical mechanics
B: Thermodynamics
C: Special relativity
D: General relativity
E: Quantum mechanics

Answer: C

Question 6: Which of the following is NOT a limitation to Newton's laws of motion?
A: Objects moving at moderate speeds
B: Very massive objects
C: Objects moving at very high speeds
D: Very small objects
E: Objects in gravitational fields

Answer: A

Question 7: Which of Newton's laws indicates that the net force acting on a body is equal to its mass times acceleration?
A: First law
B: Second law
C: Third law
D: None of the above
E: All of the above

Answer: B

Question 8: If there is no net force acting on a body (force is zero), what can be deduced about the body's acceleration according to Newton's second law?
A: The body's acceleration will be maximum.
B: The body's acceleration will be moderate.
C: The body's acceleration will be unpredictable.
D: The body's acceleration will be zero.
E: The body's acceleration will be negative.

Answer: D

Question 9: For very small objects, which theory is more appropriate than Newton's laws?
A: Classical mechanics
B: Thermodynamics
C: Special relativity
D: General relativity
E: Quantum mechanics

Answer: E

Question 10: According to Newton's first law, what force is required to keep an object moving at a constant velocity?
A: A very large force
B: A moderate force
C: A small force
D: No force is required
E: An unpredictable force

Answer: D
@
Subject:
The possibility of gravitational waves was discussed in 1893 by Oliver Heaviside, using the analogy between the inverse-square law of gravitation and the electrostatic force.[22] In 1905, Henri Poincaré proposed gravitational waves, emanating from a body and propagating at the speed of light, as being required by the Lorentz transformations[23] and suggested that, in analogy to an accelerating electrical charge producing electromagnetic waves, accelerated masses in a relativistic field theory of gravity should produce gravitational waves.[24][25] When Einstein published his general theory of relativity in 1915, he was skeptical of Poincaré's idea since the theory implied there were no "gravitational dipoles". Nonetheless, he still pursued the idea and based on various approximations came to the conclusion there must, in fact, be three types of gravitational waves (dubbed longitudinal–longitudinal, transverse–longitudinal, and transverse–transverse by Hermann Weyl).[25]

However, the nature of Einstein's approximations led many (including Einstein himself) to doubt the result. In 1922, Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could be made to propagate at any speed by choosing appropriate coordinates, leading Eddington to jest that they "propagate at the speed of thought".[26]: 72  This also cast doubt on the physicality of the third (transverse–transverse) type that Eddington showed always propagate at the speed of light regardless of coordinate system. In 1936, Einstein and Nathan Rosen submitted a paper to Physical Review in which they claimed gravitational waves could not exist in the full general theory of relativity because any such solution of the field equations would have a singularity. The journal sent their manuscript to be reviewed by Howard P. Robertson, who anonymously reported that the singularities in question were simply the harmless coordinate singularities of the employed cylindrical coordinates. Einstein, who was unfamiliar with the concept of peer review, angrily withdrew the manuscript, never to publish in Physical Review again. Nonetheless, his assistant Leopold Infeld, who had been in contact with Robertson, convinced Einstein that the criticism was correct, and the paper was rewritten with the opposite conclusion and published elsewhere.[25][26]: 79ff  In 1956, Felix Pirani remedied the confusion caused by the use of various coordinate systems by rephrasing the gravitational waves in terms of the manifestly observable Riemann curvature tensor.[27]

At the time, Pirani's work was overshadowed by the community's focus on a different question: whether gravitational waves could transmit energy. This matter was settled by a thought experiment proposed by Richard Feynman during the first "GR" conference at Chapel Hill in 1957. In short, his argument known as the "sticky bead argument" notes that if one takes a rod with beads then the effect of a passing gravitational wave would be to move the beads along the rod; friction would then produce heat, implying that the passing wave had done work. Shortly after, Hermann Bondi, published a detailed version of the "sticky bead argument".[25] This later lead to a series of articles (1959 to 1989) by Bondi and Pirani that established the existence of plane wave solutions for gravitational waves.[28]
$
10
Question 1: Who first discussed the possibility of gravitational waves in 1893?
A: Albert Einstein
B: Henri Poincaré
C: Arthur Eddington
D: Oliver Heaviside
E: Richard Feynman

Answer: D

Question 2: Henri Poincaré proposed that gravitational waves emanate from a body and propagate at the speed of what?
A: Gravity
B: Sound
C: Electricity
D: Light
E: Quantum particles

Answer: D

Question 3: How many types of gravitational waves did Einstein initially believe there were, based on his approximations?
A: One
B: Two
C: Three
D: Four
E: Five

Answer: C

Question 4: Who joked that some types of gravitational waves "propagate at the speed of thought"?
A: Hermann Weyl
B: Nathan Rosen
C: Arthur Eddington
D: Felix Pirani
E: Howard P. Robertson

Answer: C

Question 5: Why did Einstein initially withdraw a manuscript from Physical Review?
A: He disagreed with the concept of gravity waves.
B: He didn't believe in the peer review process.
C: He was unfamiliar with the concept of peer review and was angry about the criticism.
D: He found a significant error in his own work.
E: He decided to keep his findings secret.

Answer: C

Question 6: Who rephrased the gravitational waves in terms of the manifestly observable Riemann curvature tensor in 1956?
A: Richard Feynman
B: Albert Einstein
C: Hermann Bondi
D: Felix Pirani
E: Henri Poincaré

Answer: D

Question 7: What thought experiment settled the question of whether gravitational waves could transmit energy?
A: The "light wave argument"
B: The "energy bead argument"
C: The "speed of thought argument"
D: The "Riemann curvature test"
E: The "sticky bead argument"

Answer: E

Question 8: Who published a detailed version of the "sticky bead argument" after Richard Feynman?
A: Henri Poincaré
B: Hermann Bondi
C: Felix Pirani
D: Arthur Eddington
E: Oliver Heaviside

Answer: B

Question 9: The effect of a passing gravitational wave on a rod with beads would be to move the beads along the rod. What does this movement result in?
A: Light emission
B: Electric charge
C: Heat due to friction
D: Magnetic force
E: Sound waves

Answer: C

Question 10: How many years span the series of articles by Bondi and Pirani that established the existence of plane wave solutions for gravitational waves?
A: 10 years
B: 20 years
C: 30 years
D: 40 years
E: 50 years

Answer: C
@






























































































































































































































































































































