@
Antihydrogen (
H
) is the antimatter counterpart of hydrogen. Whereas the common hydrogen atom is composed of an electron and proton, the antihydrogen atom is made up of a positron and antiproton. Scientists hope that studying antihydrogen may shed light on the question of why there is more matter than antimatter in the observable universe, known as the baryon asymmetry problem.[1] Antihydrogen is produced artificially in particle accelerators.

Accelerators first detected hot antihydrogen in the 1990s. ATHENA studied cold 
H
 in 2002. It was first trapped by the Antihydrogen Laser Physics Apparatus (ALPHA) team at CERN[2][3] in 2010, who then measured the structure and other important properties.[4] ALPHA, AEGIS, and GBAR plan to further cool and study 
H
 atoms.

The CPT theorem of particle physics predicts antihydrogen atoms have many of the characteristics regular hydrogen has; i.e. the same mass, magnetic moment, and atomic state transition frequencies (see atomic spectroscopy).[7] For example, excited antihydrogen atoms are expected to glow the same color as regular hydrogen. Antihydrogen atoms should be attracted to other matter or antimatter gravitationally with a force of the same magnitude that ordinary hydrogen atoms experience.[2] This would not be true if antimatter has negative gravitational mass, which is considered highly unlikely, though not yet empirically disproven (see gravitational interaction of antimatter).[8] Recent theoretical framework for negative mass and repulsive gravity (antigravity) between matter and antimatter has been developed, and the theory is compatible with CPT theorem.[9]

When antihydrogen comes into contact with ordinary matter, its constituents quickly annihilate. The positron annihilates with an electron to produce gamma rays. The antiproton, on the other hand, is made up of antiquarks that combine with quarks in either neutrons or protons, resulting in high-energy pions, that quickly decay into muons, neutrinos, positrons, and electrons. If antihydrogen atoms were suspended in a perfect vacuum, they should survive indefinitely.

As an anti-element, it is expected to have exactly the same properties as hydrogen.[10] For example, antihydrogen would be a gas under standard conditions and combine with antioxygen to form antiwater, 
H
2
O
.

The first antihydrogen was produced in 1995 by a team led by Walter Oelert at CERN[11] using a method first proposed by Charles Munger Jr, Stanley Brodsky and Ivan Schmidt Andrade.[12]

In the LEAR, antiprotons from an accelerator were shot at xenon clusters,[13] producing electron-positron pairs. Antiprotons can capture positrons with probability about 10−19, so this method is not suited for substantial production, as calculated.[14][15][16] Fermilab measured a somewhat different cross section,[17] in agreement with predictions of quantum electrodynamics.[18] Both resulted in highly energetic, or hot, anti-atoms, unsuitable for detailed study.

Subsequently, CERN built the Antiproton Decelerator (AD) to support efforts towards low-energy antihydrogen, for tests of fundamental symmetries. The AD will supply several CERN groups. CERN expects their facilities will be capable of producing 10 million antiprotons per minute.[19]
$
5
Question: What is the antimatter counterpart of hydrogen?
A: Antihelium
B: Antioxygen
C: Antiwater
D: Antihydrogen
E: Anticarbon
Answer: D

Question: Which team first trapped antihydrogen?
A: AEGIS
B: GBAR
C: ATHENA
D: ALPHA
E: LEAR
Answer: D

Question: When antihydrogen comes into contact with ordinary matter, the positron annihilates to produce which of the following?
A: Pions
B: Electrons
C: Protons
D: Neutrons
E: Gamma rays
Answer: E

Question: If antihydrogen atoms were suspended in a perfect vacuum, what would happen to them?
A: They would quickly decay into pions and muons.
B: They would immediately produce gamma rays.
C: They would survive indefinitely.
D: They would combine with antioxygen to form antiwater.
E: They would quickly annihilate.
Answer: C

Question: Which facility was built by CERN to support efforts towards low-energy antihydrogen?
A: Atomic Decelerator
B: Baryon Asymmetry Problem
C: Antihydrogen Laser Physics Apparatus
D: Antiproton Decelerator
E: Electron-Positron Pairs
Answer: D
@
Quark–gluon plasma (or QGP and quark soup) is an interacting localized assembly of quarks and gluons at thermal (local kinetic) and (close to) chemical (abundance) equilibrium. The word plasma signals that free color charges are allowed. In a 1987 summary, Léon van Hove pointed out the equivalence of the three terms: quark gluon plasma, quark matter and a new state of matter.[2] Since the temperature is above the Hagedorn temperature—and thus above the scale of light u,d-quark mass—the pressure exhibits the relativistic Stefan-Boltzmann format governed by temperature to the fourth power (
�
4
{\displaystyle T^{4}}) and many practically massless quark and gluon constituents. It can be said that QGP emerges to be the new phase of strongly interacting matter which manifests its physical properties in terms of nearly free dynamics of practically massless gluons and quarks. Both quarks and gluons must be present in conditions near chemical (yield) equilibrium with their colour charge open for a new state of matter to be referred to as QGP.

In the Big Bang theory, quark–gluon plasma filled the entire Universe before matter as we know it was created. Theories predicting the existence of quark–gluon plasma were developed in the late 1970s and early 1980s.[3] Discussions around heavy ion experimentation followed suit[4][5][6][7][8] and the first experiment proposals were put forward at CERN[9][10][11][12][13][14] and BNL[15][16] in the following years. Quark–gluon plasma[17][18] was detected for the first time in the laboratory at CERN in the year 2000.[19][20][21]
$
5
Question: What is the quark-gluon plasma an assembly of?
A: Electrons and positrons
B: Protons and neutrons
C: Mesons and baryons
D: Quarks and leptons
E: Quarks and gluons
Answer: E

Question: Which of the following is NOT a term equivalent to quark-gluon plasma, as pointed out by Léon van Hove?
A: Quark soup
B: Quark matter
C: New state of matter
D: Hagedorn plasma
E: Quark gluon plasma
Answer: D

Question: In relation to the Big Bang theory, what role did the quark-gluon plasma play?
A: It was the final state of matter after the Big Bang.
B: It was an intermediate phase before the Universe expanded.
C: It filled the entire Universe before matter as we know it was created.
D: It was created as a result of the first stars forming.
E: It only existed in small pockets, separate from the main explosion.
Answer: C

Question: When was the quark-gluon plasma detected for the first time in a laboratory setting?
A: 1987
B: Late 1970s
C: Early 1980s
D: 1995
E: 2000
Answer: E

Question: What does the word "plasma" in quark-gluon plasma signify?
A: The presence of protons and electrons.
B: The absence of any charge.
C: The transformation of matter into energy.
D: That free color charges are allowed.
E: The cooling of matter below the Hagedorn temperature.
Answer: D
@
QCD is one part of the modern theory of particle physics called the Standard Model. Other parts of this theory deal with electroweak interactions and neutrinos. The theory of electrodynamics has been tested and found correct to a few parts in a billion. The theory of weak interactions has been tested and found correct to a few parts in a thousand. Perturbative forms of QCD have been tested to a few percent.[23] Perturbative models assume relatively small changes from the ground state, i.e. relatively low temperatures and densities, which simplifies calculations at the cost of generality. In contrast, non-perturbative forms of QCD have barely been tested. The study of the QGP, which has both a high temperature and density, is part of this effort to consolidate the grand theory of particle physics.

The study of the QGP is also a testing ground for finite temperature field theory, a branch of theoretical physics which seeks to understand particle physics under conditions of high temperature. Such studies are important to understand the early evolution of our universe: the first hundred microseconds or so. It is crucial to the physics goals of a new generation of observations of the universe (WMAP and its successors). It is also of relevance to Grand Unification Theories which seek to unify the three fundamental forces of nature (excluding gravity).

Reasons for studying the formation of quark–gluon plasma
The generally accepted model of the formation of the Universe states that it happened as the result of the Big Bang. In this model, in the time interval of 10−10–10−6 s after the Big Bang, matter existed in the form of a quark–gluon plasma. It is possible to reproduce the density and temperature of matter existing of that time in laboratory conditions to study the characteristics of the very early Universe. So far, the only possibility is the collision of two heavy atomic nuclei accelerated to energies of more than a hundred GeV. Using the result of a head-on collision in the volume approximately equal to the volume of the atomic nucleus, it is possible to model the density and temperature that existed in the first instants of the life of the Universe.

Relation to normal plasma
A plasma is matter in which charges are screened due to the presence of other mobile charges. For example: Coulomb's Law is suppressed by the screening to yield a distance-dependent charge, 
�
→
�
�
−
�
/
�{\displaystyle Q\rightarrow Qe^{-r/\alpha }}, i.e., the charge Q is reduced exponentially with the distance divided by a screening length α. In a QGP, the color charge of the quarks and gluons is screened. The QGP has other analogies with a normal plasma. There are also dissimilarities because the color charge is non-abelian, whereas the electric charge is abelian. Outside a finite volume of QGP the color-electric field is not screened, so that a volume of QGP must still be color-neutral. It will therefore, like a nucleus, have integer electric charge.

Because of the extremely high energies involved, quark-antiquark pairs are produced by pair production and thus QGP is a roughly equal mixture of quarks and antiquarks of various flavors, with only a slight excess of quarks. This property is not a general feature of conventional plasmas, which may be too cool for pair production (see however pair instability supernova).

Theory
One consequence of this difference is that the color charge is too large for perturbative computations which are the mainstay of QED. As a result, the main theoretical tools to explore the theory of the QGP is lattice gauge theory.[24][25] The transition temperature (approximately 175 MeV) was first predicted by lattice gauge theory. Since then lattice gauge theory has been used to predict many other properties of this kind of matter. The AdS/CFT correspondence conjecture may provide insights in QGP, moreover the ultimate goal of the fluid/gravity correspondence is to understand QGP. The QGP is believed to be a phase of QCD which is completely locally thermalized and thus suitable for an effective fluid dynamic description.

Production
Production of QGP in the laboratory is achieved by colliding heavy atomic nuclei (called heavy ions as in an accelerator atoms are ionized) at relativistic energy in which matter is heated well above the Hagedorn temperature TH = 150 MeV per particle, which amounts to a temperature exceeding 1.66×1012 K. This can be accomplished by colliding two large nuclei at high energy (note that 175 MeV is not the energy of the colliding beam). Lead and gold nuclei have been used for such collisions at CERN SPS and BNL RHIC, respectively. The nuclei are accelerated to ultrarelativistic speeds (contracting their length) and directed towards each other, creating a "fireball", in the rare event of a collision. Hydrodynamic simulation predicts this fireball will expand under its own pressure, and cool while expanding. By carefully studying the spherical and elliptic flow, experimentalists put the theory to test.
$
5
Question: What is the significance of studying the Quark-Gluon Plasma (QGP) in relation to the universe's early evolution?
A: To understand particle physics at cold temperatures.
B: To explore the last few milliseconds of the universe's existence.
C: To validate Grand Unification Theories.
D: To understand the universe's first hundred microseconds or so.
E: To test the latest theoretical frameworks of nuclear fusion.
Answer: D

Question: In the widely accepted model of the Universe's formation, matter existed in the form of a quark-gluon plasma within which time frame after the Big Bang?
A: 10^-10–10^-6 s
B: 10^-5–10^-3 s
C: 10^-20–10^-15 s
D: 10^-12–10^-8 s
E: 10^-7–10^-4 s
Answer: A

Question: How is a quark-antiquark pair produced in the Quark-Gluon Plasma?
A: By cooling the plasma.
B: By unifying electric and color charge.
C: By pair instability supernova.
D: By screening the color charge.
E: By pair production.
Answer: E

Question: In a Quark-Gluon Plasma, what happens to the color charge of quarks and gluons?
A: They become abelian.
B: They are screened.
C: They are amplified.
D: They unify with electric charges.
E: They dissipate.
Answer: B

Question: Which method is primarily used to explore the theory of the QGP?
A: AdS/CFT correspondence conjecture.
B: Fluid/gravity correspondence.
C: Conventional plasma modeling.
D: Lattice gauge theory.
E: Perturbative quantum electrodynamics.
Answer: D
@
The discovery of the perfect liquid was a turning point in physics. Experiments at RHIC have revealed a wealth of information about this remarkable substance, which we now know to be a QGP.[32] Nuclear matter at "room temperature" is known to behave like a superfluid. When heated the nuclear fluid evaporates and turns into a dilute gas of nucleons and, upon further heating, a gas of baryons and mesons (hadrons). At the critical temperature, TH, the hadrons melt and the gas turns back into a liquid. RHIC experiments have shown that this is the most perfect liquid ever observed in any laboratory experiment at any scale. The new phase of matter, consisting of dissolved hadrons, exhibits less resistance to flow than any other known substance. The experiments at RHIC have, already in 2005, shown that the Universe at its beginning was uniformly filled with this type of material—a super-liquid—which once the Universe cooled below TH evaporated into a gas of hadrons. Detailed measurements show that this liquid is a quark–gluon plasma where quarks, antiquarks and gluons flow independently.[33]


Schematic representation of the interaction region formed in the first moments after the collision of heavy ions with high energies in the accelerator.[34]
In short, a quark–gluon plasma flows like a splat of liquid, and because it is not "transparent" with respect to quarks, it can attenuate jets emitted by collisions. Furthermore, once formed, a ball of quark–gluon plasma, like any hot object, transfers heat internally by radiation. However, unlike in everyday objects, there is enough energy available so that gluons (particles mediating the strong force) collide and produce an excess of the heavy (i.e., high-energy) strange quarks. Whereas, if the QGP did not exist and there was a pure collision, the same energy would be converted into a non-equilibrium mixture containing even heavier quarks such as charm quarks or bottom quarks.[34][35]

The equation of state is an important input into the flow equations. The speed of sound (speed of QGP-density oscillations) is currently under investigation in lattice computations.[36][37][38] The mean free path of quarks and gluons has been computed using perturbation theory as well as string theory. Lattice computations have been slower here, although the first computations of transport coefficients have been concluded.[39][40] These indicate that the mean free time of quarks and gluons in the QGP may be comparable to the average interparticle spacing: hence the QGP is a liquid as far as its flow properties go. This is very much an active field of research, and these conclusions may evolve rapidly. The incorporation of dissipative phenomena into hydrodynamics is another active research area.[41][42][43]
$
5
Question: What does nuclear matter behave like at "room temperature"?
A: A super gas.
B: A superfluid.
C: A solid.
D: A normal liquid.
E: A plasma.
Answer: B

Question: In the experiments at RHIC, the newly discovered phase of matter showed what characteristic?
A: It exhibited more resistance to flow than any known substance.
B: It behaved like a gaseous form of quark-gluon plasma.
C: It exhibited less resistance to flow than any other known substance.
D: It formed a solid structure similar to crystal lattices.
E: It behaved unpredictably and changed its state rapidly.
Answer: C

Question: The quark-gluon plasma, when formed, transfers heat internally by which method?
A: Conduction.
B: Convection.
C: Evaporation.
D: Radiation.
E: Reflection.
Answer: D

Question: If the quark-gluon plasma did not exist and there was only a pure collision, the energy would be converted into a non-equilibrium mixture containing what?
A: Primarily gluons and photons.
B: Light quarks.
C: Heavy strange quarks.
D: Charm quarks or bottom quarks.
E: Mesons.
Answer: D

Question: What does the mean free time of quarks and gluons in the QGP indicate about its flow properties?
A: The QGP behaves as a solid.
B: The QGP behaves as a superfluid.
C: The QGP behaves as a gas.
D: The QGP is a liquid as far as its flow properties go.
E: The QGP oscillates between solid and liquid states.
Answer: D
@
The strange quark or s quark (from its symbol, s) is the third lightest of all quarks, a type of elementary particle. Strange quarks are found in subatomic particles called hadrons. Examples of hadrons containing strange quarks include kaons (
K
), strange D mesons (
D
s), Sigma baryons (
Σ
), and other strange particles.

According to the IUPAP, the symbol s is the official name, while "strange" is to be considered only as a mnemonic.[2] The name sideways has also been used because the s quark has an I3 value of 0 while the u ("up") and d ("down") quarks have values of +
1
/
2
 and −
1
/
2
 respectively.[3]

Along with the charm quark, it is part of the second generation of matter. It has an electric charge of −+
1
/
3
 e and a bare mass of 95+9
−3 MeV/c2.[1] Like all quarks, the strange quark is an elementary fermion with spin 
1
/
2
, and experiences all four fundamental interactions: gravitation, electromagnetism, weak interactions, and strong interactions. The antiparticle of the strange quark is the strange antiquark (sometimes called antistrange quark or simply antistrange), which differs from it only in that some of its properties have equal magnitude but opposite sign.

The first strange particle (a particle containing a strange quark) was discovered in 1947 (kaons), but the existence of the strange quark itself (and that of the up and down quarks) was only postulated in 1964 by Murray Gell-Mann and George Zweig to explain the eightfold way classification scheme of hadrons. The first evidence for the existence of quarks came in 1968, in deep inelastic scattering experiments at the Stanford Linear Accelerator Center. These experiments confirmed the existence of up and down quarks, and by extension, strange quarks, as they were required to explain the eightfold way.
$
5
Question: What type of elementary particle is the strange quark?
A: Meson.
B: Lepton.
C: Hadron.
D: Boson.
E: Quark.
Answer: E

Question: Which of the following is NOT an example of a hadron containing strange quarks?
A: Kaons.
B: Sigma baryons.
C: Strange neutrinos.
D: Strange D mesons.
E: Sigma baryons.
Answer: C

Question: What is the I3 value of the s quark?
A: +1/2.
B: -1/2.
C: 0.
D: 1.
E: -1.
Answer: C

Question: Along with which other quark is the strange quark part of the second generation of matter?
A: Up quark.
B: Down quark.
C: Charm quark.
D: Top quark.
E: Bottom quark.
Answer: C

Question: In what year was the first evidence for the existence of quarks obtained from experiments at the Stanford Linear Accelerator Center?
A: 1947.
B: 1964.
C: 1968.
D: 1975.
E: 1982.
Answer: C
@
In particle physics, every type of particle is associated with an antiparticle with the same mass but with opposite physical charges (such as electric charge). For example, the antiparticle of the electron is the positron (also known as an antielectron). While the electron has a negative electric charge, the positron has a positive electric charge, and is produced naturally in certain types of radioactive decay. The opposite is also true: the antiparticle of the positron is the electron.

Some particles, such as the photon, are their own antiparticle. Otherwise, for each pair of antiparticle partners, one is designated as the normal particle (the one that occurs in matter usually interacted with in daily life). The other (usually given the prefix "anti-") is designated the antiparticle.

Particle–antiparticle pairs can annihilate each other, producing photons; since the charges of the particle and antiparticle are opposite, total charge is conserved. For example, the positrons produced in natural radioactive decay quickly annihilate themselves with electrons, producing pairs of gamma rays, a process exploited in positron emission tomography.

The laws of nature are very nearly symmetrical with respect to particles and antiparticles. For example, an antiproton and a positron can form an antihydrogen atom, which is believed to have the same properties as a hydrogen atom. This leads to the question of why the formation of matter after the Big Bang resulted in a universe consisting almost entirely of matter, rather than being a half-and-half mixture of matter and antimatter. The discovery of charge parity violation helped to shed light on this problem by showing that this symmetry, originally thought to be perfect, was only approximate.

Because charge is conserved, it is not possible to create an antiparticle without either destroying another particle of the same charge (as is for instance the case when antiparticles are produced naturally via beta decay or the collision of cosmic rays with Earth's atmosphere), or by the simultaneous creation of both a particle and its antiparticle, which can occur in particle accelerators such as the Large Hadron Collider at CERN.

Particles and their antiparticles have equal and opposite charges, so that an uncharged particle also gives rise to an uncharged antiparticle. In many cases, the antiparticle and the particle coincide: pairs of photons, Z0 bosons, 
π0
 mesons, and hypothetical gravitons and some hypothetical WIMPs all self-annihilate. However, electrically neutral particles need not be identical to their antiparticles: for example, the neutron and antineutron are distinct.
$
5
Question: What is the antiparticle of an electron?
A: Neutron.
B: Photon.
C: Positron.
D: Proton.
E: Neutrino.
Answer: C

Question: Which of the following particles is its own antiparticle?
A: Electron.
B: Proton.
C: Neutron.
D: Photon.
E: Positron.
Answer: D

Question: What is produced when a particle and its antiparticle annihilate each other?
A: Protons.
B: Neutrons.
C: Electrons.
D: Positrons.
E: Photons.
Answer: E

Question: The antiproton and a positron can come together to form which atom?
A: Antihydrogen.
B: Antioxygen.
C: Antineutron.
D: Anticarbon.
E: Antihelium.
Answer: A

Question: In what type of device can both a particle and its antiparticle be created simultaneously?
A: Optical microscope.
B: X-ray machine.
C: Particle accelerator.
D: Nuclear reactor.
E: Electromagnetic coil.
Answer: C
@
In 1932, soon after the prediction of positrons by Paul Dirac, Carl D. Anderson found that cosmic-ray collisions produced these particles in a cloud chamber – a particle detector in which moving electrons (or positrons) leave behind trails as they move through the gas. The electric charge-to-mass ratio of a particle can be measured by observing the radius of curling of its cloud-chamber track in a magnetic field. Positrons, because of the direction that their paths curled, were at first mistaken for electrons travelling in the opposite direction. Positron paths in a cloud-chamber trace the same helical path as an electron but rotate in the opposite direction with respect to the magnetic field direction due to their having the same magnitude of charge-to-mass ratio but with opposite charge and, therefore, opposite signed charge-to-mass ratios.

The antiproton and antineutron were found by Emilio Segrè and Owen Chamberlain in 1955 at the University of California, Berkeley.[1] Since then, the antiparticles of many other subatomic particles have been created in particle accelerator experiments. In recent years, complete atoms of antimatter have been assembled out of antiprotons and positrons, collected in electromagnetic traps.[2]

Solutions of the Dirac equation contain negative energy quantum states. As a result, an electron could always radiate energy and fall into a negative energy state. Even worse, it could keep radiating infinite amounts of energy because there were infinitely many negative energy states available. To prevent this unphysical situation from happening, Dirac proposed that a "sea" of negative-energy electrons fills the universe, already occupying all of the lower-energy states so that, due to the Pauli exclusion principle, no other electron could fall into them. Sometimes, however, one of these negative-energy particles could be lifted out of this Dirac sea to become a positive-energy particle. But, when lifted out, it would leave behind a hole in the sea that would act exactly like a positive-energy electron with a reversed charge. These holes were interpreted as "negative-energy electrons" by Paul Dirac and mistakenly identified with protons in his 1930 paper A Theory of Electrons and Protons[4] However, these "negative-energy electrons" turned out to be positrons, and not protons.

This picture implied an infinite negative charge for the universe – a problem of which Dirac was aware. Dirac tried to argue that we would perceive this as the normal state of zero charge. Another difficulty was the difference in masses of the electron and the proton. Dirac tried to argue that this was due to the electromagnetic interactions with the sea, until Hermann Weyl proved that hole theory was completely symmetric between negative and positive charges. Dirac also predicted a reaction 
e−
 + 
p+
 → 
γ
 + 
γ
, where an electron and a proton annihilate to give two photons. Robert Oppenheimer and Igor Tamm, however, proved that this would cause ordinary matter to disappear too fast. A year later, in 1931, Dirac modified his theory and postulated the positron, a new particle of the same mass as the electron. The discovery of this particle the next year removed the last two objections to his theory.

Within Dirac's theory, the problem of infinite charge of the universe remains. Some bosons also have antiparticles, but since bosons do not obey the Pauli exclusion principle (only fermions do), hole theory does not work for them. A unified interpretation of antiparticles is now available in quantum field theory, which solves both these problems by describing antimatter as negative energy states of the same underlying matter field, i.e. particles moving backwards in time.[5]
$
5
Question: Who discovered the positrons in a cloud chamber after their prediction by Paul Dirac?
A: Owen Chamberlain.
B: Hermann Weyl.
C: Robert Oppenheimer.
D: Emilio Segrè.
E: Carl D. Anderson.
Answer: E

Question: What significant discovery was made by Emilio Segrè and Owen Chamberlain in 1955?
A: Positrons.
B: Antiproton and antineutron.
C: Electromagnetic traps.
D: Negative energy quantum states.
E: Pauli exclusion principle.
Answer: B

Question: The "sea" of negative-energy electrons proposed by Dirac prevented other electrons from falling into negative energy states because of which principle?
A: Einstein's relativity.
B: Quantum entanglement.
C: Pauli exclusion principle.
D: Heisenberg uncertainty principle.
E: Bose-Einstein condensate.
Answer: C

Question: What was the reaction predicted by Dirac where an electron and a proton annihilate?
A: e− + p+ → e+ + e−.
B: e− + e+ → γ + γ.
C: e− + p+ → γ + γ.
D: p+ + e+ → γ + γ.
E: e− + p+ → e− + p+.
Answer: C

Question: In quantum field theory, how is antimatter described?
A: As negative energy states of different matter fields.
B: As particles moving forward in time.
C: As positive energy states of the same underlying matter field.
D: As negative energy states of the same underlying matter field, i.e., particles moving backwards in time.
E: As a manifestation of the Pauli exclusion principle.
Answer: D
@
The SLAC National Accelerator Laboratory deep inelastic scattering experiments of the late 1960s showed that nucleons (protons and neutrons) contained point-like particles that scattered electrons. It was natural to identify these with quarks, but Feynman's parton model attempted to interpret the experimental data in a way that did not introduce additional hypotheses. For example, the data showed that some 45% of the energy momentum was carried by electrically neutral particles in the nucleon. These electrically neutral particles are now seen to be the gluons that carry the forces between the quarks, and their three-valued color quantum number solves the omega-minus problem. Feynman did not dispute the quark model; for example, when the fifth quark was discovered in 1977, Feynman immediately pointed out to his students that the discovery implied the existence of a sixth quark, which was discovered in the decade after his death.[142][144]

After the success of quantum electrodynamics, Feynman turned to quantum gravity. By analogy with the photon, which has spin 1, he investigated the consequences of a free massless spin 2 field and derived the Einstein field equation of general relativity, but little more. The computational device that Feynman discovered then for gravity, "ghosts", which are "particles" in the interior of his diagrams that have the "wrong" connection between spin and statistics, have proved invaluable in explaining the quantum particle behavior of the Yang–Mills theories, for example, quantum chromodynamics and the electro-weak theory.[145] He did work on all four of the forces of nature: electromagnetic, the weak force, the strong force and gravity. John and Mary Gribbin state in their book on Feynman that "Nobody else has made such influential contributions to the investigation of all four of the interactions".[146]

Partly as a way to bring publicity to progress in physics, Feynman offered $1,000 prizes for two of his challenges in nanotechnology; one was claimed by William McLellan and the other by Tom Newman.[147]

Feynman was also interested in the relationship between physics and computation. He was also one of the first scientists to conceive the possibility of quantum computers.[148][149][150] In the 1980s he began to spend his summers working at Thinking Machines Corporation, helping to build some of the first parallel supercomputers and considering the construction of quantum computers.[151][152] In 1984–1986, he developed a variational method for the approximate calculation of path integrals, which has led to a powerful method of converting divergent perturbation expansions into convergent strong-coupling expansions (variational perturbation theory) and, as a consequence, to the most accurate determination[153] of critical exponents measured in satellite experiments.[154] At Caltech, he once chalked "What I cannot create I do not understand" on his blackboard.[155]
$
5
Question: The deep inelastic scattering experiments at the SLAC National Accelerator Laboratory revealed that nucleons contained what type of particles?
A: Gluons.
B: Electrons.
C: Point-like particles.
D: Ghosts.
E: Quantum particles.
Answer: C

Question: Which quantum number solves the omega-minus problem?
A: Spin.
B: Mass.
C: Electric charge.
D: Color.
E: Statistics.
Answer: D

Question: Which computational device, used for gravity, did Feynman discover that explains the quantum particle behavior in Yang–Mills theories?
A: Neutrinos.
B: Ghosts.
C: Quarks.
D: Color fields.
E: Photons.
Answer: B

Question: Feynman's challenges in nanotechnology were aimed at bringing publicity to progress in which field?
A: Chemistry.
B: Biology.
C: Quantum mechanics.
D: Nanotechnology.
E: Astronomy.
Answer: D

Question: What concept related to future technology was Feynman one of the first to conceive?
A: Parallel supercomputers.
B: Classical computers.
C: Quantum telecommunication.
D: Quantum computers.
E: Neural networks.
Answer: D
@
Superfluidity is the characteristic property of a fluid with zero viscosity which therefore flows without any loss of kinetic energy. When stirred, a superfluid forms vortices that continue to rotate indefinitely. Superfluidity occurs in two isotopes of helium (helium-3 and helium-4) when they are liquefied by cooling to cryogenic temperatures. It is also a property of various other exotic states of matter theorized to exist in astrophysics, high-energy physics, and theories of quantum gravity.[1] The theory of superfluidity was developed by Soviet theoretical physicists Lev Landau and Isaak Khalatnikov.

Superfluidity often co-occurs with Bose–Einstein condensation, but neither phenomenon is directly related to the other; not all Bose–Einstein condensates can be regarded as superfluids, and not all superfluids are Bose–Einstein condensates.[citation needed]
$
5
Question: Superfluidity is the property of a fluid with zero what?
A: Density.
B: Thermal energy.
C: Viscosity.
D: Mass.
E: Surface tension.
Answer: C

Question: When stirred, how do superfluids behave?
A: They quickly come to a stop.
B: They form vortices that continue to rotate indefinitely.
C: They freeze.
D: They evaporate.
E: They increase in temperature.
Answer: B

Question: Superfluidity can be observed in which isotopes of helium when cooled to cryogenic temperatures?
A: Helium-1 and helium-2.
B: Helium-2 and helium-3.
C: Helium-3 and helium-4.
D: Helium-4 and helium-5.
E: Helium-5 and helium-6.
Answer: C

Question: Who developed the theory of superfluidity?
A: Albert Einstein and Niels Bohr.
B: James Clerk Maxwell and Richard Feynman.
C: Lev Landau and Isaak Khalatnikov.
D: Max Planck and Werner Heisenberg.
E: Erwin Schrödinger and Paul Dirac.
Answer: C

Question: Which statement is true regarding the relationship between Superfluidity and Bose–Einstein condensation?
A: All Bose–Einstein condensates are superfluids.
B: Superfluidity only occurs in Bose–Einstein condensates.
C: Not all Bose–Einstein condensates are superfluids and not all superfluids are Bose–Einstein condensates.
D: Superfluidity and Bose–Einstein condensation are the same phenomena.
E: Bose–Einstein condensation is a type of superfluid.
Answer: C
@
Superfluid vacuum theory (SVT) is an approach in theoretical physics and quantum mechanics where the physical vacuum is viewed as superfluid.

The ultimate goal of the approach is to develop scientific models that unify quantum mechanics (describing three of the four known fundamental interactions) with gravity. This makes SVT a candidate for the theory of quantum gravity and an extension of the Standard Model.

It is hoped that development of such theory would unify into a single consistent model of all fundamental interactions, and to describe all known interactions and elementary particles as different manifestations of the same entity, superfluid vacuum.

On the macro-scale a larger similar phenomenon has been suggested as happening in the murmurations of starlings. The rapidity of change in flight patterns mimics the phase change leading to superfluidity in some liquid states.[16]

Light behaves like a superfluid in various applications such as Poisson's Spot. As the liquid helium shown above, light will travel along the surface of an obstacle before continuing along its trajectory. Since light is not affected by local gravity its "level" becomes its own trajectory and velocity. Another example is how a beam of light travels through the hole of an aperture and along its backside before diffraction.
$
5
Question: What does the Superfluid vacuum theory (SVT) view the physical vacuum as?
A: A quantum field.
B: An empty space.
C: A plasma.
D: Superfluid.
E: A solid crystal.
Answer: D

Question: What is the ultimate goal of the Superfluid vacuum theory?
A: To prove the existence of dark matter.
B: To develop scientific models that unify quantum mechanics with gravity.
C: To disprove the theory of relativity.
D: To understand the nature of black holes.
E: To explain the behavior of electromagnetic waves.
Answer: B

Question: The development of SVT aims to describe all fundamental interactions and elementary particles as different manifestations of what?
A: Quantum fields.
B: Dark energy.
C: Superfluid vacuum.
D: Electromagnetic waves.
E: Subatomic vibrations.
Answer: C

Question: On a larger scale, what phenomenon has been suggested to exhibit behaviors similar to superfluidity?
A: The rotation of galaxies.
B: The flight patterns of drones.
C: The tidal patterns of oceans.
D: The murmurations of starlings.
E: The movement of clouds.
Answer: D

Question: How does light behave when encountering an obstacle like in Poisson's Spot?
A: It stops and reflects back.
B: It gets absorbed by the obstacle.
C: It travels along the surface of the obstacle before continuing its trajectory.
D: It splits into its component colors.
E: It accelerates and gains energy.
Answer: C
@
Relation to general relativity
Even though the predictions of both quantum theory and general relativity have been supported by rigorous and repeated empirical evidence, their abstract formalisms contradict each other and they have proven extremely difficult to incorporate into one consistent, cohesive model. Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of quantum gravity is an important issue in physical cosmology and the search by physicists for an elegant "Theory of Everything" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th- and 21st-century physics. This TOE would combine not only the models of subatomic physics but also derive the four fundamental forces of nature from a single force or phenomenon.[38]

One proposal for doing so is string theory, which posits that the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries gravitational force.[39][40]

Another popular theory is loop quantum gravity (LQG), which describes quantum properties of gravity and is thus a theory of quantum spacetime. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. This theory describes space as an extremely fine fabric "woven" of finite loops called spin networks. The evolution of a spin network over time is called a spin foam. The characteristic length scale of a spin foam is the Planck length, approximately 1.616×10−35 m, and so lengths shorter than the Planck length are not physically meaningful in LQG.[41]
$
5
Question: Why is the unification between general relativity and quantum mechanics not an urgent issue in many areas of particle physics?
A: Because gravity is extremely strong in particle physics.
B: Because quantum theory is not applicable in those areas.
C: Because gravity is negligible in those areas.
D: Because particles do not exhibit gravitational effects.
E: Because general relativity is only a theoretical concept.
Answer: C

Question: What is a major goal of 20th- and 21st-century physics concerning quantum theory and general relativity?
A: To prove that gravity does not exist.
B: To disregard all previous theories and start fresh.
C: To resolve the inconsistencies between the two theories.
D: To disprove the existence of black holes.
E: To emphasize the importance of quantum mechanics over relativity.
Answer: C

Question: Which theory proposes that the point-like particles of particle physics are replaced by one-dimensional objects?
A: Quantum foam theory.
B: Quantum loop theory.
C: Quantum field theory.
D: Loop quantum gravity.
E: String theory.
Answer: E

Question: In string theory, what determines the properties such as mass and charge of what seems like an ordinary particle from afar?
A: The atomic structure of the string.
B: The length of the string.
C: The vibrational state of the string.
D: The thickness of the string.
E: The quantum field surrounding the string.
Answer: C

Question: Loop quantum gravity describes space as a fabric made of what?
A: Quantum fields.
B: Vibrational strings.
C: Spin foams.
D: Spin networks.
E: Gravitational waves.
Answer: D
@
Electric and magnetic fields obey the properties of superposition. Thus, a field due to any particular particle or time-varying electric or magnetic field contributes to the fields present in the same space due to other causes. Further, as they are vector fields, all magnetic and electric field vectors add together according to vector addition.[12] For example, in optics two or more coherent light waves may interact and by constructive or destructive interference yield a resultant irradiance deviating from the sum of the component irradiances of the individual light waves.[13]

The electromagnetic fields of light are not affected by traveling through static electric or magnetic fields in a linear medium such as a vacuum. However, in nonlinear media, such as some crystals, interactions can occur between light and static electric and magnetic fields—these interactions include the Faraday effect and the Kerr effect.[14][15]

In refraction, a wave crossing from one medium to another of different density alters its speed and direction upon entering the new medium. The ratio of the refractive indices of the media determines the degree of refraction, and is summarized by Snell's law. Light of composite wavelengths (natural sunlight) disperses into a visible spectrum passing through a prism, because of the wavelength-dependent refractive index of the prism material (dispersion); that is, each component wave within the composite light is bent a different amount.[16]

EM radiation exhibits both wave properties and particle properties at the same time (see wave-particle duality). Both wave and particle characteristics have been confirmed in many experiments. Wave characteristics are more apparent when EM radiation is measured over relatively large timescales and over large distances while particle characteristics are more evident when measuring small timescales and distances. For example, when electromagnetic radiation is absorbed by matter, particle-like properties will be more obvious when the average number of photons in the cube of the relevant wavelength is much smaller than 1. It is not so difficult to experimentally observe non-uniform deposition of energy when light is absorbed, however this alone is not evidence of "particulate" behavior. Rather, it reflects the quantum nature of matter.[17] Demonstrating that the light itself is quantized, not merely its interaction with matter, is a more subtle affair.

Some experiments display both the wave and particle natures of electromagnetic waves, such as the self-interference of a single photon.[18] When a single photon is sent through an interferometer, it passes through both paths, interfering with itself, as waves do, yet is detected by a photomultiplier or other sensitive detector only once.

A quantum theory of the interaction between electromagnetic radiation and matter such as electrons is described by the theory of quantum electrodynamics.

Electromagnetic waves can be polarized, reflected, refracted, diffracted or interfere with each other.[19][20][21]
$
5
Question: What property allows electric and magnetic fields to contribute to fields present in the same space due to other causes?
A: They follow a linear path.
B: They obey the properties of superposition.
C: They have a constant velocity.
D: They follow the principle of dispersion.
E: They are scalar fields.
Answer: B

Question: In which medium are the electromagnetic fields of light unaffected by traveling through static electric or magnetic fields?
A: Linear medium.
B: Nonlinear medium.
C: Crystal.
D: Vacuum.
E: Prism.
Answer: D

Question: What determines the degree of refraction when a wave crosses from one medium to another?
A: The wavelength of the light.
B: The amplitude of the wave.
C: The speed of the wave.
D: The ratio of the refractive indices of the media.
E: The intensity of the light wave.
Answer: D

Question: What characteristic of electromagnetic radiation is evident when it is measured over relatively large timescales and distances?
A: Particle characteristics.
B: Refractive characteristics.
C: Wave characteristics.
D: Quantum characteristics.
E: Polarization.
Answer: C

Question: Which theory describes the interaction between electromagnetic radiation and matter such as electrons?
A: Quantum gravity.
B: Quantum electrodynamics.
C: Quantum superposition.
D: Quantum refractivity.
E: Electromagnetic relativity.
Answer: B
@
The basic structure of matter involves charged particles bound together. When electromagnetic radiation impinges on matter, it causes the charged particles to oscillate and gain energy. The ultimate fate of this energy depends on the context. It could be immediately re-radiated and appear as scattered, reflected, or transmitted radiation. It may get dissipated into other microscopic motions within the matter, coming to thermal equilibrium and manifesting itself as thermal energy, or even kinetic energy, in the material. With a few exceptions related to high-energy photons (such as fluorescence, harmonic generation, photochemical reactions, the photovoltaic effect for ionizing radiations at far ultraviolet, X-ray and gamma radiation), absorbed electromagnetic radiation simply deposits its energy by heating the material. This happens for infrared, microwave and radio wave radiation. Intense radio waves can thermally burn living tissue and can cook food. In addition to infrared lasers, sufficiently intense visible and ultraviolet lasers can easily set paper afire.[48]

Ionizing radiation creates high-speed electrons in a material and breaks chemical bonds, but after these electrons collide many times with other atoms eventually most of the energy becomes thermal energy all in a tiny fraction of a second. This process makes ionizing radiation far more dangerous per unit of energy than non-ionizing radiation. This caveat also applies to UV, even though almost all of it is not ionizing, because UV can damage molecules due to electronic excitation, which is far greater per unit energy than heating effects.[48][citation needed]

Infrared radiation in the spectral distribution of a black body is usually considered a form of heat, since it has an equivalent temperature and is associated with an entropy change per unit of thermal energy. However, "heat" is a technical term in physics and thermodynamics and is often confused with thermal energy. Any type of electromagnetic energy can be transformed into thermal energy in interaction with matter. Thus, any electromagnetic radiation can "heat" (in the sense of increase the thermal energy temperature of) a material, when it is absorbed.[49]

The inverse or time-reversed process of absorption is thermal radiation. Much of the thermal energy in matter consists of random motion of charged particles, and this energy can be radiated away from the matter. The resulting radiation may subsequently be absorbed by another piece of matter, with the deposited energy heating the material.[50]

The electromagnetic radiation in an opaque cavity at thermal equilibrium is effectively a form of thermal energy, having maximum radiation entropy.[51]
$
5
Question: What happens to the energy of absorbed electromagnetic radiation in materials like infrared, microwave, and radio wave radiation?
A: It becomes electronic excitation.
B: It breaks chemical bonds in the material.
C: It deposits its energy by heating the material.
D: It creates high-speed electrons.
E: It causes fluorescence in the material.
Answer: C

Question: Why is ionizing radiation considered more dangerous per unit of energy than non-ionizing radiation?
A: It causes electronic excitation.
B: It sets paper afire.
C: It creates high-speed electrons and breaks chemical bonds.
D: It is associated with entropy change.
E: It produces UV light.
Answer: C

Question: How is infrared radiation in the spectral distribution of a black body typically regarded?
A: As UV radiation.
B: As ionizing radiation.
C: As electronic excitation.
D: As a form of heat.
E: As a form of light.
Answer: D

Question: What is the time-reversed process of absorption in the context of thermal energy?
A: Refraction.
B: Scattering.
C: Reflection.
D: Thermal radiation.
E: Transmission.
Answer: D

Question: In an opaque cavity at thermal equilibrium, how is the electromagnetic radiation perceived?
A: As UV radiation.
B: As a random motion of charged particles.
C: As a form of thermal energy with maximum radiation entropy.
D: As non-ionizing radiation.
E: As the photovoltaic effect.
Answer: C
@
X-rays with high photon energies above 5–10 keV (below 0.2–0.1 nm wavelength) are called hard X-rays, while those with lower energy (and longer wavelength) are called soft X-rays.[71] The intermediate range with photon energies of several keV is often referred to as tender X-rays. Due to their penetrating ability, hard X-rays are widely used to image the inside of objects (e.g. in medical radiography and airport security). The term X-ray is metonymically used to refer to a radiographic image produced using this method, in addition to the method itself. Since the wavelengths of hard X-rays are similar to the size of atoms, they are also useful for determining crystal structures by X-ray crystallography. By contrast, soft X-rays are easily absorbed in air; the attenuation length of 600 eV (~2 nm) X-rays in water is less than 1 micrometer.[72]

Gamma rays
There is no consensus for a definition distinguishing between X-rays and gamma rays. One common practice is to distinguish between the two types of radiation based on their source: X-rays are emitted by electrons, while gamma rays are emitted by the atomic nucleus.[73][74][75][76] This definition has several problems: other processes can also generate these high-energy photons, or sometimes the method of generation is not known. One common alternative is to distinguish X- and gamma radiation on the basis of wavelength (or, equivalently, frequency or photon energy), with radiation shorter than some arbitrary wavelength, such as 10−11 m (0.1 Å), defined as gamma radiation.[77] This criterion assigns a photon to an unambiguous category, but is only possible if wavelength is known. (Some measurement techniques do not distinguish between detected wavelengths.) However, these two definitions often coincide since the electromagnetic radiation emitted by X-ray tubes generally has a longer wavelength and lower photon energy than the radiation emitted by radioactive nuclei.[73] Occasionally, one term or the other is used in specific contexts due to historical precedent, based on measurement (detection) technique, or based on their intended use rather than their wavelength or source. Thus, gamma-rays generated for medical and industrial uses, for example radiotherapy, in the ranges of 6–20 MeV, can in this context also be referred to as X-rays.[78]
$
5

Question: How are X-rays with photon energies above 5–10 keV categorized?
A: Soft X-rays.
B: Tender X-rays.
C: Gamma rays.
D: Hard X-rays.
E: Medium X-rays.
Answer: D

Question: What is the attenuation length of 600 eV X-rays in water?
A: More than 1 micrometer.
B: Approximately 2 nm.
C: Less than 1 micrometer.
D: Approximately 0.1 Å.
E: Approximately 10−11 m.
Answer: C

Question: According to one common distinction, X-rays are emitted by ______ while gamma rays are emitted by the ______.
A: A: atomic nucleus; electrons
B: electrons; atomic nucleus
C: atoms; molecules
D: protons; neutrons
E: photons; electrons
Answer: B

Question: Which type of radiation typically has a longer wavelength and lower photon energy when comparing the radiation emitted by X-ray tubes and radioactive nuclei?
A: Hard X-rays.
B: Soft X-rays.
C: Gamma rays.
D: Tender X-rays.
E: Both have the same wavelength.
Answer: A

Question: In specific contexts, gamma-rays generated for uses such as radiotherapy in the range of 6–20 MeV can also be referred to as what?
A: Soft X-rays.
B: Tender X-rays.
C: Gamma rays.
D: Hard X-rays.
E: X-rays.
Answer: E
@
X-rays can be generated by an X-ray tube, a vacuum tube that uses a high voltage to accelerate the electrons released by a hot cathode to a high velocity. The high velocity electrons collide with a metal target, the anode, creating the X-rays.[85] In medical X-ray tubes the target is usually tungsten or a more crack-resistant alloy of rhenium (5%) and tungsten (95%), but sometimes molybdenum for more specialized applications, such as when softer X-rays are needed as in mammography. In crystallography, a copper target is most common, with cobalt often being used when fluorescence from iron content in the sample might otherwise present a problem.

The maximum energy of the produced X-ray photon is limited by the energy of the incident electron, which is equal to the voltage on the tube times the electron charge, so an 80 kV tube cannot create X-rays with an energy greater than 80 keV. When the electrons hit the target, X-rays are created by two different atomic processes:

Characteristic X-ray emission (X-ray electroluminescence): If the electron has enough energy, it can knock an orbital electron out of the inner electron shell of the target atom. After that, electrons from higher energy levels fill the vacancies, and X-ray photons are emitted. This process produces an emission spectrum of X-rays at a few discrete frequencies, sometimes referred to as spectral lines. Usually, these are transitions from the upper shells to the K shell (called K lines), to the L shell (called L lines) and so on. If the transition is from 2p to 1s, it is called Kα, while if it is from 3p to 1s it is Kβ. The frequencies of these lines depend on the material of the target and are therefore called characteristic lines. The Kα line usually has greater intensity than the Kβ one and is more desirable in diffraction experiments. Thus the Kβ line is filtered out by a filter. The filter is usually made of a metal having one proton less than the anode material (e.g. Ni filter for Cu anode or Nb filter for Mo anode).
Bremsstrahlung: This is radiation given off by the electrons as they are scattered by the strong electric field near the nuclei. These X-rays have a continuous spectrum. The frequency of Bremsstrahlung is limited by the energy of incident electrons.
$
5
Question: Which metal is commonly used as a target in medical X-ray tubes?
A: Copper.
B: Cobalt.
C: Tungsten.
D: Molybdenum.
E: Rhenium.
Answer: C

Question: In crystallography, which metal is most commonly used as a target for X-rays to avoid fluorescence interference from iron content in the sample?
A: Tungsten.
B: Cobalt.
C: Molybdenum.
D: Rhenium.
E: Copper.
Answer: E

Question: What determines the maximum energy of the produced X-ray photon in an X-ray tube?
A: The type of target metal.
B: The energy of the incident electron, which is equal to the voltage on the tube times the electron charge.
C: The strength of the electric field near the nuclei.
D: The energy of the atomic electron shells.
E: The type of filter used.
Answer: B

Question: When an electron transitions from 3p to 1s, which spectral line does it produce?
A: Kα.
B: Kβ.
C: Lα.
D: Lβ.
E: Mα.
Answer: B

Question: Which type of radiation is emitted by electrons as they are scattered by the strong electric field near the nuclei and has a continuous spectrum?
A: Kα emission.
B: Kβ emission.
C: Lα emission.
D: Characteristic X-ray emission.
E: Bremsstrahlung.
Answer: E
@
As with all particles, electrons can act as waves. This is called the wave–particle duality and can be demonstrated using the double-slit experiment.

The wave-like nature of the electron allows it to pass through two parallel slits simultaneously, rather than just one slit as would be the case for a classical particle. In quantum mechanics, the wave-like property of one particle can be described mathematically as a complex-valued function, the wave function, commonly denoted by the Greek letter psi (ψ). When the absolute value of this function is squared, it gives the probability that a particle will be observed

Electrons are identical particles because they cannot be distinguished from each other by their intrinsic physical properties. In quantum mechanics, this means that a pair of interacting electrons must be able to swap positions without an observable change to the state of the system. The wave function of fermions, including electrons, is antisymmetric, meaning that it changes sign when two electrons are swapped; that is, ψ(r1, r2) = −ψ(r2, r1), where the variables r1 and r2 correspond to the first and second electrons, respectively. Since the absolute value is not changed by a sign swap, this corresponds to equal probabilities. Bosons, such as the photon, have symmetric wave functions instead.[96]: 162–218 

In the case of antisymmetry, solutions of the wave equation for interacting electrons result in a zero probability that each pair will occupy the same location or state. This is responsible for the Pauli exclusion principle, which precludes any two electrons from occupying the same quantum state. This principle explains many of the properties of electrons. For example, it causes groups of bound electrons to occupy different orbitals in an atom, rather than all overlapping each other in the same orbit.[96]: 162–218 
$
5
Question: What phenomenon describes the ability of electrons to act as waves?
A: Wave-electron phenomenon.
B: Quantum field theory.
C: Wave–particle duality.
D: Quantum resonance.
E: Electron diffraction.
Answer: C

Question: When the wave function's absolute value is squared, what does it represent?
A: The speed of the electron.
B: The wave amplitude of the electron.
C: The probability that a particle will be observed.
D: The energy level of the electron.
E: The momentum of the electron.
Answer: C

Question: Which property signifies that electrons cannot be distinguished from each other based on their intrinsic physical properties?
A: Heterogeneity.
B: Identical nature.
C: Dual nature.
D: Antisymmetry.
E: Pauli exclusion.
Answer: B

Question: If two electrons are swapped, how does the wave function of fermions behave?
A: It remains unchanged.
B: It doubles in value.
C: It is reduced to half.
D: It becomes zero.
E: It changes sign.
Answer: E

Question: Which principle prevents two electrons from occupying the same quantum state?
A: Heisenberg uncertainty principle.
B: Newton's third law.
C: Pauli exclusion principle.
D: Fermi level principle.
E: Quantum superposition principle.
Answer: C
@
Electron beams are used in welding.[170] They allow energy densities up to 107 W·cm−2 across a narrow focus diameter of 0.1–1.3 mm and usually require no filler material. This welding technique must be performed in a vacuum to prevent the electrons from interacting with the gas before reaching their target, and it can be used to join conductive materials that would otherwise be considered unsuitable for welding.[171][172]

Electron-beam lithography (EBL) is a method of etching semiconductors at resolutions smaller than a micrometer.[173] This technique is limited by high costs, slow performance, the need to operate the beam in the vacuum and the tendency of the electrons to scatter in solids. The last problem limits the resolution to about 10 nm. For this reason, EBL is primarily used for the production of small numbers of specialized integrated circuits.[174]

Electron beam processing is used to irradiate materials in order to change their physical properties or sterilize medical and food products.[175] Electron beams fluidise or quasi-melt glasses without significant increase of temperature on intensive irradiation: e.g. intensive electron radiation causes a many orders of magnitude decrease of viscosity and stepwise decrease of its activation energy.[176]

Linear particle accelerators generate electron beams for treatment of superficial tumors in radiation therapy. Electron therapy can treat such skin lesions as basal-cell carcinomas because an electron beam only penetrates to a limited depth before being absorbed, typically up to 5 cm for electron energies in the range 5–20 MeV. An electron beam can be used to supplement the treatment of areas that have been irradiated by X-rays.[177][178]

Particle accelerators use electric fields to propel electrons and their antiparticles to high energies. These particles emit synchrotron radiation as they pass through magnetic fields. The dependency of the intensity of this radiation upon spin polarizes the electron beam—a process known as the Sokolov–Ternov effect.[h] Polarized electron beams can be useful for various experiments. Synchrotron radiation can also cool the electron beams to reduce the momentum spread of the particles. Electron and positron beams are collided upon the particles' accelerating to the required energies; particle detectors observe the resulting energy emissions, which particle physics studies .[179]
$
5
Question: In what environment is electron beam welding typically performed?
A: In an oxygen-rich atmosphere.
B: In water.
C: In a vacuum.
D: In a helium-rich environment.
E: In open air.
Answer: C

Question: What is a primary limitation of Electron-beam lithography (EBL)?
A: It can only be used on metals.
B: Its resolution is about 10 nm.
C: It requires a high amount of energy.
D: It can only etch large patterns.
E: It produces hazardous waste.
Answer: B

Question: What is a significant change observed in glasses when exposed to intensive electron radiation?
A: A significant increase in temperature.
B: A many orders of magnitude decrease of viscosity.
C: A shift in the color spectrum.
D: Development of magnetic properties.
E: Crystallization of the glass structure.
Answer: B

Question: Why are electron beams in radiation therapy particularly suitable for treating superficial tumors?
A: They can penetrate deep into the tissue.
B: They are only effective on certain types of cancer.
C: They only penetrate up to a depth of about 5 cm.
D: They can be used without any protective measures.
E: They enhance the effects of chemotherapy.
Answer: C

Question: What is the outcome of electrons and their antiparticles passing through magnetic fields in particle accelerators?
A: They generate microwaves.
B: They emit synchrotron radiation.
C: They lose all their energy.
D: They transform into photons.
E: They generate gravitational waves.
Answer: B
@
The Big Bang theory is the most widely accepted scientific theory to explain the early stages in the evolution of the Universe.[144] For the first millisecond of the Big Bang, the temperatures were over 10 billion kelvins and photons had mean energies over a million electronvolts. These photons were sufficiently energetic that they could react with each other to form pairs of electrons and positrons. Likewise, positron-electron pairs annihilated each other and emitted energetic photons:


γ
 + 
γ
 ↔ 
e+
 + 
e−
An equilibrium between electrons, positrons and photons was maintained during this phase of the evolution of the Universe. After 15 seconds had passed, however, the temperature of the universe dropped below the threshold where electron-positron formation could occur. Most of the surviving electrons and positrons annihilated each other, releasing gamma radiation that briefly reheated the universe.[145]

For reasons that remain uncertain, during the annihilation process there was an excess in the number of particles over antiparticles. Hence, about one electron for every billion electron-positron pairs survived. This excess matched the excess of protons over antiprotons, in a condition known as baryon asymmetry, resulting in a net charge of zero for the universe.[146][147] The surviving protons and neutrons began to participate in reactions with each other—in the process known as nucleosynthesis, forming isotopes of hydrogen and helium, with trace amounts of lithium. This process peaked after about five minutes.[148] Any leftover neutrons underwent negative beta decay with a half-life of about a thousand seconds, releasing a proton and electron in the process,


n
 → 
p
 + 
e−
 + 
ν
e
For about the next 300000–400000 years, the excess electrons remained too energetic to bind with atomic nuclei.[149] What followed is a period known as recombination, when neutral atoms were formed and the expanding universe became transparent to radiation.[150]

Roughly one million years after the big bang, the first generation of stars began to form.[150] Within a star, stellar nucleosynthesis results in the production of positrons from the fusion of atomic nuclei. These antimatter particles immediately annihilate with electrons, releasing gamma rays. The net result is a steady reduction in the number of electrons, and a matching increase in the number of neutrons. However, the process of stellar evolution can result in the synthesis of radioactive isotopes. Selected isotopes can subsequently undergo negative beta decay, emitting an electron and antineutrino from the nucleus.[151] An example is the cobalt-60 (60Co) isotope, which decays to form nickel-60 (60
Ni
).[152]
$
5
Question: During the initial moments of the Big Bang, after how long did the temperature of the universe drop below the threshold where electron-positron formation could occur?
A: After 1 millisecond.
B: After 10 seconds.
C: After 15 seconds.
D: After 1 minute.
E: After 1 hour.
Answer: C

Question: What is the term for the condition where there was an excess in the number of particles over antiparticles, resulting in a net charge of zero for the universe?
A: Nucleosynthesis.
B: Baryon asymmetry.
C: Recombination.
D: Stellar nucleosynthesis.
E: Beta decay.
Answer: B

Question: Following the Big Bang, during which phase did isotopes of hydrogen and helium form, peaking after about five minutes?
A: Recombination.
B: Baryon asymmetry.
C: Nucleosynthesis.
D: Stellar nucleosynthesis.
E: Beta decay.
Answer: C

Question: How long after the Big Bang did the first generation of stars begin to form?
A: 10,000 years.
B: 100,000 years.
C: 300,000 years.
D: 1 million years.
E: 10 million years.
Answer: D

Question: What is the outcome of the decay of the cobalt-60 (60Co) isotope?
A: It decays to form helium.
B: It decays to form lithium.
C: It releases only gamma rays.
D: It decays to form cobalt-61.
E: It decays to form nickel-60 (60Ni).
Answer: E
@
Cosmic rays are high-energy particles or clusters of particles (primarily represented by protons or atomic nuclei) that move through space at nearly the speed of light. They originate from the Sun, from outside of the Solar System in our own galaxy,[1] and from distant galaxies.[2] Upon impact with Earth's atmosphere, cosmic rays produce showers of secondary particles, some of which reach the surface, although the bulk is deflected off into space by the magnetosphere or the heliosphere.

Cosmic rays were discovered by Victor Hess in 1912 in balloon experiments, for which he was awarded the 1936 Nobel Prize in Physics.[3]

Direct measurement of cosmic rays, especially at lower energies, has been possible since the launch of the first satellites in the late 1950s. Particle detectors similar to those used in nuclear and high-energy physics are used on satellites and space probes for research into cosmic rays.[4] Data from the Fermi Space Telescope (2013)[5] have been interpreted as evidence that a significant fraction of primary cosmic rays originate from the supernova explosions of stars.[6][better source needed] Based on observations of neutrinos and gamma rays from blazar TXS 0506+056 in 2018, active galactic nuclei also appear to produce cosmic rays.[7][8]
$
5
Question: What are cosmic rays primarily represented by?
A: Electrons
B: Protons or atomic nuclei
C: Neutrinos
D: Photons
E: Gamma rays
Answer: B

Question: Who discovered cosmic rays?
A: Albert Einstein
B: Richard Feynman
C: Victor Hess
D: Isaac Newton
E: Marie Curie
Answer: C

Question: What happens when cosmic rays impact Earth's atmosphere?
A: They are absorbed entirely.
B: They produce showers of secondary particles.
C: They increase the temperature of the atmosphere.
D: They are all deflected by the magnetosphere.
E: They convert into gamma rays.
Answer: B

Question: Since when has the direct measurement of cosmic rays, especially at lower energies, been possible?
A: Since the 1910s
B: Since the 1930s
C: Since the 1950s
D: Since the launch of the first satellites in the late 1950s.
E: Since the observation of neutrinos from blazar TXS 0506+056 in 2018.
Answer: D

Question: What event has been interpreted as evidence that a significant fraction of primary cosmic rays originate from?
A: Active galactic nuclei
B: The launch of the Fermi Space Telescope
C: Supernova explosions of stars
D: Cosmic ray balloon experiments by Victor Hess
E: Observations of neutrinos from blazar TXS 0506+056
Answer: C
@
Primary cosmic rays mostly originate from outside the Solar System and sometimes even outside the Milky Way. When they interact with Earth's atmosphere, they are converted to secondary particles. The mass ratio of helium to hydrogen nuclei, 28%, is similar to the primordial elemental abundance ratio of these elements, 24%.[58] The remaining fraction is made up of the other heavier nuclei that are typical nucleosynthesis end products, primarily lithium, beryllium, and boron. These nuclei appear in cosmic rays in greater abundance (≈1%) than in the solar atmosphere, where they are only about 10−3 as abundant (by number) as helium. Cosmic rays composed of charged nuclei heavier than helium are called HZE ions. Due to the high charge and heavy nature of HZE ions, their contribution to an astronaut's radiation dose in space is significant even though they are relatively scarce.

This abundance difference is a result of the way in which secondary cosmic rays are formed. Carbon and oxygen nuclei collide with interstellar matter to form lithium, beryllium and boron in a process termed cosmic ray spallation. Spallation is also responsible for the abundances of scandium, titanium, vanadium, and manganese ions in cosmic rays produced by collisions of iron and nickel nuclei with interstellar matter.[59]

At high energies the composition changes and heavier nuclei have larger abundances in some energy ranges. Current experiments aim at more accurate measurements of the composition at high energies.

When cosmic rays enter the Earth's atmosphere, they collide with atoms and molecules, mainly oxygen and nitrogen. The interaction produces a cascade of lighter particles, a so-called air shower secondary radiation that rains down, including x-rays, protons, alpha particles, pions, muons, electrons, neutrinos, and neutrons.[67] All of the secondary particles produced by the collision continue onward on paths within about one degree of the primary particle's original path.

Typical particles produced in such collisions are neutrons and charged mesons such as positive or negative pions and kaons. Some of these subsequently decay into muons and neutrinos, which are able to reach the surface of the Earth. Some high-energy muons even penetrate for some distance into shallow mines, and most neutrinos traverse the Earth without further interaction. Others decay into photons, subsequently producing electromagnetic cascades. Hence, next to photons, electrons and positrons usually dominate in air showers. These particles as well as muons can be easily detected by many types of particle detectors, such as cloud chambers, bubble chambers, water-Cherenkov or scintillation detectors. The observation of a secondary shower of particles in multiple detectors at the same time is an indication that all of the particles came from that event.

Cosmic rays impacting other planetary bodies in the Solar System are detected indirectly by observing high-energy gamma ray emissions by gamma-ray telescope. These are distinguished from radioactive decay processes by their higher energies above about 10 MeV.
$
5
Question: What are cosmic rays composed of that are heavier than helium called?
A: Alpha particles
B: Pions
C: HZE ions
D: Neutrinos
E: Secondary cosmic rays
Answer: C

Question: Which process is responsible for the formation of lithium, beryllium, and boron in cosmic rays?
A: Nucleosynthesis
B: Air shower secondary radiation
C: Cosmic ray spallation
D: Electromagnetic cascades
E: Radiative decay
Answer: C

Question: Which particles dominate in air showers resulting from cosmic rays entering Earth's atmosphere?
A: Neutrinos
B: Neutrons
C: Electrons and positrons
D: Alpha particles
E: Pions
Answer: C

Question: How are cosmic rays impacting other planetary bodies in the Solar System primarily detected?
A: By their radio waves.
B: Through infrared emissions.
C: By observing high-energy gamma ray emissions.
D: By the production of pions and kaons.
E: Through direct capture by spacecraft.
Answer: C

Question: Which particles are able to reach the surface of the Earth after being produced from cosmic ray collisions in the atmosphere?
A: Positive and negative kaons
B: Alpha particles
C: Muons and neutrinos
D: Oxygen and nitrogen molecules
E: Protons
Answer: C
@
Effects
Changes in atmospheric chemistry
Cosmic rays ionize nitrogen and oxygen molecules in the atmosphere, which leads to a number of chemical reactions. Cosmic rays are also responsible for the continuous production of a number of unstable isotopes, such as carbon-14, in the Earth's atmosphere through the reaction:

n + 14N → p + 14C
Cosmic rays kept the level of carbon-14[84] in the atmosphere roughly constant (70 tons) for at least the past 100,000 years,[citation needed] until the beginning of above-ground nuclear weapons testing in the early 1950s. This fact is used in radiocarbon dating.

Reaction products of primary cosmic rays, radioisotope half-lifetime, and production reaction[85]
Hydrogen-1 (stable): spallation from nitrogen and oxygen, decay of neutrons from such spallation
Helium-3 (stable): spallation or from tritium
Helium-4 (stable): spallation producing alpha rays
Tritium (12.3 years): 14N(n, 3H)12C (spallation)
Beryllium-7 (53.3 days)
Beryllium-10 (1.39 million years): 14N(n,p α)10Be (spallation)
Carbon-14 (5730 years): 14N(n, p)14C (neutron activation)
Sodium-22 (2.6 years)
Sodium-24 (15 hours)
Magnesium-28 (20.9 hours)
Silicon-31 (2.6 hours)
Silicon-32 (101 years)
Phosphorus-32 (14.3 days)
Sulfur-35 (87.5 days)
Sulfur-38 (2.84 hours)
Chlorine-34 m (32 minutes)
Chlorine-36 (300,000 years)
Chlorine-38 (37.2 minutes)
Chlorine-39 (56 minutes)
Argon-39 (269 years)
Krypton-85 (10.7 years)
$
5
Question: Which unstable isotope in the Earth's atmosphere is used in radiocarbon dating?
A: Carbon-10
B: Carbon-12
C: Carbon-14
D: Carbon-16
E: Nitrogen-14
Answer: C

Question: Cosmic rays have kept the level of which isotope roughly constant in the atmosphere for the past 100,000 years?
A: Helium-3
B: Carbon-14
C: Sodium-22
D: Beryllium-7
E: Tritium
Answer: B

Question: What is the half-life of Tritium?
A: 12.3 years
B: 53.3 days
C: 5730 years
D: 15 hours
E: 1.39 million years
Answer: A

Question: Which isotope has a half-life of 1.39 million years?
A: Beryllium-7
B: Carbon-14
C: Sodium-22
D: Chlorine-36
E: Beryllium-10
Answer: E

Question: What type of reaction results in the creation of Carbon-14?
A: Alpha rays spallation
B: Proton activation
C: Neutron activation
D: Electron spallation
E: Isotope fission
Answer: C
@
Symmetric multiprocessing or shared-memory multiprocessing[1] (SMP) involves a multiprocessor computer hardware and software architecture where two or more identical processors are connected to a single, shared main memory, have full access to all input and output devices, and are controlled by a single operating system instance that treats all processors equally, reserving none for special purposes. Most multiprocessor systems today use an SMP architecture. In the case of multi-core processors, the SMP architecture applies to the cores, treating them as separate processors.

SMP systems have centralized shared memory called main memory (MM) operating under a single operating system with two or more homogeneous processors. Usually each processor has an associated private high-speed memory known as cache memory (or cache) to speed up the main memory data access and to reduce the system bus traffic.

Processors may be interconnected using buses, crossbar switches or on-chip mesh networks. The bottleneck in the scalability of SMP using buses or crossbar switches is the bandwidth and power consumption of the interconnect among the various processors, the memory, and the disk arrays. Mesh architectures avoid these bottlenecks, and provide nearly linear scalability to much higher processor counts at the sacrifice of programmability:

Serious programming challenges remain with this kind of architecture because it requires two distinct modes of programming; one for the CPUs themselves and one for the interconnect between the CPUs. A single programming language would have to be able to not only partition the workload, but also comprehend the memory locality, which is severe in a mesh-based architecture.[4]

SMP systems allow any processor to work on any task no matter where the data for that task is located in memory, provided that each task in the system is not in execution on two or more processors at the same time. With proper operating system support, SMP systems can easily move tasks between processors to balance the workload efficiently.
$
5
Question: What does SMP stand for in computer architecture?
A: Symmetric Multiple Processing
B: Shared-Memory Processing
C: Symmetric Multiprocessor Programming
D: Symmetric Multiprocessing
E: Synchronous Memory Processing
Answer: D

Question: In an SMP system, how does the operating system treat the processors?
A: It reserves some processors for special purposes.
B: It treats one processor as the master and others as slaves.
C: It treats all processors equally.
D: It assigns a unique task to each processor.
E: It limits access of some processors to the main memory.
Answer: C

Question: What is the primary purpose of the cache memory associated with each processor in an SMP system?
A: To store the operating system.
B: To act as the main memory.
C: To speed up main memory data access.
D: To store data permanently.
E: To connect processors.
Answer: C

Question: What challenge does a mesh-based architecture in SMP systems introduce?
A: It reduces memory access speed.
B: It requires two distinct modes of programming.
C: It reduces processor counts.
D: It limits the scalability.
E: It eliminates the use of buses.
Answer: B

Question: In SMP systems, what ensures that a task is not executed on two or more processors at the same time?
A: Mesh-based architecture.
B: Cache memory.
C: Proper operating system support.
D: High-speed memory.
E: Interconnected buses.
Answer: C
@
In current SMP systems, all of the processors are tightly coupled inside the same box with a bus or switch; on earlier SMP systems, a single CPU took an entire cabinet. Some of the components that are shared are global memory, disks, and I/O devices. Only one copy of an OS runs on all the processors, and the OS must be designed to take advantage of this architecture. Some of the basic advantages involves cost-effective ways to increase throughput. To solve different problems and tasks, SMP applies multiple processors to that one problem, known as parallel programming.

However, there are a few limits on the scalability of SMP due to cache coherence and shared objects.

When more than one program executes at the same time, an SMP system has considerably better performance than a uni-processor, because different programs can run on different CPUs simultaneously. Conversely, asymmetric multiprocessing (AMP) usually allows only one processor to run a program or task at a time. For example, AMP can be used in assigning specific tasks to CPU based to priority and importance of task completion. AMP was created well before SMP in terms of handling multiple CPUs, which explains the lack of performance based on the example provided.

In cases where an SMP environment processes many jobs, administrators often experience a loss of hardware efficiency. Software programs have been developed to schedule jobs and other functions of the computer so that the processor utilization reaches its maximum potential. Good software packages can achieve this maximum potential by scheduling each CPU separately, as well as being able to integrate multiple SMP machines and clusters.

Access to RAM is serialized; this and cache coherency issues causes performance to lag slightly behind the number of additional processors in the system.
$
5
Question: What do current SMP systems use to connect processors within the same box?
A: RAM channels
B: Asymmetric paths
C: Uni-process pathways
D: A bus or switch
E: Parallel connectors
Answer: D

Question: Which of the following is a benefit of an SMP system when multiple programs execute simultaneously?
A: It requires more RAM.
B: It performs similarly to a uni-processor.
C: Different programs can run on different CPUs at the same time.
D: Only one program can access the global memory.
E: All CPUs focus on a single task.
Answer: C

Question: In contrast to SMP, what is a characteristic of asymmetric multiprocessing (AMP)?
A: It allows multiple processors to run a program simultaneously.
B: It provides better performance than SMP.
C: It usually allows only one processor to run a program or task at a time.
D: It serializes access to RAM.
E: It was developed after SMP.
Answer: C

Question: What problem might administrators face in an SMP environment with many jobs being processed?
A: Faster access to RAM.
B: A boost in hardware efficiency.
C: Loss of hardware efficiency.
D: Increased cache coherence.
E: Reduced software compatibility.
Answer: C

Question: What is one of the reasons for the performance lag in SMP systems as the number of processors increases?
A: Enhanced parallel programming.
B: Faster uni-processors.
C: Asymmetric multiprocessing advantages.
D: Serialized access to RAM and cache coherency issues.
E: Increased global memory access.
Answer: D
@
ARM (stylised in lowercase as arm, formerly an acronym for Advanced RISC Machines and originally Acorn RISC Machine) is a family of RISC instruction set architectures (ISAs) for computer processors. Arm Ltd. develops the ISAs and licenses them to other companies, who build the physical devices that use the instruction set. It also designs and licenses cores that implement these ISAs.

Due to their low costs, power consumption, and heat generation, ARM processors are useful for light, portable, battery-powered devices, including smartphones, laptops, and tablet computers, as well as embedded systems.[3][4][5] However, ARM processors are also used for desktops and servers, including the world's fastest supercomputer (Fugaku) from 2020[6] to 2022. With over 230 billion ARM chips produced,[7][8][9] as of 2022, ARM is the most widely used family of instruction set architectures.[10][4][11][12][13]

There have been several generations of the ARM design. The original ARM1 used a 32-bit internal structure but had a 26-bit address space that limited it to 64 MB of main memory. This limitation was removed in the ARMv3 series, which has a 32-bit address space, and several additional generations up to ARMv7 remained 32-bit. Released in 2011, the ARMv8-A architecture added support for a 64-bit address space and 64-bit arithmetic with its new 32-bit fixed-length instruction set.[14] Arm Ltd. has also released a series of additional instruction sets for different rules; the "Thumb" extension adds both 32- and 16-bit instructions for improved code density, while Jazelle added instructions for directly handling Java bytecode. More recent changes include the addition of simultaneous multithreading (SMT) for improved performance or fault tolerance.[15]
$
5
Question: What did the original acronym "ARM" stand for?
A: Acorn Review Machines
B: Advanced Real Machines
C: Advanced RISC Machines
D: Acorn Robust Machines
E: Advanced Running Machines
Answer: C

Question: Which of the following is a key advantage of ARM processors that makes them suitable for portable devices?
A: High heat generation
B: Increased power consumption
C: Low cost and low power consumption
D: 64-bit address space
E: Multithreading capabilities
Answer: C

Question: The ARMv8-A architecture, released in 2011, introduced support for what?
A: 26-bit address space
B: 32-bit address space
C: Java bytecode
D: 32- and 16-bit instructions
E: 64-bit address space and 64-bit arithmetic
Answer: E

Question: The "Thumb" extension to ARM's instruction set is primarily for what purpose?
A: Handling Java bytecode directly
B: Increasing power consumption
C: Adding support for 64-bit address space
D: Improved code density with both 32- and 16-bit instructions
E: Incorporating simultaneous multithreading
Answer: D

Question: Which ARM architecture removed the limitation of a 26-bit address space and used a 32-bit address space instead?
A: ARMv1
B: ARMv2
C: ARMv3
D: ARMv7
E: ARMv8-A
Answer: C
@
A central processing unit (CPU)—also called a central processor or main processor—is the most important processor in a given computer. Its electronic circuitry executes instructions of a computer program, such as arithmetic, logic, controlling, and input/output (I/O) operations. This role contrasts with that of external components, such as main memory and I/O circuitry,[1] and specialized coprocessors such as graphics processing units (GPUs).

The form, design, and implementation of CPUs have changed over time, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic–logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that orchestrates the fetching (from memory), decoding and execution (of instructions) by directing the coordinated operations of the ALU, registers, and other components.

Most modern CPUs are implemented on integrated circuit (IC) microprocessors, with one or more CPUs on a single IC chip. Microprocessor chips with multiple CPUs are multi-core processors. The individual physical CPUs, processor cores, can also be multithreaded to create additional virtual or logical CPUs.[2]

An IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC).

Array processors or vector processors have multiple processors that operate in parallel, with no unit considered central. Virtual CPUs are an abstraction of dynamically aggregated computational resources.[3]
$
5
Question: What primary function does a CPU perform in a computer?
A: Handling memory storage
B: Graphics rendering
C: Executing instructions of a computer program
D: Managing peripheral devices
E: Cooling the system
Answer: C

Question: Which component of a CPU performs arithmetic and logic operations?
A: Control unit
B: Main memory
C: I/O circuitry
D: Arithmetic–logic unit (ALU)
E: Processor core
Answer: D

Question: How are most modern CPUs implemented?
A: On hard drives
B: On power units
C: On integrated circuit (IC) microprocessors
D: On main memory modules
E: On GPU chips
Answer: C

Question: What term describes a microprocessor chip with multiple CPUs on it?
A: Single-core processor
B: ALU processor
C: Parallel processor
D: Logic processor
E: Multi-core processor
Answer: E

Question: What is an IC that contains not only a CPU but also memory, peripheral interfaces, and other computer components called?
A: Multi-threaded chip
B: Vector processor
C: Microcontroller or system on a chip (SoC)
D: Array processor
E: Dynamic aggregator
Answer: C
@
Early computers such as the ENIAC had to be physically rewired to perform different tasks, which caused these machines to be called "fixed-program computers".[4] The "central processing unit" term has been in use since as early as 1955.[5][6] Since the term "CPU" is generally defined as a device for software (computer program) execution, the earliest devices that could rightly be called CPUs came with the advent of the stored-program computer.

The idea of a stored-program computer had been already present in the design of J. Presper Eckert and John William Mauchly's ENIAC, but was initially omitted so that it could be finished sooner.[7] On June 30, 1945, before ENIAC was made, mathematician John von Neumann distributed a paper entitled First Draft of a Report on the EDVAC. It was the outline of a stored-program computer that would eventually be completed in August 1949.[8] EDVAC was designed to perform a certain number of instructions (or operations) of various types. Significantly, the programs written for EDVAC were to be stored in high-speed computer memory rather than specified by the physical wiring of the computer.[9] This overcame a severe limitation of ENIAC, which was the considerable time and effort required to reconfigure the computer to perform a new task.[10] With von Neumann's design, the program that EDVAC ran could be changed simply by changing the contents of the memory. EDVAC was not the first stored-program computer; the Manchester Baby, which was a small-scale experimental stored-program computer, ran its first program on 21 June 1948[11] and the Manchester Mark 1 ran its first program during the night of 16–17 June 1949.[12]
$
5
Question: What caused early computers like ENIAC to be termed "fixed-program computers"?
A: They used high-speed memory.
B: They were designed by John von Neumann.
C: They had to be physically rewired to perform different tasks.
D: They had multiple CPUs.
E: They were the first to use the term "central processing unit".
Answer: C

Question: Who distributed a paper entitled "First Draft of a Report on the EDVAC"?
A: J. Presper Eckert
B: John William Mauchly
C: ENIAC
D: John von Neumann
E: Manchester Baby
Answer: D

Question: What was a significant feature of the programs written for EDVAC?
A: They were determined by the physical wiring of the computer.
B: They were stored in high-speed computer memory.
C: They had to be manually rewritten for each task.
D: They were only experimental.
E: They were the first to use stored-program design.
Answer: B

Question: Why was the stored-program concept crucial for the design of computers like EDVAC?
A: It allowed for a multi-CPU configuration.
B: It reduced the physical size of the computer.
C: It eliminated the need for reconfiguration to perform a new task.
D: It allowed for faster arithmetic operations.
E: It made the computer cheaper to produce.
Answer: C

Question: Which was the first stored-program computer to run a program?
A: ENIAC
B: EDVAC
C: Manchester Baby
D: Manchester Mark 1
E: First Draft of a Report on the EDVAC
Answer: C
@
The performance or speed of a processor depends on, among many other factors, the clock rate (generally given in multiples of hertz) and the instructions per clock (IPC), which together are the factors for the instructions per second (IPS) that the CPU can perform.[81] Many reported IPS values have represented "peak" execution rates on artificial instruction sequences with few branches, whereas realistic workloads consist of a mix of instructions and applications, some of which take longer to execute than others. The performance of the memory hierarchy also greatly affects processor performance, an issue barely considered in MIPS calculations. Because of these problems, various standardized tests, often called "benchmarks" for this purpose‍—‌such as SPECint‍—‌have been developed to attempt to measure the real effective performance in commonly used applications.

Processing performance of computers is increased by using multi-core processors, which essentially is plugging two or more individual processors (called cores in this sense) into one integrated circuit.[82] Ideally, a dual core processor would be nearly twice as powerful as a single core processor. In practice, the performance gain is far smaller, only about 50%, due to imperfect software algorithms and implementation.[83] Increasing the number of cores in a processor (i.e. dual-core, quad-core, etc.) increases the workload that can be handled. This means that the processor can now handle numerous asynchronous events, interrupts, etc. which can take a toll on the CPU when overwhelmed. These cores can be thought of as different floors in a processing plant, with each floor handling a different task. Sometimes, these cores will handle the same tasks as cores adjacent to them if a single core is not enough to handle the information.

Due to specific capabilities of modern CPUs, such as simultaneous multithreading and uncore, which involve sharing of actual CPU resources while aiming at increased utilization, monitoring performance levels and hardware use gradually became a more complex task.[84] As a response, some CPUs implement additional hardware logic that monitors actual use of various parts of a CPU and provides various counters accessible to software; an example is Intel's Performance Counter Monitor technology.[2]
$
5
Question: What two factors determine the instructions per second (IPS) that a CPU can perform?
A: Clock rate and memory hierarchy
B: Clock rate and instructions per clock (IPC)
C: Instructions per clock (IPC) and memory hierarchy
D: Clock rate and SPECint benchmark
E: Memory hierarchy and benchmarks
Answer: B

Question: Why is there a need for standardized tests like SPECint in measuring CPU performance?
A: To measure the clock rate of CPUs
B: To determine the number of cores in a CPU
C: To handle numerous asynchronous events
D: To account for variations in real-world workloads and instruction sequences
E: To determine the IPC of a CPU
Answer: D

Question: In an ideal scenario, how much performance gain should a dual-core processor provide over a single-core processor?
A: 25% increase
B: 50% increase
C: Twice as powerful
D: Four times as powerful
E: No significant difference
Answer: C

Question: What analogy is used to describe the functioning of different cores in a processor?
A: Different departments in a company
B: Different lanes on a highway
C: Different floors in a processing plant
D: Different units in an army
E: Different stations on a production line
Answer: C

Question: What is the purpose of additional hardware logic in some modern CPUs, like Intel's Performance Counter Monitor technology?
A: To increase the clock rate
B: To add more cores to the CPU
C: To monitor actual use of various parts of a CPU and provide counters accessible to software
D: To enhance simultaneous multithreading capabilities
E: To reduce power consumption
Answer: C
@
The description of the basic operation of a CPU offered in the previous section describes the simplest form that a CPU can take. This type of CPU, usually referred to as subscalar, operates on and executes one instruction on one or two pieces of data at a time, that is less than one instruction per clock cycle (IPC < 1).

This process gives rise to an inherent inefficiency in subscalar CPUs. Since only one instruction is executed at a time, the entire CPU must wait for that instruction to complete before proceeding to the next instruction. As a result, the subscalar CPU gets "hung up" on instructions which take more than one clock cycle to complete execution. Even adding a second execution unit (see below) does not improve performance much; rather than one pathway being hung up, now two pathways are hung up and the number of unused transistors is increased. This design, wherein the CPU's execution resources can operate on only one instruction at a time, can only possibly reach scalar performance (one instruction per clock cycle, IPC = 1). However, the performance is nearly always subscalar (less than one instruction per clock cycle, IPC < 1).

Attempts to achieve scalar and better performance have resulted in a variety of design methodologies that cause the CPU to behave less linearly and more in parallel. When referring to parallelism in CPUs, two terms are generally used to classify these design techniques:

instruction-level parallelism (ILP), which seeks to increase the rate at which instructions are executed within a CPU (that is, to increase the use of on-die execution resources);
task-level parallelism (TLP), which purposes to increase the number of threads or processes that a CPU can execute simultaneously.
Each methodology differs both in the ways in which they are implemented, as well as the relative effectiveness they afford in increasing the CPU's performance for an application.[i]
$
5
Question: What term describes a CPU that operates on and executes one instruction on one or two pieces of data at a time, with an IPC of less than 1?
A: Scalar
B: Parallel
C: Subscalar
D: Superscalar
E: Task-level
Answer: C

Question: What inefficiency arises in subscalar CPUs due to their operation?
A: They can execute multiple instructions simultaneously.
B: They cannot achieve scalar performance.
C: The entire CPU waits for an instruction to complete before moving to the next.
D: They always achieve one instruction per clock cycle.
E: They focus primarily on task-level parallelism.
Answer: C

Question: If a CPU can only reach one instruction per clock cycle, what performance level is it achieving?
A: Subscalar
B: Parallel
C: Scalar
D: Superscalar
E: Task-level
Answer: C

Question: What type of parallelism seeks to increase the rate at which instructions are executed within a CPU?
A: Execution-level parallelism
B: Data-level parallelism
C: Scalar parallelism
D: Instruction-level parallelism (ILP)
E: Task-level parallelism (TLP)
Answer: D

Question: Which form of parallelism aims to increase the number of threads or processes a CPU can handle simultaneously?
A: Execution-level parallelism
B: Data-level parallelism
C: Scalar parallelism
D: Instruction-level parallelism (ILP)
E: Task-level parallelism (TLP)
Answer: E
@
A CPU cache[64] is a hardware cache used by the central processing unit (CPU) of a computer to reduce the average cost (time or energy) to access data from the main memory. A cache is a smaller, faster memory, closer to a processor core, which stores copies of the data from frequently used main memory locations. Most CPUs have different independent caches, including instruction and data caches, where the data cache is usually organized as a hierarchy of more cache levels (L1, L2, L3, L4, etc.).

All modern (fast) CPUs (with few specialized exceptions[f]) have multiple levels of CPU caches. The first CPUs that used a cache had only one level of cache; unlike later level 1 caches, it was not split into L1d (for data) and L1i (for instructions). Almost all current CPUs with caches have a split L1 cache. They also have L2 caches and, for larger processors, L3 caches as well. The L2 cache is usually not split and acts as a common repository for the already split L1 cache. Every core of a multi-core processor has a dedicated L2 cache and is usually not shared between the cores. The L3 cache, and higher-level caches, are shared between the cores and are not split. An L4 cache is currently uncommon, and is generally on dynamic random-access memory (DRAM), rather than on static random-access memory (SRAM), on a separate die or chip. That was also the case historically with L1, while bigger chips have allowed integration of it and generally all cache levels, with the possible exception of the last level. Each extra level of cache tends to be bigger and be optimized differently.

Other types of caches exist (that are not counted towards the "cache size" of the most important caches mentioned above), such as the translation lookaside buffer (TLB) that is part of the memory management unit (MMU) that most CPUs have.

Caches are generally sized in powers of two: 2, 8, 16 etc. KiB or MiB (for larger non-L1) sizes, although the IBM z13 has a 96 KiB L1 instruction cache.[65]
$
5
Question: What is the primary purpose of a CPU cache?
A: To enhance the graphics of the computer.
B: To reduce the average cost to access data from the main memory.
C: To store backup data for the CPU.
D: To manage power consumption of the CPU.
E: To interface with peripheral devices.
Answer: B

Question: What does a cache primarily store?
A: Original data from the hard drive.
B: Unused data.
C: Data from external storage devices.
D: Copies of the data from frequently used main memory locations.
E: Data for the operating system.
Answer: D

Question: Which cache is typically not split and acts as a common repository for the already split L1 cache?
A: L4 cache
B: Translation lookaside buffer
C: L3 cache
D: L1 cache
E: L2 cache
Answer: E

Question: Every core of a multi-core processor typically has a dedicated ________ that is not shared between the cores.
A: L4 cache
B: L3 cache
C: L2 cache
D: L1 cache
E: Translation lookaside buffer
Answer: C

Question: Which of the following is NOT counted towards the "cache size" of the main caches?
A: L1 cache
B: L2 cache
C: L3 cache
D: Translation lookaside buffer (TLB)
E: L4 cache
Answer: D
@
Stars spend about 90% of their lifetimes fusing hydrogen into helium in high-temperature-and-pressure reactions in their cores. Such stars are said to be on the main sequence and are called dwarf stars. Starting at zero-age main sequence, the proportion of helium in a star's core will steadily increase, the rate of nuclear fusion at the core will slowly increase, as will the star's temperature and luminosity.[75] The Sun, for example, is estimated to have increased in luminosity by about 40% since it reached the main sequence 4.6 billion (4.6×109) years ago.[76]

Every star generates a stellar wind of particles that causes a continual outflow of gas into space. For most stars, the mass lost is negligible. The Sun loses 10−14 M☉ every year,[77] or about 0.01% of its total mass over its entire lifespan. However, very massive stars can lose 10−7 to 10−5 M☉ each year, significantly affecting their evolution.[78] Stars that begin with more than 50 M☉ can lose over half their total mass while on the main sequence.[79]


An example of a Hertzsprung–Russell diagram for a set of stars that includes the Sun (center) (see Classification)
The time a star spends on the main sequence depends primarily on the amount of fuel it has and the rate at which it fuses it. The Sun is expected to live 10 billion (1010) years. Massive stars consume their fuel very rapidly and are short-lived. Low mass stars consume their fuel very slowly. Stars less massive than 0.25 M☉, called red dwarfs, are able to fuse nearly all of their mass while stars of about 1 M☉ can only fuse about 10% of their mass. The combination of their slow fuel-consumption and relatively large usable fuel supply allows low mass stars to last about one trillion (10×1012) years; the most extreme of 0.08 M☉ will last for about 12 trillion years. Red dwarfs become hotter and more luminous as they accumulate helium. When they eventually run out of hydrogen, they contract into a white dwarf and decline in temperature.[59] Since the lifespan of such stars is greater than the current age of the universe (13.8 billion years), no stars under about 0.85 M☉[80] are expected to have moved off the main sequence.

Besides mass, the elements heavier than helium can play a significant role in the evolution of stars. Astronomers label all elements heavier than helium "metals", and call the chemical concentration of these elements in a star, its metallicity. A star's metallicity can influence the time the star takes to burn its fuel, and controls the formation of its magnetic fields,[81] which affects the strength of its stellar wind.[82] Older, population II stars have substantially less metallicity than the younger, population I stars due to the composition of the molecular clouds from which they formed. Over time, such clouds become increasingly enriched in heavier elements as older stars die and shed portions of their atmospheres.[83]
$
5
Question: During which phase do stars spend approximately 90% of their lifetimes fusing hydrogen into helium?
A: Stellar wind phase
B: White dwarf phase
C: Off the main sequence phase
D: Main sequence phase
E: Red giant phase
Answer: D

Question: Which type of stars can lose more than half of their total mass while on the main sequence?
A: Red dwarfs
B: Stars of about 1 M☉
C: Stars with over 50 M☉
D: White dwarfs
E: Stars with less than 0.25 M☉
Answer: C

Question: What is the primary factor determining the time a star remains on the main sequence?
A: Its metallicity
B: The strength of its stellar wind
C: The presence of elements heavier than helium
D: The rate of nuclear fusion in its core
E: The amount of fuel it possesses and its rate of fusion
Answer: E

Question: How long are red dwarfs expected to last, given their slow fuel consumption?
A: 10 billion years
B: 12 trillion years
C: 13.8 billion years
D: 4.6 billion years
E: 1010 years
Answer: B

Question: What term do astronomers use to describe elements heavier than helium in a star?
A: Gases
B: Fuels
C: Isotopes
D: Metals
E: Fusion elements
Answer: D
@
As a star's core shrinks, the intensity of radiation from that surface increases, creating such radiation pressure on the outer shell of gas that it will push those layers away, forming a planetary nebula. If what remains after the outer atmosphere has been shed is less than roughly 1.4 M☉, it shrinks to a relatively tiny object about the size of Earth, known as a white dwarf. White dwarfs lack the mass for further gravitational compression to take place.[93] The electron-degenerate matter inside a white dwarf is no longer a plasma. Eventually, white dwarfs fade into black dwarfs over a very long period of time.[94]


The Crab Nebula, remnants of a supernova that was first observed around 1050 AD
In massive stars, fusion continues until the iron core has grown so large (more than 1.4 M☉) that it can no longer support its own mass. This core will suddenly collapse as its electrons are driven into its protons, forming neutrons, neutrinos, and gamma rays in a burst of electron capture and inverse beta decay. The shockwave formed by this sudden collapse causes the rest of the star to explode in a supernova. Supernovae become so bright that they may briefly outshine the star's entire home galaxy. When they occur within the Milky Way, supernovae have historically been observed by naked-eye observers as "new stars" where none seemingly existed before.[95]

A supernova explosion blows away the star's outer layers, leaving a remnant such as the Crab Nebula.[95] The core is compressed into a neutron star, which sometimes manifests itself as a pulsar or X-ray burster. In the case of the largest stars, the remnant is a black hole greater than 4 M☉.[96] In a neutron star the matter is in a state known as neutron-degenerate matter, with a more exotic form of degenerate matter, QCD matter, possibly present in the core.[97]

The blown-off outer layers of dying stars include heavy elements, which may be recycled during the formation of new stars. These heavy elements allow the formation of rocky planets. The outflow from supernovae and the stellar wind of large stars play an important part in shaping the interstellar medium.[95]
$
5
Question: What is formed when a star's outer layers are pushed away due to the intensity of radiation from its shrinking core?
A: Supernova
B: Black dwarf
C: Neutron star
D: Planetary nebula
E: Black hole
Answer: D

Question: What happens to a remnant of a star after a supernova if it is greater than 4 M☉?
A: It becomes a white dwarf.
B: It transforms into a pulsar.
C: It forms a black dwarf.
D: It becomes a neutron star.
E: It turns into a black hole.
Answer: E

Question: In the core of massive stars, when fusion continues until it can no longer support its own mass, it collapses. What event does this sudden collapse lead to?
A: Formation of a white dwarf
B: Creation of a planetary nebula
C: Formation of a black dwarf
D: Explosion in a supernova
E: Creation of a neutron-degenerate matter
Answer: D

Question: The matter inside a neutron star is known as:
A: Plasma
B: Electron-degenerate matter
C: Proton-rich matter
D: Neutron-degenerate matter
E: Gamma rays
Answer: D

Question: What significance do the blown-off outer layers of dying stars hold?
A: They cause the star to expand again.
B: They form new black holes.
C: They can be recycled in the formation of new stars and contribute to the formation of rocky planets.
D: They fuse to form helium.
E: They create new galaxies.
Answer: C
@
The energy produced by stars, a product of nuclear fusion, radiates to space as both electromagnetic radiation and particle radiation. The particle radiation emitted by a star is manifested as the stellar wind,[171] which streams from the outer layers as electrically charged protons and alpha and beta particles. A steady stream of almost massless neutrinos emanate directly from the star's core.[172]

The production of energy at the core is the reason stars shine so brightly: every time two or more atomic nuclei fuse together to form a single atomic nucleus of a new heavier element, gamma ray photons are released from the nuclear fusion product. This energy is converted to other forms of electromagnetic energy of lower frequency, such as visible light, by the time it reaches the star's outer layers.[173]

The color of a star, as determined by the most intense frequency of the visible light, depends on the temperature of the star's outer layers, including its photosphere.[174] Besides visible light, stars emit forms of electromagnetic radiation that are invisible to the human eye. In fact, stellar electromagnetic radiation spans the entire electromagnetic spectrum, from the longest wavelengths of radio waves through infrared, visible light, ultraviolet, to the shortest of X-rays, and gamma rays. From the standpoint of total energy emitted by a star, not all components of stellar electromagnetic radiation are significant, but all frequencies provide insight into the star's physics.[172]

Using the stellar spectrum, astronomers can determine the surface temperature, surface gravity, metallicity and rotational velocity of a star. If the distance of the star is found, such as by measuring the parallax, then the luminosity of the star can be derived. The mass, radius, surface gravity, and rotation period can then be estimated based on stellar models. (Mass can be calculated for stars in binary systems by measuring their orbital velocities and distances. Gravitational microlensing has been used to measure the mass of a single star.[175]) With these parameters, astronomers can estimate the age of the star.[176]
$
5
Question: What is the primary reason stars shine so brightly?
A: They reflect light from neighboring stars.
B: Gamma ray photons are released during nuclear fusion in their cores.
C: They have a constant external energy source.
D: They absorb and re-emit cosmic radiation.
E: They undergo a continuous chemical reaction on their surface.
Answer: B

Question: What determines the color of a star?
A: The number of planets orbiting it.
B: Its distance from the Earth.
C: The type of fuel it uses for nuclear fusion.
D: The intensity of gamma rays it emits.
E: The temperature of its outer layers, including its photosphere.
Answer: E

Question: Which form of radiation is NOT emitted by stars according to the given information?
A: Radio waves
B: Visible light
C: Ultraviolet
D: Microwaves
E: Gamma rays
Answer: D

Question: Using the stellar spectrum, what can astronomers NOT determine about a star?
A: Surface temperature
B: Number of moons around the star
C: Metallicity
D: Rotational velocity
E: Surface gravity
Answer: B

Question: How can the mass of a star in a binary system be calculated?
A: By the intensity of light it emits.
B: By measuring its visible size.
C: By measuring its orbital velocities and distances.
D: By the number of neutrinos it releases.
E: By the color of its photosphere.
Answer: C
@
The current stellar classification system originated in the early 20th century, when stars were classified from A to Q based on the strength of the hydrogen line.[190] It was thought that the hydrogen line strength was a simple linear function of temperature. Instead, it was more complicated: it strengthened with increasing temperature, peaked near 9000 K, and then declined at greater temperatures. The classifications were since reordered by temperature, on which the modern scheme is based.[191]

Stars are given a single-letter classification according to their spectra, ranging from type O, which are very hot, to M, which are so cool that molecules may form in their atmospheres. The main classifications in order of decreasing surface temperature are: O, B, A, F, G, K, and M. A variety of rare spectral types are given special classifications. The most common of these are types L and T, which classify the coldest low-mass stars and brown dwarfs. Each letter has 10 sub-divisions, numbered from 0 to 9, in order of decreasing temperature. However, this system breaks down at extreme high temperatures as classes O0 and O1 may not exist.[192]

In addition, stars may be classified by the luminosity effects found in their spectral lines, which correspond to their spatial size and is determined by their surface gravity. These range from 0 (hypergiants) through III (giants) to V (main sequence dwarfs); some authors add VII (white dwarfs). Main sequence stars fall along a narrow, diagonal band when graphed according to their absolute magnitude and spectral type.[192] The Sun is a main sequence G2V yellow dwarf of intermediate temperature and ordinary size.[193]

There is additional nomenclature in the form of lower-case letters added to the end of the spectral type to indicate peculiar features of the spectrum. For example, an "e" can indicate the presence of emission lines; "m" represents unusually strong levels of metals, and "var" can mean variations in the spectral type.[192]

White dwarf stars have their own class that begins with the letter D. This is further sub-divided into the classes DA, DB, DC, DO, DZ, and DQ, depending on the types of prominent lines found in the spectrum. This is followed by a numerical value that indicates the temperature.[194]
$
5
Question: In the early 20th century, what was the basis for the initial classification of stars?
A: Their luminosity
B: Their distance from Earth
C: The strength of the hydrogen line
D: Their surface gravity
E: The presence of specific metals in their spectra
Answer: C

Question: Which classification corresponds to very hot stars?
A: A
B: K
C: M
D: L
E: O
Answer: E

Question: How are main sequence stars classified in terms of their spatial size and surface gravity?
A: 0 (hypergiants)
B: III (giants)
C: IV (sub-giants)
D: V (main sequence dwarfs)
E: VII (white dwarfs)
Answer: D

Question: What does the "e" indicate when added to the end of a star's spectral type?
A: Presence of emission lines
B: Elevated temperature
C: Excess metals
D: End of life cycle
E: Extremely large size
Answer: A

Question: What does the class D represent in the context of stellar classification?
A: Main sequence dwarfs
B: Giants
C: Hypergiants
D: White dwarfs
E: Stars with a dominant hydrogen line
Answer: D
@
The interior of a stable star is in a state of hydrostatic equilibrium: the forces on any small volume almost exactly counterbalance each other. The balanced forces are inward gravitational force and an outward force due to the pressure gradient within the star. The pressure gradient is established by the temperature gradient of the plasma; the outer part of the star is cooler than the core. The temperature at the core of a main sequence or giant star is at least on the order of 107 K. The resulting temperature and pressure at the hydrogen-burning core of a main sequence star are sufficient for nuclear fusion to occur and for sufficient energy to be produced to prevent further collapse of the star.[198][199]

As atomic nuclei are fused in the core, they emit energy in the form of gamma rays. These photons interact with the surrounding plasma, adding to the thermal energy at the core. Stars on the main sequence convert hydrogen into helium, creating a slowly but steadily increasing proportion of helium in the core. Eventually the helium content becomes predominant, and energy production ceases at the core. Instead, for stars of more than 0.4 M☉, fusion occurs in a slowly expanding shell around the degenerate helium core.[200]

In addition to hydrostatic equilibrium, the interior of a stable star will maintain an energy balance of thermal equilibrium. There is a radial temperature gradient throughout the interior that results in a flux of energy flowing toward the exterior. The outgoing flux of energy leaving any layer within the star will exactly match the incoming flux from below.[201]
$
5
Question: What maintains the balance of forces within a stable star's interior?
A: The pull of dark matter
B: The force due to electromagnetic waves
C: Inward gravitational force and outward force due to the pressure gradient
D: The rotation of the star
E: The force from external celestial bodies
Answer: C

Question: Why is the outer part of the star cooler than the core?
A: Due to the presence of helium
B: Because of the rotation of the star
C: Because it's farther from the center of the galaxy
D: Due to the temperature gradient of the plasma
E: Because of the influence of dark matter
Answer: D

Question: What happens in the core of a main sequence star in terms of atomic fusion?
A: Fusion of helium into hydrogen
B: Fusion of hydrogen into lithium
C: Fusion of hydrogen into helium
D: Fusion of carbon into neon
E: No fusion takes place
Answer: C

Question: As the helium content in the core becomes predominant, what ceases?
A: Gravitational pull
B: Energy production at the core
C: Gamma ray emissions
D: The star's rotation
E: Expansion of the star
Answer: B

Question: What ensures thermal equilibrium inside a stable star?
A: A consistent temperature throughout the star
B: Flux of energy flowing outward matching the incoming flux from below
C: The balance between gamma rays and X-rays
D: The balance between the star's rotation and its gravity
E: A consistent pressure throughout the star
Answer: B
@
Stochastic computing was first introduced in a pioneering paper by von Neumann in 1953.[307] However, the theory could not be implemented until advances in computing of the 1960s.[308][309] Around 1950 he was also among the first people to talk about the time complexity of computations, which eventually evolved into the field of computational complexity theory.[310]

Herman Goldstine once described how he felt that even in comparison to all his technical achievements in computer science, it was the fact that he was held in such high esteem, had such a reputation, that the digital computer was accepted so quickly and worked on by others.[311] As an example, he talked about Tom Watson, Jr.'s meetings with von Neumann at the Institute for Advanced Study, whom he had come to see after having heard of von Neumann's work and wanting to know what was happening for himself personally. IBM, which Watson Jr. later became CEO and president of, would play an enormous role in the forthcoming computer industry. The second example was that once von Neumann was elected Commissioner of the Atomic Energy Commission, he would exert great influence over the commission's laboratories to promote the use of computers and to spur competition between IBM and Sperry-Rand, which would result in the Stretch and LARC computers that lead to further developments in the field. Goldstine also notes how von Neumann's expository style when speaking about technical subjects, particularly to non-technical audiences, was very attractive.[312] This view was held not just by him but by many other mathematicians and scientists of the time too.[313]
$
5
Question: Who introduced the concept of stochastic computing?
A: Herman Goldstine
B: Tom Watson, Jr.
C: von Neumann
D: IBM
E: Sperry-Rand
Answer: C

Question: When did the theory of stochastic computing become implementable due to advances in computing?
A: 1950s
B: 1960s
C: 1970s
D: 1980s
E: 1990s
Answer: B

Question: Which individual sought von Neumann's insights after hearing about his work and later became CEO and president of IBM?
A: Herman Goldstine
B: Sperry-Rand
C: The Atomic Energy Commission
D: Tom Watson, Jr.
E: von Neumann
Answer: D

Question: After being elected Commissioner of the Atomic Energy Commission, what influence did von Neumann exert?
A: He worked on the development of the first digital computer
B: He focused on stochastic computing exclusively
C: He promoted the use of computers and spurred competition between IBM and Sperry-Rand
D: He started his own computer company
E: He collaborated closely with Herman Goldstine
Answer: C

Question: What was notable about von Neumann's expository style when discussing technical subjects?
A: He used complex terminologies
B: He primarily spoke to technical audiences only
C: He was concise and straight to the point
D: His style was attractive, especially to non-technical audiences
E: He rarely spoke about his own achievements
Answer: D
@
Evolution of Complexity
Von Neumann's goal, as specified in his lectures at the University of Illinois in 1949,[2] was to design a machine whose complexity could grow automatically akin to biological organisms under natural selection. He asked what is the threshold of complexity that must be crossed for machines to be able to evolve and grow in complexity.[4][3] His “proof-of-principle” designs showed how it is logically possible. By using an architecture that separates a general purpose programmable (“universal”) constructor from a general purpose copier, he showed how the descriptions (tapes) of machines could accumulate mutations in self-replication and thus evolve more complex machines (the image below illustrates this possibility.). This is a very important result, as prior to that, it might have been conjectured that there is a fundamental logical barrier to the existence of such machines; in which case, biological organisms, which do evolve and grow in complexity, could not be “machines”, as conventionally understood. Von Neumann's insight was to think of life as a Turing Machine, which, is similarly defined by a state-determined machine "head" separated from a memory tape.[5]

In practice, when we consider the particular automata implementation Von Neumann pursued, we conclude that it does not yield much evolutionary dynamics because the machines are too fragile - the vast majority of perturbations cause them effectively to disintegrate.[3] Thus, it is the conceptual model outlined in his Illinois lectures[2] that is of greater interest today because it shows how a machine can in principle evolve.[7][4] This insight is all the more remarkable because the model preceded the discovery of the structure of the DNA molecule as discussed above.[6] It is also noteworthy that Von Neumann's design considers that mutations towards greater complexity need to occur in the (descriptions of) subsystems not involved in self-reproduction itself, as conceptualized by the additional automaton D he considered to perform all functions not directly involved in reproduction (see Figure above with Von Neumann's System of Self-Replication Automata with the ability to evolve.) Indeed, in biological organisms only very minor variations of the genetic code have been observed, which matches Von Neumann's rationale that the universal constructor (A) and Copier (B) would not themselves evolve, leaving all evolution (and growth of complexity) to automaton D.[4] In his unfinished work, Von Neumann also briefly considers conflict and interactions between his self-reproducing machines, towards understanding the evolution of ecological and social interactions from his theory of self-reproducing machines.[2]: 147 
$
5
Question: What was Von Neumann's primary goal as outlined in his lectures at the University of Illinois in 1949?
A: To design machines with fixed complexity
B: To model the DNA molecule structure
C: To understand how machines interact socially
D: To design a machine that can automatically grow in complexity like biological organisms
E: To analyze the fragility of machines
Answer: D

Question: Which aspect of life did Von Neumann relate to a Turing Machine?
A: The concept of DNA
B: The state-determined machine "head" separated from a memory tape
C: The automaton D
D: The ability to disintegrate when perturbed
E: The process of self-reproduction
Answer: B

Question: Why do the particular automata implementations pursued by Von Neumann not show much evolutionary dynamics?
A: They were not based on the principles of natural selection
B: They are too robust and resilient
C: They disintegrate with most perturbations
D: They evolve but only in one specific direction
E: They were only conceptual and not practical
Answer: C

Question: Which part of Von Neumann's design is responsible for functions not directly involved in reproduction?
A: The Turing Machine
B: The memory tape
C: Automaton D
D: The universal constructor (A)
E: The Copier (B)
Answer: C

Question: What does Von Neumann's design suggest regarding the evolution and growth of complexity in relation to the universal constructor (A) and Copier (B)?
A: Both A and B are the primary drivers of evolution
B: A and B would evolve rapidly, leading to increased complexity
C: Neither A nor B would evolve significantly; the complexity would be attributed to another part of the system
D: Only A would evolve, while B remains constant
E: Only minor variations in A and B have been observed, so they don't contribute to evolution
Answer: C
@
Von Neumann's crucial insight is that the description of the machine, which is copied and passed to offspring separately via the universal copier, has a double use; being both an active component of the construction mechanism in reproduction, and being the target of a passive copying process. This part is played by the description (akin to Turing's tape of instructions) in Von Neumann's combination of universal constructor and universal copier.[4] The combination of a universal constructor and copier, plus a tape of instructions conceptualizes and formalizes i) self-replication, and ii) open-ended evolution, or growth of complexity observed in biological organisms.[3]

This insight is all the more remarkable because it preceded the discovery of the structure of the DNA molecule by Watson and Crick and how it is separately translated and replicated in the cell—though it followed the Avery–MacLeod–McCarty experiment which identified DNA as the molecular carrier of genetic information in living organisms.[6] The DNA molecule is processed by separate mechanisms that carry out its instructions (translation) and copy (replicate) the DNA for newly constructed cells. The ability to achieve open-ended evolution lies in the fact that, just as in nature, errors (mutations) in the copying of the genetic tape can lead to viable variants of the automaton, which can then evolve via natural selection.[4] As Brenner put it:

Turing invented the stored-program computer, and von Neumann showed that the description is separate from the universal constructor. This is not trivial. Physicist Erwin Schrödinger confused the program and the constructor in his 1944 book What is Life?, in which he saw chromosomes as ″architect's plan and builder's craft in one″. This is wrong. The code script contains only a description of the executive function, not the function itself.[5]

— Sydney Brenner
$
5
Question: What role does the description of the machine play in Von Neumann's insight?
A: It acts as the energy source for the machine.
B: It serves both as an active part in the reproduction mechanism and as the target of passive copying.
C: It provides the visual representation of the machine.
D: It is the core of the universal constructor.
E: It is responsible for mutations in the system.
Answer: B

Question: What two main concepts are conceptualized and formalized by Von Neumann's combination of a universal constructor and copier, plus a tape of instructions?
A: Energy consumption and resource management
B: Error correction and replication
C: Self-replication and open-ended evolution
D: Instruction translation and error mutation
E: Machine learning and automation
Answer: C

Question: What significant discovery followed Von Neumann's insight but was a landmark in understanding the nature of genetic information in living organisms?
A: The Avery–MacLeod–McCarty experiment identifying DNA as the genetic carrier
B: The discovery of the RNA structure
C: Turing's invention of the stored-program computer
D: The discovery of chromosomes as the center of reproduction
E: Watson and Crick's unveiling of the structure of the DNA molecule
Answer: E

Question: What error can lead to the evolution of the automaton in a manner similar to natural processes?
A: Errors in the energy supply of the automaton
B: Errors in the constructor's processing capability
C: Errors (mutations) in the copying of the genetic tape
D: Errors in the transmission of signals in the system
E: Errors in the architectural design of the machine
Answer: C

Question: What misconception did physicist Erwin Schrödinger have in his 1944 book "What is Life?"?
A: He believed the DNA molecule had no significant role in genetics.
B: He saw chromosomes as both the "architect's plan and builder's craft in one".
C: He dismissed the idea of mutations leading to evolution.
D: He confused the function of RNA and DNA.
E: He believed that only the constructor was necessary for evolution.
Answer: B
@
In 2012, NASA researchers Metzger, Muscatello, Mueller, and Mantovani argued for a so-called "bootstrapping approach" to start self-replicating factories in space.[40] They developed this concept on the basis of In Situ Resource Utilization (ISRU) technologies that NASA has been developing to "live off the land" on the Moon or Mars. Their modeling showed that in just 20 to 40 years this industry could become self-sufficient then grow to large size, enabling greater exploration in space as well as providing benefits back to Earth. In 2014, Thomas Kalil of the White House Office of Science and Technology Policy published on the White House blog an interview with Metzger on bootstrapping solar system civilization through self-replicating space industry.[41] Kalil requested the public submit ideas for how "the Administration, the private sector, philanthropists, the research community, and storytellers can further these goals." Kalil connected this concept to what former NASA Chief technologist Mason Peck has dubbed "Massless Exploration", the ability to make everything in space so that you do not need to launch it from Earth. Peck has said, "...all the mass we need to explore the solar system is already in space. It's just in the wrong shape."[42] In 2016, Metzger argued that fully self-replicating industry can be started over several decades by astronauts at a lunar outpost for a total cost (outpost plus starting the industry) of about a third of the space budgets of the International Space Station partner nations, and that this industry would solve Earth's energy and environmental problems in addition to providing massless exploration.[43]
$
5
Question: What was the primary objective behind NASA researchers' "bootstrapping approach" proposed in 2012?
A: To develop new space vehicles for faster travel
B: To create self-replicating factories in space based on In Situ Resource Utilization technologies
C: To study the lunar atmosphere and environment
D: To ensure safe human travel between Earth and Mars
E: To study the potential benefits of self-replicating factories on Earth
Answer: B

Question: How long did NASA researchers estimate the self-replicating space industry to become self-sufficient?
A: Less than 10 years
B: Between 50 and 60 years
C: Between 20 and 40 years
D: Over 70 years
E: Exactly 25 years
Answer: C

Question: Who, from the White House Office of Science and Technology Policy, highlighted the discussion on bootstrapping solar system civilization?
A: Mason Peck
B: Thomas Kalil
C: John F. Kennedy
D: Metzger
E: Mueller
Answer: B

Question: What concept did Mason Peck introduce related to exploring the solar system?
A: Resource Utilization
B: Solar System Bootstrapping
C: Massless Exploration
D: Lunar Outpost Development
E: Earth Energy Solutions
Answer: C

Question: What did Metzger suggest could be achieved by astronauts at a lunar outpost with respect to self-replicating industry?
A: The industry could be started and would only be beneficial for space exploration.
B: It would lead to more job opportunities for astronauts.
C: The industry would be detrimental to Earth's environment.
D: The industry can be initiated within several decades at a fraction of the space budgets of the International Space Station partner nations and would have benefits for Earth's energy and environment.
E: Only energy problems on Earth would be addressed by the industry.
Answer: D
@
Aluminium (aluminum in North American English) is a chemical element with the symbol Al and atomic number 13. Aluminium has a density lower than those of other common metals; about one-third that of steel. It has a great affinity towards oxygen, forming a protective layer of oxide on the surface when exposed to air. Aluminium visually resembles silver, both in its color and in its great ability to reflect light. It is soft, nonmagnetic and ductile. It has one stable isotope: 27Al, which is highly abundant, making aluminium the twelfth-most common element in the universe. The radioactivity of 26Al is used in radiometric dating.

Chemically, aluminium is a post-transition metal in the boron group; as is common for the group, aluminium forms compounds primarily in the +3 oxidation state. The aluminium cation Al3+ is small and highly charged; as such, it is polarizing, and bonds aluminium forms tend towards covalency. The strong affinity towards oxygen leads to aluminium's common association with oxygen in nature in the form of oxides; for this reason, aluminium is found on Earth primarily in rocks in the crust, where it is the third-most abundant element, after oxygen and silicon, rather than in the mantle, and virtually never as the free metal. It is obtained industrially by mining bauxite, a sedimentary rock rich in aluminium minerals.
$
5
Question: What is the atomic number of Aluminium?
A: 12
B: 15
C: 13
D: 10
E: 14
Answer: C

Question: When exposed to air, what layer forms on the surface of Aluminium?
A: Layer of gold
B: Layer of rust
C: Protective layer of oxide
D: Protective layer of sulphate
E: Layer of nitrogen
Answer: C

Question: Which of the following describes Aluminium's appearance?
A: Golden and matte
B: Resembles silver in color and reflective ability
C: Dark and non-reflective
D: Reddish-brown
E: Transparent and shiny
Answer: B

Question: What is the primary source from which Aluminium is obtained industrially?
A: Silicate rock
B: Pure metal deposits
C: Iron ores
D: Bauxite
E: Sulfuric rock
Answer: D

Question: In terms of abundance on Earth's crust, where does Aluminium rank among elements?
A: First
B: Second
C: Third
D: Fourth
E: Fifth
Answer: C
@
An aluminium atom has 13 electrons, arranged in an electron configuration of [Ne] 3s2 3p1,[15] with three electrons beyond a stable noble gas configuration. Accordingly, the combined first three ionization energies of aluminium are far lower than the fourth ionization energy alone.[16] Such an electron configuration is shared with the other well-characterized members of its group, boron, gallium, indium, and thallium; it is also expected for nihonium. Aluminium can surrender its three outermost electrons in many chemical reactions (see below). The electronegativity of aluminium is 1.61 (Pauling scale).[17]

M. Tunes & S. Pogatscher, Montanuniversität Leoben 2019 No copyrights =)
High-resolution STEM-HAADF micrograph of Al atoms viewed along the [001] zone axis.
A free aluminium atom has a radius of 143 pm.[18] With the three outermost electrons removed, the radius shrinks to 39 pm for a 4-coordinated atom or 53.5 pm for a 6-coordinated atom.[18] At standard temperature and pressure, aluminium atoms (when not affected by atoms of other elements) form a face-centered cubic crystal system bound by metallic bonding provided by atoms' outermost electrons; hence aluminium (at these conditions) is a metal.[19] This crystal system is shared by many other metals, such as lead and copper; the size of a unit cell of aluminium is comparable to that of those other metals.[19] The system, however, is not shared by the other members of its group; boron has ionization energies too high to allow metallization, thallium has a hexagonal close-packed structure, and gallium and indium have unusual structures that are not close-packed like those of aluminium and thallium. The few electrons that are available for metallic bonding in aluminium metal are a probable cause for it being soft with a low melting point and low electrical resistivity.[20]
$
5
Question: How many electrons does an aluminium atom possess?
A: 10
B: 11
C: 12
D: 13
E: 14
Answer: D

Question: Which scale is used to measure the electronegativity of aluminium mentioned in the text?
A: Richter scale
B: Mohs scale
C: Kelvin scale
D: Pauling scale
E: Newton scale
Answer: D

Question: What is the radius of a free aluminium atom?
A: 53.5 pm
B: 143 pm
C: 200 pm
D: 39 pm
E: 100 pm
Answer: B

Question: At standard temperature and pressure, what crystal system do aluminium atoms form?
A: Simple cubic crystal system
B: Hexagonal close-packed system
C: Face-centered cubic crystal system
D: Body-centered cubic crystal system
E: Rhombohedral crystal system
Answer: C

Question: Which of the following is NOT a shared characteristic of aluminium and thallium according to the text?
A: Both have the same electron configuration
B: Both have the same ionization energies
C: Both form a face-centered cubic crystal system
D: Both are metals
E: Both have the same number of electrons
Answer: B
@
The production of aluminium starts with the extraction of bauxite rock from the ground. The bauxite is processed and transformed using the Bayer process into alumina, which is then processed using the Hall–Héroult process, resulting in the final aluminium metal.

Aluminium production is highly energy-consuming, and so the producers tend to locate smelters in places where electric power is both plentiful and inexpensive.[132] As of 2019, the world's largest smelters of aluminium are located in China, India, Russia, Canada, and the United Arab Emirates,[133] while China is by far the top producer of aluminium with a world share of fifty-five percent.

According to the International Resource Panel's Metal Stocks in Society report, the global per capita stock of aluminium in use in society (i.e. in cars, buildings, electronics, etc.) is 80 kg (180 lb). Much of this is in more-developed countries (350–500 kg (770–1,100 lb) per capita) rather than less-developed countries (35 kg (77 lb) per capita).[134]

Bauxite is converted to alumina by the Bayer process. Bauxite is blended for uniform composition and then is ground. The resulting slurry is mixed with a hot solution of sodium hydroxide; the mixture is then treated in a digester vessel at a pressure well above atmospheric, dissolving the aluminium hydroxide in bauxite while converting impurities into relatively insoluble compounds:[135]

Al(OH)3 + Na+ + OH− → Na+ + [Al(OH)4]−
After this reaction, the slurry is at a temperature above its atmospheric boiling point. It is cooled by removing steam as pressure is reduced. The bauxite residue is separated from the solution and discarded. The solution, free of solids, is seeded with small crystals of aluminium hydroxide; this causes decomposition of the [Al(OH)4]− ions to aluminium hydroxide. After about half of aluminium has precipitated, the mixture is sent to classifiers. Small crystals of aluminium hydroxide are collected to serve as seeding agents; coarse particles are converted to alumina by heating; the excess solution is removed by evaporation, (if needed) purified, and recycled.[135]
$
5
Question: What process is used to transform bauxite into alumina?
A: Bayer process
B: Hall-Héroult process
C: Fischer-Tropsch process
D: Haber process
E: Ostwald process
Answer: A

Question: Which country is the top producer of aluminium with a world share of fifty-five percent as of 2019?
A: United Arab Emirates
B: Canada
C: Russia
D: India
E: China
Answer: E

Question: According to the International Resource Panel's Metal Stocks in Society report, what is the global per capita stock of aluminium in use in society?
A: 35 kg
B: 77 lb
C: 180 lb
D: 80 kg
E: 100 kg
Answer: D

Question: In the Bayer process, after the reaction involving sodium hydroxide, the slurry is at a temperature:
A: Just below its freezing point
B: Above its atmospheric boiling point
C: Exactly at its atmospheric boiling point
D: At room temperature
E: Just above its freezing point
Answer: B

Question: What is done with the bauxite residue after the slurry has been treated with sodium hydroxide in the Bayer process?
A: It is transformed into alumina
B: It is used as seeding agents
C: It is separated from the solution and discarded
D: It is used to make aluminium metal directly
E: It is purified and recycled
Answer: C
@
Aluminium metal has an appearance ranging from silvery white to dull gray, depending on the surface roughness.[d] Aluminium mirrors are the most reflective of all metal mirrors for the near ultraviolet and far infrared light, and one of the most reflective in the visible spectrum, nearly on par with silver, and the two therefore look similar. Aluminium is also good at reflecting solar radiation, although prolonged exposure to sunlight in air adds wear to the surface of the metal; this may be prevented if aluminium is anodized, which adds a protective layer of oxide on the surface.

The density of aluminium is 2.70 g/cm3, about 1/3 that of steel, much lower than other commonly encountered metals, making aluminium parts easily identifiable through their lightness.[23] Aluminium's low density compared to most other metals arises from the fact that its nuclei are much lighter, while difference in the unit cell size does not compensate for this difference. The only lighter metals are the metals of groups 1 and 2, which apart from beryllium and magnesium are too reactive for structural use (and beryllium is very toxic).[24] Aluminium is not as strong or stiff as steel, but the low density makes up for this in the aerospace industry and for many other applications where light weight and relatively high strength are crucial.[25]

Pure aluminium is quite soft and lacking in strength. In most applications various aluminium alloys are used instead because of their higher strength and hardness.[26] The yield strength of pure aluminium is 7–11 MPa, while aluminium alloys have yield strengths ranging from 200 MPa to 600 MPa.[27] Aluminium is ductile, with a percent elongation of 50-70%,[28] and malleable allowing it to be easily drawn and extruded.[29] It is also easily machined and cast.[29]

Aluminium is an excellent thermal and electrical conductor, having around 60% the conductivity of copper, both thermal and electrical, while having only 30% of copper's density.[30] Aluminium is capable of superconductivity, with a superconducting critical temperature of 1.2 kelvin and a critical magnetic field of about 100 gauss (10 milliteslas).[31] It is paramagnetic and thus essentially unaffected by static magnetic fields.[32] The high electrical conductivity, however, means that it is strongly affected by alternating magnetic fields through the induction of eddy currents.[33]
$
5
Question: Which type of light is aluminium the most reflective for?
A: Visible spectrum
B: Near ultraviolet and far infrared light
C: X-ray
D: Near infrared light
E: Radio waves
Answer: B

Question: What is the density of aluminium?
A: 1.20 g/cm3
B: 7.80 g/cm3
C: 5.50 g/cm3
D: 2.70 g/cm3
E: 3.50 g/cm3
Answer: D

Question: Which of the following describes the strength of pure aluminium?
A: Very strong and stiff
B: Quite soft and lacking in strength
C: Moderately strong with high ductility
D: Brittle and fragile
E: As strong as steel
Answer: B

Question: How does the electrical conductivity of aluminium compare to that of copper?
A: It has 60% the conductivity of copper and 30% of its density
B: It has 30% the conductivity of copper and 60% of its density
C: It has equal conductivity to copper
D: It has twice the conductivity of copper
E: It has half the conductivity of copper
Answer: A

Question: What is the effect of static magnetic fields on aluminium?
A: It becomes superconductive
B: It is strongly affected and induces eddy currents
C: It becomes highly magnetic
D: It is essentially unaffected and is paramagnetic
E: It loses its electrical conductivity
Answer: D
@
The quantum harmonic oscillator is the quantum-mechanical analog of the classical harmonic oscillator. Because an arbitrary smooth potential can usually be approximated as a harmonic potential at the vicinity of a stable equilibrium point, it is one of the most important model systems in quantum mechanics. Furthermore, it is one of the few quantum-mechanical systems for which an exact, analytical solution is known.[1][2][3]

Hamiltonian and energy eigenstates

Wavefunction representations for the first eight bound eigenstates, n = 0 to 7. The horizontal axis shows the position x.

Corresponding probability densities.
The Hamiltonian of the particle is:

�
^
=
�
^
2
2
�
+
1
2
�
�
^
2
=
�
^
2
2
�
+
1
2
�
�
2
�
^
2
,
{\displaystyle {\hat {H}}={\frac {{\hat {p}}^{2}}{2m}}+{\frac {1}{2}}k{\hat {x}}^{2}={\frac {{\hat {p}}^{2}}{2m}}+{\frac {1}{2}}m\omega ^{2}{\hat {x}}^{2}\,,}
where m is the particle's mass, k is the force constant, 
�
=
�
/
�
{\textstyle \omega ={\sqrt {k/m}}} is the angular frequency of the oscillator, 
�
^{\hat {x}} is the position operator (given by x in the coordinate basis), and 
�
^{\hat {p}} is the momentum operator (given by 
�
^
=
−
�
ℏ
∂
/
∂
�
{\displaystyle {\hat {p}}=-i\hbar \,\partial /\partial x} in the coordinate basis). The first term in the Hamiltonian represents the kinetic energy of the particle, and the second term represents its potential energy, as in Hooke's law.
The time-independent Schrödinger equation is,

�
^
|
�
⟩
=
�
|
�
⟩
 
,
{\displaystyle {\hat {H}}\left|\psi \right\rangle =E\left|\psi \right\rangle ~,}
where E denotes a real number (which needs to be determined) that will specify a time-independent energy level, or eigenvalue, and the solution |ψ⟩ denotes that level's energy eigenstate.
Then solve the differential equation representing this eigenvalue problem in the coordinate basis, for the wave function ⟨x|ψ⟩ = ψ(x), using a spectral method. It turns out that there is a family of solutions. In this basis, they amount to Hermite functions,

�
�
(
�
)
=
1
2
�
�
!
(
�
�
�
ℏ
)
1
/
4
�
−
�
�
�
2
2
ℏ
�
�
(
�
�
ℏ
�
)
,
�
=
0
,
1
,
2
,
…
.
{\displaystyle \psi _{n}(x)={\frac {1}{\sqrt {2^{n}\,n!}}}\left({\frac {m\omega }{\pi \hbar }}\right)^{1/4}e^{-{\frac {m\omega x^{2}}{2\hbar }}}H_{n}\left({\sqrt {\frac {m\omega }{\hbar }}}x\right),\qquad n=0,1,2,\ldots .}
The functions Hn are the physicists' Hermite polynomials,

�
�
(
�
)
=
(
−
1
)
�
 
�
�
2
�
�
�
�
�
(
�
−
�
2
)
.
{\displaystyle H_{n}(z)=(-1)^{n}~e^{z^{2}}{\frac {d^{n}}{dz^{n}}}\left(e^{-z^{2}}\right).}
The corresponding energy levels are

�
�
=
ℏ
�
(
�
+
1
2
)
=
(
2
�
+
1
)
ℏ
2
�
 
.
{\displaystyle E_{n}=\hbar \omega {\bigl (}n+{\tfrac {1}{2}}{\bigr )}=(2n+1){\hbar  \over 2}\omega ~.}
This energy spectrum is noteworthy for three reasons. First, the energies are quantized, meaning that only discrete energy values (integer-plus-half multiples of ħω) are possible; this is a general feature of quantum-mechanical systems when a particle is confined. Second, these discrete energy levels are equally spaced, unlike in the Bohr model of the atom, or the particle in a box. Third, the lowest achievable energy (the energy of the n = 0 state, called the ground state) is not equal to the minimum of the potential well, but ħω/2 above it; this is called zero-point energy. Because of the zero-point energy, the position and momentum of the oscillator in the ground state are not fixed (as they would be in a classical oscillator), but have a small range of variance, in accordance with the Heisenberg uncertainty principle.

The ground state probability density is concentrated at the origin, which means the particle spends most of its time at the bottom of the potential well, as one would expect for a state with little energy. As the energy increases, the probability density peaks at the classical "turning points", where the state's energy coincides with the potential energy. (See the discussion below of the highly excited states.) This is consistent with the classical harmonic oscillator, in which the particle spends more of its time (and is therefore more likely to be found) near the turning points, where it is moving the slowest. The correspondence principle is thus satisfied. Moreover, special nondispersive wave packets, with minimum uncertainty, called coherent states oscillate very much like classical objects, as illustrated in the figure; they are not eigenstates of the Hamiltonian.
$
5
Question: Which system is the quantum harmonic oscillator a quantum-mechanical analog of?
A: Classical wave oscillator
B: Quantum wave oscillator
C: Classical harmonic oscillator
D: Quantum pendulum oscillator
E: Quantum field oscillator
Answer: C

Question: What does the first term in the Hamiltonian of the particle represent?
A: Potential energy
B: Frequency energy
C: Quantum energy
D: Kinetic energy
E: Wave energy
Answer: D

Question: What is the lowest achievable energy of the quantum harmonic oscillator referred to as?
A: Minimum potential energy
B: Ground state
C: Bohr energy
D: Quantum base energy
E: Base state
Answer: B

Question: What is the term used to describe the energy which is ħω/2 above the minimum of the potential well?
A: Turning point energy
B: Quantized energy
C: Bohr energy
D: Zero-point energy
E: Classical energy
Answer: D

Question: In the ground state, where is the probability density of the quantum harmonic oscillator concentrated?
A: At the top of the potential well
B: Equally distributed throughout the well
C: At the origin
D: At the classical turning points
E: Equally spaced along the horizontal axis
Answer: C
@
The US National Ignition Facility, which uses laser-driven inertial confinement fusion, was designed with a goal of break-even fusion; the first large-scale laser target experiments were performed in June 2009 and ignition experiments began in early 2011.[9][10] On 13 December 2022, the United States Department of Energy announced that on 5 December 2022, they had successfully accomplished break-even fusion, "delivering 2.05 megajoules (MJ) of energy to the target, resulting in 3.15 MJ of fusion energy output."[11]

Prior to this breakthrough, controlled fusion reactions had been unable to produce break-even (self-sustaining) controlled fusion.[12] The two most advanced approaches for it are magnetic confinement (toroid designs) and inertial confinement (laser designs). Workable designs for a toroidal reactor that theoretically will deliver ten times more fusion energy than the amount needed to heat plasma to the required temperatures are in development (see ITER). The ITER facility is expected to finish its construction phase in 2025. It will start commissioning the reactor that same year and initiate plasma experiments in 2025, but is not expected to begin full deuterium–tritium fusion until 2035.[13]

Private companies pursuing the commercialization of nuclear fusion received $2.6 billion in private funding in 2021 alone, going to many notable startups including but not limited to Commonwealth Fusion Systems, Helion Energy Inc., General Fusion, TAE Technologies Inc. and Zap Energy Inc.[14]
$
5
Question: What type of fusion does the US National Ignition Facility use?
A: Magnetic confinement fusion
B: Plasma confinement fusion
C: Toroid confinement fusion
D: Laser-driven inertial confinement fusion
E: Deuterium-tritium fusion
Answer: D

Question: When did the US Department of Energy announce that they had successfully accomplished break-even fusion?
A: June 2009
B: Early 2011
C: 5 December 2022
D: 13 December 2022
E: 2025
Answer: D

Question: Which facility is expected to finish its construction phase in 2025?
A: US National Ignition Facility
B: Helion Energy Inc.
C: TAE Technologies Inc.
D: Zap Energy Inc.
E: ITER
Answer: E

Question: How much private funding did private companies pursuing the commercialization of nuclear fusion receive in 2021?
A: $1.5 billion
B: $2.6 billion
C: $3.1 billion
D: $5 billion
E: $7.2 billion
Answer: B

Question: Which of the following is NOT a company mentioned as receiving private funding for nuclear fusion in 2021?
A: Commonwealth Fusion Systems
B: General Energy Solutions
C: Helion Energy Inc.
D: TAE Technologies Inc.
E: Zap Energy Inc.
Answer: B
@
The National Ignition Facility (NIF) is a laser-based inertial confinement fusion (ICF) research device, located at Lawrence Livermore National Laboratory in Livermore, California, United States. NIF's mission is to achieve fusion ignition with high energy gain. It achieved the first instance of scientific breakeven controlled fusion in an experiment on December 5, 2022, with an energy gain factor of 1.5.[1][2] It supports nuclear weapon maintenance and design by studying the behavior of matter under the conditions found within nuclear explosions.[3]

NIF is the largest and most powerful ICF device built to date.[4] The basic ICF concept is to squeeze a small amount of fuel to reach pressure and temperature necessary for fusion. NIF hosts the world's most energetic laser. The laser heats the outer layer of a small sphere. The energy is so intense that it causes the sphere to implode, squeezing the fuel inside. The implosion reaches a peak speed of 350 km/s (0.35 mm/ns),[5] raising the fuel density from about that of water to about 100 times that of lead. The delivery of energy and the adiabatic process during implosion raises the temperature of the fuel to hundreds of millions of degrees. At these temperatures, fusion processes occur in the tiny interval before the fuel explodes outward.

Construction on the NIF began in 1997. NIF was completed five years behind schedule and cost almost four times its original budget. Construction was certified complete on March 31, 2009, by the U.S. Department of Energy.[6] The first large-scale experiments were performed in June 2009[7] and the first "integrated ignition experiments" (which tested the laser's power) were declared completed in October 2010.[8]

From 2009 to 2012 experiments were conducted under the National Ignition Campaign, with the goal of reaching ignition just after the laser reached full power, some time in the second half of 2012. The campaign officially ended in September 2012, at about 1⁄10 the conditions needed for ignition.[9][10] Thereafter NIF has been used primarily for materials science and weapons research. In 2021, after improvements in fuel target design, NIF produced 70% of the energy of the laser, beating the record set in 1997 by the JET reactor at 67% and achieving a burning plasma.[11] On December 5, 2022, after further technical improvements, NIF reached "ignition", or scientific breakeven, for the first time, achieving a 154% energy yield.[12]
$
5
Question: Where is the National Ignition Facility (NIF) located?
A: Los Angeles, California
B: Livermore, California
C: San Francisco, California
D: San Diego, California
E: Sacramento, California
Answer: B

Question: What is the mission of the NIF?
A: To produce the world's most energetic laser
B: To achieve fusion ignition with low energy gain
C: To achieve fusion ignition with high energy gain
D: To study nuclear weapon maintenance only
E: To conduct materials science research
Answer: C

Question: When was construction on the NIF completed?
A: March 31, 2005
B: March 31, 2009
C: March 31, 2010
D: March 31, 2012
E: March 31, 2017
Answer: B

Question: By the end of the National Ignition Campaign in 2012, the conditions achieved for ignition were about:
A: 1⁄2 the required conditions
B: 1⁄4 the required conditions
C: 1⁄10 the required conditions
D: 1⁄5 the required conditions
E: 1⁄8 the required conditions
Answer: C

Question: In 2021, what percentage of the energy of the laser did NIF produce?
A: 50%
B: 60%
C: 67%
D: 70%
E: 75%
Answer: D
@
A substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through coulomb forces.

When a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbors due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface-area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.

The electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from all the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.
$
5
Question: What must be overcome for fusion to occur between two nuclei?
A: Electrostatic attraction
B: Electrostatic repulsion
C: Nuclear attraction
D: Quantum tunneling
E: Gravitational forces
Answer: B

Question: Which force can nuclei tunnel through due to the quantum effect?
A: Nuclear force
B: Electrostatic attraction
C: Electrostatic repulsion
D: Inverse-square force
E: Gravitational force
Answer: C

Question: What happens when a nucleon is added to a small nucleus?
A: It is repelled by all other nucleons.
B: It attracts primarily to nucleons far from it.
C: It is attracted to all the other nucleons, mainly its immediate neighbors.
D: It becomes isolated from other nucleons.
E: It repels primarily from its immediate neighbors.
Answer: C

Question: As the size of the nucleus grows, the binding energy per nucleon due to the nuclear force:
A: Decreases without limit
B: Increases and then decreases
C: Stays constant
D: Generally increases but approaches a limiting value
E: Is not affected by the size of the nucleus
Answer: D

Question: What is the nature of the electrostatic force between protons in a nucleus?
A: Inverse-linear force
B: Direct-square force
C: Inverse-square force
D: Direct-linear force
E: Cubic force
Answer: C
@
Coulomb's inverse-square law, or simply Coulomb's law, is an experimental law[1] of physics that calculates the amount of force between two electrically charged particles at rest. This electric force is conventionally called electrostatic force or Coulomb force.[2] Although the law was known earlier, it was first published in 1785 by French physicist Charles-Augustin de Coulomb, hence the name. Coulomb's law was essential to the development of the theory of electromagnetism and maybe even its starting point,[1] as it allowed meaningful discussions of the amount of electric charge in a particle.[3]

The law states that the magnitude, or absolute value, of the attractive or repulsive electrostatic force between two point charges is directly proportional to the product of the magnitudes of their charges and inversely proportional to the squared distance between them.[4] Coulomb discovered that bodies with like electrical charges repel:

It follows therefore from these three tests, that the repulsive force that the two balls – [that were] electrified with the same kind of electricity – exert on each other, follows the inverse proportion of the square of the distance.[5]
$
5
Question: Who first published Coulomb's law?
A: Albert Einstein
B: Isaac Newton
C: James Clerk Maxwell
D: Nikola Tesla
E: Charles-Augustin de Coulomb
Answer: E

Question: Coulomb's law was crucial in the development of which theory?
A: Quantum mechanics
B: Theory of relativity
C: Thermodynamics
D: Electromagnetism
E: Gravitation
Answer: D

Question: According to Coulomb's law, how is the electrostatic force between two point charges related to the distance between them?
A: Directly proportional to the distance
B: Inversely proportional to the distance
C: Inversely proportional to the square of the distance
D: Directly proportional to the square of the distance
E: Not dependent on the distance
Answer: C

Question: Coulomb's law allows for meaningful discussions about what?
A: The speed of light
B: The gravitational pull of an object
C: The amount of electric charge in a particle
D: The wave-particle duality
E: The relativity of time
Answer: C

Question: Based on the law, how do bodies with like electrical charges interact?
A: They attract each other.
B: They do not influence each other.
C: They repel each other.
D: They neutralize each other.
E: They oscillate between attraction and repulsion.
Answer: C
@
Some other confinement principles have been investigated.

Antimatter-initialized fusion uses small amounts of antimatter to trigger a tiny fusion explosion. This has been studied primarily in the context of making nuclear pulse propulsion, and pure fusion bombs feasible. This is not near becoming a practical power source, due to the cost of manufacturing antimatter alone.
Pyroelectric fusion was reported in April 2005 by a team at UCLA. The scientists used a pyroelectric crystal heated from −34 to 7 °C (−29 to 45 °F), combined with a tungsten needle to produce an electric field of about 25 gigavolts per meter to ionize and accelerate deuterium nuclei into an erbium deuteride target. At the estimated energy levels,[27] the D–D fusion reaction may occur, producing helium-3 and a 2.45 MeV neutron. Although it makes a useful neutron generator, the apparatus is not intended for power generation since it requires far more energy than it produces.[28][29][30][31] D–T fusion reactions have been observed with a tritiated erbium target.[32]
Nuclear fusion–fission hybrid (hybrid nuclear power) is a proposed means of generating power by use of a combination of nuclear fusion and fission processes. The concept dates to the 1950s, and was briefly advocated by Hans Bethe during the 1970s, but largely remained unexplored until a revival of interest in 2009, due to the delays in the realization of pure fusion.[33]
Project PACER, carried out at Los Alamos National Laboratory (LANL) in the mid-1970s, explored the possibility of a fusion power system that would involve exploding small hydrogen bombs (fusion bombs) inside an underground cavity. As an energy source, the system is the only fusion power system that could be demonstrated to work using existing technology. However it would also require a large, continuous supply of nuclear bombs, making the economics of such a system rather questionable.
Bubble fusion also called sonofusion was a proposed mechanism for achieving fusion via sonic cavitation which rose to prominence in the early 2000s. Subsequent attempts at replication failed and the principal investigator, Rusi Taleyarkhan, was judged guilty of research misconduct in 2008.[34]
$
5
Question: What does antimatter-initialized fusion primarily study for?
A: Creating fusion reactors
B: Nuclear pulse propulsion and pure fusion bombs
C: Renewable energy sources
D: Bubble fusion
E: Powering large cities
Answer: B

Question: In pyroelectric fusion, what is the outcome of the D–D fusion reaction?
A: Hydrogen and electrons
B: Helium-4 and protons
C: Helium-3 and a 2.45 MeV neutron
D: Deuterium and tritium
E: Positrons and gamma rays
Answer: C

Question: Who briefly advocated for the nuclear fusion–fission hybrid during the 1970s?
A: Rusi Taleyarkhan
B: Hans Bethe
C: Albert Einstein
D: Richard Feynman
E: Marie Curie
Answer: B

Question: Project PACER explored the fusion power system by:
A: Using sonic cavitation
B: Using a pyroelectric crystal
C: Exploding small hydrogen bombs inside an underground cavity
D: Using a combination of nuclear fusion and fission processes
E: Ionizing and accelerating deuterium nuclei
Answer: C

Question: Why was the principal investigator, Rusi Taleyarkhan, judged guilty in 2008?
A: For developing an unstable fusion reactor
B: For research misconduct related to bubble fusion
C: For advocating the nuclear fusion-fission hybrid
D: For overestimating the results of his experiments
E: For creating a fusion bomb
Answer: B
@
Inertial electrostatic confinement, or IEC, is a class of fusion power devices that use electric fields to confine the plasma rather than the more common approach using magnetic fields found in magnetic confinement fusion (MCF) designs. Most IEC devices directly accelerate their fuel to fusion conditions, thereby avoiding energy losses seen during the longer heating stages of MCF devices. In theory, this makes them more suitable for using alternative aneutronic fusion fuels, which offer a number of major practical benefits and makes IEC devices one of the more widely studied approaches to fusion.

As the negatively charged electrons and positively charged ions in the plasma move in different directions in an electric field, the field has to be arranged in some fashion so that the two particles remain close together. Most IEC designs achieve this by pulling the electrons or ions across a potential well, beyond which the potential drops and the particles continue to move due to their inertia. Fusion occurs in this lower-potential area when ions moving in different directions collide. Because the motion provided by the field creates the energy levels needed for fusion, not random collisions with the rest of the fuel, the bulk of the plasma does not have to be hot and the systems as a whole work at much lower temperatures and energy levels than MCF devices.

One of the simpler IEC devices is the fusor, which consists of two concentric metal wire spherical grids. When the grids are charged to a high voltage, the fuel gas ionizes. The field between the two then accelerates the fuel inward, and when it passes the inner grid the field drops and the ions continue inward toward the center. If they impact with another ion they may undergo fusion. If they do not, they travel out of the reaction area into the charged area again, where they are re-accelerated inward. Overall the physical process is similar to the colliding beam fusion, although beam devices are linear instead of spherical. Other IEC designs, like the polywell, differ largely in the arrangement of the fields used to create the potential well.

A number of detailed theoretical studies have pointed out that the IEC approach is subject to a number of energy loss mechanisms that are not present if the fuel is evenly heated, or "Maxwellian". These loss mechanisms appear to be greater than the rate of fusion in such devices, meaning they can never reach fusion breakeven and thus be used for power production. These mechanisms are more powerful when the atomic mass of the fuel increases, which suggests IEC also does not have any advantage with aneutronic fuels. Whether these critiques apply to specific IEC devices remains highly contentious.
$
5
Question: What primarily differentiates Inertial electrostatic confinement (IEC) from magnetic confinement fusion (MCF) designs?
A: The type of fuel used
B: The way they create fusion reactions
C: The use of electric fields for confinement in IEC versus magnetic fields in MCF
D: The temperature of the plasma
E: The type of particles used for reactions
Answer: C

Question: Why are IEC devices considered more suitable for using alternative aneutronic fusion fuels?
A: They use a faster acceleration method, avoiding longer heating stages of MCF devices
B: They operate at higher temperatures
C: They use different types of metal grids for reactions
D: They are more cost-effective to produce
E: They use magnetic fields for confinement
Answer: A

Question: What is the primary function of the electric field in the IEC design?
A: To keep negatively charged electrons and positively charged ions close together
B: To ionize the fuel gas
C: To prevent fusion from occurring
D: To separate the electrons and ions
E: To increase the overall temperature of the plasma
Answer: A

Question: In a fusor, what happens to the ions if they do not undergo fusion at the center?
A: They are neutralized and expelled from the device
B: They travel out of the reaction area and are re-accelerated inward
C: They settle at the bottom of the fusor
D: They undergo a chemical reaction
E: They combine with electrons to form neutral particles
Answer: B

Question: Why do some theoretical studies critique the IEC approach?
A: The approach is too costly
B: It has a number of energy loss mechanisms not present if the fuel is evenly heated
C: The use of electric fields is considered unsafe
D: It is too similar to magnetic confinement fusion (MCF) designs
E: The fusion reactions are not sustainable
Answer: B
@
In his work with vacuum tubes, Philo Farnsworth observed that electric charge would accumulate in regions of the tube. Today, this effect is known as the multipactor effect.[9] Farnsworth reasoned that if ions were concentrated high enough they could collide, and fuse. In 1962, he filed a patent on a design using a positive inner cage to concentrate plasma, in order to achieve nuclear fusion.[10] During this time, Robert L. Hirsch joined the Farnsworth Television labs and began work on what became the fusor. Hirsch patented the design in 1966[11] and published the design in 1967.[12] The Hirsch machine was a 17.8 cm diameter machine with 150 kV voltage drop across it and used ion beams to help inject material.

Simultaneously, a key plasma physics text was published by Lyman Spitzer at Princeton in 1963.[13] Spitzer took the ideal gas laws and adapted them to an ionized plasma, developing many of the fundamental equations used to model a plasma. Meanwhile, magnetic mirror theory and direct energy conversion were developed by Richard F. Post's group at LLNL.[14][15] A magnetic mirror or magnetic bottle is similar to a biconic cusp except that the poles are reversed.
$
5
Question: What effect did Philo Farnsworth observe in his work with vacuum tubes?
A: The multipactor effect
B: The fusion effect
C: The ionization effect
D: The plasma conversion effect
E: The magnetic mirror effect
Answer: A

Question: What was Philo Farnsworth's idea regarding the concentration of ions?
A: They could amplify signals
B: They could be used for television broadcasting
C: They could be dispersed evenly in a vacuum tube
D: They could collide and fuse
E: They could be magnetically controlled
Answer: D

Question: Who joined the Farnsworth Television labs and began work on the fusor?
A: Richard F. Post
B: Lyman Spitzer
C: Robert L. Hirsch
D: Philo Farnsworth
E: None of the above
Answer: C

Question: What significant contribution did Lyman Spitzer make to plasma physics in 1963?
A: He developed the fusor
B: He patented the design of a plasma concentrator
C: He adapted the ideal gas laws to an ionized plasma
D: He introduced the multipactor effect
E: He developed magnetic mirrors for plasma confinement
Answer: C

Question: How does a magnetic mirror differ from a biconic cusp?
A: It uses a higher voltage
B: It relies solely on ion beams
C: The poles are reversed
D: It does not use magnetic fields
E: It is primarily used for television tubes
Answer: C
@
Magnetic confinement fusion (MCF) is an approach to generate thermonuclear fusion power that uses magnetic fields to confine fusion fuel in the form of a plasma. Magnetic confinement is one of two major branches of controlled fusion research, along with inertial confinement fusion.

Fusion reactions for reactors usually combine light atomic nuclei of deuterium and tritium to form an alpha particle (Helium-4 nucleus) and a neutron, where the energy is released in the form of the kinetic energy of the reaction products. In order to overcome the electrostatic repulsion between the nuclei, the fuel must have a temperature of hundreds of millions of degrees, at which the fuel is fully ionized and becomes a plasma. In addition, the plasma must be at a sufficient density, and the energy must remain in the reacting region for a sufficient time, as specified by the Lawson criterion (triple product). The high temperature of a fusion plasma precludes the use of material vessels for direct containment. Magnetic confinement fusion attempts to use the physics of charged particle motion to contain the plasma particles by applying strong magnetic fields.

Tokamaks and stellarators are the two leading MCF device candidates as of today. Investigation of using various magnetic configurations to confine fusion plasma began in the 1950s. Early simple mirror and toroidal machines showed disappointing results of low confinement. After the declassification of fusion research by the United States, United Kingdom and Soviet Union in 1958, a breakthrough on toroidal devices was reported by the Kurchatov Institute in 1968, where its tokamak demonstrated a temperature of 1 kilo-electronvolts (around 11.6 million degree Kelvin) and some milliseconds of confinement time, and was confirmed by a visiting team from the Culham Laboratory using the Thomson scattering technique.[1][2] Since then, tokamaks became the dominant line of research globally with large tokamaks such as JET, TFTR and JT-60 being constructed and operated. The ITER tokamak experiment under construction, which aims to demonstrate scientific breakeven, will be the world's largest MCF device. While early stellarators of low confinement in the 1950s were overshadowed by the initial success of tokamaks, interests in stellarators re-emerged attributing to their inherent capability for steady-state and disruption-free operation distinct from tokamaks. The world's largest stellarator experiment, Wendelstein 7-X, began operation in 2015.

The current record of fusion power generated by MCF devices is held by JET. In 1997, JET's set the record of 16 megawatts of transient fusion power with a gain factor of Q = 0.62 and 4 megawatts steady state fusion power with Q = 0.18 for 4 seconds.[3] In 2021, JET sustained Q = 0.33 for 5 seconds and produced 59 megajoules of energy, beating the record 21.7 megajoules released in 1997 over around 4 seconds.[4]

One of the challenges of MCF research is the development and extrapolation of plasma scenarios to power plant conditions, where good fusion performance and energy confinement must be maintained. Potential solutions to other problems such as divertor power exhaust, mitigation of transients (disruptions, runaway electrons, edge-localized modes), handling of neutron flux, tritium breeding and the physics of burning plasmas are being actively studied. Development of new technologies in plasma diagnostics, real-time control, plasma-facing materials, high-power microwave sources, vacuum engineering, cryogenics and superconducting magnets are essential in MCF research.
$
5
Question: What is the primary approach of Magnetic confinement fusion (MCF)?
A: To use gravitational forces to confine plasma
B: To use electric fields to contain fusion fuel
C: To use magnetic fields to confine fusion fuel in the form of plasma
D: To use inertial forces for plasma containment
E: To combine fusion fuel with external heat sources
Answer: C

Question: Which particles are primarily fused in the fusion reactions for reactors?
A: Hydrogen and Helium
B: Oxygen and Carbon
C: Deuterium and tritium
D: Argon and Neon
E: Helium and Neon
Answer: C

Question: What is the major factor that prevents the use of material vessels for direct containment of a fusion plasma?
A: The corrosiveness of the plasma
B: The electromagnetic interference of the plasma
C: The high temperature of a fusion plasma
D: The instability of the plasma's magnetic field
E: The excessive brightness of the plasma
Answer: C

Question: Which experiment aims to demonstrate scientific breakeven and will be the world's largest MCF device?
A: JET
B: Wendelstein 7-X
C: ITER tokamak
D: TFTR
E: JT-60
Answer: C

Question: As of the mentioned dates, which MCF device holds the record for fusion power generated?
A: ITER tokamak
B: Wendelstein 7-X
C: JET
D: TFTR
E: JT-60
Answer: C
@
Inertial confinement fusion (ICF) is a fusion energy process that initiates nuclear fusion reactions by compressing and heating targets filled with fuel. The targets are small pellets, typically containing deuterium (2H) and tritium (3H).

Energy is deposited in the target's outer layer, which explodes outward. This produces a reaction force in the form of shock waves that travel through the target. The waves compress and heat it. Sufficiently powerful shock waves generate fusion.

ICF is one of two major branches of fusion energy research; the other is magnetic confinement fusion (MCF). When first proposed in the early 1970s, ICF appeared to be a practical approach to power production and the field flourished. Experiments demonstrated that the efficiency of these devices was much lower than expected. Throughout the 1980s and '90s, experiments were conducted in order to understand the interaction of high-intensity laser light and plasma. These led to the design of much larger machines that achieved ignition-generating energies.

The largest operational ICF experiment is the National Ignition Facility (NIF) in the US. In 2022, the NIF produced fusion, delivering 2.05 megajoules (MJ) of energy to the target which produced 3.15 MJ, the first time that an ICF device produced more energy than was delivered to the target.[1][2]

Inertial confinement fusion using lasers rapidly progressed in the late 1970s and early 1980s from being able to deliver only a few joules of laser energy (per pulse) to being able to deliver tens of kilojoules to a target. At this point, very large scientific devices were needed for experimentation. Here, a view of the 10 beam LLNL Nova laser, shown shortly after the laser's completion in 1984. Around the time of the construction of its predecessor, the Shiva laser, laser fusion had entered the realm of "big science".
$
10
Question: What process is initiated in inertial confinement fusion (ICF)?
A: Nuclear fission reactions by cooling targets
B: Nuclear fusion reactions by compressing and heating targets
C: Nuclear decay reactions by electromagnetic stimulation
D: Nuclear absorption reactions by radiative cooling
E: Nuclear fusion reactions by magnetic confinement
Answer: B

Question: What are the typical targets used in ICF filled with?
A: Hydrogen and Helium
B: Argon and Neon
C: Deuterium and tritium
D: Oxygen and Nitrogen
E: Carbon and Boron
Answer: C

Question: How is the reaction force generated in ICF after energy is deposited in the target's outer layer?
A: By absorbing energy inward
B: By producing shock waves traveling through the target
C: By creating magnetic fields that attract fusion material
D: By cooling and solidifying the outer layer
E: By releasing energy as electromagnetic radiation
Answer: B

Question: What is the other major branch of fusion energy research apart from ICF?
A: Gravitational confinement fusion
B: Electrostatic confinement fusion
C: Inertial confinement decay
D: Magnetic confinement fusion (MCF)
E: Radiative confinement fusion
Answer: D

Question: When was ICF first proposed?
A: Late 1980s
B: Early 2000s
C: Mid 1990s
D: Early 1970s
E: Late 1950s
Answer: D

Question: Which is the largest operational ICF experiment as of the mentioned date?
A: Shiva laser
B: Nova laser
C: ITER facility
D: National Ignition Facility (NIF)
E: Stellarator experiment
Answer: D

Question: In 2022, how much energy was produced by the NIF compared to what was delivered to the target?
A: Delivered 2.05 MJ, produced 1.5 MJ
B: Delivered 3.15 MJ, produced 2.05 MJ
C: Delivered 2.05 MJ, produced 3.15 MJ
D: Delivered 1 MJ, produced 2 MJ
E: Delivered 3 MJ, produced 1.5 MJ
Answer: C

Question: During which time period did inertial confinement fusion using lasers show rapid progress?
A: Mid 1990s to early 2000s
B: Late 1970s to early 1980s
C: Early 1970s to mid 1970s
D: Mid 1980s to early 1990s
E: Late 1980s to mid 1990s
Answer: B

Question: Which laser facility was completed in 1984 and signified the entry of laser fusion into "big science"?
A: Shiva laser
B: ITER laser
C: Stellarator laser
D: National Ignition Laser
E: LLNL Nova laser
Answer: E

Question: By the time of the Shiva laser's construction, into what realm had laser fusion entered?
A: Small-scale research
B: Practical energy production
C: Big science
D: Commercial viability
E: Military applications
Answer: C
@
Subject:
The basic principle of the Teller–Ulam configuration is the idea that different parts of a thermonuclear weapon can be chained together in "stages", with the detonation of each stage providing the energy to ignite the next stage. At a bare minimum, this implies a primary section that consists of an implosion-type fission bomb (a "trigger"), and a secondary section that consists of fusion fuel. The energy released by the primary compresses the secondary through the process of radiation implosion, at which point it is heated and undergoes nuclear fusion. This process could be continued, with energy from the secondary igniting a third fusion stage; the Soviet Union's AN602 "Tsar Bomba" is thought to have been a three-stage fission-fusion-fusion device. Theoretically by continuing this process thermonuclear weapons with arbitrarily high yield could be constructed.[citation needed] This contrasts with fission weapons, which are limited in yield because only so much fission fuel can be amassed in one place before the danger of its accidentally becoming supercritical becomes too great.

Surrounding the other components is a hohlraum or radiation case, a container that traps the first stage or primary's energy inside temporarily. The outside of this radiation case, which is also normally the outside casing of the bomb, is the only direct visual evidence publicly available of any thermonuclear bomb component's configuration. Numerous photographs of various thermonuclear bomb exteriors have been declassified.[10]

The primary is thought to be a standard implosion method fission bomb, though likely with a core boosted by small amounts of fusion fuel (usually 50/50% deuterium/tritium gas) for extra efficiency; the fusion fuel releases excess neutrons when heated and compressed, inducing additional fission. When fired, the 239
Pu
 or 235
U
 core would be compressed to a smaller sphere by special layers of conventional high explosives arranged around it in an explosive lens pattern, initiating the nuclear chain reaction that powers the conventional "atomic bomb".

The secondary is usually shown as a column of fusion fuel and other components wrapped in many layers. Around the column is first a "pusher-tamper", a heavy layer of uranium-238 (238
U
) or lead that helps compress the fusion fuel (and, in the case of uranium, may eventually undergo fission itself). Inside this is the fusion fuel itself, usually a form of lithium deuteride, which is used because it is easier to weaponize than liquefied tritium/deuterium gas. This dry fuel, when bombarded by neutrons, produces tritium, a heavy isotope of hydrogen that can undergo nuclear fusion, along with the deuterium present in the mixture. (See the article on nuclear fusion for a more detailed technical discussion of fusion reactions.) Inside the layer of fuel is the "spark plug", a hollow column of fissile material (239
Pu
 or 235
U
) often boosted by deuterium gas. The spark plug, when compressed, can itself undergo nuclear fission (because of the shape, it is not a critical mass without compression). The tertiary, if one is present, would be set below the secondary and probably be made of the same materials.[11][12]

Separating the secondary from the primary is the interstage. The fissioning primary produces four types of energy: 1) expanding hot gases from high explosive charges that implode the primary; 2) superheated plasma that was originally the bomb's fissile material and its tamper; 3) the electromagnetic radiation; and 4) the neutrons from the primary's nuclear detonation. The interstage is responsible for accurately modulating the transfer of energy from the primary to the secondary. It must direct the hot gases, plasma, electromagnetic radiation and neutrons toward the right place at the right time. Less than optimal interstage designs have resulted in the secondary failing to work entirely on multiple shots, known as a "fissile fizzle". The Castle Koon shot of Operation Castle is a good example; a small flaw allowed the neutron flux from the primary to prematurely begin heating the secondary, weakening the compression enough to prevent any fusion.
Question: What is the fundamental concept behind the Teller–Ulam configuration?
A: Storing energy for extended periods.
B: Utilizing only fusion reactions.
C: Chaining parts of a weapon in "stages" where one ignites the next.
D: Increasing the number of fissile materials.
E: Creating a multi-layered bomb without a primary.
Answer: C

Question: What initiates the energy for the secondary in a thermonuclear weapon?
A: The detonation of the tertiary.
B: The explosion of the fusion fuel.
C: The detonation of the primary.
D: External electromagnetic radiation.
E: Compression of the spark plug alone.
Answer: C

Question: The Soviet Union's AN602 "Tsar Bomba" is
$
10
Question: What is the fundamental concept behind the Teller–Ulam configuration?
A: Storing energy for extended periods.
B: Utilizing only fusion reactions.
C: Chaining parts of a weapon in "stages" where one ignites the next.
D: Increasing the number of fissile materials.
E: Creating a multi-layered bomb without a primary.
Answer: C

Question: What initiates the energy for the secondary in a thermonuclear weapon?
A: The detonation of the tertiary.
B: The explosion of the fusion fuel.
C: The detonation of the primary.
D: External electromagnetic radiation.
E: Compression of the spark plug alone.
Answer: C

Question: The Soviet Union's AN602 "Tsar Bomba" is believed to have been which type of device?
A: Single-stage fission.
B: Two-stage fusion.
C: Three-stage fission-fusion-fusion.
D: Pure fusion.
E: Fission-only.
Answer: C

Question: What is the role of the hohlraum or radiation case?
A: To undergo nuclear fusion.
B: To trap the primary's energy temporarily.
C: To initiate the detonation of the primary.
D: To serve as the main fusion fuel.
E: To provide an explosive lens pattern.
Answer: B

Question: What is the primary thought to be?
A: A simple fusion bomb.
B: An implosion method fission bomb.
C: A pure lithium deuteride device.
D: A radiation case with fusion fuel.
E: A tertiary fission device.
Answer: B

Question: What is the "spark plug" in a thermonuclear weapon?
A: The explosive that initiates the detonation.
B: The outer casing of the bomb.
C: The fusion fuel.
D: A hollow column of fissile material.
E: The interstage separating primary and secondary.
Answer: D

Question: Why is lithium deuteride typically used in the secondary?
A: It is the primary fission material.
B: It is the main component of the radiation case.
C: It produces a brighter explosion.
D: It is easier to weaponize than liquefied tritium/deuterium gas.
E: It acts as the spark plug.
Answer: D

Question: What are the four types of energy produced by the fissioning primary?
A: Plasma, neutrons, X-rays, and gamma rays.
B: Hot gases, plasma, electromagnetic radiation, and neutrons.
C: Fusion fuel, fissile material, hot gases, and plasma.
D: Electromagnetic radiation, fissile material, X-rays, and plasma.
E: Neutrons, gamma rays, X-rays, and hot gases.
Answer: B

Question: What role does the interstage play in the Teller–Ulam configuration?
A: It stores additional fusion fuel.
B: It modulates the transfer of energy from the primary to the secondary.
C: It acts as the primary explosive.
D: It undergoes the first nuclear detonation.
E: It captures the electromagnetic radiation.
Answer: B

Question: What term is used for the secondary failing to work due to suboptimal interstage designs?
A: Fusion failure.
B: Incomplete detonation.
C: Primary fizzle.
D: Fissile fizzle.
E: Secondary dampening.
Answer: D
@
In nuclear physics, a nuclear chain reaction occurs when one single nuclear reaction causes an average of one or more subsequent nuclear reactions, thus leading to the possibility of a self-propagating series of these reactions. The specific nuclear reaction may be the fission of heavy isotopes (e.g., uranium-235, 235U). A nuclear chain reaction releases several million times more energy per reaction than any chemical reaction.

Chemical chain reactions were first proposed by German chemist Max Bodenstein in 1913, and were reasonably well understood before nuclear chain reactions were proposed.[1] It was understood that chemical chain reactions were responsible for exponentially increasing rates in reactions, such as produced in chemical explosions.

The concept of a nuclear chain reaction was reportedly first hypothesized by Hungarian scientist Leó Szilárd on September 12, 1933.[2] Szilárd that morning had been reading in a London paper of an experiment in which protons from an accelerator had been used to split lithium-7 into alpha particles, and the fact that much greater amounts of energy were produced by the reaction than the proton supplied. Ernest Rutherford commented in the article that inefficiencies in the process precluded use of it for power generation. However, the neutron had been discovered by James Chadwick in 1932, shortly before, as the product of a nuclear reaction. Szilárd, who had been trained as an engineer and physicist, put the two nuclear experimental results together in his mind and realized that if a nuclear reaction produced neutrons, which then caused further similar nuclear reactions, the process might be a self-perpetuating nuclear chain-reaction, spontaneously producing new isotopes and power without the need for protons or an accelerator. Szilárd, however, did not propose fission as the mechanism for his chain reaction, since the fission reaction was not yet discovered, or even suspected. Instead, Szilárd proposed using mixtures of lighter known isotopes which produced neutrons in copious amounts. He filed a patent for his idea of a simple nuclear reactor the following year.[3]

In 1936, Szilárd attempted to create a chain reaction using beryllium and indium, but was unsuccessful. Nuclear fission was discovered by Otto Hahn and Fritz Strassmann in December 1938[4] and explained theoretically in January 1939 by Lise Meitner and her nephew Otto Robert Frisch.[5] In their second publication on nuclear fission in February of 1939, Hahn and Strassmann used the term Uranspaltung (uranium fission) for the first time, and predicted the existence and liberation of additional neutrons during the fission process, opening up the possibility of a nuclear chain reaction.[6]

A few months later, Frédéric Joliot-Curie, H. Von Halban and L. Kowarski in Paris[7] searched for, and discovered, neutron multiplication in uranium, proving that a nuclear chain reaction by this mechanism was indeed possible.

On May 4, 1939, Joliot-Curie, Halban, and Kowarski filed three patents. The first two described power production from a nuclear chain reaction, the last one called Perfectionnement aux charges explosives was the first patent for the atomic bomb and is filed as patent No. 445686 by the Caisse nationale de Recherche Scientifique.[8]

In parallel, Szilárd and Enrico Fermi in New York made the same analysis.[9] This discovery prompted the letter from Szilárd and signed by Albert Einstein to President Franklin D. Roosevelt, warning of the possibility that Nazi Germany might be attempting to build an atomic bomb.[10]

On December 2, 1942, a team led by Fermi (and including Szilárd) produced the first artificial self-sustaining nuclear chain reaction with the Chicago Pile-1 (CP-1) experimental reactor in a racquets court below the bleachers of Stagg Field at the University of Chicago. Fermi's experiments at the University of Chicago were part of Arthur H. Compton's Metallurgical Laboratory of the Manhattan Project; the lab was later renamed Argonne National Laboratory, and tasked with conducting research in harnessing fission for nuclear energy.[11]

In 1956, Paul Kuroda of the University of Arkansas postulated that a natural fission reactor may have once existed. Since nuclear chain reactions may only require natural materials (such as water and uranium, if the uranium has sufficient amounts of 235U), it was possible to have these chain reactions occur in the distant past when uranium-235 concentrations were higher than today, and where there was the right combination of materials within the Earth's crust. 235
U made up a larger share of uranium on earth in the geological past due to the different half life of the isotopes 235
U and 238
U, the former decaying almost an order of magnitude faster than the latter. Kuroda's prediction was verified with the discovery of evidence of natural self-sustaining nuclear chain reactions in the past at Oklo in Gabon in September 1972.[12] To sustain a nuclear fission chain reaction at present isotope ratios in natural uranium on earth would require the presence of a neutron moderator like heavy water or high purity carbon (e.g. graphite) in the absence of neutron poisons, which is even more unlikely to arise by natural geological processes than the conditions at Oklo some two billion years ago.
$
10
Question: What event takes place when a single nuclear reaction leads to an average of one or more subsequent nuclear reactions?
A: Nuclear decay.
B: Fusion reaction.
C: Atomic explosion.
D: Nuclear chain reaction.
E: Chemical reaction.
Answer: D

Question: Who first proposed the concept of chemical chain reactions?
A: Otto Hahn.
B: Lise Meitner.
C: Leó Szilárd.
D: Max Bodenstein.
E: James Chadwick.
Answer: D

Question: On which date is Leó Szilárd believed to have first hypothesized the concept of a nuclear chain reaction?
A: December 2, 1942.
B: February 1939.
C: May 4, 1939.
D: September 12, 1933.
E: January 1939.
Answer: D

Question: What significant discovery did James Chadwick make in 1932?
A: Nuclear fission.
B: Chemical chain reaction.
C: The proton.
D: The neutron.
E: Lithium-7.
Answer: D

Question: Who discovered nuclear fission in December 1938?
A: Albert Einstein and Franklin D. Roosevelt.
B: Paul Kuroda and Leó Szilárd.
C: Otto Hahn and Fritz Strassmann.
D: Enrico Fermi and Arthur H. Compton.
E: Lise Meitner and Otto Robert Frisch.
Answer: C

Question: In which city was the first artificial self-sustaining nuclear chain reaction achieved?
A: New York.
B: Paris.
C: Berlin.
D: Chicago.
E: Arkansas.
Answer: D

Question: What was the name of the experimental reactor where the first artificial self-sustaining nuclear chain reaction took place?
A: Manhattan Project.
B: Oklo Reactor.
C: Chicago Pile-1.
D: Stagg Field Reactor.
E: Argonne National Laboratory.
Answer: C

Question: Who postulated in 1956 that a natural fission reactor might have once existed?
A: Ernest Rutherford.
B: Frédéric Joliot-Curie.
C: Paul Kuroda.
D: Enrico Fermi.
E: H. Von Halban.
Answer: C

Question: Where was the evidence of natural self-sustaining nuclear chain reactions from the past discovered?
A: Paris.
B: New York.
C: Chicago.
D: Oklo in Gabon.
E: Arkansas.
Answer: D

Question: What is required to sustain a nuclear fission chain reaction in today's natural uranium on earth?
A: Presence of 238U only.
B: The absence of lithium-7.
C: Presence of a neutron moderator like heavy water or high purity carbon.
D: High amounts of protons.
E: An accelerator.
Answer: C
@
In physics, mass–energy equivalence is the relationship between mass and energy in a system's rest frame, where the two quantities differ only by a multiplicative constant and the units of measurement.[1][2] The principle is described by the physicist Albert Einstein's formula: 
�
=
�
�
2
E = mc^2.[3] In a reference frame where the system is moving, its relativistic energy and relativistic mass (instead of rest mass) obey the same formula.

The formula defines the energy E of a particle in its rest frame as the product of mass (m) with the speed of light squared (c2). Because the speed of light is a large number in everyday units (approximately 300000 km/s or 186000 mi/s), the formula implies that a small amount of "rest mass", measured when the system is at rest, corresponds to an enormous amount of energy, which is independent of the composition of the matter.

Rest mass, also called invariant mass, is a fundamental physical property that is independent of momentum, even at extreme speeds approaching the speed of light. Its value is the same in all inertial frames of reference. Massless particles such as photons have zero invariant mass, but massless free particles have both momentum and energy.

The equivalence principle implies that when energy is lost in chemical reactions, nuclear reactions, and other energy transformations, the system will also lose a corresponding amount of mass. The energy, and mass, can be released to the environment as radiant energy, such as light, or as thermal energy. The principle is fundamental to many fields of physics, including nuclear and particle physics.

Mass–energy equivalence arose from special relativity as a paradox described by the French polymath Henri Poincaré (1854–1912).[4] Einstein was the first to propose the equivalence of mass and energy as a general principle and a consequence of the symmetries of space and time. The principle first appeared in "Does the inertia of a body depend upon its energy-content?", one of his annus mirabilis papers, published on 21 November 1905.[5] The formula and its relationship to momentum, as described by the energy–momentum relation, were later developed by other physicists.

An object moves at different speeds in different frames of reference, depending on the motion of the observer. This implies the kinetic energy, in both Newtonian mechanics and relativity, is 'frame dependent', so that the amount of relativistic energy that an object is measured to have depends on the observer. The relativistic mass of an object is given by the relativistic energy divided by c2.[9] Because the relativistic mass is exactly proportional to the relativistic energy, relativistic mass and relativistic energy are nearly synonymous; the only difference between them is the units. The rest mass or invariant mass of an object is defined as the mass an object has in its rest frame, when it is not moving with respect to the observer. Physicists typically use the term mass, though experiments have shown an object's gravitational mass depends on its total energy and not just its rest mass.[citation needed] The rest mass is the same for all inertial frames, as it is independent of the motion of the observer, it is the smallest possible value of the relativistic mass of the object. Because of the attraction between components of a system, which results in potential energy, the rest mass is almost never additive; in general, the mass of an object is not the sum of the masses of its parts.[8] The rest mass of an object is the total energy of all the parts, including kinetic energy, as observed from the center of momentum frame, and potential energy. The masses add up only if the constituents are at rest (as observed from the center of momentum frame) and do not attract or repel, so that they do not have any extra kinetic or potential energy.[note 1] Massless particles are particles with no rest mass, and therefore have no intrinsic energy; their energy is due only to their momentum.
$
10
Question: What principle describes the relationship between mass and energy in a system's rest frame?
A: Energy-momentum relation.
B: Relativity principle.
C: Kinetic energy principle.
D: Mass–energy equivalence.
E: Inertia principle.
Answer: D

Question: Whose formula describes the mass–energy equivalence principle?
A: Henri Poincaré.
B: Isaac Newton.
C: Albert Einstein.
D: James Clerk Maxwell.
E: Richard Feynman.
Answer: C

Question: In the formula E = mc^2, what does 'c' represent?
A: Mass.
B: Energy.
C: Atomic number.
D: Speed of light.
E: Temperature.
Answer: D

Question: What kind of mass is described as being the same in all inertial frames of reference?
A: Relativistic mass.
B: Photonic mass.
C: Proportional mass.
D: Newtonian mass.
E: Rest (or invariant) mass.
Answer: E

Question: Particles such as photons have what kind of invariant mass?
A: Positive.
B: Negative.
C: Zero.
D: Infinite.
E: Oscillating.
Answer: C

Question: Where can energy be released to when it is lost in reactions, based on the equivalence principle?
A: Only as kinetic energy.
B: Only as potential energy.
C: As radiant or thermal energy.
D: As gravitational energy.
E: As magnetic energy.
Answer: C

Question: Who described the paradox from which mass–energy equivalence arose?
A: Albert Einstein.
B: Richard Feynman.
C: Henri Poincaré.
D: James Clerk Maxwell.
E: Niels Bohr.
Answer: C

Question: When did Einstein's principle of mass and energy equivalence first appear in publication?
A: 1854.
B: 1905.
C: 1912.
D: 1921.
E: 1930.
Answer: B

Question: What is the relativistic mass of an object given by?
A: Kinetic energy.
B: Total energy.
C: Potential energy.
D: Relativistic energy divided by c^2.
E: Invariant energy multiplied by c^2.
Answer: D

Question: The rest mass of an object includes which types of energy from its components?
A: Only kinetic energy.
B: Only potential energy.
C: Kinetic and potential energy.
D: Photonic energy.
E: Thermal energy.
Answer: C
@
In Newtonian mechanics, momentum (pl: momenta or momentums; more specifically linear momentum or translational momentum) is the product of the mass and velocity of an object. It is a vector quantity, possessing a magnitude and a direction. If m is an object's mass and v is its velocity (also a vector quantity), then the object's momentum p (from Latin pellere "push, drive") is: 
�
=
�
�
.
\mathbf{p} = m \mathbf{v}.

In the International System of Units (SI), the unit of measurement of momentum is the kilogram metre per second (kg⋅m/s), which is equivalent to the newton-second.

Newton's second law of motion states that the rate of change of a body's momentum is equal to the net force acting on it. Momentum depends on the frame of reference, but in any inertial frame it is a conserved quantity, meaning that if a closed system is not affected by external forces, its total linear momentum does not change. Momentum is also conserved in special relativity (with a modified formula) and, in a modified form, in electrodynamics, quantum mechanics, quantum field theory, and general relativity. It is an expression of one of the fundamental symmetries of space and time: translational symmetry.

Advanced formulations of classical mechanics, Lagrangian and Hamiltonian mechanics, allow one to choose coordinate systems that incorporate symmetries and constraints. In these systems the conserved quantity is generalized momentum, and in general this is different from the kinetic momentum defined above. The concept of generalized momentum is carried over into quantum mechanics, where it becomes an operator on a wave function. The momentum and position operators are related by the Heisenberg uncertainty principle.

In continuous systems such as electromagnetic fields, fluid dynamics and deformable bodies, a momentum density can be defined, and a continuum version of the conservation of momentum leads to equations such as the Navier–Stokes equations for fluids or the Cauchy momentum equation for deformable solids or fluids.
$
10
Question: What is momentum the product of in Newtonian mechanics?
A: Mass and temperature.
B: Mass and volume.
C: Velocity and acceleration.
D: Mass and velocity.
E: Mass and force.
Answer: D

Question: Which of the following best describes momentum?
A: Scalar quantity.
B: A product of force and time.
C: Quantity possessing only magnitude.
D: Vector quantity with magnitude and direction.
E: Integral of force over time.
Answer: D

Question: In the formula 
�
=
�
�
p=mv, what does the symbol 
�
p represent?
A: Position.
B: Pressure.
C: Power.
D: Momentum.
E: Potential.
Answer: D

Question: What is the unit of measurement for momentum in the International System of Units?
A: Newton.
B: Joule.
C: Kilogram metre per second.
D: Metre per second squared.
E: Watt.
Answer: C

Question: According to Newton's second law, the rate of change of momentum is equal to what?
A: The gravitational force.
B: The net force acting on the body.
C: The sum of all forces.
D: The initial force applied.
E: The difference in force over time.
Answer: B

Question: In which frame of reference is momentum conserved?
A: Only in accelerating frames.
B: In any inertial frame.
C: Only in decelerating frames.
D: In gravitational frames.
E: In quantum frames.
Answer: B

Question: Momentum is an expression of which fundamental symmetry?
A: Rotational symmetry.
B: Inverse symmetry.
C: Translational symmetry.
D: Reflective symmetry.
E: Bilateral symmetry.
Answer: C

Question: What does the concept of generalized momentum transform into in quantum mechanics?
A: A scalar.
B: A wave function.
C: An operator on a wave function.
D: A vector.
E: A quantum state.
Answer: C

Question: In the Heisenberg uncertainty principle, which two operators are related?
A: Energy and time.
B: Position and energy.
C: Momentum and force.
D: Position and velocity.
E: Momentum and position.
Answer: E

Question: In continuous systems like fluid dynamics, which equation is a continuum version of the conservation of momentum?
A: Maxwell's equations.
B: Bernoulli's equation.
C: Newton's second law.
D: The Navier–Stokes equations.
E: Heisenberg's equation.
Answer: D
@
Werner Karl Heisenberg (pronounced [ˈvɛʁnɐ kaʁl ˈhaɪzn̩bɛʁk] i; 5 December 1901 – 1 February 1976)[2] was a German theoretical physicist and one of the main pioneers of the theory of quantum mechanics. He published his work in 1925 in a major breakthrough paper. In the subsequent series of papers with Max Born and Pascual Jordan, during the same year, his matrix formulation of quantum mechanics was substantially elaborated. He is known for the uncertainty principle, which he published in 1927. Heisenberg was awarded the 1932 Nobel Prize in Physics "for the creation of quantum mechanics".[3][a]

Heisenberg also made contributions to the theories of the hydrodynamics of turbulent flows, the atomic nucleus, ferromagnetism, cosmic rays, and subatomic particles. He was a principal scientist in the Nazi nuclear weapons program during World War II. He was also instrumental in planning the first West German nuclear reactor at Karlsruhe, together with a research reactor in Munich, in 1957.

Following World War II, he was appointed director of the Kaiser Wilhelm Institute for Physics, which soon thereafter was renamed the Max Planck Institute for Physics. He was director of the institute until it was moved to Munich in 1958. He then became director of the Max Planck Institute for Physics and Astrophysics from 1960 to 1970.

Heisenberg was also president of the German Research Council,[4] chairman of the Commission for Atomic Physics, chairman of the Nuclear Physics Working Group, and president of the Alexander von Humboldt Foundation.[1]

From 1920 to 1923, he studied physics and mathematics at the Ludwig Maximilian University of Munich under Arnold Sommerfeld and Wilhelm Wien and at the Georg-August University of Göttingen with Max Born and James Franck and mathematics with David Hilbert. He received his doctorate in 1923 at Munich under Sommerfeld.

At Göttingen, under Born, he completed his habilitation in 1924 with a Habilitationsschrift (habilitation thesis) on the anomalous Zeeman effect.[13][2][14][15]

In June 1922, Sommerfeld took Heisenberg to Göttingen to attend the Bohr Festival, because Sommerfeld had a sincere interest in his students and knew of Heisenberg's interest in Niels Bohr's theories on atomic physics. At the event, Bohr was a guest lecturer and gave a series of comprehensive lectures on quantum atomic physics and Heisenberg met Bohr for the first time, which had a lasting effect on him.[16][17][18]

Heisenberg's doctoral thesis, the topic of which was suggested by Sommerfeld, was on turbulence;[19] the thesis discussed both the stability of laminar flow and the nature of turbulent flow. The problem of stability was investigated by the use of the Orr–Sommerfeld equation, a fourth-order linear differential equation for small disturbances from laminar flow. He briefly returned to this topic after World War II.[20]

In his youth he was a member and Scoutleader of the Neupfadfinder, a German Scout association and part of the German Youth Movement.[21][22][23] In August 1923 Robert Honsell and Heisenberg organized a trip to Finland with a Scout group of this association from Munich.[24]
$
10
Question: Which scientific principle is Heisenberg best known for?
A: Principle of Relativity.
B: Theory of Evolution.
C: Uncertainty Principle.
D: Principle of Determination.
E: Quantum Field Theory.
Answer: C

Question: For what specific achievement was Heisenberg awarded the Nobel Prize in Physics in 1932?
A: Development of the atomic bomb.
B: Research on the atomic nucleus.
C: Discovery of cosmic rays.
D: Creation of quantum mechanics.
E: Development of the hydrodynamics of turbulent flows.
Answer: D

Question: Which institution was renamed the Max Planck Institute for Physics after World War II?
A: Ludwig Maximilian University of Munich.
B: Georg-August University of Göttingen.
C: Kaiser Wilhelm Institute for Physics.
D: Alexander von Humboldt Foundation.
E: Nuclear Physics Working Group.
Answer: C

Question: Who was Heisenberg's doctoral advisor?
A: Niels Bohr.
B: Max Born.
C: James Franck.
D: Arnold Sommerfeld.
E: David Hilbert.
Answer: D

Question: Heisenberg's habilitation thesis focused on which phenomenon?
A: Uncertainty principle.
B: Quantum mechanics matrix formulation.
C: Anomalous Zeeman effect.
D: Hydrodynamics of turbulent flows.
E: Stability of laminar flow.
Answer: C

Question: During which event did Heisenberg first meet Niels Bohr?
A: The Munich Science Symposium.
B: The Göttingen Physics Conference.
C: The Bohr Festival.
D: The Sommerfeld Lectures.
E: The Heisenberg Seminars.
Answer: C

Question: What was the primary subject of Heisenberg's doctoral thesis?
A: Quantum mechanics.
B: Stability of laminar flow and turbulent flow.
C: The Uncertainty Principle.
D: The hydrodynamics of ferromagnetism.
E: The atomic structure of elements.
Answer: B

Question: Where did Heisenberg study physics and mathematics from 1920 to 1923?
A: Max Planck Institute for Physics.
B: Ludwig Maximilian University of Munich and Georg-August University of Göttingen.
C: Alexander von Humboldt Foundation.
D: Kaiser Wilhelm Institute for Physics.
E: University of Berlin.
Answer: B

Question: In his younger days, what youth organization was Heisenberg a member of?
A: The Youth League of Munich.
B: The German Academic Society.
C: The Neupfadfinder.
D: The Göttingen Young Scientists Group.
E: The Quantum Scouts.
Answer: C

Question: Who accompanied Heisenberg on a trip to Finland with a Scout group in August 1923?
A: Niels Bohr.
B: Arnold Sommerfeld.
C: Max Born.
D: Robert Honsell.
E: David Hilbert.
Answer: D
@
J. Robert Oppenheimer (born Julius Robert Oppenheimer; /ˈɒpənhaɪmər/ OP-ən-hy-mər; April 22, 1904 – February 18, 1967) was an American theoretical physicist and director of the Manhattan Project's Los Alamos Laboratory during World War II. He is often called the "father of the atomic bomb".

Born in New York City, Oppenheimer earned a bachelor of arts degree in chemistry from Harvard University in 1925 and a doctorate in physics from the University of Göttingen in Germany in 1927, where he studied under Max Born. After research at other institutions, he joined the physics department at the University of California, Berkeley, where he became a full professor in 1936. He made significant contributions to theoretical physics, including achievements in quantum mechanics and nuclear physics such as the Born–Oppenheimer approximation for molecular wave functions, work on the theory of electrons and positrons, the Oppenheimer–Phillips process in nuclear fusion, and early work on quantum tunneling. With his students, he also made contributions to the theory of neutron stars and black holes, quantum field theory, and the interactions of cosmic rays.

In 1942, Oppenheimer was recruited to work on the Manhattan Project, and in 1943 he was appointed director of the project's Los Alamos Laboratory in New Mexico, tasked with developing the first nuclear weapons. His leadership and scientific expertise were instrumental in the project's success. On July 16, 1945, he was present at the first test of the atomic bomb, Trinity. In August 1945, the weapons were used against Japan in the bombings of Hiroshima and Nagasaki, the only use of nuclear weapons in an armed conflict.

In 1947, Oppenheimer became the director of the Institute for Advanced Study in Princeton, New Jersey, and chaired the influential General Advisory Committee of the newly created U.S. Atomic Energy Commission. He lobbied for international control of nuclear power to avert nuclear proliferation and a nuclear arms race with the Soviet Union. He opposed the development of the hydrogen bomb during a 1949–1950 governmental debate on the question and subsequently took positions on defense-related issues that provoked the ire of some U.S. government and military factions. During the second Red Scare, Oppenheimer's stances, together with his past associations with the Communist Party USA, led to the revocation of his security clearance following a 1954 security hearing. This effectively ended his access to the government's atomic secrets and thus his career as a nuclear physicist. Stripped also of his direct political influence, Oppenheimer continued to lecture, write, and work in physics. In 1963, he was awarded the Enrico Fermi Award as a gesture of political rehabilitation. He died four years later of throat cancer. In 2022, the federal government vacated the 1954 revocation of Oppenheimer's security clearance.

Oppenheimer did important research in theoretical astronomy (especially as related to general relativity and nuclear theory), nuclear physics, spectroscopy, and quantum field theory, including its extension into quantum electrodynamics. The formal mathematics of relativistic quantum mechanics also attracted his attention, although he doubted its validity. His work predicted many later finds, including the neutron, meson and neutron star.[45]

Initially, his major interest was the theory of the continuous spectrum. His first published paper, in 1926, concerned the quantum theory of molecular band spectra. He developed a method to carry out calculations of its transition probabilities. He calculated the photoelectric effect for hydrogen and X-rays, obtaining the absorption coefficient at the K-edge. His calculations accorded with observations of the X-ray absorption of the Sun, but not helium. Years later, it was realized that the Sun was largely composed of hydrogen and that his calculations were correct.[46][47]

Groves selected Oppenheimer to head the project's secret weapons laboratory, although it is not known precisely when.[118] This decision surprised many, because Oppenheimer had left-wing political views and no record as a leader of large projects. Groves worried that because Oppenheimer did not have a Nobel Prize, he might not have had the prestige to direct fellow scientists,[119] but Groves was impressed by Oppenheimer's singular grasp of the practical aspects of the project and by the breadth of his knowledge. As a military engineer, Groves knew that this would be vital in an interdisciplinary project that would involve not just physics but also chemistry, metallurgy, ordnance, and engineering. Groves also detected in Oppenheimer something that many others did not, an "overweening ambition",[120] which Groves reckoned would supply the drive necessary to push the project to a successful conclusion.[120] Oppenheimer's past associations were not overlooked, but on July 20, 1943, Groves directed that he receive a security clearance "without delay irrespective of the information which you have concerning Mr Oppenheimer. He is absolutely essential to the project."[121] Rabi considered Oppenheimer's appointment "a real stroke of genius on the part of General Groves, who was not generally considered to be a genius".[122]

In the early morning hours of July 16, 1945, near Alamogordo, New Mexico, the work at Los Alamos culminated in the test of the world's first nuclear weapon. Oppenheimer had code-named the site "Trinity" in mid-1944, saying later that the name came from John Donne's Holy Sonnets; he had been introduced to Donne's work in the 1930s by Jean Tatlock, who killed herself in January 1944.[140][141]
$
10
Question: What title is often given to J. Robert Oppenheimer in relation to the atomic bomb?
A: Father of Nuclear Energy.
B: Director of the Manhattan Project.
C: Father of the Atomic Bomb.
D: Creator of Nuclear Fusion.
E: Lead of Nuclear Studies.
Answer: C

Question: Where did Oppenheimer earn his bachelor of arts degree in chemistry?
A: University of Göttingen.
B: University of California, Berkeley.
C: Harvard University.
D: Princeton University.
E: Stanford University.
Answer: C

Question: Which approximation for molecular wave functions is associated with Oppenheimer?
A: Los Alamos Approximation.
B: Quantum Tunneling Approximation.
C: Born–Oppenheimer Approximation.
D: Manhattan Approximation.
E: Trinity Approximation.
Answer: C

Question: What position did Oppenheimer hold during the Manhattan Project's development of the atomic bomb?
A: Chief Scientist of the Manhattan Project.
B: Head of Nuclear Fusion Studies.
C: Director of the Los Alamos Laboratory.
D: Lead of Trinity Test Site.
E: Head of Nuclear Strategy.
Answer: C

Question: In which city was J. Robert Oppenheimer born?
A: Berkeley.
B: Princeton.
C: Göttingen.
D: New York City.
E: Los Alamos.
Answer: D

Question: Which award was given to Oppenheimer in 1963 as a gesture of political rehabilitation?
A: Nobel Prize in Physics.
B: Los Alamos Recognition Award.
C: Enrico Fermi Award.
D: Manhattan Medal of Honor.
E: General Groves Trophy.
Answer: C

Question: In 1947, which institute appointed Oppenheimer as its director?
A: Los Alamos Laboratory.
B: University of California, Berkeley.
C: Manhattan Project Institute.
D: U.S. Atomic Energy Commission.
E: Institute for Advanced Study in Princeton.
Answer: E

Question: Which event directly led to the end of Oppenheimer's career as a nuclear physicist?
A: Development of the hydrogen bomb.
B: Revocation of his security clearance after a 1954 security hearing.
C: Decision to lobby for international control of nuclear power.
D: His association with the Communist Party USA.
E: The bombings of Hiroshima and Nagasaki.
Answer: B

Question: What did Oppenheimer name the site of the world's first nuclear weapon test?
A: Hiroshima.
B: Atomic Site.
C: Manhattan.
D: Trinity.
E: Nuclear Dawn.
Answer: D

Question: Which General was responsible for selecting Oppenheimer to head the project's secret weapons laboratory?
A: General Patton.
B: General Eisenhower.
C: General MacArthur.
D: General Groves.
E: General Bradley.
Answer: D
@
Plutonium-239 (239Pu or Pu-239) is an isotope of plutonium. Plutonium-239 is the primary fissile isotope used for the production of nuclear weapons, although uranium-235 is also used for that purpose. Plutonium-239 is also one of the three main isotopes demonstrated usable as fuel in thermal spectrum nuclear reactors, along with uranium-235 and uranium-233. Plutonium-239 has a half-life of 24,110 years.[1]

Nuclear properties
The nuclear properties of plutonium-239, as well as the ability to produce large amounts of nearly pure 239Pu more cheaply than highly enriched weapons-grade uranium-235, led to its use in nuclear weapons and nuclear power plants. The fissioning of an atom of uranium-235 in the reactor of a nuclear power plant produces two to three neutrons, and these neutrons can be absorbed by uranium-238 to produce plutonium-239 and other isotopes. Plutonium-239 can also absorb neutrons and fission along with the uranium-235 in a reactor.

Of all the common nuclear fuels, 239Pu has the smallest critical mass. A spherical untamped critical mass is about 11 kg (24.2 lbs),[2] 10.2 cm (4") in diameter. Using appropriate triggers, neutron reflectors, implosion geometry and tampers, the critical mass can be less than half of that.

The fission of one atom of 239Pu generates 207.1 MeV = 3.318 × 10−11 J, i.e. 19.98 TJ/mol = 83.61 TJ/kg,[3] or about 23 gigawatt hours/kg.

radiation source (thermal fission of 239Pu)	average energy released [MeV][3]
Kinetic energy of fission fragments	175.8
Kinetic energy of prompt neutrons	    5.9
Energy carried by prompt γ-rays	    7.8
Total instantaneous energy	189.5
Energy of β− particles	    5.3
Energy of antineutrinos	    7.1
Energy of delayed γ-rays	    5.2
Total from decaying fission products	  17.6
Energy released by radiative capture of prompt neutrons	  11.5
Total heat released in a thermal-spectrum reactor (anti-neutrinos do not contribute)	211.5
$
10
Question: What is the primary use of Plutonium-239?
A: Fuel for cars.
B: Production of nuclear weapons.
C: Catalyst in chemical reactions.
D: Battery for electronic devices.
E: Radiation treatment in hospitals.
Answer: B

Question: Which of the following is NOT demonstrated as usable fuel in thermal spectrum nuclear reactors?
A: Uranium-235.
B: Plutonium-241.
C: Plutonium-239.
D: Uranium-233.
E: Uranium-238.
Answer: B

Question: What is the half-life of Plutonium-239?
A: 1,110 years.
B: 5,000 years.
C: 10,210 years.
D: 24,110 years.
E: 50,000 years.
Answer: D

Question: The fission of uranium-235 in a reactor produces how many neutrons?
A: One.
B: Two to three.
C: Four to five.
D: Six to seven.
E: Eight to nine.
Answer: B

Question: Which nuclear fuel has the smallest critical mass among the common nuclear fuels?
A: Uranium-233.
B: Uranium-238.
C: Uranium-235.
D: Plutonium-240.
E: Plutonium-239.
Answer: E

Question: Approximately how much is the spherical untamped critical mass of 239Pu in kilograms?
A: 5 kg.
B: 15 kg.
C: 11 kg.
D: 20 kg.
E: 25 kg.
Answer: C

Question: How much energy does the fission of one atom of 239Pu generate in TJ/kg?
A: 10.5 TJ/kg.
B: 83.61 TJ/kg.
C: 150 TJ/kg.
D: 60 TJ/kg.
E: 200 TJ/kg.
Answer: B

Question: The kinetic energy of fission fragments from the thermal fission of 239Pu is approximately:
A: 175.8 MeV.
B: 5.9 MeV.
C: 7.8 MeV.
D: 5.3 MeV.
E: 7.1 MeV.
Answer: A

Question: What is the total instantaneous energy released from the radiation source (thermal fission of 239Pu)?
A: 211.5 MeV.
B: 189.5 MeV.
C: 175.8 MeV.
D: 11.5 MeV.
E: 5.2 MeV.
Answer: B

Question: Which energy does NOT contribute to the total heat released in a thermal-spectrum reactor?
A: Energy of β− particles.
B: Energy of delayed γ-rays.
C: Energy of antineutrinos.
D: Kinetic energy of prompt neutrons.
E: Kinetic energy of fission fragments.
Answer: C
@
In nuclear engineering, a critical mass is the smallest amount of fissile material needed for a sustained nuclear chain reaction. The critical mass of a fissionable material depends upon its nuclear properties (specifically, its nuclear fission cross-section), density, shape, enrichment, purity, temperature, and surroundings. The concept is important in nuclear weapon design.

When a nuclear chain reaction in a mass of fissile material is self-sustaining, the mass is said to be in a critical state in which there is no increase or decrease in power, temperature, or neutron population.

A numerical measure of a critical mass is dependent on the effective neutron multiplication factor k, the average number of neutrons released per fission event that go on to cause another fission event rather than being absorbed or leaving the material. When k = 1, the mass is critical, and the chain reaction is self-sustaining.

A subcritical mass is a mass of fissile material that does not have the ability to sustain a fission chain reaction. A population of neutrons introduced to a subcritical assembly will exponentially decrease. In this case, k < 1. A steady rate of spontaneous fissions causes a proportionally steady level of neutron activity. The constant of proportionality increases as k increases.

A supercritical mass is one which, once fission has started, will proceed at an increasing rate. The material may settle into equilibrium (i.e. become critical again) at an elevated temperature/power level or destroy itself. In the case of supercriticality, k > 1.

Due to spontaneous fission a supercritical mass will undergo a chain reaction. For example, a spherical critical mass of pure uranium-235 (235U) with a mass of about 52 kilograms (115 lb) would experience around 15 spontaneous fission events per second.[citation needed] The probability that one such event will cause a chain reaction depends on how much the mass exceeds the critical mass. If there is uranium-238 (238U) present, the rate of spontaneous fission will be much higher. Fission can also be initiated by neutrons produced by cosmic rays.
$
10
Question: What does a critical mass refer to in nuclear engineering?
A: The maximum amount of fissile material that can be stored safely.
B: The amount of energy produced by a nuclear reaction.
C: The smallest amount of fissile material needed for a sustained nuclear chain reaction.
D: The temperature at which a nuclear reaction occurs.
E: The rate of neutron production in a fissile material.
Answer: C

Question: What determines the critical mass of a fissionable material?
A: Only its nuclear fission cross-section.
B: Its density and shape.
C: Its temperature and surroundings.
D: Its nuclear properties, density, shape, enrichment, purity, temperature, and surroundings.
E: Only its enrichment and purity.
Answer: D

Question: When is a mass of fissile material said to be in a critical state?
A: When there is a decrease in power and temperature.
B: When the neutron population decreases.
C: When there is no change in power, temperature, or neutron population.
D: When the chain reaction is not self-sustaining.
E: When the mass is at its highest temperature.
Answer: C

Question: What does the effective neutron multiplication factor k represent?
A: The temperature of the fissile material.
B: The size of the fissile material.
C: The speed of neutrons in the material.
D: The average number of neutrons released per fission event that cause another fission event.
E: The density of the fissile material.
Answer: D

Question: What is the condition for a mass to be critical in terms of k?
A: k < 1.
B: k = 1.
C: k > 1.
D: k = 0.
E: k > 2.
Answer: B

Question: Which statement best describes a subcritical mass?
A: The chain reaction will proceed at an increasing rate.
B: Fission events will decrease exponentially.
C: It will destroy itself.
D: There is an increase in power and temperature.
E: The mass has reached maximum density.
Answer: B

Question: What characterizes a supercritical mass?
A: It cannot sustain a chain reaction.
B: The material settles into equilibrium at a basic temperature.
C: The chain reaction decreases over time.
D: Once fission starts, it will proceed at an increasing rate.
E: It has the least amount of spontaneous fission events.
Answer: D

Question: Which statement is true about spontaneous fission in a supercritical mass of pure uranium-235?
A: It would experience around 50 spontaneous fission events per second.
B: It would experience no spontaneous fission events.
C: It would experience around 5 spontaneous fission events per second.
D: It would experience around 15 spontaneous fission events per second.
E: The number of spontaneous fission events would be negligible.
Answer: D

Question: What can potentially initiate fission besides spontaneous fission?
A: Gamma radiation.
B: Alpha particles.
C: Protons.
D: Neutrons produced by cosmic rays.
E: Electrons.
Answer: D

Question: If uranium-238 is present, how is the rate of spontaneous fission affected?
A: It remains the same.
B: It decreases.
C: It increases.
D: It becomes unpredictable.
E: It stops completely.
Answer: C
@
The shape with minimal critical mass and the smallest physical dimensions is a sphere. Bare-sphere critical masses at normal density of some actinides are listed in the following table. Most information on bare sphere masses is considered classified, since it is critical to nuclear weapons design, but some documents have been declassified.[2]

Nuclide	Half-life
(y)	Critical mass
(kg)	Diameter
(cm)	Ref
uranium-233	159,200	15	11	[3]
uranium-235	703,800,000	52	17	[3]
neptunium-236	154,000	7	8.7	[4]
neptunium-237	2,144,000	60	18	[5][6]
plutonium-238	87.7	9.04–10.07	9.5–9.9	[7]
plutonium-239	24,110	10	9.9	[3][7]
plutonium-240	6561	40	15	[3]
plutonium-241	14.3	12	10.5	[8]
plutonium-242	375,000	75–100	19–21	[8]
americium-241	432.2	55–77	20–23	[9]
americium-242m	141	9–14	11–13	[9]
americium-243	7370	180–280	30–35	[9]
curium-243	29.1	7.34–10	10–11	[10]
curium-244	18.1	13.5–30	12.4–16	[10]
curium-245	8500	9.41–12.3	11–12	[10]
curium-246	4760	39–70.1	18–21	[10]
curium-247	15,600,000	6.94–7.06	9.9	[10]
berkelium-247	1380	75.7	11.8-12.2	[11]
berkelium-249	0.9	192	16.1-16.6	[11]
californium-249	351	6	9	[4]
californium-251	900	5.46	8.5	[4]
californium-252	2.6	2.73	6.9	[12]
einsteinium-254	0.755	9.89	7.1	[11]
The critical mass for lower-grade uranium depends strongly on the grade: with 20% 235U it is over 400 kg; with 15% 235U, it is well over 600 kg.

The critical mass is inversely proportional to the square of the density. If the density is 1% more and the mass 2% less, then the volume is 3% less and the diameter 1% less. The probability for a neutron per cm travelled to hit a nucleus is proportional to the density. It follows that 1% greater density means that the distance travelled before leaving the system is 1% less. This is something that must be taken into consideration when attempting more precise estimates of critical masses of plutonium isotopes than the approximate values given above, because plutonium metal has a large number of different crystal phases which can have widely varying densities.

Note that not all neutrons contribute to the chain reaction. Some escape and others undergo radiative capture.
$
10
Question: What shape has the minimal critical mass and the smallest physical dimensions?
A: Cylinder
B: Rectangle
C: Sphere
D: Cube
E: Ellipsoid
Answer: C

Question: Which actinide has a half-life of 159,200 years and a critical mass of 15 kg?
A: plutonium-238
B: neptunium-236
C: uranium-233
D: curium-245
E: americium-241
Answer: C

Question: How much is the critical mass for uranium-235 in kilograms?
A: 52
B: 15
C: 703,800,000
D: 17
E: 10
Answer: A

Question: If the density is increased by 1%, by how much is the diameter reduced?
A: 2%
B: 3%
C: 1%
D: 4%
E: 0.5%
Answer: C

Question: For lower-grade uranium with 20% 235U, what is the approximate critical mass in kilograms?
A: Over 400 kg
B: About 250 kg
C: Over 600 kg
D: About 350 kg
E: Over 800 kg
Answer: A

Question: The critical mass is inversely proportional to the square of what factor?
A: Temperature
B: Diameter
C: Purity
D: Density
E: Volume
Answer: D

Question: Which statement is true regarding neutrons in a nuclear chain reaction?
A: All neutrons contribute to the chain reaction.
B: Neutrons never escape the reaction.
C: Only neutrons with high energy contribute to the chain reaction.
D: Some neutrons undergo radiative capture.
E: All neutrons undergo fission.
Answer: D

Question: What factor is critical to nuclear weapons design that is considered classified?
A: The cost of producing actinides.
B: The exact mechanism of a nuclear explosion.
C: Most information on bare sphere masses.
D: The methods of enrichment.
E: The location of actinide mines.
Answer: C

Question: What nuclide has a critical mass of 6 kg?
A: americium-242m
B: californium-249
C: berkelium-247
D: curium-243
E: plutonium-241
Answer: B

Question: Which of the following does NOT affect the critical mass?
A: Neutrons undergoing fission.
B: Some neutrons escaping.
C: Neutrons undergoing radiative capture.
D: The shape of the material.
E: The surrounding temperature.
Answer: A
@
Plutonium is a radioactive chemical element with the symbol Pu and atomic number 94. It is an actinide metal of silvery-gray appearance that tarnishes when exposed to air, and forms a dull coating when oxidized. The element normally exhibits six allotropes and four oxidation states. It reacts with carbon, halogens, nitrogen, silicon, and hydrogen. When exposed to moist air, it forms oxides and hydrides that can expand the sample up to 70% in volume, which in turn flake off as a powder that is pyrophoric. It is radioactive and can accumulate in bones, which makes the handling of plutonium dangerous.

Plutonium was first synthetically produced and isolated in late 1940 and early 1941, by a deuteron bombardment of uranium-238 in the 1.5-metre (60 in) cyclotron at the University of California, Berkeley. First, neptunium-238 (half-life 2.1 days) was synthesized, which subsequently beta-decayed to form the new element with atomic number 94 and atomic weight 238 (half-life 88 years). Since uranium had been named after the planet Uranus and neptunium after the planet Neptune, element 94 was named after Pluto, which at the time was considered to be a planet as well. Wartime secrecy prevented the University of California team from publishing its discovery until 1948.

Plutonium is the element with the highest atomic number to occur in nature. Trace quantities arise in natural uranium-238 deposits when uranium-238 captures neutrons emitted by decay of other uranium-238 atoms.

Both plutonium-239 and plutonium-241 are fissile, meaning that they can sustain a nuclear chain reaction, leading to applications in nuclear weapons and nuclear reactors. Plutonium-240 exhibits a high rate of spontaneous fission, raising the neutron flux of any sample containing it. The presence of plutonium-240 limits a plutonium sample's usability for weapons or its quality as reactor fuel, and the percentage of plutonium-240 determines its grade (weapons-grade, fuel-grade, or reactor-grade). Plutonium-238 has a half-life of 87.7 years and emits alpha particles. It is a heat source in radioisotope thermoelectric generators, which are used to power some spacecraft. Plutonium isotopes are expensive and inconvenient to separate, so particular isotopes are usually manufactured in specialized reactors.

Producing plutonium in useful quantities for the first time was a major part of the Manhattan Project during World War II that developed the first atomic bombs. The Fat Man bombs used in the Trinity nuclear test in July 1945, and in the bombing of Nagasaki in August 1945, had plutonium cores. Human radiation experiments studying plutonium were conducted without informed consent, and several criticality accidents, some lethal, occurred after the war. Disposal of plutonium waste from nuclear power plants and dismantled nuclear weapons built during the Cold War is a nuclear-proliferation and environmental concern. Other sources of plutonium in the environment are fallout from numerous above-ground nuclear tests, which are now banned.
$
10
Question: What is the atomic number of plutonium?
A: 92
B: 93
C: 94
D: 95
E: 96
Answer: C

Question: Which of the following happens when plutonium is exposed to moist air?
A: It shines brightly.
B: It contracts in volume.
C: It forms oxides and hydrides.
D: It becomes transparent.
E: It liquifies instantly.
Answer: C

Question: What event prevented the University of California team from publishing its discovery of plutonium immediately?
A: An accidental spill at the lab.
B: Wartime secrecy.
C: The Cold War.
D: The loss of research data.
E: A disagreement within the research team.
Answer: B

Question: Which planet inspired the naming of plutonium?
A: Mars
B: Jupiter
C: Saturn
D: Neptune
E: Pluto
Answer: E

Question: Plutonium-239 and plutonium-241 have which characteristic in common?
A: They have the same half-life.
B: They are fissile.
C: They are stable isotopes.
D: They cannot sustain a nuclear chain reaction.
E: They are found abundantly in nature.
Answer: B

Question: What is the significance of plutonium-240 in a plutonium sample?
A: It increases the sample's reactivity.
B: It raises the neutron flux.
C: It makes the sample glow.
D: It decreases the sample's atomic weight.
E: It enhances the sample's stability.
Answer: B

Question: Which application uses plutonium-238 due to its emission of alpha particles and heat generation?
A: Production of nuclear weapons.
B: Propulsion of naval ships.
C: In radioisotope thermoelectric generators for spacecraft.
D: For use in medical radiation therapy.
E: In commercial power plants.
Answer: C

Question: During which major event was producing plutonium in useful quantities a significant part?
A: The Cold War.
B: The Korean War.
C: The Vietnam War.
D: The Gulf War.
E: The Manhattan Project during World War II.
Answer: E

Question: The Fat Man bombs, used in 1945, had cores made of what material?
A: Uranium-235
B: Uranium-238
C: Neptunium
D: Plutonium
E: Thorium
Answer: D

Question: What is a major environmental concern related to plutonium?
A: The extraction process harming wildlife.
B: The bright light emitted by plutonium.
C: Disposal of plutonium waste.
D: The use of plutonium in household products.
E: The evaporation of plutonium into the atmosphere.
Answer: C
@
Plutonium, like most metals, has a bright silvery appearance at first, much like nickel, but it oxidizes very quickly to a dull gray, although yellow and olive green are also reported.[4][5] At room temperature plutonium is in its α (alpha) form. This, the most common structural form of the element (allotrope), is about as hard and brittle as gray cast iron unless it is alloyed with other metals to make it soft and ductile. Unlike most metals, it is not a good conductor of heat or electricity. It has a low melting point (640 °C, 1,184 °F) and an unusually high boiling point (3,228 °C, 5,842 °F).[4] This gives a large range of temperatures (over 2,500 kelvin wide) at which plutonium is liquid, but this range is neither the greatest among all actinides nor among all metals.[6] The low melting point as well as the reactivity of the native metal compared to the oxide leads to plutonium oxides being a preferred form for applications such as nuclear fission reactor fuel (MOX-fuel).

Alpha decay, the release of a high-energy helium nucleus, is the most common form of radioactive decay for plutonium.[7] A 5 kg mass of 239Pu contains about 12.5×1024 atoms. With a half-life of 24,100 years, about 11.5×1012 of its atoms decay each second by emitting a 5.157 MeV alpha particle. This amounts to 9.68 watts of power. Heat produced by the deceleration of these alpha particles makes it warm to the touch.[8][9] 238
Pu due to its much shorter half life heats up to much higher temperatures and glows red hot with blackbody radiation if left without external heating or cooling. This heat has been used in Radioisotope thermoelectric generators (see below).

Resistivity is a measure of how strongly a material opposes the flow of electric current. The resistivity of plutonium at room temperature is very high for a metal, and it gets even higher with lower temperatures, which is unusual for metals.[10] This trend continues down to 100 K, below which resistivity rapidly decreases for fresh samples.[10] Resistivity then begins to increase with time at around 20 K due to radiation damage, with the rate dictated by the isotopic composition of the sample.[10]

Because of self-irradiation, a sample of plutonium fatigues throughout its crystal structure, meaning the ordered arrangement of its atoms becomes disrupted by radiation with time.[11] Self-irradiation can also lead to annealing which counteracts some of the fatigue effects as temperature increases above 100 K.[12]

Unlike most materials, plutonium increases in density when it melts, by 2.5%, but the liquid metal exhibits a linear decrease in density with temperature.[10] Near the melting point, the liquid plutonium has very high viscosity and surface tension compared to other metals.[11]

Plutonium normally has six allotropes and forms a seventh (zeta, ζ) at high temperature within a limited pressure range.[13] These allotropes, which are different structural modifications or forms of an element, have very similar internal energies but significantly varying densities and crystal structures. This makes plutonium very sensitive to changes in temperature, pressure, or chemistry, and allows for dramatic volume changes following phase transitions from one allotropic form to another.[11] The densities of the different allotropes vary from 16.00 g/cm3 to 19.86 g/cm3.[14]

The presence of these many allotropes makes machining plutonium very difficult, as it changes state very readily. For example, the α form exists at room temperature in unalloyed plutonium. It has machining characteristics similar to cast iron but changes to the plastic and malleable β (beta) form at slightly higher temperatures.[15] The reasons for the complicated phase diagram are not entirely understood. The α form has a low-symmetry monoclinic structure, hence its brittleness, strength, compressibility, and poor thermal conductivity.[13]

Plutonium in the δ (delta) form normally exists in the 310 °C to 452 °C range but is stable at room temperature when alloyed with a small percentage of gallium, aluminium, or cerium, enhancing workability and allowing it to be welded.[15] The δ form has more typical metallic character, and is roughly as strong and malleable as aluminium.[13] In fission weapons, the explosive shock waves used to compress a plutonium core will also cause a transition from the usual δ phase plutonium to the denser α form, significantly helping to achieve supercriticality.[citation needed] The ε phase, the highest temperature solid allotrope, exhibits anomalously high atomic self-diffusion compared to other elements.[11]
$
10
Question: How does plutonium appear when first exposed to air?
A: Silvery and bright
B: Dull gray
C: Olive green
D: Yellow
E: Transparent
Answer: A

Question: What form is plutonium in at room temperature?
A: α (alpha) form
B: β (beta) form
C: δ (delta) form
D: ε (epsilon) form
E: ζ (zeta) form
Answer: A

Question: Which of the following is NOT a characteristic of the α form of plutonium?
A: Soft and ductile
B: Brittle
C: Poor conductor of heat
D: High melting point
E: Good conductor of electricity
Answer: E

Question: How does the density of plutonium change when it melts?
A: Decreases by 2.5%
B: Remains the same
C: Increases by 1.5%
D: Increases by 2.5%
E: Decreases linearly with temperature
Answer: D

Question: Which type of radioactive decay is most common for plutonium?
A: Beta decay
B: Gamma radiation
C: Alpha decay
D: Neutron emission
E: Spontaneous fission
Answer: C

Question: Which plutonium isotope glows red hot if left without external heating or cooling due to its heat?
A: 235Pu
B: 237Pu
C: 238Pu
D: 239Pu
E: 240Pu
Answer: C

Question: At what temperature range does the δ (delta) form of plutonium typically exist?
A: 100 °C to 250 °C
B: 250 °C to 310 °C
C: 310 °C to 452 °C
D: 452 °C to 500 °C
E: 500 °C to 550 °C
Answer: C

Question: What happens to a plutonium sample due to self-irradiation?
A: It becomes highly magnetic.
B: Its crystal structure fatigues.
C: It liquifies instantly.
D: It increases in volume exponentially.
E: It turns transparent.
Answer: B

Question: How many allotropes of plutonium are typically known?
A: Three
B: Four
C: Five
D: Six
E: Seven
Answer: D

Question: What is the effect of alloying plutonium with a small percentage of gallium, aluminium, or cerium?
A: It turns transparent.
B: The α form becomes more stable.
C: The δ form becomes stable at room temperature.
D: The ε phase becomes the dominant form.
E: It prevents plutonium from melting.
Answer: C
@
Trace amounts of plutonium-238, plutonium-239, plutonium-240, and plutonium-244 can be found in nature. Small traces of plutonium-239, a few parts per trillion, and its decay products are naturally found in some concentrated ores of uranium,[50] such as the natural nuclear fission reactor in Oklo, Gabon.[51] The ratio of plutonium-239 to uranium at the Cigar Lake Mine uranium deposit ranges from 2.4×10−12 to 44×10−12.[52] These trace amounts of 239Pu originate in the following fashion: on rare occasions, 238U undergoes spontaneous fission, and in the process, the nucleus emits one or two free neutrons with some kinetic energy. When one of these neutrons strikes the nucleus of another 238U atom, it is absorbed by the atom, which becomes 239U. With a relatively short half-life, 239U decays to 239Np, which decays into 239Pu.[53][54] Finally, exceedingly small amounts of plutonium-238, attributed to the extremely rare double beta decay of uranium-238, have been found in natural uranium samples.[55]

Due to its relatively long half-life of about 80 million years, it was suggested that plutonium-244 occurs naturally as a primordial nuclide, but early reports of its detection could not be confirmed.[56] However, its long half-life ensured its circulation across the solar system before its extinction,[57] and indeed, evidence of the spontaneous fission of extinct 244Pu has been found in meteorites.[58] The former presence of 244Pu in the early Solar System has been confirmed, since it manifests itself today as an excess of its daughters, either 232Th (from the alpha decay pathway) or xenon isotopes (from its spontaneous fission). The latter are generally more useful, because the chemistries of thorium and plutonium are rather similar (both are predominantly tetravalent) and hence an excess of thorium would not be strong evidence that some of it was formed as a plutonium daughter.[59] 244Pu has the longest half-life of all transuranic nuclides and is produced only in the r-process in supernovae and colliding neutron stars; when nuclei are ejected from these events at high speed to reach Earth, 244Pu alone among transuranic nuclides has a long enough half-life to survive the journey, and hence tiny traces of live interstellar 244Pu have been found in the deep sea floor. Because 240Pu also occurs in the decay chain of 244Pu, it must thus also be present in secular equilibrium, albeit in even tinier quantities.[60]

Minute traces of plutonium are usually found in the human body due to the 550 atmospheric and underwater nuclear tests that have been carried out, and to a small number of major nuclear accidents.[38] Most atmospheric and underwater nuclear testing was stopped by the Limited Test Ban Treaty in 1963, which of the nuclear powers was signed and ratified by the United States, United Kingdom and Soviet Union. France would continue atmospheric nuclear testing until 1974 and China would continue atmospheric nuclear testing until 1980. All subsequent nuclear testing was conducted underground.[61]
$
10
Question: Where can small traces of plutonium-239 be naturally found?
A: In meteorites
B: In every uranium deposit
C: In the natural nuclear fission reactor in Oklo, Gabon
D: In the human body
E: In the atmosphere
Answer: C

Question: How does 239Pu typically originate?
A: Spontaneous fission of 238U
B: Double beta decay of uranium-238
C: Direct absorption of energy from the sun
D: Neutron absorption by 238U atom followed by decay
E: Direct decay of 238U
Answer: D

Question: Plutonium-244 was suggested to occur naturally as a what kind of nuclide?
A: Primordial
B: Radiogenic
C: Secondary
D: Transuranic
E: Proton-rich
Answer: A

Question: Evidence of the spontaneous fission of extinct 244Pu has been found where?
A: Deep sea floor
B: Human body
C: Earth's core
D: Moon rocks
E: Meteorites
Answer: E

Question: What does an excess of 232Th or xenon isotopes indicate?
A: Presence of uranium deposits
B: Previous existence of 244Pu
C: A recent nuclear explosion
D: Pollution from nuclear accidents
E: Active decay of 239Pu
Answer: B

Question: How is 244Pu produced?
A: Atmospheric nuclear tests
B: Underground nuclear explosions
C: In the r-process in supernovae and colliding neutron stars
D: During uranium extraction processes
E: By direct decay of 240Pu
Answer: C

Question: Why are minute traces of plutonium found in the human body?
A: Consumption of plutonium-rich foods
B: Natural genetic mutations
C: Atmospheric and underwater nuclear tests and nuclear accidents
D: Inhalation of air from polluted cities
E: Direct exposure to uranium deposits
Answer: C

Question: Which treaty stopped most atmospheric and underwater nuclear testing in 1963?
A: Nuclear Non-Proliferation Treaty
B: Comprehensive Nuclear-Test-Ban Treaty
C: Strategic Arms Reduction Treaty
D: Outer Space Treaty
E: Limited Test Ban Treaty
Answer: E

Question: Which country continued atmospheric nuclear testing until 1974?
A: United States
B: Soviet Union
C: United Kingdom
D: China
E: France
Answer: E

Question: How was all nuclear testing conducted after the Limited Test Ban Treaty in 1963?
A: Atmospheric
B: Underwater
C: In space
D: Underground
E: Not conducted at all
Answer: D
@
The isotope plutonium-238 has a half-life of 87.74 years.[123] It emits a large amount of thermal energy with low levels of both gamma rays/photons and spontaneous neutron rays/particles.[124] Being an alpha emitter, it combines high energy radiation with low penetration and thereby requires minimal shielding. A sheet of paper can be used to shield against the alpha particles emitted by plutonium-238. One kilogram of the isotope can generate about 570 watts of heat.[8][124]

These characteristics make it well-suited for electrical power generation for devices that must function without direct maintenance for timescales approximating a human lifetime. It is therefore used in radioisotope thermoelectric generators and radioisotope heater units such as those in the Cassini,[125] Voyager, Galileo and New Horizons[126] space probes, and the Curiosity [127] and Perseverance (Mars 2020) Mars rovers.

The twin Voyager spacecraft were launched in 1977, each containing a 500 watt plutonium power source. Over 30 years later, each source is still producing about 300 watts which allows limited operation of each spacecraft.[128] An earlier version of the same technology powered five Apollo Lunar Surface Experiment Packages, starting with Apollo 12 in 1969.[38]

Plutonium-238 has also been used successfully to power artificial heart pacemakers, to reduce the risk of repeated surgery.[129][130] It has been largely replaced by lithium-based primary cells, but as of 2003 there were somewhere between 50 and 100 plutonium-powered pacemakers still implanted and functioning in living patients in the United States.[131] By the end of 2007, the number of plutonium-powered pacemakers was reported to be down to just nine.[132] Plutonium-238 was studied as a way to provide supplemental heat to scuba diving.[133] Plutonium-238 mixed with beryllium is used to generate neutrons for research purposes.[38]
$
10
Question: What is the half-life of plutonium-238?
A: 50 years
B: 80 years
C: 87.74 years
D: 100 years
E: 120 years
Answer: C

Question: Which type of radiation does plutonium-238 primarily emit?
A: Beta radiation
B: Gamma rays
C: Alpha radiation
D: X-rays
E: Ultraviolet radiation
Answer: C

Question: What can be used to shield against the alpha particles emitted by plutonium-238?
A: A lead wall
B: A sheet of paper
C: A thick glass window
D: A metal box
E: A wooden shield
Answer: B

Question: Approximately how much heat can one kilogram of plutonium-238 generate?
A: 100 watts
B: 250 watts
C: 400 watts
D: 500 watts
E: 570 watts
Answer: E

Question: What type of device is plutonium-238 used in for long-duration electrical power generation in space?
A: Solar panels
B: Fusion reactors
C: Radioisotope thermoelectric generators
D: Electric capacitors
E: Fuel cells
Answer: C

Question: Which Mars rover was NOT powered by plutonium-238?
A: Cassini
B: Voyager
C: Curiosity
D: Perseverance (Mars 2020)
E: Galileo
Answer: A

Question: When were the twin Voyager spacecraft launched?
A: 1969
B: 1972
C: 1977
D: 1985
E: 1991
Answer: C

Question: How many plutonium-powered pacemakers were reported to be still functioning by the end of 2007?
A: Between 50 and 100
B: 20
C: 500
D: 9
E: 15
Answer: D

Question: What was an application of plutonium-238 studied in relation to scuba diving?
A: As a breathing apparatus
B: To power underwater vehicles
C: As a lighting source
D: To provide supplemental heat
E: As a buoyancy aid
Answer: D

Question: When mixed with beryllium, what does plutonium-238 generate for research purposes?
A: X-rays
B: Gamma rays
C: Electromagnetic radiation
D: Neutrons
E: Beta radiation
Answer: D
@
Thorium is a weakly radioactive metallic chemical element with the symbol Th and atomic number 90. Thorium is light silver and tarnishes olive gray when it is exposed to air, forming thorium dioxide; it is moderately soft and malleable and has a high melting point. Thorium is an electropositive actinide whose chemistry is dominated by the +4 oxidation state; it is quite reactive and can ignite in air when finely divided.

All known thorium isotopes are unstable. The most stable isotope, 232Th, has a half-life of 14.05 billion years, or about the age of the universe; it decays very slowly via alpha decay, starting a decay chain named the thorium series that ends at stable 208Pb. On Earth, thorium and uranium are the only significantly radioactive elements that still occur naturally in large quantities as primordial elements.[a] Thorium is estimated to be over three times as abundant as uranium in the Earth's crust, and is chiefly refined from monazite sands as a by-product of extracting rare-earth metals.

Thorium was discovered in 1828 by the Norwegian amateur mineralogist Morten Thrane Esmark and identified by the Swedish chemist Jöns Jacob Berzelius, who named it after Thor, the Norse god of thunder. Its first applications were developed in the late 19th century. Thorium's radioactivity was widely acknowledged during the first decades of the 20th century. In the second half of the century, thorium was replaced in many uses due to concerns about its radioactivity.

Thorium is still being used as an alloying element in TIG welding electrodes but is slowly being replaced in the field with different compositions. It was also material in high-end optics and scientific instrumentation, used in some broadcast vacuum tubes, and as the light source in gas mantles, but these uses have become marginal. It has been suggested as a replacement for uranium as nuclear fuel in nuclear reactors, and several thorium reactors have been built. Thorium is also used in strengthening magnesium, coating tungsten wire in electrical equipment, controlling the grain size of tungsten in electric lamps, high-temperature crucibles, and glasses including camera and scientific instrument lenses. Other uses for thorium include heat-resistant ceramics, aircraft engines, and in light bulbs. Ocean science has utilised 231Pa/230Th isotope ratios to understand the ancient ocean.[7]
$
10
Question: What is the atomic number of Thorium?
A: 85
B: 88
C: 90
D: 92
E: 94
Answer: C

Question: What happens to thorium when it is exposed to air?
A: It becomes transparent.
B: It tarnishes olive gray, forming thorium dioxide.
C: It glows in the dark.
D: It dissolves.
E: It turns gold in color.
Answer: B

Question: Which oxidation state dominates the chemistry of thorium?
A: +1
B: +2
C: +3
D: +4
E: +5
Answer: D

Question: What is the half-life of the isotope 232Th?
A: 4.05 billion years
B: 10.05 billion years
C: 14.05 billion years
D: 20.05 billion years
E: 25.05 billion years
Answer: C

Question: Compared to uranium, how abundant is thorium in the Earth's crust?
A: About the same
B: Half as abundant
C: Twice as abundant
D: Three times as abundant
E: Over three times as abundant
Answer: E

Question: Who discovered thorium?
A: Albert Einstein
B: Marie Curie
C: Jöns Jacob Berzelius
D: Morten Thrane Esmark
E: Richard Feynman
Answer: D

Question: Who named thorium after the Norse god of thunder, Thor?
A: Morten Thrane Esmark
B: Albert Einstein
C: Jöns Jacob Berzelius
D: Marie Curie
E: Richard Feynman
Answer: C

Question: What is one use of thorium in the welding field?
A: As a cooling agent
B: As a fuel source
C: In the creation of masks
D: As an alloying element in TIG welding electrodes
E: As a lubricant
Answer: D

Question: Which of the following was NOT a past use of thorium?
A: In broadcast vacuum tubes
B: As a light source in gas mantles
C: In high-end optics and scientific instrumentation
D: As a primary material in building bridges
E: In heat-resistant ceramics
Answer: D

Question: For what purpose has the 231Pa/230Th isotope ratio been utilized in ocean science?
A: To detect underwater volcanoes
B: To study marine life behavior
C: To understand the ancient ocean
D: To locate sunken ships
E: To map the ocean floor
Answer: C
@
The thorium fuel cycle is a nuclear fuel cycle that uses an isotope of thorium, 232
Th
, as the fertile material. In the reactor, 232
Th
 is transmuted into the fissile artificial uranium isotope 233
U
 which is the nuclear fuel. Unlike natural uranium, natural thorium contains only trace amounts of fissile material (such as 231
Th
), which are insufficient to initiate a nuclear chain reaction. Additional fissile material or another neutron source is necessary to initiate the fuel cycle. In a thorium-fuelled reactor, 232
Th
 absorbs neutrons to produce 233
U
. This parallels the process in uranium breeder reactors whereby fertile 238
U
 absorbs neutrons to form fissile 239
Pu
. Depending on the design of the reactor and fuel cycle, the generated 233
U
 either fissions in situ or is chemically separated from the used nuclear fuel and formed into new nuclear fuel.

The thorium fuel cycle has several potential advantages over a uranium fuel cycle, including thorium's greater abundance, superior physical and nuclear properties, reduced plutonium and actinide production,[1] and better resistance to nuclear weapons proliferation when used in a traditional light water reactor[1][2] though not in a molten salt reactor.[3][4][5]
$
10
Question: Which isotope of thorium is used as the fertile material in the thorium fuel cycle?
A: 230Th
B: 231Th
C: 232Th
D: 233Th
E: 234Th
Answer: C

Question: What is the result of 232Th absorbing neutrons in a thorium-fuelled reactor?
A: 230U
B: 231U
C: 232U
D: 233U
E: 234U
Answer: D

Question: In uranium breeder reactors, which isotope of uranium absorbs neutrons to form fissile material?
A: 234U
B: 235U
C: 236U
D: 237U
E: 238U
Answer: E

Question: What is the fissile material formed in uranium breeder reactors when 238U absorbs neutrons?
A: 235U
B: 236U
C: 237U
D: 238U
E: 239Pu
Answer: E

Question: Natural thorium contains only trace amounts of fissile material such as:
A: 229Th
B: 230Th
C: 231Th
D: 232Th
E: 233Th
Answer: C

Question: What is necessary to initiate the thorium fuel cycle due to insufficient fissile material in natural thorium?
A: Additional fissile material or another neutron source
B: Extra heat
C: Chemical additives
D: Additional thorium isotopes
E: Radiation shielding
Answer: A

Question: How does the generated 233U behave in the thorium fuel cycle?
A: It remains stable and unused.
B: It decays into another isotope immediately.
C: It either fissions in situ or is chemically separated and formed into new nuclear fuel.
D: It is expelled as waste.
E: It is converted back into 232Th.
Answer: C

Question: Which of the following is NOT a potential advantage of the thorium fuel cycle over the uranium fuel cycle?
A: Thorium's lesser abundance
B: Superior physical and nuclear properties of thorium
C: Reduced plutonium and actinide production
D: Better resistance to nuclear weapons proliferation
E: Thorium's greater abundance
Answer: A

Question: In which type of reactor does thorium fuel cycle not show better resistance to nuclear weapons proliferation?
A: Heavy water reactor
B: Boiling water reactor
C: Fast breeder reactor
D: Traditional light water reactor
E: Molten salt reactor
Answer: E

Question: Which isotope of uranium acts as the nuclear fuel in a thorium fuel cycle?
A: 230U
B: 231U
C: 232U
D: 233U
E: 234U
Answer: D
@
Radioactive decay (also known as nuclear decay, radioactivity, radioactive disintegration, or nuclear disintegration) is the process by which an unstable atomic nucleus loses energy by radiation. A material containing unstable nuclei is considered radioactive. Three of the most common types of decay are alpha, beta, and gamma decay, all of which involve emitting particles. The weak force is the mechanism that is responsible for beta decay, while the other two are governed by the electromagnetism and nuclear force.[1]

Radioactive decay is a stochastic (i.e. random) process at the level of single atoms. According to quantum theory, it is impossible to predict when a particular atom will decay, regardless of how long the atom has existed.[2][3][4] However, for a significant number of identical atoms, the overall decay rate can be expressed as a decay constant or as half-life. The half-lives of radioactive atoms have a huge range; from nearly instantaneous to far longer than the age of the universe.

The decaying nucleus is called the parent radionuclide (or parent radioisotope[note 1]), and the process produces at least one daughter nuclide. Except for gamma decay or internal conversion from a nuclear excited state, the decay is a nuclear transmutation resulting in a daughter containing a different number of protons or neutrons (or both). When the number of protons changes, an atom of a different chemical element is created.

There are 28 naturally occurring chemical elements on Earth that are radioactive, consisting of 34 radionuclides (6 elements have 2 different radionuclides) that date before the time of formation of the Solar System. These 34 are known as primordial nuclides. Well-known examples are uranium and thorium, but also included are naturally occurring long-lived radioisotopes, such as potassium-40.
$
10
Question: What is the process by which an unstable atomic nucleus loses energy through radiation called?
A: Radiant disintegration
B: Nuclear radiation
C: Radioactive decay
D: Nuclear dispersion
E: Atomic radiation
Answer: C

Question: Which of the following is NOT a common type of radioactive decay?
A: Alpha
B: Beta
C: Gamma
D: Delta
E: Epsilon
Answer: D

Question: Which force is responsible for beta decay?
A: Gravitational force
B: Strong nuclear force
C: Electromagnetic force
D: Weak force
E: Frictional force
Answer: D

Question: On the level of single atoms, how can radioactive decay be best described?
A: Predictable
B: Stochastic
C: Linear
D: Static
E: Recurrent
Answer: B

Question: Can one predict when a particular atom will decay based on its age?
A: Yes, always
B: Yes, but only for certain atoms
C: No, it is impossible
D: Yes, but only for a group of atoms
E: Only if it's a primordial nuclide
Answer: C

Question: What term describes the time over which half of a given sample will have decayed?
A: Decay constant
B: Decay period
C: Nuclear time
D: Radioactive period
E: Half-life
Answer: E

Question: What is the nucleus called that is undergoing decay?
A: Daughter radionuclide
B: Decaying atom
C: Parent radionuclide
D: Radioactive particle
E: Decay initiator
Answer: C

Question: What is the outcome when the number of protons in a decaying nucleus changes?
A: An atom remains the same chemical element
B: An atom of a different chemical element is created
C: The atom becomes non-radioactive
D: The atom splits into multiple smaller atoms
E: The atom merges with another to form a larger atom
Answer: B

Question: How many naturally occurring chemical elements on Earth are radioactive?
A: 6
B: 14
C: 28
D: 34
E: 40
Answer: C

Question: Which of the following is NOT known as a primordial nuclide?
A: Uranium
B: Thorium
C: Potassium-40
D: Carbon-14
E: Hydrogen
Answer: D
@
Radioactivity was discovered in 1896 by scientists Henri Becquerel and Marie Skłodowska-Curie, while working with phosphorescent materials.[5][6][7][8][9] These materials glow in the dark after exposure to light, and Becquerel suspected that the glow produced in cathode ray tubes by X-rays might be associated with phosphorescence. He wrapped a photographic plate in black paper and placed various phosphorescent salts on it. All results were negative until he used uranium salts. The uranium salts caused a blackening of the plate in spite of the plate being wrapped in black paper. These radiations were given the name "Becquerel Rays".

It soon became clear that the blackening of the plate had nothing to do with phosphorescence, as the blackening was also produced by non-phosphorescent salts of uranium and by metallic uranium. It became clear from these experiments that there was a form of invisible radiation that could pass through paper and was causing the plate to react as if exposed to light.

At first, it seemed as though the new radiation was similar to the then recently discovered X-rays. Further research by Becquerel, Ernest Rutherford, Paul Villard, Pierre Curie, Marie Curie, and others showed that this form of radioactivity was significantly more complicated. Rutherford was the first to realize that all such elements decay in accordance with the same mathematical exponential formula. Rutherford and his student Frederick Soddy were the first to realize that many decay processes resulted in the transmutation of one element to another. Subsequently, the radioactive displacement law of Fajans and Soddy was formulated to describe the products of alpha and beta decay.[10][11]

The early researchers also discovered that many other chemical elements, besides uranium, have radioactive isotopes. A systematic search for the total radioactivity in uranium ores also guided Pierre and Marie Curie to isolate two new elements: polonium and radium. Except for the radioactivity of radium, the chemical similarity of radium to barium made these two elements difficult to distinguish.

Marie and Pierre Curie's study of radioactivity is an important factor in science and medicine. After their research on Becquerel's rays led them to the discovery of both radium and polonium, they coined the term "radioactivity"[12] to define the emission of ionizing radiation by some heavy elements.[13] (Later the term was generalized to all elements.) Their research on the penetrating rays in uranium and the discovery of radium launched an era of using radium for the treatment of cancer. Their exploration of radium could be seen as the first peaceful use of nuclear energy and the start of modern nuclear medicine.[12]v
$
10
Question: Who discovered radioactivity in 1896?
A: Henri Poincaré and Albert Einstein
B: Pierre Curie and Paul Villard
C: Henri Becquerel and Marie Skłodowska-Curie
D: Ernest Rutherford and Frederick Soddy
E: Fajans and Soddy
Answer: C

Question: What did Becquerel initially suspect was associated with the glow produced in cathode ray tubes by X-rays?
A: Radioactivity
B: Phosphorescence
C: Radium
D: Uranium radiation
E: Ionizing radiation
Answer: B

Question: Which substance caused a blackening of the photographic plate even when wrapped in black paper?
A: Radium
B: Polonium
C: Phosphorescent salts
D: Uranium salts
E: Barium
Answer: D

Question: The blackening of the plate was not related to phosphorescence but was a result of what?
A: Exposure to light
B: Invisible radiation passing through paper
C: Chemical reactions of the plate
D: Heat from the substances
E: Alpha particles from radium
Answer: B

Question: Who first realized that radioactive elements decay according to a mathematical exponential formula?
A: Marie Skłodowska-Curie
B: Pierre Curie
C: Paul Villard
D: Fajans
E: Ernest Rutherford
Answer: E

Question: The radioactive displacement law was formulated to describe the products of which types of decay?
A: Gamma and delta
B: X-ray and ultraviolet
C: Alpha and beta
D: Beta and gamma
E: Delta and alpha
Answer: C

Question: Which two new elements were isolated by Pierre and Marie Curie from uranium ores?
A: Thorium and beryllium
B: Polonium and radium
C: Barium and plutonium
D: Uranium and plutonium
E: Radium and thorium
Answer: B

Question: What made radium and barium difficult to distinguish from one another?
A: Their radioactive properties
B: Their chemical similarity
C: Their position on the periodic table
D: Their similar atomic weights
E: Their discovery by the same scientists
Answer: B

Question: Who coined the term "radioactivity"?
A: Ernest Rutherford and Frederick Soddy
B: Paul Villard and Fajans
C: Henri Becquerel and Paul Villard
D: Pierre and Marie Curie
E: Henri Poincaré and Albert Einstein
Answer: D

Question: The exploration of radium by the Curies can be seen as the starting point for which field?
A: Quantum mechanics
B: Electromagnetic studies
C: Modern nuclear medicine
D: Chemical bonding research
E: Astrophysics
Answer: C
@
Radioactive decay results in a reduction of summed rest mass, once the released energy (the disintegration energy) has escaped in some way. Although decay energy is sometimes defined as associated with the difference between the mass of the parent nuclide products and the mass of the decay products, this is true only of rest mass measurements, where some energy has been removed from the product system. This is true because the decay energy must always carry mass with it, wherever it appears (see mass in special relativity) according to the formula E = mc2. The decay energy is initially released as the energy of emitted photons plus the kinetic energy of massive emitted particles (that is, particles that have rest mass). If these particles come to thermal equilibrium with their surroundings and photons are absorbed, then the decay energy is transformed to thermal energy, which retains its mass.

Decay energy, therefore, remains associated with a certain measure of the mass of the decay system, called invariant mass, which does not change during the decay, even though the energy of decay is distributed among decay particles. The energy of photons, the kinetic energy of emitted particles, and, later, the thermal energy of the surrounding matter, all contribute to the invariant mass of the system. Thus, while the sum of the rest masses of the particles is not conserved in radioactive decay, the system mass and system invariant mass (and also the system total energy) is conserved throughout any decay process. This is a restatement of the equivalent laws of conservation of energy and conservation of mass.

Early researchers found that an electric or magnetic field could split radioactive emissions into three types of beams. The rays were given the names alpha, beta, and gamma, in increasing order of their ability to penetrate matter. Alpha decay is observed only in heavier elements of atomic number 52 (tellurium) and greater, with the exception of beryllium-8 (which decays to two alpha particles). The other two types of decay are observed in all the elements. Lead, atomic number 82, is the heaviest element to have any isotopes stable (to the limit of measurement) to radioactive decay. Radioactive decay is seen in all isotopes of all elements of atomic number 83 (bismuth) or greater. Bismuth-209, however, is only very slightly radioactive, with a half-life greater than the age of the universe; radioisotopes with extremely long half-lives are considered effectively stable for practical purposes.
$
10
Question: Radioactive decay leads to a reduction in which of the following?
A: Total energy
B: Photon energy
C: Summed rest mass
D: Invariant mass
E: Atomic number
Answer: C

Question: What carries mass with it according to the formula E = mc^2?
A: Kinetic energy
B: Thermal energy
C: Decay energy
D: Photon energy
E: Invariant mass
Answer: C

Question: Decay energy eventually transforms into which kind of energy if particles come to thermal equilibrium and photons are absorbed?
A: Alpha energy
B: Photon energy
C: Kinetic energy
D: Gamma energy
E: Thermal energy
Answer: E

Question: The invariant mass of a decay system remains unchanged during what process?
A: Absorption of photons
B: Emission of beta particles
C: Radioactive decay
D: Release of kinetic energy
E: Thermal equilibrium
Answer: C

Question: Which two laws are restated by the principle that system mass and system invariant mass are conserved throughout any decay process?
A: Conservation of energy and conservation of momentum
B: Conservation of energy and conservation of mass
C: Conservation of mass and conservation of velocity
D: Conservation of momentum and conservation of velocity
E: Conservation of thermal energy and conservation of photon energy
Answer: B

Question: Electric or magnetic fields were used by early researchers to divide radioactive emissions into how many types of beams?
A: Two
B: Three
C: Four
D: Five
E: Six
Answer: B

Question: Which type of decay is typically observed in elements with atomic number 52 and greater, except for beryllium-8?
A: Gamma decay
B: Beta decay
C: Alpha decay
D: Photon decay
E: Invariant decay
Answer: C

Question: Which is the heaviest element to have isotopes that are stable with respect to radioactive decay?
A: Bismuth
B: Tellurium
C: Lead
D: Uranium
E: Thorium
Answer: C

Question: All isotopes of elements with an atomic number of 83 or greater exhibit what characteristic?
A: They are stable.
B: They undergo gamma decay.
C: They are radioactive.
D: They undergo beta decay.
E: They have short half-lives.
Answer: C

Question: Bismuth-209 is unique because its half-life is:
A: Less than a second.
B: Equal to the age of the Earth.
C: The shortest among radioactive elements.
D: Greater than the age of the universe.
E: Equivalent to that of uranium.
Answer: D
@
The Big Bang event is a physical theory that describes how the universe expanded from an initial state of high density and temperature.[1] Various cosmological models of the Big Bang explain the evolution of the observable universe from the earliest known periods through its subsequent large-scale form.[2][3][4] These models offer a comprehensive explanation for a broad range of observed phenomena, including the abundance of light elements, the cosmic microwave background (CMB) radiation, and large-scale structure. The overall uniformity of the Universe, known as the flatness problem, is explained through cosmic inflation: a sudden and very rapid expansion of space during the earliest moments. However, physics currently lacks a widely accepted theory of quantum gravity that can successfully model the earliest conditions of the Big Bang.

Crucially, these models are compatible with the Hubble–Lemaître law—the observation that the farther away a galaxy is, the faster it is moving away from Earth. Extrapolating this cosmic expansion backwards in time using the known laws of physics, the models describe an increasingly concentrated cosmos preceded by a singularity in which space and time lose meaning (typically named "the Big Bang singularity").[5] In 1964 the CMB was discovered, which convinced many cosmologists that the competing steady-state model of cosmic evolution was falsified,[6] since the Big Bang models predict a uniform background radiation caused by high temperatures and densities in the distant past. A wide range of empirical evidence strongly favors the Big Bang event, which is now essentially universally accepted.[7] Detailed measurements of the expansion rate of the universe place the Big Bang singularity at an estimated 13.787±0.020 billion years ago, which is considered the age of the universe.[8]

There remain aspects of the observed universe that are not yet adequately explained by the Big Bang models. After its initial expansion, the universe cooled sufficiently to allow the formation of subatomic particles, and later atoms. The unequal abundances of matter and antimatter that allowed this to occur is an unexplained effect known as baryon asymmetry. These primordial elements—mostly hydrogen, with some helium and lithium—later coalesced through gravity, forming early stars and galaxies. Astronomers observe the gravitational effects of an unknown dark matter surrounding galaxies. Most of the gravitational potential in the universe seems to be in this form, and the Big Bang models and various observations indicate that this excess gravitational potential is not created by baryonic matter, such as normal atoms. Measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating, an observation attributed to an unexplained phenomenon known as dark energy.[9]
$
10
Question: What does the Big Bang theory describe?
A: The formation of galaxies and stars.
B: The universe's contraction from an expansive state.
C: The universe's expansion from a state of high density and temperature.
D: The emergence of life in the universe.
E: The creation of planets and solar systems.
Answer: C

Question: Which phenomenon is explained by cosmic inflation during the earliest moments of the universe?
A: Baryon asymmetry
B: Gravitational effects of dark matter
C: The flatness problem
D: Formation of subatomic particles
E: Hubble–Lemaître law
Answer: C

Question: What current limitation does physics face with regard to the Big Bang?
A: Inability to explain dark energy.
B: Lack of a theory of quantum gravity for the earliest conditions.
C: Absence of data supporting cosmic microwave background.
D: Lack of compatibility with the Hubble–Lemaître law.
E: Inadequate understanding of cosmic inflation.
Answer: B

Question: What evidence in 1964 strengthened the case for the Big Bang theory?
A: Discovery of cosmic inflation.
B: Discovery of the Hubble–Lemaître law.
C: Observation of dark energy.
D: Discovery of the cosmic microwave background.
E: Observation of dark matter surrounding galaxies.
Answer: D

Question: The Big Bang singularity is estimated to have occurred how long ago?
A: 10.000±0.005 billion years ago.
B: 15.000±0.025 billion years ago.
C: 13.787±0.020 billion years ago.
D: 12.500±0.015 billion years ago.
E: 11.250±0.010 billion years ago.
Answer: C

Question: Baryon asymmetry refers to:
A: The rapid expansion of space.
B: The gravitational effects of an unknown dark matter.
C: The uneven distribution of matter and antimatter.
D: The uniformity of the universe.
E: The redshifts of supernovae.
Answer: C

Question: Which elements primarily formed through gravity after the Big Bang event?
A: Carbon, nitrogen, and oxygen.
B: Iron, magnesium, and sulfur.
C: Hydrogen, helium, and lithium.
D: Gold, silver, and copper.
E: Neptunium, plutonium, and americium.
Answer: C

Question: The majority of the gravitational potential in the universe is attributed to:
A: Baryonic matter.
B: Cosmic microwave background.
C: Dark energy.
D: Dark matter.
E: Helium and lithium.
Answer: D

Question: What observation is linked to the phenomenon known as dark energy?
A: The rapid cooling of the universe.
B: The even distribution of matter and antimatter.
C: The accelerating expansion of the universe.
D: The creation of the cosmic microwave background.
E: The formation of the earliest galaxies.
Answer: C

Question: Which of the following is not explained adequately by the Big Bang models?
A: The redshifts of supernovae.
B: The formation of subatomic particles.
C: Baryon asymmetry.
D: The uniformity of the universe.
E: Cosmic inflation.
Answer: C
@
The earliest phases of the Big Bang are subject to much speculation, since astronomical data about them are not available. In the most common models the universe was filled homogeneously and isotropically with a very high energy density and huge temperatures and pressures, and was very rapidly expanding and cooling. The period up to 10−43 seconds into the expansion, the Planck epoch, was a phase in which the four fundamental forces — the electromagnetic force, the strong nuclear force, the weak nuclear force, and the gravitational force, were unified as one.[24] In this stage, the characteristic scale length of the universe was the Planck length, 1.6×10−35 m, and consequently had a temperature of approximately 1032 degrees Celsius. Even the very concept of a particle breaks down in these conditions. A proper understanding of this period awaits the development of a theory of quantum gravity.[25][26] The Planck epoch was succeeded by the grand unification epoch beginning at 10−43 seconds, where gravitation separated from the other forces as the universe's temperature fell.[24]

At approximately 10−37 seconds into the expansion, a phase transition caused a cosmic inflation, during which the universe grew exponentially, unconstrained by the light speed invariance, and temperatures dropped by a factor of 100,000. This concept is motivated by the flatness problem, where the density of matter and energy is very close to the critical density needed to produce a flat universe. That is, the shape of the universe has no overall geometric curvature due to gravitational influence. Microscopic quantum fluctuations that occurred because of Heisenberg's uncertainty principle were "frozen in" by inflation, becoming amplified into the seeds that would later form the large-scale structure of the universe.[27] At a time around 10−36 seconds, the electroweak epoch begins when the strong nuclear force separates from the other forces, with only the electromagnetic force and weak nuclear force remaining unified.[28]

Inflation stopped locally at around the 10−33 to 10−32 seconds mark, with the observable universe's volume having increased by a factor of at least 1078. Reheating occurred until the universe obtained the temperatures required for the production of a quark–gluon plasma as well as all other elementary particles.[29][30] Temperatures were so high that the random motions of particles were at relativistic speeds, and particle–antiparticle pairs of all kinds were being continuously created and destroyed in collisions.[1] At some point, an unknown reaction called baryogenesis violated the conservation of baryon number, leading to a very small excess of quarks and leptons over antiquarks and antileptons—of the order of one part in 30 million. This resulted in the predominance of matter over antimatter in the present universe.[31]
$
10
Question: In the earliest phases of the Big Bang, how were the four fundamental forces characterized?
A: They were separate and distinct.
B: Only the electromagnetic force existed.
C: They were unified as one.
D: They alternated in dominance.
E: They had no influence at all.
Answer: C

Question: Which epoch began at 10−43 seconds into the expansion?
A: Electroweak epoch
B: Cosmic inflation epoch
C: Reheating epoch
D: Grand unification epoch
E: Baryogenesis epoch
Answer: D

Question: What was the characteristic scale length of the universe during the Planck epoch?
A: 1.6×10^35 m
B: 10^43 m
C: 1.6 m
D: 1.6×10^−35 m
E: 10^−43 m
Answer: D

Question: Cosmic inflation caused the universe to expand:
A: At the speed of light.
B: Exponentially without the constraint of light speed.
C: Linearly with time.
D: At half the speed of light.
E: Only at a microscopic level.
Answer: B

Question: The flatness problem is related to:
A: The rapid expansion of the universe.
B: The dominance of matter over antimatter.
C: The critical density needed for a flat universe.
D: The separation of the strong nuclear force.
E: The concept of a particle breaking down.
Answer: C

Question: Which principle is responsible for microscopic quantum fluctuations during inflation?
A: Principle of relativity
B: Newton's third law
C: Heisenberg's uncertainty principle
D: Planck's radiation law
E: Bernoulli's principle
Answer: C

Question: At what point did the electroweak epoch begin?
A: 10^36 seconds into the expansion
B: 10^−36 seconds into the expansion
C: 10^−33 seconds into the expansion
D: 10^−43 seconds into the expansion
E: 10^−32 seconds into the expansion
Answer: B

Question: The process of baryogenesis resulted in:
A: Equal amounts of matter and antimatter.
B: Predominance of antimatter over matter.
C: Exponential growth of the universe.
D: Predominance of matter over antimatter.
E: Separation of the four fundamental forces.
Answer: D

Question: Which particles were being continuously created and destroyed in collisions due to high temperatures?
A: Only quarks
B: Only leptons
C: Particle–antiparticle pairs of all kinds
D: Only baryons
E: Only electrons
Answer: C

Question: By how much did the observable universe's volume increase by the end of the local inflation?
A: By a factor of 10^78
B: By a factor of 10^−33
C: By a factor of 10^−37
D: By a factor of 10^−32
E: By a factor of at least 10^78
Answer: E
@
The Big Bang models developed from observations of the structure of the universe and from theoretical considerations. In 1912, Vesto Slipher measured the first Doppler shift of a "spiral nebula" (spiral nebula is the obsolete term for spiral galaxies), and soon discovered that almost all such nebulae were receding from Earth. He did not grasp the cosmological implications of this fact, and indeed at the time it was highly controversial whether or not these nebulae were "island universes" outside our Milky Way.[56][57] Ten years later, Alexander Friedmann, a Russian cosmologist and mathematician, derived the Friedmann equations from the Einstein field equations, showing that the universe might be expanding in contrast to the static universe model advocated by Albert Einstein at that time.[58]

In 1924, American astronomer Edwin Hubble's measurement of the great distance to the nearest spiral nebulae showed that these systems were indeed other galaxies. Starting that same year, Hubble painstakingly developed a series of distance indicators, the forerunner of the cosmic distance ladder, using the 100-inch (2.5 m) Hooker telescope at Mount Wilson Observatory. This allowed him to estimate distances to galaxies whose redshifts had already been measured, mostly by Slipher. In 1929, Hubble discovered a correlation between distance and recessional velocity—now known as Hubble's law.[59][60]

Independently deriving Friedmann's equations in 1927, Georges Lemaître, a Belgian physicist and Roman Catholic priest, proposed that the recession of the nebulae was due to the expansion of the universe.[61] He inferred the relation that Hubble would later observe, given the cosmological principle.[9] In 1931, Lemaître went further and suggested that the evident expansion of the universe, if projected back in time, meant that the further in the past the smaller the universe was, until at some finite time in the past all the mass of the universe was concentrated into a single point, a "primeval atom" where and when the fabric of time and space came into existence.[62]

In the 1920s and 1930s, almost every major cosmologist preferred an eternal steady-state universe, and several complained that the beginning of time implied by the Big Bang imported religious concepts into physics; this objection was later repeated by supporters of the steady-state theory.[63] This perception was enhanced by the fact that the originator of the Big Bang concept, Lemaître, was a Roman Catholic priest.[64] Arthur Eddington agreed with Aristotle that the universe did not have a beginning in time, viz., that matter is eternal. A beginning in time was "repugnant" to him.[65][66] Lemaître, however, disagreed:

If the world has begun with a single quantum, the notions of space and time would altogether fail to have any meaning at the beginning; they would only begin to have a sensible meaning when the original quantum had been divided into a sufficient number of quanta. If this suggestion is correct, the beginning of the world happened a little before the beginning of space and time.[67]
$
10
Question: Who measured the first Doppler shift of a "spiral nebula"?
A: Vesto Slipher
B: Edwin Hubble
C: Alexander Friedmann
D: Georges Lemaître
E: Arthur Eddington
Answer: A

Question: What did Vesto Slipher discover about almost all "spiral nebulae"?
A: They were static.
B: They were receding from Earth.
C: They were moving towards Earth.
D: They were part of the Milky Way.
E: They were rapidly rotating.
Answer: B

Question: Alexander Friedmann derived equations that suggested the universe:
A: Was contracting.
B: Was static.
C: Was fluctuating.
D: Might be expanding.
E: Had no structure.
Answer: D

Question: Who proposed the concept of the "primeval atom"?
A: Vesto Slipher
B: Edwin Hubble
C: Alexander Friedmann
D: Georges Lemaître
E: Arthur Eddington
Answer: D

Question: Hubble's discovery of the correlation between distance and recessional velocity is known as:
A: Friedmann's principle.
B: Lemaître's law.
C: Eddington's theory.
D: Doppler's shift.
E: Hubble's law.
Answer: E

Question: In 1924, Edwin Hubble confirmed that "spiral nebulae" were actually:
A: Nebulous clouds.
B: Stars within the Milky Way.
C: Other galaxies.
D: Gas formations.
E: Asteroid clusters.
Answer: C

Question: Who independently derived Friedmann's equations and inferred the relation that would later be observed by Hubble?
A: Vesto Slipher
B: Edwin Hubble
C: Alexander Friedmann
D: Georges Lemaître
E: Arthur Eddington
Answer: D

Question: Many cosmologists in the 1920s and 1930s preferred which model of the universe?
A: Expanding universe
B: Contracting universe
C: Big Bang universe
D: Steady-state universe
E: Primeval atom universe
Answer: D

Question: The objection to the beginning of time implied by the Big Bang was seen as importing what into physics?
A: Mathematical concepts
B: Religious concepts
C: Aesthetic concepts
D: Philosophical concepts
E: Quantum mechanics
Answer: B

Question: Lemaître suggested that the beginning of the world occurred a little before:
A: The beginning of galaxies.
B: The beginning of the universe.
C: The formation of the first star.
D: The beginning of space and time.
E: The Big Bang.
Answer: D
@
The cosmic microwave background (CMB, CMBR) is microwave radiation that fills all space in the observable universe. It is a remnant that provides an important source of data on the primordial universe.[1] With a standard optical telescope, the background space between stars and galaxies is almost completely dark. However, a sufficiently sensitive radio telescope detects a faint background glow that is almost uniform and is not associated with any star, galaxy, or other object. This glow is strongest in the microwave region of the radio spectrum. The accidental discovery of the CMB in 1965 by American radio astronomers Arno Penzias and Robert Wilson was the culmination of work initiated in the 1940s.[2][3]

CMB is landmark evidence of the Big Bang theory for the origin of the universe. In the Big Bang cosmological models, during the earliest periods, the universe was filled with an opaque fog of dense, hot plasma of sub-atomic particles. As the universe expanded, this plasma cooled to the point where protons and electrons combined to form neutral atoms of mostly hydrogen. Unlike the plasma, these atoms could not scatter thermal radiation by Thomson scattering, and so the universe became transparent.[4] Known as the recombination epoch, this decoupling event released photons to travel freely through space – sometimes referred to as relic radiation.[1] However, the photons have grown less energetic due to the cosmological redshift associated with the expansion of the universe. The surface of last scattering refers to a shell at the right distance in space so photons are now received that were originally emitted at the time of decoupling.[5]
$
10
Question: What is the cosmic microwave background (CMB) radiation?
A: A type of radio signal from galaxies.
B: Radiation that fills all space in the observable universe.
C: Radiation emitted from stars.
D: A form of energy produced by black holes.
E: A signal from extraterrestrial civilizations.
Answer: B

Question: Using a standard optical telescope, the space between stars and galaxies appears to be:
A: Filled with faint microwave glow.
B: Reflective and shimmering.
C: Dominated by relic radiation.
D: Almost completely dark.
E: Dominated by the signals of the CMB.
Answer: D

Question: In which part of the radio spectrum is the glow of the CMB strongest?
A: Infrared
B: Ultraviolet
C: Gamma
D: X-ray
E: Microwave
Answer: E

Question: Who accidentally discovered the CMB in 1965?
A: Thomson and Redshift
B: Arno Penzias and Robert Wilson
C: Neutral Atom and Recombination Epoch
D: Plasma and Photon
E: Proton and Electron
Answer: B

Question: What is the CMB considered as evidence for?
A: The Steady State theory.
B: The Big Crunch hypothesis.
C: The Black Hole theory.
D: The Big Bang theory for the origin of the universe.
E: The Cosmic Inflation theory.
Answer: D

Question: During the earliest periods of the universe, it was filled with:
A: Cold hydrogen gas.
B: An opaque fog of dense, hot plasma of sub-atomic particles.
C: Highly energetic gamma rays.
D: Neutral atoms of helium.
E: Empty space and voids.
Answer: B

Question: As the universe expanded and cooled, what did protons and electrons combine to form?
A: Atoms of helium.
B: Thermal radiation.
C: Neutrons.
D: Neutral atoms of mostly hydrogen.
E: Dark matter particles.
Answer: D

Question: What is the recombination epoch associated with?
A: The time when galaxies first formed.
B: The period when plasma turned into gases.
C: The event when photons were released to travel freely through space.
D: The moment of the Big Bang.
E: The creation of black holes.
Answer: C

Question: Over time, why have the photons from the recombination epoch grown less energetic?
A: Due to their interaction with dark matter.
B: Due to collisions with other photons.
C: Because of absorption by galaxies.
D: Due to the cosmological redshift associated with the expansion of the universe.
E: Due to the cooling effects of space.
Answer: D

Question: What does the term "surface of last scattering" refer to?
A: The final state of photons before they disappear.
B: The layer in a star where photons are emitted.
C: The shell in space where photons are now received that were originally emitted at the time of decoupling.
D: The boundary of the observable universe.
E: The point where photons scatter the most.
Answer: C
@
The cosmic microwave background radiation is an emission of uniform, black body thermal energy coming from all parts of the sky. The radiation is isotropic to roughly one part in 100,000: the root mean square variations are only 18 μK,[11] after subtracting out a dipole anisotropy from the Doppler shift of the background radiation. The latter is caused by the peculiar velocity of the Sun relative to the comoving cosmic rest frame as it moves at 369.82 ± 0.11 km/s towards the constellation Leo (galactic longitude 264.021 ± 0.011, galactic latitude 48.253 ± 0.005).[12] The CMB dipole and aberration at higher multipoles have been measured, consistent with galactic motion.[13]

In the Big Bang model for the formation of the universe, inflationary cosmology predicts that after about 10−37 seconds[14] the nascent universe underwent exponential growth that smoothed out nearly all irregularities. The remaining irregularities were caused by quantum fluctuations in the inflaton field that caused the inflation event.[15] Long before the formation of stars and planets, the early universe was smaller, much hotter and, starting 10−6 seconds after the Big Bang, filled with a uniform glow from its white-hot fog of interacting plasma of photons, electrons, and baryons.

As the universe expanded, adiabatic cooling caused the energy density of the plasma to decrease until it became favorable for electrons to combine with protons, forming hydrogen atoms. This recombination event happened when the temperature was around 3000 K or when the universe was approximately 379,000 years old.[16] As photons did not interact with these electrically neutral atoms, the former began to travel freely through space, resulting in the decoupling of matter and radiation.[17]

The color temperature of the ensemble of decoupled photons has continued to diminish ever since; now down to 2.7260±0.0013 K,[6] it will continue to drop as the universe expands. The intensity of the radiation corresponds to black-body radiation at 2.726 K because red-shifted black-body radiation is just like black-body radiation at a lower temperature. According to the Big Bang model, the radiation from the sky we measure today comes from a spherical surface called the surface of last scattering. This represents the set of locations in space at which the decoupling event is estimated to have occurred[18] and at a point in time such that the photons from that distance have just reached observers. Most of the radiation energy in the universe is in the cosmic microwave background,[19] making up a fraction of roughly 6×10−5 of the total density of the universe.[20]

Two of the greatest successes of the Big Bang theory are its prediction of the almost perfect black body spectrum and its detailed prediction of the anisotropies in the cosmic microwave background. The CMB spectrum has become the most precisely measured black body spectrum in nature.[10]

The energy density of the CMB is 0.260 eV/cm3 (4.17×10−14 J/m3) which yields about 411 photons/cm3.[21]
$
10
Question: What is the cosmic microwave background radiation characterized by?
A: A non-uniform, fluctuating energy emission.
B: A burst of energy in one particular direction of the sky.
C: An emission of uniform, black body thermal energy from all parts of the sky.
D: A series of high-frequency gamma rays.
E: A reflection of energy from stars and galaxies.
Answer: C

Question: To what precision is the radiation of CMB isotropic?
A: Roughly one part in 1,000.
B: Roughly one part in 10,000.
C: Roughly one part in 100,000.
D: Roughly one part in 500,000.
E: Roughly one part in 1,000,000.
Answer: C

Question: Which constellation is the Sun moving towards, causing the Doppler shift of the background radiation?
A: Taurus
B: Ursa Major
C: Orion
D: Scorpius
E: Leo
Answer: E

Question: Inflationary cosmology predicts that after about how long did the universe undergo exponential growth?
A: 10 seconds
B: 10^-37 seconds
C: 1 minute
D: 10^-6 seconds
E: 1 hour
Answer: B

Question: What caused the remaining irregularities after the inflation event?
A: Cosmic dust and dark matter.
B: Collisions between galaxies.
C: Quantum fluctuations in the inflaton field.
D: The movement of stars and planets.
E: The decay of primordial elements.
Answer: C

Question: The recombination event occurred when the universe's temperature was approximately:
A: 1000 K.
B: 5000 K.
C: 2000 K.
D: 4500 K.
E: 3000 K.
Answer: E

Question: The radiation from the sky measured today comes from:
A: The center of the Milky Way.
B: The edges of our solar system.
C: The surface of last scattering.
D: The time of the Big Bang.
E: Nearby star systems.
Answer: C

Question: How much of the total density of the universe is made up by most of the radiation energy?
A: 6x10^-3
B: 6x10^-5
C: 6x10^-2
D: 6x10^-4
E: 6x10^-1
Answer: B

Question: Which spectrum has the cosmic microwave background become the most precisely measured example of?
A: Gamma-ray spectrum.
B: Ultraviolet spectrum.
C: Infrared spectrum.
D: X-ray spectrum.
E: Black body spectrum.
Answer: E

Question: What is the energy density of the CMB?
A: 0.260 eV/cm^3
B: 0.500 eV/cm^3
C: 1.00 eV/cm^3
D: 0.100 eV/cm^3
E: 2.726 eV/cm^3
Answer: A
@
Subsequent to the discovery of the CMB, hundreds of cosmic microwave background experiments have been conducted to measure and characterize the signatures of the radiation. The most famous experiment is probably the NASA Cosmic Background Explorer (COBE) satellite that orbited in 1989–1996 and which detected and quantified the large scale anisotropies at the limit of its detection capabilities. Inspired by the initial COBE results of an extremely isotropic and homogeneous background, a series of ground- and balloon-based experiments quantified CMB anisotropies on smaller angular scales over the next decade. The primary goal of these experiments was to measure the angular scale of the first acoustic peak, for which COBE did not have sufficient resolution. These measurements were able to rule out cosmic strings as the leading theory of cosmic structure formation, and suggested cosmic inflation was the right theory.

During the 1990s, the first peak was measured with increasing sensitivity and by 2000 the BOOMERanG experiment reported that the highest power fluctuations occur at scales of approximately one degree. Together with other cosmological data, these results implied that the geometry of the universe is flat. A number of ground-based interferometers provided measurements of the fluctuations with higher accuracy over the next three years, including the Very Small Array, Degree Angular Scale Interferometer (DASI), and the Cosmic Background Imager (CBI). DASI made the first detection of the polarization of the CMB and the CBI provided the first E-mode polarization spectrum with compelling evidence that it is out of phase with the T-mode spectrum.
$
10
Question: What was the main purpose of the NASA Cosmic Background Explorer (COBE) satellite?
A: To detect cosmic strings in the universe.
B: To measure the density of dark matter.
C: To measure and quantify the large scale anisotropies of the CMB.
D: To explore the possibility of multiple universes.
E: To monitor the movement of galaxies.
Answer: C

Question: What was a major finding of the COBE satellite about the CMB?
A: It found high-frequency fluctuations.
B: It found the CMB to be extremely isotropic and homogeneous.
C: It detected the presence of cosmic strings.
D: It confirmed the theory of cosmic inflation was incorrect.
E: It suggested that the universe is not flat.
Answer: B

Question: What was the primary aim of the ground- and balloon-based experiments following COBE's findings?
A: To study the polarization of the CMB.
B: To detect the presence of dark energy.
C: To measure the angular scale of the first acoustic peak.
D: To understand the nature of cosmic strings.
E: To explore the possibility of a multiverse.
Answer: C

Question: By the year 2000, which experiment reported that the highest power fluctuations of the CMB occur at scales of about one degree?
A: Cosmic Background Imager (CBI).
B: Very Small Array.
C: Degree Angular Scale Interferometer (DASI).
D: BOOMERanG experiment.
E: NASA Cosmic Background Explorer (COBE).
Answer: D

Question: What did the results of the BOOMERanG experiment, combined with other data, suggest about the geometry of the universe?
A: It is circular.
B: It is irregular.
C: It is flat.
D: It is a closed loop.
E: It is helical.
Answer: C

Question: Which experiment made the first detection of the polarization of the CMB?
A: BOOMERanG experiment.
B: Cosmic Background Imager (CBI).
C: Very Small Array.
D: Degree Angular Scale Interferometer (DASI).
E: NASA Cosmic Background Explorer (COBE).
Answer: D

Question: The Cosmic Background Imager (CBI) provided evidence that the E-mode polarization spectrum is:
A: In phase with the T-mode spectrum.
B: Higher in frequency than the T-mode spectrum.
C: Out of phase with the T-mode spectrum.
D: Non-existent in the CMB.
E: Equal in intensity to the T-mode spectrum.
Answer: C

Question: Which theory of cosmic structure formation was ruled out by measurements of the CMB anisotropies?
A: Cosmic inflation.
B: Dark matter theory.
C: Multiverse hypothesis.
D: Cosmic strings.
E: Big Crunch theory.
Answer: D

Question: As a result of the CMB measurements, which theory gained more credibility for cosmic structure formation?
A: Cosmic inflation.
B: Dark matter theory.
C: Multiverse hypothesis.
D: Cosmic strings.
E: Big Crunch theory.
Answer: A

Question: The Very Small Array, Degree Angular Scale Interferometer (DASI), and the Cosmic Background Imager (CBI) were all designed to provide:
A: Initial measurements of CMB.
B: Measurements of the fluctuations with higher accuracy.
C: Data on cosmic strings and their impact on the universe.
D: Evidence against the Big Bang theory.
E: Understanding of the multiverse hypothesis.
Answer: B
@
Cosmic strings are hypothetical 1-dimensional topological defects which may have formed during a symmetry-breaking phase transition in the early universe when the topology of the vacuum manifold associated to this symmetry breaking was not simply connected. Their existence was first contemplated by the theoretical physicist Tom Kibble in the 1970s.[1]

The formation of cosmic strings is somewhat analogous to the imperfections that form between crystal grains in solidifying liquids, or the cracks that form when water freezes into ice. The phase transitions leading to the production of cosmic strings are likely to have occurred during the earliest moments of the universe's evolution, just after cosmological inflation, and are a fairly generic prediction in both quantum field theory and string theory models of the early universe.

In string theory, the role of cosmic strings can be played by the fundamental strings (or F-strings) themselves that define the theory perturbatively, by D-strings which are related to the F-strings by weak-strong or so called S-duality, or higher-dimensional D-, NS- or M-branes that are partially wrapped on compact cycles associated to extra spacetime dimensions so that only one non-compact dimension remains.[2]

The prototypical example of a quantum field theory with cosmic strings is the Abelian Higgs model. The quantum field theory and string theory cosmic strings are expected to have many properties in common, but more research is needed to determine the precise distinguishing features. The F-strings for instance are fully quantum-mechanical and do not have a classical definition, whereas the field theory cosmic strings are almost exclusively treated classically.
$
10
Question: What are cosmic strings?
A: 2-dimensional objects formed after the Big Bang.
B: Prototypes used to understand quantum field theory.
C: Hypothetical 1-dimensional topological defects from the early universe.
D: Threads that tie galaxies together.
E: Oscillating energy waves detected by radio telescopes.
Answer: C

Question: Who first contemplated the existence of cosmic strings in the 1970s?
A: Albert Einstein.
B: Richard Feynman.
C: Tom Kibble.
D: Stephen Hawking.
E: Brian Greene.
Answer: C

Question: Which analogy is used to describe the formation of cosmic strings?
A: The spiral of galaxies.
B: Black holes and wormholes connecting different parts of the universe.
C: The imperfections formed between crystal grains when liquids solidify.
D: The radiation emitted by a supernova explosion.
E: The orbits of planets around the sun.
Answer: C

Question: When are the phase transitions that lead to the production of cosmic strings believed to have occurred?
A: Just before the Big Bang.
B: Right after the occurrence of dark energy.
C: Millions of years after the Big Bang.
D: Just after cosmological inflation.
E: During the formation of galaxies.
Answer: D

Question: Which of the following is NOT a type of string or object related to string theory's role in cosmic strings?
A: P-strings.
B: F-strings.
C: D-strings.
D: NS-branes.
E: M-branes.
Answer: A

Question: What dimensionality do the F-strings, D-strings, NS-branes, and M-branes possess after they are wrapped on compact cycles?
A: Two non-compact dimensions remain.
B: Zero non-compact dimensions remain.
C: Only one non-compact dimension remains.
D: Three non-compact dimensions remain.
E: Four non-compact dimensions remain.
Answer: C

Question: The quantum field theory which serves as a typical example for cosmic strings is:
A: General Relativity.
B: The Grand Unified Theory.
C: The Abelian Higgs model.
D: Quantum Chromodynamics.
E: The Kaluza-Klein theory.
Answer: C

Question: How do F-strings primarily differ from field theory cosmic strings?
A: F-strings are 2-dimensional, while field theory cosmic strings are 1-dimensional.
B: F-strings have a classical definition, whereas field theory cosmic strings are quantum-mechanical.
C: F-strings are fully quantum-mechanical and lack a classical definition, while field theory cosmic strings are usually treated classically.
D: F-strings are derived from general relativity, while field theory cosmic strings arise from quantum mechanics.
E: There's no difference; they are the same.
Answer: C

Question: Cosmic strings may have formed during a phase transition when the topology of the vacuum manifold was:
A: Fully connected.
B: Simply connected.
C: Not simply connected.
D: Completely isolated.
E: Entangled with quantum states.
Answer: C

Question: In the models of the early universe, the production of cosmic strings is a prediction of:
A: Only quantum field theory.
B: Only general relativity.
C: Both quantum field theory and string theory.
D: Neither quantum field theory nor string theory.
E: Only string theory.
Answer: C
@
In physics, string theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries the gravitational force. Thus, string theory is a theory of quantum gravity.

String theory is a broad and varied subject that attempts to address a number of deep questions of fundamental physics. String theory has contributed a number of advances to mathematical physics, which have been applied to a variety of problems in black hole physics, early universe cosmology, nuclear physics, and condensed matter physics, and it has stimulated a number of major developments in pure mathematics. Because string theory potentially provides a unified description of gravity and particle physics, it is a candidate for a theory of everything, a self-contained mathematical model that describes all fundamental forces and forms of matter. Despite much work on these problems, it is not known to what extent string theory describes the real world or how much freedom the theory allows in the choice of its details.

String theory was first studied in the late 1960s as a theory of the strong nuclear force, before being abandoned in favor of quantum chromodynamics. Subsequently, it was realized that the very properties that made string theory unsuitable as a theory of nuclear physics made it a promising candidate for a quantum theory of gravity. The earliest version of string theory, bosonic string theory, incorporated only the class of particles known as bosons. It later developed into superstring theory, which posits a connection called supersymmetry between bosons and the class of particles called fermions. Five consistent versions of superstring theory were developed before it was conjectured in the mid-1990s that they were all different limiting cases of a single theory in 11 dimensions known as M-theory. In late 1997, theorists discovered an important relationship called the anti-de Sitter/conformal field theory correspondence (AdS/CFT correspondence), which relates string theory to another type of physical theory called a quantum field theory.

One of the challenges of string theory is that the full theory does not have a satisfactory definition in all circumstances. Another issue is that the theory is thought to describe an enormous landscape of possible universes, which has complicated efforts to develop theories of particle physics based on string theory. These issues have led some in the community to criticize these approaches to physics, and to question the value of continued research on string theory unification.
$
10
Question: In string theory, what replaces the point-like particles of particle physics?
A: Bosons.
B: Fermions.
C: One-dimensional objects called strings.
D: Gravitons.
E: Quarks.
Answer: C

Question: On scales larger than the string scale, how does a string appear?
A: As a wave.
B: As an energy field.
C: As an ordinary particle with properties determined by its vibrations.
D: As a quantum state.
E: As a photon.
Answer: C

Question: Which particle in string theory carries the gravitational force?
A: Proton.
B: Electron.
C: Neutron.
D: Graviton.
E: Gluon.
Answer: D

Question: What fundamental force and forms of matter does string theory potentially provide a unified description of?
A: Electromagnetism and neutrons.
B: Weak nuclear force and protons.
C: Strong nuclear force and quarks.
D: Gravity and particle physics.
E: Magnetic force and electrons.
Answer: D

Question: Originally, for what force was string theory first studied?
A: Gravitational force.
B: Electromagnetic force.
C: Weak nuclear force.
D: Strong nuclear force.
E: Quantum force.
Answer: D

Question: Which version of string theory only incorporated bosons?
A: Superstring theory.
B: Fermionic string theory.
C: Quantum string theory.
D: Bosonic string theory.
E: M-theory.
Answer: D

Question: Superstring theory establishes a connection between which two classes of particles?
A: Protons and electrons.
B: Quarks and leptons.
C: Photons and gluons.
D: Bosons and fermions.
E: Neutrinos and gravitons.
Answer: D

Question: How many consistent versions of superstring theory were developed before the introduction of M-theory?
A: Two.
B: Three.
C: Four.
D: Five.
E: Six.
Answer: D

Question: The AdS/CFT correspondence relates string theory to which other type of physical theory?
A: General relativity.
B: Quantum mechanics.
C: Electromagnetism.
D: Quantum field theory.
E: Thermodynamics.
Answer: D

Question: Which of the following is a challenge associated with string theory?
A: It only works in 2-dimensional space.
B: It doesn’t incorporate gravitational force.
C: The full theory lacks a satisfactory definition in all circumstances.
D: It can only describe the behavior of fermions.
E: It doesn't involve quantum mechanics.
Answer: C
@
In the 20th century, two theoretical frameworks emerged for formulating the laws of physics. The first is Albert Einstein's general theory of relativity, a theory that explains the force of gravity and the structure of spacetime at the macro-level. The other is quantum mechanics, a completely different formulation, which uses known probability principles to describe physical phenomena at the micro-level. By the late 1970s, these two frameworks had proven to be sufficient to explain most of the observed features of the universe, from elementary particles to atoms to the evolution of stars and the universe as a whole.[1]

In spite of these successes, there are still many problems that remain to be solved. One of the deepest problems in modern physics is the problem of quantum gravity.[1] The general theory of relativity is formulated within the framework of classical physics, whereas the other fundamental forces are described within the framework of quantum mechanics. A quantum theory of gravity is needed in order to reconcile general relativity with the principles of quantum mechanics, but difficulties arise when one attempts to apply the usual prescriptions of quantum theory to the force of gravity.[2] In addition to the problem of developing a consistent theory of quantum gravity, there are many other fundamental problems in the physics of atomic nuclei, black holes, and the early universe.[a]

String theory is a theoretical framework that attempts to address these questions and many others. The starting point for string theory is the idea that the point-like particles of particle physics can also be modeled as one-dimensional objects called strings. String theory describes how strings propagate through space and interact with each other. In a given version of string theory, there is only one kind of string, which may look like a small loop or segment of ordinary string, and it can vibrate in different ways. On distance scales larger than the string scale, a string will look just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In this way, all of the different elementary particles may be viewed as vibrating strings. In string theory, one of the vibrational states of the string gives rise to the graviton, a quantum mechanical particle that carries gravitational force. Thus string theory is a theory of quantum gravity.[3]
$
10
Question: Which theory explains the force of gravity and the structure of spacetime at the macro-level?
A: Quantum mechanics.
B: Electromagnetism.
C: String theory.
D: Albert Einstein's general theory of relativity.
E: Classical physics.
Answer: D

Question: Quantum mechanics is used primarily to describe phenomena at which level?
A: Macro-level.
B: Middle-level.
C: Intergalactic level.
D: Sub-atomic level.
E: Astronomical level.
Answer: D

Question: By the late 1970s, the two theoretical frameworks of the 20th century had been sufficient to explain most of the observed features of what?
A: Black holes.
B: Quantum particles.
C: The universe.
D: Atomic nuclei.
E: Electromagnetic fields.
Answer: C

Question: What is one of the deepest unsolved problems in modern physics?
A: Quantum electrodynamics.
B: Quantum gravity.
C: Quantum entanglement.
D: Quantum chromodynamics.
E: Quantum teleportation.
Answer: B

Question: Why is a quantum theory of gravity needed?
A: To explain atomic structures.
B: To measure quantum states.
C: To reconcile general relativity with quantum mechanics.
D: To define the behavior of light.
E: To classify fundamental particles.
Answer: C

Question: In string theory, how are the point-like particles of particle physics modeled?
A: As three-dimensional cubes.
B: As quantum waves.
C: As one-dimensional objects called strings.
D: As two-dimensional membranes.
E: As loops of energy.
Answer: C

Question: On larger distance scales compared to the string scale, how does a string appear?
A: As an energy wave.
B: As a continuous loop.
C: As an ordinary particle determined by its vibrations.
D: As a quantum field.
E: As a black hole.
Answer: C

Question: What determines the properties of a string in string theory?
A: Its charge only.
B: Its rotational motion.
C: The space it occupies.
D: The vibrational state of the string.
E: The type of particle it represents.
Answer: D

Question: Which particle in string theory carries the gravitational force?
A: Electron.
B: Photon.
C: Neutrino.
D: Proton.
E: Graviton.
Answer: E

Question: What overarching problem does string theory aim to address?
A: The behavior of light.
B: The nature of space.
C: The issues of quantum gravity and fundamental problems in physics.
D: The properties of atomic structures.
E: The origin of quantum mechanics.
Answer: C
@
M-theory is a theory in physics that unifies all consistent versions of superstring theory. Edward Witten first conjectured the existence of such a theory at a string theory conference at the University of Southern California in 1995 (M-Theory - Edward Witten (1995)). Witten's announcement initiated a flurry of research activity known as the second superstring revolution. Prior to Witten's announcement, string theorists had identified five versions of superstring theory. Although these theories initially appeared to be very different, work by many physicists showed that the theories were related in intricate and nontrivial ways. Physicists found that apparently distinct theories could be unified by mathematical transformations called S-duality and T-duality. Witten's conjecture was based in part on the existence of these dualities and in part on the relationship of the string theories to a field theory called eleven-dimensional supergravity.

Although a complete formulation of M-theory is not known, such a formulation should describe two- and five-dimensional objects called branes and should be approximated by eleven-dimensional supergravity at low energies. Modern attempts to formulate M-theory are typically based on matrix theory or the AdS/CFT correspondence. According to Witten, M should stand for "magic", "mystery" or "membrane" according to taste, and the true meaning of the title should be decided when a more fundamental formulation of the theory is known.[1]

Investigations of the mathematical structure of M-theory have spawned important theoretical results in physics and mathematics. More speculatively, M-theory may provide a framework for developing a unified theory of all of the fundamental forces of nature. Attempts to connect M-theory to experiment typically focus on compactifying its extra dimensions to construct candidate models of the four-dimensional world, although so far none has been verified to give rise to physics as observed in high-energy physics experiments.
$
10
Question: M-theory seeks to unify how many previously identified versions of superstring theory?
A: Three
B: Four
C: Five
D: Six
E: Seven
Answer: C

Question: Who first conjectured the existence of M-theory?
A: Stephen Hawking
B: Richard Feynman
C: Leonard Susskind
D: Edward Witten
E: Roger Penrose
Answer: D

Question: When was the existence of M-theory first conjectured by Edward Witten?
A: 1985
B: 1990
C: 1995
D: 2000
E: 2005
Answer: C

Question: The period of intense research following Witten's announcement is referred to as what?
A: The golden age of string theory
B: The superstring expansion
C: The first string theory evolution
D: The second superstring revolution
E: The M-theory era
Answer: D

Question: What two types of mathematical transformations showed that the different versions of superstring theory were related?
A: U-duality and V-duality
B: R-duality and S-duality
C: S-duality and T-duality
D: Q-duality and P-duality
E: A-duality and B-duality
Answer: C

Question: What is the relationship between M-theory and eleven-dimensional supergravity?
A: Eleven-dimensional supergravity is a precursor to M-theory.
B: Eleven-dimensional supergravity approximates M-theory at high energies.
C: Eleven-dimensional supergravity is a competitor to M-theory.
D: Eleven-dimensional supergravity approximates M-theory at low energies.
E: There is no relationship between the two.
Answer: D

Question: Which objects should a complete formulation of M-theory describe?
A: One- and three-dimensional objects
B: Two- and five-dimensional objects
C: Three- and seven-dimensional objects
D: Four- and six-dimensional objects
E: One- and four-dimensional objects
Answer: B

Question: According to Witten, what does the "M" in M-theory potentially stand for?
A: Matrix
B: Momentum
C: Magic, mystery, or membrane
D: Matter
E: Mass
Answer: C

Question: Modern formulations of M-theory are often based on which of the following?
A: Quantum mechanics
B: Matrix theory or the AdS/CFT correspondence
C: The Schrödinger equation
D: General relativity
E: The Standard Model
Answer: B

Question: What do investigations into M-theory aim to produce in a more speculative sense?
A: A model for dark matter
B: A unified theory of all fundamental forces of nature
C: A new interpretation of quantum mechanics
D: A replacement for general relativity
E: A mechanism for faster-than-light travel
Answer: B
@
Field theory usually refers to a construction of the dynamics of a field, i.e., a specification of how a field changes with time or with respect to other independent physical variables on which the field depends. Usually this is done by writing a Lagrangian or a Hamiltonian of the field, and treating it as a classical or quantum mechanical system with an infinite number of degrees of freedom. The resulting field theories are referred to as classical or quantum field theories.

The dynamics of a classical field are usually specified by the Lagrangian density in terms of the field components; the dynamics can be obtained by using the action principle.

It is possible to construct simple fields without any prior knowledge of physics using only mathematics from multivariable calculus, potential theory and partial differential equations (PDEs). For example, scalar PDEs might consider quantities such as amplitude, density and pressure fields for the wave equation and fluid dynamics; temperature/concentration fields for the heat/diffusion equations. Outside of physics proper (e.g., radiometry and computer graphics), there are even light fields. All these previous examples are scalar fields. Similarly for vectors, there are vector PDEs for displacement, velocity and vorticity fields in (applied mathematical) fluid dynamics, but vector calculus may now be needed in addition, being calculus for vector fields (as are these three quantities, and those for vector PDEs in general). More generally problems in continuum mechanics may involve for example, directional elasticity (from which comes the term tensor, derived from the Latin word for stretch), complex fluid flows or anisotropic diffusion, which are framed as matrix-tensor PDEs, and then require matrices or tensor fields, hence matrix or tensor calculus. The scalars (and hence the vectors, matrices and tensors) can be real or complex as both are fields in the abstract-algebraic/ring-theoretic sense.

In a general setting, classical fields are described by sections of fiber bundles and their dynamics is formulated in the terms of jet manifolds (covariant classical field theory).[21]

In modern physics, the most often studied fields are those that model the four fundamental forces which one day may lead to the Unified Field Theory.
$
10
Question: What is field theory primarily concerned with?
A: The study of solid objects
B: The dynamics of a field
C: The interactions of subatomic particles
D: The gravitational pull of celestial bodies
E: The mathematical structures of geometry
Answer: B

Question: Which two methods are commonly used for constructing the dynamics of a field?
A: Differentiation and Integration
B: Multiplication and Division
C: Lagrangian and Hamiltonian
D: Pythagorean and Eulerian
E: Cartesian and Polar
Answer: C

Question: What type of field theories result from treating the field as a system with an infinite number of degrees of freedom?
A: Modern and ancient field theories
B: Macroscopic and microscopic field theories
C: Classical or quantum field theories
D: Theoretical or applied field theories
E: Positive and negative field theories
Answer: C

Question: The dynamics of a classical field are usually specified by what?
A: The Hamiltonian density
B: The Action principle
C: The Lagrangian density
D: The Quantum principle
E: The Tensorial density
Answer: C

Question: Scalar PDEs can consider quantities related to which fields?
A: Wave equation and fluid dynamics
B: Electrostatics and magnetic resonance
C: Atomic structures and quantum relativity
D: Celestial mechanics and photon dynamics
E: Nuclear fusion and fission reactions
Answer: A

Question: In fields outside of physics, such as radiometry and computer graphics, what kind of fields can be found?
A: Magnetic fields
B: Wave fields
C: Light fields
D: Gravitational fields
E: Electron fields
Answer: C

Question: For problems related to fluid dynamics requiring vector calculus, what type of field is often involved?
A: Scalar fields
B: Photon fields
C: Vector fields
D: Magnetic fields
E: Quantum fields
Answer: C

Question: In the context of continuum mechanics, what term is derived from the Latin word for "stretch"?
A: Scalar
B: Vector
C: Matrix
D: Tensor
E: Lagrangian
Answer: D

Question: In a general setting, how are classical fields typically described?
A: By sections of fiber bundles
B: Through quantum mechanics
C: By the principles of thermodynamics
D: Using Newton's laws
E: Through wave-particle duality
Answer: A

Question: What ultimate goal is modern physics aiming to achieve regarding fields?
A: The discovery of a fifth fundamental force
B: The Quantum Field Theory
C: The Unified Field Theory
D: The Relativistic Field Theory
E: The Grand Unified Theory
Answer: C
@
It is now believed that quantum mechanics should underlie all physical phenomena, so that a classical field theory should, at least in principle, permit a recasting in quantum mechanical terms; success yields the corresponding quantum field theory. For example, quantizing classical electrodynamics gives quantum electrodynamics. Quantum electrodynamics is arguably the most successful scientific theory; experimental data confirm its predictions to a higher precision (to more significant digits) than any other theory.[19] The two other fundamental quantum field theories are quantum chromodynamics and the electroweak theory.

In quantum chromodynamics, the color field lines are coupled at short distances by gluons, which are polarized by the field and line up with it. This effect increases within a short distance (around 1 fm from the vicinity of the quarks) making the color force increase within a short distance, confining the quarks within hadrons. As the field lines are pulled together tightly by gluons, they do not "bow" outwards as much as an electric field between electric charges.[20]

These three quantum field theories can all be derived as special cases of the so-called standard model of particle physics. General relativity, the Einsteinian field theory of gravity, has yet to be successfully quantized. However an extension, thermal field theory, deals with quantum field theory at finite temperatures, something seldom considered in quantum field theory.

In BRST theory one deals with odd fields, e.g. Faddeev–Popov ghosts. There are different descriptions of odd classical fields both on graded manifolds and supermanifolds.

As above with classical fields, it is possible to approach their quantum counterparts from a purely mathematical view using similar techniques as before. The equations governing the quantum fields are in fact PDEs (specifically, relativistic wave equations (RWEs)). Thus one can speak of Yang–Mills, Dirac, Klein–Gordon and Schrödinger fields as being solutions to their respective equations. A possible problem is that these RWEs can deal with complicated mathematical objects with exotic algebraic properties (e.g. spinors are not tensors, so may need calculus for spinor fields), but these in theory can still be subjected to analytical methods given appropriate mathematical generalization.
$
10
Question: Quantum mechanics is believed to underlie which of the following?
A: Only subatomic phenomena
B: All physical phenomena
C: Only macroscopic phenomena
D: Only electromagnetic phenomena
E: Only gravitational phenomena
Answer: B

Question: When classical electrodynamics is quantized, what theory is produced?
A: Quantum chromodynamics
B: Quantum electrodynamics
C: Electroweak theory
D: Quantum field theory
E: Classical field theory
Answer: B

Question: Which theory is considered to be the most successful scientific theory based on its precision to experimental data?
A: General relativity
B: Quantum chromodynamics
C: Electroweak theory
D: Quantum electrodynamics
E: Standard model of particle physics
Answer: D

Question: In quantum chromodynamics, which particles are responsible for coupling the color field lines at short distances?
A: Photons
B: Quarks
C: Gluons
D: Electrons
E: Neutrinos
Answer: C

Question: Within what distance does the color force increase, confining quarks within hadrons?
A: Around 10 fm
B: Around 1 fm
C: Around 100 fm
D: Around 0.1 fm
E: Around 0.01 fm
Answer: B

Question: The three fundamental quantum field theories are special cases of which broader theory?
A: Quantum mechanics
B: General relativity
C: The standard model of particle physics
D: Electroweak theory
E: Thermal field theory
Answer: C

Question: Which field theory of gravity has yet to be successfully quantized?
A: Quantum electrodynamics
B: Quantum chromodynamics
C: Electroweak theory
D: General relativity
E: Thermal field theory
Answer: D

Question: What does thermal field theory deal with?
A: Quantum field theory at zero temperatures
B: Quantum field theory at infinite temperatures
C: Quantum field theory at finite temperatures
D: Classical field theory at zero temperatures
E: Classical field theory at infinite temperatures
Answer: C

Question: In BRST theory, what type of fields are dealt with?
A: Gravitational fields
B: Electric fields
C: Odd fields
D: Yang–Mills fields
E: Electromagnetic fields
Answer: C

Question: Which among the following is NOT a solution to their respective relativistic wave equations (RWEs)?
A: Klein–Gordon
B: Schrödinger
C: Gluon
D: Dirac
E: Yang–Mills
Answer: C
@
In physics, a field is a physical quantity, represented by a scalar, vector, or tensor, that has a value for each point in space and time.[1][2][3] For example, on a weather map, the surface temperature is described by assigning a number to each point on the map; the temperature can be considered at a certain point in time or over some interval of time, to study the dynamics of temperature change. A surface wind map,[4] assigning an arrow to each point on a map that describes the wind speed and direction at that point, is an example of a vector field, i.e. a 1-dimensional (rank-1) tensor field. Field theories, mathematical descriptions of how field values change in space and time, are ubiquitous in physics. For instance, the electric field is another rank-1 tensor field, while electrodynamics can be formulated in terms of two interacting vector fields at each point in spacetime, or as a single-rank 2-tensor field.[5][6][7]

In the modern framework of the quantum theory of fields, even without referring to a test particle, a field occupies space, contains energy, and its presence precludes a classical "true vacuum".[8] This has led physicists to consider electromagnetic fields to be a physical entity, making the field concept a supporting paradigm of the edifice of modern physics. "The fact that the electromagnetic field can possess momentum and energy makes it very real ... a particle makes a field, and a field acts on another particle, and the field has such familiar properties as energy content and momentum, just as particles can have."[9] In practice, the strength of most fields diminishes with distance, eventually becoming undetectable. For instance the strength of many relevant classical fields, such as the gravitational field in Newton's theory of gravity or the electrostatic field in classical electromagnetism, is inversely proportional to the square of the distance from the source (i.e., they follow Gauss's law).

A field can be classified as a scalar field, a vector field, a spinor field or a tensor field according to whether the represented physical quantity is a scalar, a vector, a spinor, or a tensor, respectively. A field has a consistent tensorial character wherever it is defined: i.e. a field cannot be a scalar field somewhere and a vector field somewhere else. For example, the Newtonian gravitational field is a vector field: specifying its value at a point in spacetime requires three numbers, the components of the gravitational field vector at that point. Moreover, within each category (scalar, vector, tensor), a field can be either a classical field or a quantum field, depending on whether it is characterized by numbers or quantum operators respectively. In this theory an equivalent representation of field is a field particle, for instance a boson.[10]
$
10
Question: In physics, what is a field?
A: A method for measuring temperature
B: A physical space occupied by an object
C: A particle that exists in space and time
D: A physical quantity with a value for each point in space and time
E: A specific region in spacetime
Answer: D

Question: On a weather map, what does the surface temperature assigning a number to each point represent?
A: A vector field
B: A tensor field
C: A scalar field
D: A spinor field
E: A quantum field
Answer: C

Question: A surface wind map assigning an arrow to each point indicating wind speed and direction is an example of what?
A: A scalar field
B: A spinor field
C: A tensor field
D: A vector field
E: A quantum field
Answer: D

Question: Electrodynamics can be formulated in terms of how many interacting vector fields at each point in spacetime?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: B

Question: In modern quantum theory of fields, a field occupies space and contains what?
A: Sound
B: Vacuum
C: Energy
D: Particles
E: Waves
Answer: C

Question: The strength of classical fields like gravitational field or electrostatic field typically diminishes with what relationship to distance from the source?
A: Directly proportional
B: Inversely proportional
C: Inversely proportional to the square of the distance
D: Remains constant
E: Increases exponentially
Answer: C

Question: What kind of field is the Newtonian gravitational field?
A: Scalar field
B: Tensor field
C: Quantum field
D: Vector field
E: Spinor field
Answer: D

Question: A field's tensorial character is:
A: Different at different locations
B: Consistent wherever it is defined
C: Only applicable in scalar fields
D: Only relevant in quantum fields
E: Defined by its interaction with particles
Answer: B

Question: Within each category (scalar, vector, tensor), a field can be classified as classical or quantum based on what distinction?
A: Its location in spacetime
B: Whether it interacts with gravity
C: Its relationship with distance
D: Whether it is characterized by numbers or quantum operators
E: Its tensorial character
Answer: D

Question: In the theory, an equivalent representation of field is a:
A: Neutron
B: Photon
C: Boson
D: Proton
E: Electron
Answer: C
@
In a CNN, the input is a tensor with shape: (number of inputs) × (input height) × (input width) × (input channels). After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape: (number of inputs) × (feature map height) × (feature map width) × (feature map channels).

Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus.[20] Each convolutional neuron processes data only for its receptive field. Although fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 × 100 has 10,000 weights for each neuron in the second layer. Convolution reduces the number of free parameters, allowing the network to be deeper.[21] For example, using a 5 × 5 tiling region, each with the same shared weights, requires only 25 neurons. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks.[22][23]

To speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers,[24] which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor, while the pointwise convolution is a standard convolution restricted to the use of 
1
×
1
{\displaystyle 1\times 1} kernels.
$
10
Question: In a CNN, what does the input tensor shape represent?
A: The depth of the feature maps.
B: The number of weights in each neuron.
C: The image's resolution.
D: (number of inputs) × (input height) × (input width) × (input channels).
E: The number of convolutional layers.
Answer: D

Question: After processing through a convolutional layer, what does the image get abstracted to?
A: An input tensor.
B: A receptive field.
C: An activation map.
D: A depthwise convolution.
E: A pointwise convolution.
Answer: C

Question: The process in a convolutional neuron is similar to the response of what?
A: A neuron in the auditory cortex.
B: A neuron in the motor cortex.
C: A neuron in the visual cortex.
D: A neuron in the sensory cortex.
E: A neuron in the somatosensory cortex.
Answer: C

Question: Why can using fully connected feedforward neural networks for larger inputs, like high-resolution images, be impractical?
A: They process data slower.
B: They need massive numbers of neurons for each pixel.
C: They use depthwise separable convolutional layers.
D: They cannot be used for image classification.
E: They use the same shared weights for each neuron.
Answer: B

Question: If a fully connected layer is used for an image of size 100 × 100, how many weights are required for each neuron in the second layer?
A: 25
B: 100
C: 1,000
D: 10,000
E: 100,000
Answer: D

Question: Convolution in CNNs reduces the number of free parameters, allowing the network to be what?
A: Wider.
B: Shorter.
C: Less complex.
D: Faster.
E: Deeper.
Answer: E

Question: To avoid issues like vanishing gradients and exploding gradients, what approach is used in CNNs?
A: Increasing the size of receptive fields.
B: Using regularized weights over fewer parameters.
C: Applying standard convolutional layers.
D: Increasing the number of neurons.
E: Using depthwise separable convolutional layers.
Answer: B

Question: What kind of convolution is applied independently over each channel of the input tensor?
A: Receptive convolution.
B: Pointwise convolution.
C: Depthwise convolution.
D: Standard convolution.
E: Activation convolution.
Answer: C

Question: What kind of convolution is restricted to the use of 
1
×
1
1×1 kernels?
A: Standard convolution.
B: Receptive convolution.
C: Pointwise convolution.
D: Activation convolution.
E: Depthwise convolution.
Answer: C

Question: What can replace standard convolutional layers to speed up processing?
A: Activation convolutional layers.
B: Regularized weight layers.
C: Fully connected feedforward layers.
D: Depthwise separable convolutional layers.
E: Receptive field layers.
Answer: D
@
The observable universe is a ball-shaped region of the universe comprising all matter that can be observed from Earth or its space-based telescopes and exploratory probes at the present time; the electromagnetic radiation from these objects has had time to reach the Solar System and Earth since the beginning of the cosmological expansion. Initially, it was estimated that there may be 2 trillion galaxies in the observable universe,[7][8] although that number was reduced in 2021 to only several hundred billion based on data from New Horizons.[9][10][11] Assuming the universe is isotropic, the distance to the edge of the observable universe is roughly the same in every direction. That is, the observable universe is a spherical region centered on the observer. Every location in the universe has its own observable universe, which may or may not overlap with the one centered on Earth.

The word observable in this sense does not refer to the capability of modern technology to detect light or other information from an object, or whether there is anything to be detected. It refers to the physical limit created by the speed of light itself. No signal can travel faster than light; hence there is a maximum distance (called the particle horizon) beyond which nothing can be detected, as the signals could not have reached us yet. Sometimes astrophysicists distinguish between the visible universe, which includes only signals emitted since recombination (when hydrogen atoms were formed from protons and electrons and photons were emitted)—and the observable universe, which includes signals since the beginning of the cosmological expansion (the Big Bang in traditional physical cosmology, the end of the inflationary epoch in modern cosmology).
$
5
Question: What is the observable universe?
A: The entire universe as we know it.
B: Only the planets within our Solar System.
C: A region of the universe containing all matter that can currently be observed from Earth.
D: The total number of galaxies in the universe.
E: The region that only advanced telescopes can see.
Answer: C

Question: How was the estimate for the number of galaxies in the observable universe changed in 2021?
A: Increased to several trillion.
B: Reduced to 2 trillion.
C: Reduced to several hundred billion.
D: Increased to 4 trillion.
E: Stayed the same.
Answer: C

Question: What does the term "observable" refer to in this context?
A: The modern technological capabilities.
B: Whether there is anything to be detected.
C: The physical limit set by the speed of light.
D: The human eye's ability to see.
E: The capabilities of current space probes.
Answer: C

Question: What is the particle horizon?
A: The estimated edge of the observable universe.
B: The boundary of our Solar System.
C: The maximum distance where signals have not reached us due to the speed of light.
D: The limit where technology can detect signals.
E: The beginning of the cosmological expansion.
Answer: C

Question: What differentiates the "visible universe" from the "observable universe"?
A: The visible universe is smaller in size.
B: The visible universe only includes signals emitted since hydrogen atoms were formed.
C: The observable universe includes all galaxies, the visible does not.
D: The observable universe refers to what can be seen with the naked eye.
E: The visible universe only includes objects within our galaxy.
Answer: B
@
According to calculations, the current comoving distance to particles from which the cosmic microwave background radiation (CMBR) was emitted, which represents the radius of the visible universe, is about 14.0 billion parsecs (about 45.7 billion light-years); the comoving distance to the edge of the observable universe is about 14.3 billion parsecs (about 46.6 billion light-years),[12] about 2% larger. The radius of the observable universe is therefore estimated to be about 46.5 billion light-years.[13][14] Using the critical density and the diameter of the observable universe, the total mass of ordinary matter in the universe can be calculated to be about 1.5×1053 kg.[15] In November 2018, astronomers reported that extragalactic background light (EBL) amounted to 4×1084 photons.[16][17]

As the universe's expansion is accelerating, all currently observable objects, outside the local supercluster, will eventually appear to freeze in time, while emitting progressively redder and fainter light. For instance, objects with the current redshift z from 5 to 10 will remain observable for no more than 4–6 billion years. In addition, light emitted by objects currently situated beyond a certain comoving distance (currently about 19 billion parsecs) will never reach Earth.[18]
$
5
Question: How far are the particles from which the cosmic microwave background radiation was emitted?
A: 14.0 billion parsecs.
B: 46.6 billion light-years.
C: 19 billion parsecs.
D: 45.7 billion light-years.
E: 14.3 billion parsecs.
Answer: A

Question: What is the estimated radius of the observable universe?
A: 14.0 billion light-years.
B: 46.6 billion light-years.
C: 19 billion parsecs.
D: 45.7 billion light-years.
E: 46.5 billion light-years.
Answer: E

Question: Using the critical density and the diameter of the observable universe, what is the total mass of ordinary matter in the universe?
A: 1.5×1054 kg.
B: 4×1084 photons.
C: 1.5×1053 kg.
D: 1×1050 kg.
E: 5×1055 kg.
Answer: C

Question: What will eventually happen to all currently observable objects outside the local supercluster due to the universe's accelerating expansion?
A: They will move closer to Earth.
B: They will appear to freeze in time and emit redder, fainter light.
C: They will disappear completely.
D: They will emit brighter, blue light.
E: They will remain unchanged.
Answer: B

Question: For how long will objects with the current redshift z from 5 to 10 remain observable?
A: 10-12 billion years.
B: 1-2 billion years.
C: 4-6 billion years.
D: 20-25 billion years.
E: 15-17 billion years.
Answer: C
@
The abundance of the chemical elements is a measure of the occurrence of the chemical elements relative to all other elements in a given environment. Abundance is measured in one of three ways: by mass fraction (in commercial contexts often called weight fraction), by mole fraction (fraction of atoms by numerical count, or sometimes fraction of molecules in gases), or by volume fraction. Volume fraction is a common abundance measure in mixed gases such as planetary atmospheres, and is similar in value to molecular mole fraction for gas mixtures at relatively low densities and pressures, and ideal gas mixtures. Most abundance values in this article are given as mass fractions.

For example, the abundance of oxygen in pure water can be measured in two ways: the mass fraction is about 89%, because that is the fraction of water's mass which is oxygen. However, the mole fraction is about 33% because only 1 atom of 3 in water, H2O, is oxygen. As another example, looking at the mass fraction abundance of hydrogen and helium in both the Universe as a whole and in the atmospheres of gas-giant planets such as Jupiter, it is 74% for hydrogen and 23–25% for helium; while the (atomic) mole fraction for hydrogen is 92%, and for helium is 8%, in these environments. Changing the given environment to Jupiter's outer atmosphere, where hydrogen is diatomic while helium is not, changes the molecular mole fraction (fraction of total gas molecules), as well as the fraction of atmosphere by volume, of hydrogen to about 86%, and of helium to 13%
$
5
Question: What does the abundance of chemical elements refer to?
A: The total amount of elements on Earth.
B: The relative presence of elements in a given environment.
C: The concentration of elements in solid matter.
D: The percentage of elements in the human body.
E: The most dominant element in the universe.
Answer: B

Question: How is abundance commonly measured in mixed gases like planetary atmospheres?
A: By mass fraction.
B: By mole fraction.
C: By atomic number.
D: By volume fraction.
E: By density.
Answer: D

Question: What is the mole fraction of oxygen in pure water?
A: 50%
B: 89%
C: 33%
D: 74%
E: 25%
Answer: C

Question: In the Universe, what is the mole fraction abundance of hydrogen?
A: 74%
B: 23–25%
C: 92%
D: 8%
E: 86%
Answer: C

Question: In the outer atmosphere of Jupiter, what is the molecular mole fraction of helium?
A: 86%
B: 8%
C: 13%
D: 23–25%
E: 92%
Answer: C
@
The elements – that is, ordinary (baryonic) matter made of protons, neutrons, and electrons, are only a small part of the content of the Universe. Cosmological observations suggest that only 4.6% of the universe's energy (including the mass contributed by energy, E = mc2 ⇔ m = E / c2) comprises the visible baryonic matter that constitutes stars, planets, and living beings. The rest is thought to be made up of dark energy (68%) and dark matter (27%).[4] These are forms of matter and energy believed to exist on the basis of scientific theory and inductive reasoning based on observations, but they have not been directly observed and their nature is not well understood.

Most standard (baryonic) matter is found in intergalactic gas, stars, and interstellar clouds, in the form of atoms or ions (plasma), although it can be found in degenerate forms in extreme astrophysical settings, such as the high densities inside white dwarfs and neutron stars.

Hydrogen is the most abundant element in the Universe; helium is second. However, after this, the rank of abundance does not continue to correspond to the atomic number; oxygen has abundance rank 3, but atomic number 8. All others are substantially less common.

The abundance of the lightest elements is well predicted by the standard cosmological model, since they were mostly produced shortly (i.e., within a few hundred seconds) after the Big Bang, in a process known as Big Bang nucleosynthesis. Heavier elements were mostly produced much later, inside of stars.

Hydrogen and helium are estimated to make up roughly 74% and 24% of all baryonic matter in the universe respectively. Despite comprising only a very small fraction of the universe, the remaining "heavy elements" can greatly influence astronomical phenomena. Only about 2% (by mass) of the Milky Way galaxy's disk is composed of heavy elements.
$
5
Question: What percentage of the universe's energy consists of visible baryonic matter?
A: 68%
B: 4.6%
C: 27%
D: 74%
E: 24%
Answer: B

Question: Which element is the most abundant in the Universe?
A: Oxygen
B: Helium
C: Nitrogen
D: Hydrogen
E: Carbon
Answer: D

Question: In what process were the lightest elements primarily produced?
A: Stellar nucleosynthesis.
B: Fusion in stars.
C: Atomic collisions.
D: Galactic evolution.
E: Big Bang nucleosynthesis.
Answer: E

Question: What is the estimated percentage of hydrogen in all baryonic matter in the universe?
A: 2%
B: 24%
C: 68%
D: 74%
E: 27%
Answer: D

Question: Approximately what fraction (by mass) of the Milky Way galaxy's disk is composed of heavy elements?
A: 68%
B: 2%
C: 74%
D: 24%
E: 27%
Answer: B
@
In 1944, Walter Baade categorized groups of stars within the Milky Way into stellar populations. In the abstract of the article by Baade, he recognizes that Jan Oort originally conceived this type of classification in 1926.[1]

Baade observed that bluer stars were strongly associated with the spiral arms, and yellow stars dominated near the central galactic bulge and within globular star clusters.[2] Two main divisions were defined as population I and population II, with another newer, hypothetical division called population III added in 1978.

Among the population types, significant differences were found with their individual observed stellar spectra. These were later shown to be very important and were possibly related to star formation, observed kinematics,[3] stellar age, and even galaxy evolution in both spiral and elliptical galaxies. These three simple population classes usefully divided stars by their chemical composition or metallicity.[4][5][3]

By definition, each population group shows the trend where decreasing metal content indicates increasing age of stars. Hence, the first stars in the universe (very low metal content) were deemed population III, old stars (low metallicity) as population II, and recent stars (high metallicity) as population I.[6] The Sun is considered population I, a recent star with a relatively high 1.4% metallicity. Note that astrophysics nomenclature considers any element heavier than helium to be a "metal", including chemical non-metals such as oxygen.[7]
$
5
Question: Who originally conceived the classification of stars into stellar populations?
A: Walter Baade
B: Jan Oort
C: Albert Einstein
D: Isaac Newton
E: Carl Sagan
Answer: B

Question: Where are yellow stars primarily found in the Milky Way?
A: Spiral arms
B: Intergalactic space
C: Galactic bulge
D: Milky Way’s outer rim
E: Near black holes
Answer: C

Question: What does population I, II, and III classification primarily divide stars by?
A: Their size
B: Their brightness
C: Their temperature
D: Their chemical composition
E: Their distance from the Milky Way
Answer: D

Question: Which stellar population represents the oldest stars?
A: Population I
B: Population II
C: Population III
D: Population IV
E: None of the above
Answer: C

Question: In astrophysics, what elements are considered "metals"?
A: Elements heavier than carbon
B: Elements lighter than helium
C: Elements heavier than helium
D: Elements between helium and carbon
E: Elements found in terrestrial crust
Answer: C
@
Formation of the first stars
Under current cosmological models, all matter created in the Big Bang was mostly hydrogen (75%) and helium (25%), with only a very tiny fraction consisting of other light elements such as lithium and beryllium.[8] When the universe had cooled sufficiently, the first stars were born as population III stars, without any contaminating heavier metals. This is postulated to have affected their structure so that their stellar masses became hundreds of times more than that of the Sun. In turn, these massive stars also evolved very quickly, and their nucleosynthetic processes created the first 26 elements (up to iron in the periodic table).[9]

Many theoretical stellar models show that most high-mass population III stars rapidly exhausted their fuel and likely exploded in extremely energetic pair-instability supernovae. Those explosions would have thoroughly dispersed their material, ejecting metals into the interstellar medium (ISM), to be incorporated into the later generations of stars. Their destruction suggests that no galactic high-mass population III stars should be observable.[10] However, some population III stars might be seen in high-redshift galaxies whose light originated during the earlier history of the universe.[11] Scientists have found evidence of an extremely small ultra metal-poor star, slightly smaller than the Sun, found in a binary system of the spiral arms in the Milky Way. The discovery opens up the possibility of observing even older stars.[12]

Stars too massive to produce pair-instability supernovae would have likely collapsed into black holes through a process known as photodisintegration. Here some matter may have escaped during this process in the form of relativistic jets, and this could have distributed the first metals into the universe.[13][14][a]
$
5
Question: What were the primary elements created in the Big Bang?
A: Lithium and Beryllium
B: Hydrogen and Helium
C: Oxygen and Neon
D: Carbon and Nitrogen
E: Iron and Silicon
Answer: B

Question: Population III stars were different from later stars because they lacked what?
A: Hydrogen
B: Helium
C: Heavier metals
D: Light elements
E: Gamma rays
Answer: C

Question: What process might cause extremely massive stars to collapse into black holes?
A: Nuclear fusion
B: Stellar nucleosynthesis
C: Photodisintegration
D: Accretion
E: Pair-instability reactions
Answer: C

Question: The first 26 elements were created inside what type of stars?
A: Population I stars
B: Population II stars
C: Population III stars
D: White dwarfs
E: Neutron stars
Answer: C

Question: Where might some Population III stars still be observed today?
A: Within the Milky Way’s center
B: Within the spiral arms of the Milky Way
C: In globular clusters
D: In high-redshift galaxies
E: Within black holes
Answer: D
@
Population I, or metal-rich, stars are young stars with the highest metallicity out of all three populations and are more commonly found in the spiral arms of the Milky Way galaxy. The Sun is an example of a metal-rich star and is considered as an intermediate population I star, while the sun-like μ Arae is much richer in metals.[17]

Population I stars usually have regular elliptical orbits of the Galactic Center, with a low relative velocity. It was earlier hypothesized that the high metallicity of population I stars makes them more likely to possess planetary systems than the other two populations, because planets, particularly terrestrial planets, are thought to be formed by the accretion of metals.[18] However, observations of the Kepler Space Telescope data have found smaller planets around stars with a range of metallicities, while only larger, potential gas giant planets are concentrated around stars with relatively higher metallicity – a finding that has implications for theories of gas-giant formation.[19] Between the intermediate population I and the population II stars comes the intermediate disc population.
$
5
Question: Where are Population I stars most commonly found?
A: Galactic halo
B: Globular clusters
C: Spiral arms of the Milky Way
D: Intergalactic space
E: Galactic bulge
Answer: C

Question: The Sun is considered to be which type of star?
A: Intermediate Population II
B: High-mass Population III
C: Intermediate Population I
D: Ultra metal-poor
E: Population IV
Answer: C

Question: Which type of planets were initially thought to be more likely around high metallicity stars?
A: Gas giants
B: Brown dwarfs
C: White dwarfs
D: Terrestrial planets
E: Ice giants
Answer: D

Question: Which astronomical tool provided observations about planets around stars of different metallicities?
A: Hubble Space Telescope
B: James Webb Space Telescope
C: Kepler Space Telescope
D: Galileo Telescope
E: SkyMapper
Answer: C

Question: What comes between the intermediate Population I and Population II stars?
A: Population III stars
B: Intermediate disc population
C: Galactic halo stars
D: Ultra metal-poor stars
E: High-redshift stars
Answer: B
@
Population II, or metal-poor, stars are those with relatively little of the elements heavier than helium. These objects were formed during an earlier time of the universe. Intermediate population II stars are common in the bulge near the centre of the Milky Way, whereas population II stars found in the galactic halo are older and thus more metal-deficient. Globular clusters also contain high numbers of population II stars.[20]

A characteristic of population II stars is that despite their lower overall metallicity, they often have a higher ratio of "alpha elements" (elements produced by the alpha process, like oxygen and neon) relative to iron (Fe) as compared with population I stars; current theory suggests that this is the result of type II supernovas being more important contributors to the interstellar medium at the time of their formation, whereas type Ia supernova metal-enrichment came at a later stage in the universe's development.[21]

Scientists have targeted these oldest stars in several different surveys, including the HK objective-prism survey of Timothy C. Beers et al.[22] and the Hamburg-ESO survey of Norbert Christlieb et al.,[23] originally started for faint quasars. Thus far, they have uncovered and studied in detail about ten ultra-metal-poor (UMP) stars (such as Sneden's Star, Cayrel's Star, BD +17° 3248) and three of the oldest stars known to date: HE 0107-5240, HE 1327-2326 and HE 1523-0901. Caffau's star was identified as the most metal-poor star yet when it was found in 2012 using Sloan Digital Sky Survey data. However, in February 2014 the discovery of an even lower-metallicity star was announced, SMSS J031300.36-670839.3 located with the aid of SkyMapper astronomical survey data. Less extreme in their metal deficiency, but nearer and brighter and hence longer known, are HD 122563 (a red giant) and HD 140283 (a subgiant).
$
5
Question: Which stellar population is common in the bulge near the center of the Milky Way?
A: Population I
B: Population II
C: Population III
D: Population IV
E: Intermediate disc population
Answer: B

Question: Despite their lower overall metallicity, Population II stars often have a higher ratio of which elements relative to iron?
A: Gamma elements
B: Beta elements
C: Delta elements
D: Alpha elements
E: Omega elements
Answer: D

Question: Which type of supernovas contributed more to the interstellar medium at the time of Population II stars' formation?
A: Type I
B: Type Ia
C: Type Ib
D: Type II
E: Type III
Answer: D

Question: Which star was identified as the most metal-poor star in 2012?
A: SMSS J031300.36-670839.3
B: Caffau's star
C: Sneden's Star
D: HE 0107-5240
E: HD 140283
Answer: B

Question: Which star was discovered in 2014 with an even lower metallicity than Caffau's star?
A: HD 122563
B: Cayrel's Star
C: HE 1523-0901
D: BD +17° 3248
E: SMSS J031300.36-670839.3
Answer: E
@
The European Organisation for Astronomical Research in the Southern Hemisphere,[2] commonly referred to as the European Southern Observatory (ESO), is an intergovernmental research organisation made up of 16 member states for ground-based astronomy. Created in 1962, ESO has provided astronomers with state-of-the-art research facilities and access to the southern sky. The organisation employs over 750 staff members and receives annual member state contributions of approximately €162 million.[3] Its observatories are located in northern Chile.

ESO has built and operated some of the largest and most technologically advanced telescopes. These include the 3.6 m New Technology Telescope, an early pioneer in the use of active optics, and the Very Large Telescope (VLT), which consists of four individual 8.2 m telescopes and four smaller auxiliary telescopes which can all work together or separately. The Atacama Large Millimeter Array observes the universe in the millimetre and submillimetre wavelength ranges, and is the world's largest ground-based astronomy project to date. It was completed in March 2013 in an international collaboration by Europe (represented by ESO), North America, East Asia and Chile.[4][5]
$
5
Question: Which organization is responsible for the Atacama Large Millimeter Array?
A: NASA
B: ISRO
C: European Southern Observatory (ESO)
D: Canadian Space Agency
E: Russian Space Agency
Answer: C

Question: In what year was the European Southern Observatory created?
A: 1972
B: 1992
C: 1962
D: 1982
E: 2002
Answer: C

Question: What technology did the 3.6 m New Technology Telescope pioneer?
A: Passive optics
B: Digital optics
C: Active optics
D: Reflective optics
E: Adaptive optics
Answer: C

Question: As of the provided text, how many member states are part of ESO?
A: 10
B: 16
C: 20
D: 25
E: 30
Answer: B

Question: Where are ESO's observatories primarily located?
A: Southern France
B: Eastern Australia
C: Northern Chile
D: Western USA
E: South Africa
Answer: C
@
Currently under construction is the Extremely Large Telescope. It will use a 39.3-metre-diameter segmented mirror, and become the world's largest optical reflecting telescope when operational towards the end of this decade. Its light-gathering power will allow detailed studies of planets around other stars, the first objects in the universe, supermassive black holes, and the nature and distribution of the dark matter and dark energy which dominate the universe.

ESO's observing facilities have made astronomical discoveries and produced several astronomical catalogues.[6] Its findings include the discovery of the most distant gamma-ray burst and evidence for a black hole at the centre of the Milky Way.[7][8] In 2004, the VLT allowed astronomers to obtain the first picture of an extrasolar planet (2M1207b) orbiting a brown dwarf 173 light-years away.[9] The High Accuracy Radial Velocity Planet Searcher (HARPS) instrument installed on the older ESO 3.6 m telescope led to the discovery of extrasolar planets, including Gliese 581c—one of the smallest planets seen outside the Solar System.[10]
$
5
Question: What will be the distinguishing feature of the Extremely Large Telescope when operational?
A: The world's largest infrared telescope
B: The world's largest optical reflecting telescope
C: The world's largest segmented telescope
D: The world's most advanced adaptive optics system
E: The world's highest altitude telescope
Answer: B

Question: In which year did the VLT provide the first picture of an extrasolar planet orbiting a brown dwarf?
A: 1998
B: 2004
C: 2010
D: 2016
E: 2019
Answer: B

Question: What type of celestial object was found at the center of the Milky Way, according to ESO's findings?
A: A neutron star
B: A gamma-ray burst
C: A black hole
D: A white dwarf
E: A blue supergiant
Answer: C

Question: Which instrument led to the discovery of extrasolar planets like Gliese 581c?
A: ELT
B: NTT
C: HARPS
D: VLT
E: ALMA
Answer: C

Question: The High Accuracy Radial Velocity Planet Searcher was installed on which telescope?
A: 2.4 m telescope
B: 3.6 m telescope
C: 4.2 m telescope
D: 5.0 m telescope
E: 5.8 m telescope
Answer: B
@
Although ESO is headquartered in Germany, its telescopes and observatories are in northern Chile, where the organisation operates advanced ground-based astronomical facilities:

La Silla, which hosts the New Technology Telescope (NTT)
Paranal, where the Very Large Telescope (VLT) is located
Llano de Chajnantor, where ALMA, the Atacama Large Millimeter/submillimeter Array, is located
These are among the best locations for astronomical observations in the southern hemisphere.[20] An ESO project is the Extremely Large Telescope (ELT), a 40-metre-class telescope based on a five-mirror design and the formerly planned Overwhelmingly Large Telescope. The ELT will be the largest visible and near-infrared telescope in the world. ESO began its design in early 2006, and aimed to begin construction in 2012.[21] Construction work at the ELT site started in June 2014.[22] As decided by the ESO council on 26 April 2010, a fourth site (Cerro Armazones) is to be home to ELT.[23][24][25]

Each year about 2,000 requests are made for the use of ESO telescopes, for four to six times more nights than are available. Observations made with these instruments appear in a number of peer-reviewed publications annually; in 2017, more than 1,000 reviewed papers based on ESO data were published.[26]

ESO telescopes generate large amounts of data at a high rate, which are stored in a permanent archive facility at ESO headquarters. The archive contains more than 1.5 million images (or spectra) with a total volume of about 65 terabytes (65,000,000,000,000 bytes) of data.
$
5
Question: Where is the headquarters of ESO located?
A: Northern Chile
B: France
C: Germany
D: Spain
E: Italy
Answer: C

Question: Which facility hosts the New Technology Telescope (NTT)?
A: Llano de Chajnantor
B: Paranal
C: Cerro Armazones
D: La Silla
E: Mauna Kea
Answer: D

Question: How many peer-reviewed papers based on ESO data were published in 2017?
A: 500
B: 800
C: 1000
D: 1200
E: 1500
Answer: C

Question: As of the data provided, how much data is stored in ESO's archive facility?
A: 25 terabytes
B: 45 terabytes
C: 65 terabytes
D: 85 terabytes
E: 105 terabytes
Answer: C

Question: What is the size class of the Extremely Large Telescope (ELT) being developed by ESO?
A: 20-metre-class
B: 30-metre-class
C: 40-metre-class
D: 50-metre-class
E: 60-metre-class
Answer: C
@
The Danish 1.54-metre telescope at La Silla participated in the discovery of one of the most Earth-like planets found to date. The planet, detected using the microlensing technique and about five times as massive as Earth, circles its parent star in about 10 years and most certainly has a rocky and icy surface.[84][85]

In 2017, Breakthrough Initiatives and the European Southern Observatory (ESO) entered a collaboration[79][86] to enable and implement a search for habitable planets in the nearby star system, Alpha Centauri. The agreement involves Breakthrough Initiatives providing funding for an upgrade to the VISIR (VLT Imager and Spectrometer for mid-Infrared)[87] instrument on ESO's Very Large Telescope (VLT) in Chile. This upgrade will greatly increase the likelihood of planet detection in the system.

In August 2016, the European Southern Observatory announced the detection of a planet orbiting the third star in the Alpha Centauri system, Proxima Centauri.[88][89] The planet, called Proxima Centauri b, could be a potential target for one of the projects of Breakthrough Initiatives.

Breakthrough Starshot[90] is a proof of concept mission to send a fleet of ultra-fast light-driven nanocraft to explore the Alpha Centauri star system, which could pave the way for a first launch within the next generation. An objective of the mission would be to make a fly-by of and possibly photograph any Earth-like worlds that might exist in the system.

In March 2019, ESO astronomers, employing the GRAVITY instrument on their Very Large Telescope Interferometer (VLTI), announced the first direct detection of an exoplanet, HR 8799 e, using optical interferometry.[91]
$
5
Question: Which technique was used to detect one of the most Earth-like planets by the Danish 1.54-metre telescope at La Silla?
A: Radial velocity
B: Transit photometry
C: Astrometry
D: Microlensing
E: Direct imaging
Answer: D

Question: Which star system is the closest to our Solar System?
A: Sirius
B: Barnard's Star
C: Alpha Centauri
D: Vega
E: Betelgeuse
Answer: C

Question: Which initiative aims to send a fleet of ultra-fast light-driven nanocraft to explore the Alpha Centauri star system?
A: Breakthrough Listen
B: Breakthrough Watch
C: Breakthrough Starshot
D: Breakthrough Message
E: Breakthrough Encounters
Answer: C

Question: Which instrument allowed ESO astronomers to make the first direct detection of an exoplanet using optical interferometry in March 2019?
A: HARPS
B: VISIR
C: GRAVITY
D: ESPRESSO
E: NIRPS
Answer: C

Question: The discovery of which planet suggests the possibility of a rocky and icy surface and orbits its parent star in about 10 years?
A: Proxima Centauri b
B: Gliese 581c
C: 2M1207b
D: HR 8799 e
E: Kepler-22b
Answer: A
@
In gamma-ray astronomy, gamma-ray bursts (GRBs) are immensely energetic explosions that have been observed in distant galaxies. They are the most energetic and luminous electromagnetic events since the Big Bang.[1] Bursts can last from ten milliseconds to several hours.[2][3] After an initial flash of gamma rays, a longer-lived "afterglow" is usually emitted at longer wavelengths (X-ray, ultraviolet, optical, infrared, microwave and radio).[4]

The intense radiation of most observed GRBs is thought to be released during a supernova or superluminous supernova as a high-mass star implodes to form a neutron star or a black hole. A subclass of GRBs appears to originate from the merger of binary neutron stars.[5]
$
5
Question: What are gamma-ray bursts (GRBs) in the context of gamma-ray astronomy?
A: Short-lived electromagnetic signals from the Sun.
B: Rapid movements of stars within a galaxy.
C: Immensely energetic explosions observed in distant galaxies.
D: A type of radio wave from nearby star systems.
E: Light emissions from supernovae.
Answer: C

Question: Which of these describes the afterglow of GRBs?
A: A shorter, more intense gamma-ray emission.
B: A continued emission at longer wavelengths.
C: A sudden burst of X-rays without any other signals.
D: A precursor signal before the main gamma-ray burst.
E: An intense infrared burst lasting several hours.
Answer: B

Question: Most of the radiation from observed GRBs is thought to be released during what event?
A: A solar flare.
B: The merger of two galaxies.
C: A supernova or when a high-mass star implodes.
D: The collision of two neutron stars.
E: The expansion of a black hole.
Answer: C

Question: What event is associated with a subclass of GRBs?
A: Formation of a white dwarf.
B: Collision of two galaxies.
C: Supernova explosion.
D: Formation of a magnetar.
E: Merger of binary neutron stars.
Answer: E

Question: How long can GRBs last?
A: A few microseconds.
B: A few seconds.
C: From ten milliseconds to several hours.
D: Several days.
E: A few weeks.
Answer: C
@
The sources of most GRBs are billions of light years away from Earth, implying that the explosions are both extremely energetic (a typical burst releases as much energy in a few seconds as the Sun will in its entire 10-billion-year lifetime)[6] and extremely rare (a few per galaxy per million years[7]). All observed GRBs have originated from outside the Milky Way galaxy, although a related class of phenomena, soft gamma repeaters, are associated with magnetars within the Milky Way. It has been hypothesized that a gamma-ray burst in the Milky Way, pointing directly towards the Earth, could cause a mass extinction event.[8] The Late Ordovician mass extinction has been hypothesised by some researchers to have occurred as a result of such a gamma-ray burst.[9][10][11]
$
5
Question: Where have all observed GRBs originated from?
A: Within the Milky Way galaxy.
B: From outside the Milky Way galaxy.
C: From our neighboring galaxy, Andromeda.
D: From black holes.
E: From neutron stars within the Milky Way.
Answer: B

Question: What phenomenon within the Milky Way is related to gamma-ray bursts?
A: White dwarf explosions.
B: Magnetars.
C: Black hole formations.
D: Stellar wind patterns.
E: Galactic mergers.
Answer: B

Question: How does the energy of a typical GRB compare to the Sun's energy output?
A: It releases the same amount of energy as the Sun in a day.
B: It releases the same amount of energy as the Sun in a year.
C: It releases less energy than the Sun in its entire lifetime.
D: It releases as much energy in a few seconds as the Sun will in its entire 10-billion-year lifetime.
E: It releases the same amount of energy as the Sun in a millennium.
Answer: D

Question: What could be a potential consequence of a gamma-ray burst in the Milky Way pointing directly towards Earth?
A: A supernova explosion.
B: A new star formation.
C: A solar flare.
D: A mass extinction event.
E: Formation of a black hole.
Answer: D

Question: Which mass extinction is hypothesized by some researchers to have occurred due to a gamma-ray burst?
A: Cretaceous-Paleogene mass extinction.
B: Permian mass extinction.
C: Triassic-Jurassic mass extinction.
D: Devonian mass extinction.
E: Late Ordovician mass extinction.
Answer: E
@
Gamma-ray bursts were first observed in the late 1960s by the U.S. Vela satellites, which were built to detect gamma radiation pulses emitted by nuclear weapons tested in space. The United States suspected that the Soviet Union might attempt to conduct secret nuclear tests after signing the Nuclear Test Ban Treaty in 1963.[14] On July 2, 1967, at 14:19 UTC, the Vela 4 and Vela 3 satellites detected a flash of gamma radiation unlike any known nuclear weapons signature.[15] Uncertain what had happened but not considering the matter particularly urgent, the team at the Los Alamos National Laboratory, led by Ray Klebesadel, filed the data away for investigation. As additional Vela satellites were launched with better instruments, the Los Alamos team continued to find inexplicable gamma-ray bursts in their data. By analyzing the different arrival times of the bursts as detected by different satellites, the team was able to determine rough estimates for the sky positions of 16 bursts[15] and definitively rule out a terrestrial or solar origin. Contrary to popular belief, the data was never classified.[16] After thorough analysis, the findings were published in 1973 as an Astrophysical Journal article entitled "Observations of Gamma-Ray Bursts of Cosmic Origin".[12]

Most early theories of gamma-ray bursts posited nearby sources within the Milky Way Galaxy. From 1991, the Compton Gamma Ray Observatory (CGRO) and its Burst and Transient Source Explorer (BATSE) instrument, an extremely sensitive gamma-ray detector, provided data that showed the distribution of GRBs is isotropic – not biased towards any particular direction in space.[17] If the sources were from within our own galaxy, they would be strongly concentrated in or near the galactic plane. The absence of any such pattern in the case of GRBs provided strong evidence that gamma-ray bursts must come from beyond the Milky Way.[18][19][20][21] However, some Milky Way models are still consistent with an isotropic distribution.[18][22]
$
5
Question: Why were the U.S. Vela satellites originally launched?
A: To detect alien communications.
B: To study gamma-ray bursts.
C: To detect gamma radiation from nuclear weapons in space.
D: To map the Milky Way galaxy.
E: To monitor solar flares.
Answer: C

Question: When did the Vela satellites first detect an unexplained gamma radiation flash?
A: July 2, 1965
B: July 2, 1967
C: July 2, 1969
D: July 2, 1971
E: July 2, 1973
Answer: B

Question: Which journal published the findings on the cosmic origin of gamma-ray bursts in 1973?
A: Nature
B: Science
C: Physical Review Letters
D: Journal of Astrophysics
E: Astrophysical Journal
Answer: E

Question: What did the data from the Compton Gamma Ray Observatory (CGRO) suggest about the origin of GRBs?
A: They are concentrated near the galactic plane.
B: They originate only from black holes.
C: They come from within our Milky Way galaxy.
D: They must come from beyond the Milky Way.
E: They are uniformly distributed across the universe.
Answer: D

Question: What was the main argument against the Milky Way origin of GRBs based on the data from CGRO?
A: The isotropic distribution of GRBs.
B: The high energy of GRBs.
C: The short duration of GRBs.
D: The afterglow associated with GRBs.
E: The distance of GRBs from Earth.
Answer: A
@
Events with a duration of less than about two seconds are classified as short gamma-ray bursts. These account for about 30% of gamma-ray bursts, but until 2005, no afterglow had been successfully detected from any short event and little was known about their origins.[59] Since then, several dozen short gamma-ray burst afterglows have been detected and localized, several of which are associated with regions of little or no star formation, such as large elliptical galaxies.[60][61][62] This rules out a link to massive stars, confirming that short events are physically distinct from long events. In addition, there has been no association with supernovae.[63]

The true nature of these objects was initially unknown, and the leading hypothesis was that they originated from the mergers of binary neutron stars or a neutron star with a black hole. Such mergers were theorized to produce kilonovae,[64] and evidence for a kilonova associated with GRB 130603B was seen.[65][66] The mean duration of these events of 0.2 seconds suggests (because of causality) a source of very small physical diameter in stellar terms; less than 0.2 light-seconds (about 60,000 km or 37,000 miles – four times the Earth's diameter). The observation of minutes to hours of X-ray flashes after a short gamma-ray burst is consistent with small particles of a primary object like a neutron star initially swallowed by a black hole in less than two seconds, followed by some hours of lesser energy events, as remaining fragments of tidally disrupted neutron star material (no longer neutronium) remain in orbit to spiral into the black hole, over a longer period of time.[59] A small fraction of short gamma-ray bursts are probably produced by giant flares from soft gamma repeaters in nearby galaxies.[67][68]

The origin of short GRBs in kilonovae was confirmed when short GRB 170817A was detected only 1.7 s after the detection of gravitational wave GW170817, which was a signal from the merger of two neutron stars.[5]
$
5
Question: What is the classification for gamma-ray bursts with a duration of less than two seconds?
A: Long gamma-ray bursts.
B: Mini gamma-ray bursts.
C: Intermediate gamma-ray bursts.
D: Soft gamma repeaters.
E: Short gamma-ray bursts.
Answer: E

Question: What type of regions are several short gamma-ray burst afterglows associated with?
A: Regions with intense star formation.
B: Regions near black holes.
C: Regions of little or no star formation.
D: Regions near the center of galaxies.
E: Regions near supernovae.
Answer: C

Question: What event was confirmed as the origin of short GRBs with the detection of GRB 170817A and gravitational wave GW170817?
A: Supernova explosion.
B: Merger of two neutron stars.
C: Formation of a black hole.
D: Collision of two galaxies.
E: Explosion of a massive star.
Answer: B

Question: What object's giant flares can produce a small fraction of short gamma-ray bursts?
A: Magnetars.
B: Neutron stars.
C: Black holes.
D: Dwarf stars.
E: White dwarfs.
Answer: A

Question: Which technique has not been linked to the origin of short gamma-ray bursts?
A: Binary neutron star mergers.
B: Neutron star and black hole mergers.
C: Giant flares from soft gamma repeaters.
D: Supernovae.
E: Tidal disruptions of neutron stars by black holes.
Answer: D
@
The Galactic Center is the rotational center, the barycenter, of the Milky Way galaxy.[1][2] Its central massive object is a supermassive black hole of about 4 million solar masses, which is called Sagittarius A*,[3][4][5] a compact radio source which is almost exactly at the galactic rotational center. The Galactic Center is approximately 8 kiloparsecs (26,000 ly) away from Earth[3] in the direction of the constellations Sagittarius, Ophiuchus, and Scorpius, where the Milky Way appears brightest, visually close to the Butterfly Cluster (M6) or the star Shaula, south to the Pipe Nebula.

There are around 10 million stars within one parsec of the Galactic Center, dominated by red giants, with a significant population of massive supergiants and Wolf–Rayet stars from star formation in the region around 1 million years ago. The core stars are a small part within the much wider galactic bulge.
$
5
Question: What is the name of the supermassive black hole at the Galactic Center?
A: Orion A*
B: Cygnus X-1
C: Sagittarius A*
D: Centaurus B*
E: Vega Z*
Answer: C

Question: How far away is the Galactic Center from Earth?
A: 800,000 light years
B: 26,000 light years
C: 4 million light years
D: 100,000 light years
E: 50,000 light years
Answer: B

Question: Which star or cluster is visually close to the Galactic Center?
A: Pleiades Cluster
B: Betelgeuse
C: Butterfly Cluster (M6)
D: Sirius
E: Polaris
Answer: C

Question: Which type of stars dominate within one parsec of the Galactic Center?
A: White dwarfs
B: Supergiants
C: Red giants
D: Blue giants
E: Neutron stars
Answer: C

Question: The core stars of the Galactic Center are part of what larger structure?
A: Galactic halo
B: Galactic spiral
C: Galactic arm
D: Galactic disk
E: Galactic bulge
Answer: E
@
Because of interstellar dust along the line of sight, the Galactic Center cannot be studied at visible, ultraviolet, or soft (low-energy) X-ray wavelengths. The available information about the Galactic Center comes from observations at gamma ray, hard (high-energy) X-ray, infrared, submillimetre, and radio wavelengths.

Immanuel Kant stated in Universal Natural History and Theory of the Heavens (1755) that a large star was at the center of the Milky Way Galaxy, and that Sirius might be the star.[6] Harlow Shapley stated in 1918 that the halo of globular clusters surrounding the Milky Way seemed to be centered on the star swarms in the constellation of Sagittarius, but the dark molecular clouds in the area blocked the view for optical astronomy.[7] In the early 1940s Walter Baade at Mount Wilson Observatory took advantage of wartime blackout conditions in nearby Los Angeles to conduct a search for the center with the 100-inch (250 cm) Hooker Telescope. He found that near the star Alnasl (Gamma Sagittarii) there is a one-degree-wide void in the interstellar dust lanes, which provides a relatively clear view of the swarms of stars around the nucleus of the Milky Way Galaxy.[8] This gap has been known as Baade's Window ever since.[9]
$
5
Question: Which star did Immanuel Kant hypothesize might be at the center of the Milky Way Galaxy?
A: Alpha Centauri
B: Vega
C: Sirius
D: Polaris
E: Betelgeuse
Answer: C

Question: The clear view near Alnasl is known as?
A: Kant's Window
B: Shapley's View
C: Baade's Window
D: Sagittarius Sight
E: Milky Way Mirror
Answer: C

Question: Because of what obstacle is the Galactic Center not visible at certain wavelengths?
A: Solar interference
B: Intergalactic gas
C: Nebula clouds
D: Interstellar dust
E: Black holes
Answer: D

Question: Harlow Shapley in 1918 stated that the halo of globular clusters seemed to center on star swarms in which constellation?
A: Orion
B: Draco
C: Leo
D: Taurus
E: Sagittarius
Answer: E

Question: Who conducted a search for the Galactic Center using the 100-inch Hooker Telescope during wartime blackout conditions?
A: Immanuel Kant
B: Harlow Shapley
C: Walter Baade
D: G. Dobler
E: Ray Klebesadel
Answer: C
@
In November 2010, it was announced that two large elliptical lobe structures of energetic plasma, termed bubbles, which emit gamma- and X-rays, were detected astride the Milky Way galaxy's core.[37] Termed Fermi or eRosita bubbles,[38] they extend up to about 25,000 light years above and below the Galactic Center.[37] The galaxy's diffuse gamma-ray fog hampered prior observations, but the discovery team led by D. Finkbeiner, building on research by G. Dobler, worked around this problem.[37] The 2014 Bruno Rossi Prize went to Tracy Slatyer, Douglas Finkbeiner, and Meng Su "for their discovery, in gamma rays, of the large unanticipated Galactic structure called the Fermi bubbles".[39]

The origin of the bubbles is being researched.[40][41] The bubbles are connected and seemingly coupled, via energy transport, to the galactic core by columnar structures of energetic plasma termed chimneys.[42] In 2020, for the first time, the lobes were seen in visible light[43] and optical measurements were made.[44] By 2022, detailed computer simulations further confirmed that the bubbles were caused by the Sagittarius A* black hole.[45][38]
$
5
Question: In what year were the Fermi or eRosita bubbles announced?
A: 2005
B: 2010
C: 2014
D: 2017
E: 2020
Answer: B

Question: Which award was given in recognition of the discovery of the Fermi bubbles?
A: Nobel Prize in Physics
B: Shaw Prize
C: Gruber Prize in Cosmology
D: Bruno Rossi Prize
E: Breakthrough of the Year Award
Answer: D

Question: In what year were the Fermi bubbles first observed in visible light?
A: 2015
B: 2016
C: 2017
D: 2019
E: 2020
Answer: E

Question: What is believed to have caused the Fermi bubbles?
A: Supernova explosions
B: Intergalactic winds
C: Sagittarius A* black hole
D: Colliding galaxies
E: Pulsars
Answer: C

Question: What structures connect and transport energy from the galactic core to the bubbles?
A: Star trails
B: Gamma beams
C: Energy filaments
D: Plasma chimneys
E: Galactic bridges
Answer: D
@
The central cubic parsec around Sagittarius A* contains around 10 million stars.[46] Although most of them are old red giant stars, the Galactic Center is also rich in massive stars. More than 100 OB and Wolf–Rayet stars have been identified there so far.[47] They seem to have all been formed in a single star formation event a few million years ago. The existence of these relatively young stars was a surprise to experts, who expected the tidal forces from the central black hole to prevent their formation. This paradox of youth is even stronger for stars that are on very tight orbits around Sagittarius A*, such as S2 and S0-102. The scenarios invoked to explain this formation involve either star formation in a massive star cluster offset from the Galactic Center that would have migrated to its current location once formed, or star formation within a massive, compact gas accretion disk around the central black-hole. Current evidence favors the latter theory, as formation through a large accretion disk is more likely to lead to the observed discrete edge of the young stellar cluster at roughly 0.5 parsec.[48] Most of these 100 young, massive stars seem to be concentrated within one or two disks, rather than randomly distributed within the central parsec.[49][50] This observation however does not allow definite conclusions to be drawn at this point.
$
5
Question: Approximately how many stars are contained in the central cubic parsec around Sagittarius A*?
A: 1 million
B: 100,000
C: 10 million
D: 1 billion
E: 500,000
Answer: C

Question: What type of stars were unexpected to be found near the central black hole due to tidal forces?
A: Neutron stars
B: Old red giant stars
C: Young massive stars
D: White dwarfs
E: Brown dwarfs
Answer: C

Question: The observed discrete edge of the young stellar cluster is approximately how far from the center?
A: 0.1 parsec
B: 0.5 parsec
C: 1 parsec
D: 2 parsecs
E: 5 parsecs
Answer: B

Question: Stars like S2 and S0-102 that are on tight orbits around Sagittarius A* present what kind of paradox?
A: Paradox of mass
B: Paradox of age
C: Paradox of rotation
D: Paradox of youth
E: Paradox of distance
Answer: D

Question: The majority of the young massive stars near Sagittarius A* appear to be concentrated within?
A: Random regions
B: Galactic arms
C: One or two disks
D: Spiral formations
E: Black hole accretion zones
Answer: C
@
Sagittarius A* (/ˈeɪ stɑːr/ AY star), abbreviated Sgr A* (/ˈsædʒ ˈeɪ stɑːr/ SAJ AY star[3]), is the supermassive black hole[4][5][6] at the Galactic Center of the Milky Way. It is located near the border of the constellations Sagittarius and Scorpius, about 5.6° south of the ecliptic,[7] visually close to the Butterfly Cluster (M6) and Lambda Scorpii.

The object is a bright and very compact astronomical radio source. The name Sagittarius A* follows from historical reasons. In 1954,[8] John D. Kraus, Hsien-Ching Ko, and Sean Matt listed the radio sources they identified with the Ohio State University radio telescope at 250 MHz. They arranged these sources by constellation and then assigned capital letters in order of brightness within each constellation, with A denoting the brightest radio source within the constellation. The asterisk * is a later addition and was added because its discovery was considered "exciting",[9] in parallel with the nomenclature for excited state atoms which are denoted with an asterisk (for example, the excited state of helium would be He*). The asterisk was assigned in 1982 by Robert L. Brown,[10] who understood that the strongest radio emission from the center of the galaxy appeared to be due to a compact nonthermal radio object.
$
5
Question: Which object is located at the Galactic Center of the Milky Way?
A: Andromeda A*
B: Orion B*
C: Sagittarius A*
D: Virgo C*
E: Canis Majoris Z*
Answer: C

Question: Why was an asterisk () added to the name Sagittarius A?
A: To denote its northern location.
B: To represent its binary nature.
C: To indicate it is an excited state.
D: To signify its large mass.
E: To mark it as the primary source.
Answer: C

Question: Who added the asterisk to Sagittarius A* in 1982?
A: Hsien-Ching Ko
B: John D. Kraus
C: Sean Matt
D: Robert L. Brown
E: Andrea Ghez
Answer: D

Question: Sagittarius A* is particularly associated with which kind of astronomical radio source?
A: Distant
B: Intermittent
C: Weak
D: Compact
E: Variable
Answer: D

Question: What determined the assignment of capital letters to radio sources according to the 1954 system?
A: Their size.
B: Their distance from Earth.
C: Their brightness within each constellation.
D: Their discovery date.
E: Their stability over time.
Answer: C
@
The observations of several stars orbiting Sagittarius A*, particularly star S2, have been used to determine the mass and upper limits on the radius of the object. Based on mass and increasingly precise radius limits, astronomers concluded that Sagittarius A* must be the Milky Way's central supermassive black hole.[11] The current value of its mass is 4.154±0.014 million solar masses.[2]

Reinhard Genzel and Andrea Ghez were awarded the 2020 Nobel Prize in Physics for their discovery that Sagittarius A* is a supermassive compact object, for which a black hole was the only plausible explanation at the time.[12]

In May 2022, astronomers released the first image of the accretion disk around the horizon of Sagittarius A*, confirming it to be a black hole, using the Event Horizon Telescope, a world-wide network of radio observatories.[13] This is the second confirmed image of a black hole, after Messier 87's supermassive black hole in 2019.[14][15] The black hole itself is not seen, only nearby objects whose behavior is influenced by the black hole. The observed radio and infrared energy emanates from gas and dust heated to millions of degrees while falling into the black hole.[16]
$
5
Question: Who was awarded the 2020 Nobel Prize in Physics for the discovery related to Sagittarius A*?
A: John D. Kraus and Hsien-Ching Ko
B: Reinhard Genzel and Andrea Ghez
C: Robert L. Brown and Sean Matt
D: Stephen Hawking and Roger Penrose
E: Karl Schwarzschild and Jocelyn Bell Burnell
Answer: B

Question: How was the mass of Sagittarius A* primarily determined?
A: By studying its gravitational waves.
B: By its radiation emissions.
C: By the observations of stars orbiting it.
D: By its magnetic field strength.
E: By measuring its diameter directly.
Answer: C

Question: What was the primary significance of the image captured in May 2022 related to Sagittarius A*?
A: It showed the actual black hole.
B: It showed a new galaxy.
C: It showed the accretion disk around the black hole.
D: It showed a supernova event.
E: It showed the birth of a star.
Answer: C

Question: The Event Horizon Telescope, which captured the image of Sagittarius A*'s accretion disk, previously imaged a black hole in which galaxy?
A: Andromeda
B: Triangulum
C: Whirlpool
D: Messier 87
E: NGC 1300
Answer: D

Question: What do the observed radio and infrared energy near a black hole primarily come from?
A: Nearby stars.
B: Cosmic background radiation.
C: Gas and dust heated to millions of degrees.
D: Electromagnetic pulses from neutron stars.
E: Gamma-ray bursts from star collisions.
Answer: C
@
First noticed as something unusual in images of the center of the Milky Way in 2002,[72] the gas cloud G2, which has a mass about three times that of Earth, was confirmed to be likely on a course taking it into the accretion zone of Sgr A* in a paper published in Nature in 2012.[73] Predictions of its orbit suggested it would make its closest approach to the black hole (a perinigricon) in early 2014, when the cloud was at a distance of just over 3,000 times the radius of the event horizon (or ≈260 AU, 36 light-hours) from the black hole. G2 has been observed to be disrupting since 2009,[73] and was predicted by some to be completely destroyed by the encounter, which could have led to a significant brightening of X-ray and other emission from the black hole. Other astronomers suggested the gas cloud could be hiding a dim star, or a binary star merger product, which would hold it together against the tidal forces of Sgr A*, allowing the ensemble to pass by without any effect.[74] In addition to the tidal effects on the cloud itself, it was proposed in May 2013[75] that, prior to its perinigricon, G2 might experience multiple close encounters with members of the black-hole and neutron-star populations thought to orbit near the Galactic Center, offering some insight to the region surrounding the supermassive black hole at the center of the Milky Way.[76]

The average rate of accretion onto Sgr A* is unusually small for a black hole of its mass[77] and is only detectable because it is so close to Earth. It was thought that the passage of G2 in 2013 might offer astronomers the chance to learn much more about how material accretes onto supermassive black holes. Several astronomical facilities observed this closest approach, with observations confirmed with Chandra, XMM, VLA, INTEGRAL, Swift, Fermi and requested at VLT and Keck.[78]
$
5
Question: What is unusual about the rate of accretion onto Sgr A* for its mass?
A: It is unusually high.
B: It is consistent with other black holes.
C: It is unusually small.
D: It changes unpredictably.
E: It is primarily sourced from gas clouds.
Answer: C

Question: In what year did the gas cloud G2 make its closest approach to the black hole?
A: 2002
B: 2009
C: 2012
D: 2013
E: 2014
Answer: E

Question: What was the original expectation for G2 when it approached Sgr A*?
A: It would become a star.
B: It would brighten X-ray and other emission from the black hole.
C: It would merge with Sgr A*.
D: It would deflect its path around the black hole.
E: It would cause a supernova.
Answer: B

Question: Which telescope confirmed observations during G2's closest approach?
A: Hubble Space Telescope
B: James Webb Space Telescope
C: Fermi Telescope
D: Chandra Telescope
E: Spitzer Space Telescope
Answer: D

Question: It was suggested in 2013 that before its closest approach, G2 might encounter what type of astronomical bodies near the Galactic Center?
A: Comets
B: Dwarf planets
C: Black holes and neutron stars
D: Pulsars
E: Rogue planets
Answer: C
@
An analysis published on July 21, 2014, based on observations by the ESO's Very Large Telescope in Chile, concluded alternatively that the cloud, rather than being isolated, might be a dense clump within a continuous but thinner stream of matter, and would act as a constant breeze on the disk of matter orbiting the black hole, rather than sudden gusts that would have caused high brightness as they hit, as originally expected. Supporting this hypothesis, G1, a cloud that passed near the black hole 13 years ago, had an orbit almost identical to G2, consistent with both clouds, and a gas tail thought to be trailing G2, all being denser clumps within a large single gas stream.[82][84]

Professor Andrea Ghez et al. suggested in 2014 that G2 is not a gas cloud but rather a pair of binary stars that had been orbiting the black hole in tandem and merged into an extremely large star.[74][85]
$
5
Question: According to a 2014 analysis based on ESO's Very Large Telescope, G2 might not be an isolated cloud but a dense clump within what?
A: A meteor shower
B: A black hole's event horizon
C: A thinner stream of matter
D: A supernova remnant
E: A binary star system
Answer: C

Question: G1, a cloud that passed the black hole 13 years before G2, had an orbit that was:
A: Completely different from G2
B: In the opposite direction of G2
C: Almost identical to G2
D: Perpendicular to G2
E: Much farther from the black hole than G2
Answer: C

Question: Professor Andrea Ghez suggested in 2014 that G2 might actually be what?
A: A rogue planet
B: A neutron star
C: A comet cloud
D: A binary star merger product
E: A pulsar
Answer: D

Question: The gas tail thought to be trailing G2 and G1's almost identical orbit to G2 suggest that these objects might be:
A: From a different galaxy
B: Completely independent of each other
C: The remnants of a supernova
D: Denser clumps within a single gas stream
E: Part of a planetary system
Answer: D

Question: The idea that G2 could be a pair of binary stars that merged is supported by the possibility of it forming a(n):
A: Neutron star
B: Supernova
C: Extremely large star
D: New galaxy
E: Gas cloud
Answer: C
@
The kelvin, symbol K, is a unit of measurement for temperature.[1] The Kelvin scale is an absolute scale, which is defined such that 0 K is absolute zero and a change of thermodynamic temperature T by 1 kelvin corresponds to a change of thermal energy kT by 1.380649×10−23 J. The Boltzmann constant k = 1.380649×10−23 J⋅K−1 was exactly defined in the 2019 redefinition of the SI base units such that the triple point of water is 273.16±0.0001 K.[2] The kelvin is the base unit of temperature in the International System of Units (SI), used alongside its prefixed forms.[2][3][4] It is named after the Belfast-born and University of Glasgow-based engineer and physicist William Thomson, 1st Baron Kelvin (1824–1907).[5]

Historically, the Kelvin scale was developed from the Celsius scale, such that 273.15 K was 0 °C (the approximate melting point of ice) and a change of one kelvin was exactly equal to a change of one degree Celsius.[1][5] This relationship remains accurate, but the Celsius, Fahrenheit, and Rankine scales are now defined in terms of the Kelvin scale.[2][6][7] The kelvin is the primary unit of temperature for engineering and the physical sciences, while in most countries the Celsius scale remains the dominant scale outside of these fields.[5] In the United States, outside of the physical sciences, the Fahrenheit scale predominates, with the kelvin or Rankine scale employed for absolute temperature.[6]
$
5
Question: What is the Kelvin scale's starting point, known as absolute zero?
A: 100 K
B: 0 K
C: 273.15 K
D: 373.15 K
E: -273.15 K
Answer: B

Question: The Kelvin scale was historically developed from which temperature scale?
A: Fahrenheit
B: Rankine
C: Celsius
D: Newton
E: Réaumur
Answer: C

Question: Who is the Kelvin scale named after?
A: James Joule
B: Isaac Newton
C: William Thomson
D: Michael Faraday
E: Anders Celsius
Answer: C

Question: What is the base unit of temperature in the International System of Units?
A: Celsius
B: Fahrenheit
C: Rankine
D: Joule
E: Kelvin
Answer: E

Question: The kelvin is the primary temperature unit for which fields?
A: Cooking and gastronomy
B: Journalism
C: Meteorology in most countries
D: Engineering and physical sciences
E: Daily communication in the United States
Answer: D
@
During the 18th century, multiple temperature scales were developed,[8] notably Fahrenheit and centigrade (later Celsius). These scales predated much of the modern science of thermodynamics, including atomic theory and the kinetic theory of gases which underpin the concept of absolute zero. Instead, they chose defining points within the range of human experience that could be reproduced easily and with reasonable accuracy, but lacked any deep significance in thermal physics. In the case of the Celsius scale (and the long since defunct Newton scale and Réaumur scale) the melting point of water served as such a starting point, with Celsius being defined, from the 1740s up until the 1940s, by calibrating a thermometer such that

The freezing point of water is 0 degrees.
The boiling point of water is 100 degrees.
This definition assumes pure water at a specific pressure chosen to approximate the natural air pressure at sea level. Thus an increment of 1 °C equals 
1
/
100
 of the temperature difference between the melting and boiling points. This temperature interval would go on to become the template for the kelvin.
$
5
Question: During which century were the Fahrenheit and Celsius scales developed?
A: 16th century
B: 17th century
C: 18th century
D: 19th century
E: 20th century
Answer: C

Question: On the original Celsius scale, what was the boiling point of water?
A: 0 degrees
B: 50 degrees
C: 100 degrees
D: 212 degrees
E: 373.15 degrees
Answer: C

Question: What pressure condition is assumed for the boiling and freezing points of water in the Celsius scale definition?
A: Absolute zero pressure
B: Natural air pressure at sea level
C: Pressure at Mount Everest summit
D: Pressure in space
E: Pressure in deep sea trenches
Answer: B

Question: Before the development of the Kelvin scale, what was the key difference in older temperature scales like Celsius?
A: They were based on quantum mechanics
B: They were defined using absolute zero
C: They were based on daily average temperatures
D: They chose defining points easily reproduced and within human experience
E: They used the Boltzmann constant
Answer: D

Question: The temperature interval between the melting and boiling points of water on the Celsius scale became a template for which other scale?
A: Fahrenheit
B: Rankine
C: Newton
D: Kelvin
E: Réaumur
Answer: D
@
In 1848, William Thomson, who was later ennobled as Lord Kelvin, published a paper On an Absolute Thermometric Scale.[12][13][14] Using the soon-to-be-defunct caloric theory, he proposed an "absolute" scale based on the following parameters:

The melting point of water is 0 degrees.
The boiling point of water is 100 degrees.
"The arbitrary points which coincide on the two scales are 0° and 100°"

Any two heat engines whose heat source and heat sink are both separated by the same number of degrees will, per Carnot's theorem, be capable of producing the same amount of mechanical work per unit of "caloric" passing through.
$
3
Question: Who proposed the "absolute" thermometric scale in 1848?
A: Isaac Newton
B: Michael Faraday
C: William Thomson
D: James Joule
E: Anders Celsius
Answer: C

Question: In Lord Kelvin's "absolute" scale, which points coincided on both the Kelvin and Celsius scales?
A: -273.15° and 0°
B: 0° and 100°
C: -100° and 0°
D: 100° and 273.15°
E: -273.15° and 273.15°
Answer: B

Question: Which theorem suggests that two heat engines with the same degree of separation between their heat source and sink will produce the same amount of work per unit?
A: Newton's third law
B: Joule's law
C: Carnot's theorem
D: Faraday's law
E: Boyle's law
Answer: C
@
In 2005, the CIPM began a programme to redefine the kelvin (along with the other SI units) using a more experimentally rigorous method. In particular, the committee proposed redefining the kelvin such that the Boltzmann constant takes the exact value 1.3806505×10−23 J/K.[29] The committee had hoped that the program would be completed in time for its adoption by the CGPM at its 2011 meeting, but at the 2011 meeting the decision was postponed to the 2014 meeting when it would be considered as part of a larger program.[30]

The redefinition was further postponed in 2014, pending more accurate measurements of the Boltzmann constant in terms of the current definition,[31] but was finally adopted at the 26th CGPM in late 2018, with a value of k = 1.380649×10−23 J⋅K−1.[32][29][1][2][4][33]

For scientific purposes, the main advantage is that this allows measurements at very low and very high temperatures to be made more accurately, as the techniques used depend on the Boltzmann constant.
$
4
Question: When did the CIPM begin a program to redefine the kelvin using a more rigorous method?
A: 1995
B: 2000
C: 2005
D: 2010
E: 2015
Answer: C

Question: In the redefined kelvin scale, the Boltzmann constant takes an exact value of:
A: 1.3806505×10−23 J/K
B: 1.380649×10−22 J/K
C: 1.3806505×10−22 J/K
D: 1.380649×10−23 J⋅K−1
E: 2.380649×10−23 J/K
Answer: D

Question: Why was the redefinition of the Kelvin scale seen as advantageous for scientific purposes?
A: It simplified calculations in classical mechanics
B: It made temperature scales more comprehensible for the general public
C: It allowed for more accurate measurements at very low and high temperatures
D: It reduced the need for experimental setups
E: It was more compatible with the Fahrenheit scale
Answer: C

Question: The redefinition of the Kelvin scale was finally adopted in which year?
A: 2014
B: 2015
C: 2016
D: 2017
E: 2018
Answer: E
@
The concept entropy was first developed by German physicist Rudolf Clausius in the mid-nineteenth century as a thermodynamic property that predicts that certain spontaneous processes are irreversible or impossible. In statistical mechanics, entropy is formulated as a statistical property using probability theory. The statistical entropy perspective was introduced in 1870 by Austrian physicist Ludwig Boltzmann, who established a new field of physics that provided the descriptive linkage between the macroscopic observation of nature and the microscopic view based on the rigorous treatment of large ensembles of microstates that constitute thermodynamic systems.
$
5
Question: Who first developed the concept of entropy as a thermodynamic property?
A: Ludwig Boltzmann
B: William Thomson
C: Rudolf Clausius
D: Richard Feynman
E: Albert Einstein
Answer: C

Question: In what field was the statistical perspective of entropy introduced?
A: Quantum mechanics
B: Relativity
C: Statistical mechanics
D: Electromagnetism
E: Fluid dynamics
Answer: C

Question: What does statistical mechanics link together?
A: Large ensembles of microstates and thermodynamic systems
B: Quantum particles and waves
C: Energy and motion
D: Gravity and time
E: Mass and energy
Answer: A

Question: In which century was the concept of entropy introduced?
A: 17th century
B: 18th century
C: 19th century
D: 20th century
E: 21st century
Answer: C

Question: Who established the statistical entropy perspective?
A: Isaac Newton
B: Rudolf Clausius
C: Albert Einstein
D: Ludwig Boltzmann
E: Niels Bohr
Answer: D
@
Ludwig Boltzmann defined entropy as a measure of the number of possible microscopic states (microstates) of a system in thermodynamic equilibrium, consistent with its macroscopic thermodynamic properties, which constitute the macrostate of the system. A useful illustration is the example of a sample of gas contained in a container. The easily measurable parameters volume, pressure, and temperature of the gas describe its macroscopic condition (state). At a microscopic level, the gas consists of a vast number of freely moving atoms or molecules, which randomly collide with one another and with the walls of the container. The collisions with the walls produce the macroscopic pressure of the gas, which illustrates the connection between microscopic and macroscopic phenomena.

A microstate of the system is a description of the positions and momenta of all its particles. The large number of particles of the gas provides an infinite number of possible microstates for the sample, but collectively they exhibit a well-defined average of configuration, which is exhibited as the macrostate of the system, to which each individual microstate contribution is negligibly small. The ensemble of microstates comprises a statistical distribution of probability for each microstate, and the group of most probable configurations accounts for the macroscopic state. Therefore, the system can be described as a whole by only a few macroscopic parameters, called the thermodynamic variables: the total energy E, volume V, pressure P, temperature T, and so forth. However, this description is relatively simple only when the system is in a state of equilibrium.
$
5
Question: What does entropy measure in terms of a system in thermodynamic equilibrium?
A: Total energy
B: Molecular speed
C: Pressure
D: Number of possible microscopic states
E: Volume of the system
Answer: D

Question: What parameters describe the macroscopic condition of a gas sample?
A: Mass, charge, and spin
B: Density, viscosity, and surface tension
C: Volume, pressure, and temperature
D: Atomic number, valency, and radius
E: Humidity, opacity, and conductivity
Answer: C

Question: A microstate of a system gives information about:
A: The average conditions of a system
B: Positions and momenta of all its particles
C: The temperature and pressure of a system
D: The macrostate of the system
E: The volume and surface area of a system
Answer: B

Question: What does a macrostate of the system represent?
A: The individual behavior of each particle
B: The average behavior over all the microstates
C: The speed of the particles
D: The energy levels of the particles
E: The molecular structure of the particles
Answer: B

Question: Systems are simply described by thermodynamic variables when:
A: They are undergoing a phase change
B: They are at a very high temperature
C: They are in a state of equilibrium
D: They are under extreme pressure
E: They are in motion
Answer: C
@
We can view Ω as a measure of our lack of knowledge about a system. As an illustration of this idea, consider a set of 100 coins, each of which is either heads up or tails up. The macrostates are specified by the total number of heads and tails, whereas the microstates are specified by the facings of each individual coin. For the macrostates of 100 heads or 100 tails, there is exactly one possible configuration, so our knowledge of the system is complete. At the opposite extreme, the macrostate which gives us the least knowledge about the system consists of 50 heads and 50 tails in any order, for which there are 100,891,344,545,564,193,334,812,497,256 (100 choose 50) ≈ 1029 possible microstates.

Even when a system is entirely isolated from external influences, its microstate is constantly changing. For instance, the particles in a gas are constantly moving, and thus occupy a different position at each moment of time; their momenta are also constantly changing as they collide with each other or with the container walls. Suppose we prepare the system in an artificially highly ordered equilibrium state. For instance, imagine dividing a container with a partition and placing a gas on one side of the partition, with a vacuum on the other side. If we remove the partition and watch the subsequent behavior of the gas, we will find that its microstate evolves according to some chaotic and unpredictable pattern, and that on average these microstates will correspond to a more disordered macrostate than before. It is possible, but extremely unlikely, for the gas molecules to bounce off one another in such a way that they remain in one half of the container. It is overwhelmingly probable for the gas to spread out to fill the container evenly, which is the new equilibrium macrostate of the system.
$
5
Question: Ω can be viewed as a measure of:
A: Energy
B: Volume
C: Pressure
D: Lack of knowledge about a system
E: Temperature
Answer: D

Question: For a macrostate of 50 heads and 50 tails in a 100 coin system, approximately how many possible microstates are there?
A: 105
B: 1015
C: 1020
D: 1029
E: 1035
Answer: D

Question: An example of an artificially highly ordered equilibrium state is:
A: A gas filling an entire container evenly
B: A container half-filled with gas and half with a vacuum, separated by a partition
C: Gas molecules moving randomly in a container
D: A gas at very high pressure
E: A gas at very high temperature
Answer: B

Question: In the above example, the overwhelmingly probable macrostate after removing the partition is for the gas to:
A: Stay in its original half
B: Move to the other half
C: Spread out and fill the container evenly
D: Condense into a liquid
E: Evaporate out of the container
Answer: C

Question: An isolated system's microstate is constantly changing due to:
A: External forces
B: Magnetic fields
C: Collisions among particles
D: Gravitational pull
E: Radiation
Answer: C
@
In classical statistical mechanics, the number of microstates is actually uncountably infinite, since the properties of classical systems are continuous. For example, a microstate of a classical ideal gas is specified by the positions and momenta of all the atoms, which range continuously over the real numbers. If we want to define Ω, we have to come up with a method of grouping the microstates together to obtain a countable set. This procedure is known as coarse graining. In the case of the ideal gas, we count two states of an atom as the "same" state if their positions and momenta are within δx and δp of each other. Since the values of δx and δp can be chosen arbitrarily, the entropy is not uniquely defined. It is defined only up to an additive constant. (As we will see, the thermodynamic definition of entropy is also defined only up to a constant.)

However, this ambiguity can be resolved with quantum mechanics. The quantum state of a system can be expressed as a superposition of "basis" states, which can be chosen to be energy eigenstates (i.e. eigenstates of the quantum Hamiltonian). Usually, the quantum states are discrete, even though there may be an infinite number of them. For a system with some specified energy E, one takes Ω to be the number of energy eigenstates within a macroscopically small energy range between E and E + δE. In the thermodynamical limit, the specific entropy becomes independent on the choice of δE.
$
5
Question: In classical statistical mechanics, microstates are:
A: Countable and discrete
B: Uncountably infinite
C: Limited to a finite range
D: Restricted to quantum states
E: Based on external conditions
Answer: B

Question: The grouping of microstates to obtain a countable set is known as:
A: Quantum superposition
B: Entropy stabilization
C: Coarse graining
D: Fine tuning
E: Thermodynamic adjustment
Answer: C

Question: The quantum state of a system is expressed as a superposition of:
A: Classical states
B: Thermodynamic variables
C: Energy eigenstates
D: Entropy states
E: Microscopic states
Answer: C

Question: In the context of entropy, Ω represents:
A: Total energy
B: Volume
C: Number of microstates
D: Pressure
E: Temperature
Answer: C

Question: The thermodynamic definition of entropy is defined:
A: Accurately with no variations
B: Only in terms of quantum mechanics
C: In terms of classical mechanics
D: Only up to an additive constant
E: Based on the observer's perspective
Answer: D
@
In mathematics, an uncountable set (or uncountably infinite set)[1] is an infinite set that contains too many elements to be countable. The uncountability of a set is closely related to its cardinal number: a set is uncountable if its cardinal number is larger than that of the set of all natural numbers.

There are many equivalent characterizations of uncountability. A set X is uncountable if and only if any of the following conditions hold:

There is no injective function (hence no bijection) from X to the set of natural numbers.
X is nonempty and for every ω-sequence of elements of X, there exists at least one element of X not included in it. That is, X is nonempty and there is no surjective function from the natural numbers to X.
The cardinality of X is neither finite nor equal to 
ℵ
0
\aleph _{0} (aleph-null, the cardinality of the natural numbers).
The set X has cardinality strictly greater than 
ℵ
0
\aleph _{0}.
The first three of these characterizations can be proven equivalent in Zermelo–Fraenkel set theory without the axiom of choice, but the equivalence of the third and fourth cannot be proved without additional choice principles.

If an uncountable set X is a subset of set Y, then Y is uncountable.
$
5
Question: Which of the following defines a set to be uncountable?
A: The set has more elements than the set of all natural numbers.
B: The set is countably infinite.
C: The set has the same number of elements as the set of real numbers.
D: The set is a finite subset of natural numbers.
E: The set is empty.
Answer: A

Question: According to the given text, if set Y contains an uncountable set X, then what can be said about set Y?
A: Y is a finite set.
B: Y is also uncountable.
C: Y becomes countable.
D: Y is an empty set.
E: The cardinality of Y is equal to that of X.
Answer: B

Question: What does ℵ0 (aleph-null) represent?
A: The cardinality of real numbers.
B: The cardinality of all natural numbers.
C: An uncountable infinite set.
D: A finite set.
E: A set containing no elements.
Answer: B

Question: Which axiom cannot prove the equivalence of certain characterizations of uncountability without additional principles?
A: Axiom of Power Set.
B: Axiom of Infinity.
C: Axiom of Replacement.
D: Axiom of Union.
E: Zermelo–Fraenkel set theory.
Answer: E

Question: What does the absence of a bijection from set X to the set of natural numbers signify?
A: X is countable.
B: X has the same size as natural numbers.
C: X is uncountable.
D: X has a finite number of elements.
E: X is an empty set.
Answer: C
@
The best known example of an uncountable set is the set R of all real numbers; Cantor's diagonal argument shows that this set is uncountable. The diagonalization proof technique can also be used to show that several other sets are uncountable, such as the set of all infinite sequences of natural numbers and the set of all subsets of the set of natural numbers. The cardinality of R is often called the cardinality of the continuum, and denoted by 
�
{\displaystyle {\mathfrak {c}}}, or 
2
ℵ
0
2^{\aleph _{0}}, or 
ℶ
1
\beth _{1} (beth-one).

The Cantor set is an uncountable subset of R. The Cantor set is a fractal and has Hausdorff dimension greater than zero but less than one (R has dimension one). This is an example of the following fact: any subset of R of Hausdorff dimension strictly greater than zero must be uncountable.

Another example of an uncountable set is the set of all functions from R to R. This set is even "more uncountable" than R in the sense that the cardinality of this set is 
ℶ
2
\beth _{2} (beth-two), which is larger than 
ℶ
1
\beth _{1}.

A more abstract example of an uncountable set is the set of all countable ordinal numbers, denoted by Ω or ω1.[1] The cardinality of Ω is denoted 
ℵ
1
\aleph _{1} (aleph-one). It can be shown, using the axiom of choice, that 
ℵ
1
\aleph _{1} is the smallest uncountable cardinal number. Thus either 
ℶ
1
\beth _{1}, the cardinality of the reals, is equal to 
ℵ
1
\aleph _{1} or it is strictly larger. Georg Cantor was the first to propose the question of whether 
ℶ
1
\beth _{1} is equal to 
ℵ
1
\aleph _{1}. In 1900, David Hilbert posed this question as the first of his 23 problems. The statement that 
ℵ
1
=
ℶ
1
\aleph _{1}=\beth _{1} is now called the continuum hypothesis, and is known to be independent of the Zermelo–Fraenkel axioms for set theory (including the axiom of choice).
$
5
Question: Which of the following sets is known to be uncountable?
A: The set of all prime numbers.
B: The set of natural numbers.
C: The set of all rational numbers.
D: The set of all real numbers.
E: The set of all even numbers.
Answer: D

Question: What is the cardinality of the set of real numbers denoted by?
A: ℵ0
B: ℵ1
C: ℶ1
D: ℶ2
E: ℵ2
Answer: C

Question: The continuum hypothesis proposes the equality of which two cardinalities?
A: ℵ0 and ℵ1
B: ℵ1 and ℶ1
C: ℵ0 and ℶ0
D: ℵ2 and ℶ2
E: ℶ0 and ℶ1
Answer: B

Question: Which set is considered a subset of R and is also uncountable?
A: The set of all rational numbers.
B: The set of all integers.
C: The Cantor set.
D: The set of all prime numbers.
E: The set of all whole numbers.
Answer: C

Question: Which set is considered "more uncountable" than the set of real numbers due to its larger cardinality?
A: Set of all rational numbers.
B: Set of all countable ordinal numbers.
C: Set of all natural numbers.
D: Set of all functions from R to R.
E: Set of all prime numbers.
Answer: D
@
In mathematics, the natural numbers are the numbers 1, 2, 3, etc., possibly including 0 as well. Some definitions, including the standard ISO 80000-2,[1] begin the natural numbers with 0, corresponding to the non-negative integers 0, 1, 2, 3, ..., whereas others start with 1, corresponding to the positive integers 1, 2, 3, ...[2][a] Texts that exclude zero from the natural numbers sometimes refer to the natural numbers together with zero as the whole numbers, while in other writings, that term is used instead for the integers (including negative integers).[4] In common language, particularly in primary school education, natural numbers may be called counting numbers[5] to intuitively exclude the negative integers and zero, and also to contrast the discreteness of counting to the continuity of measurement—a hallmark characteristic of real numbers.
$
5
Question: In most mathematical contexts, from which number do natural numbers start?
A: -1
B: 0
C: 1
D: 2
E: None of the above.
Answer: C

Question: Which of the following might NOT be considered a natural number in some definitions?
A: 2
B: 3
C: 5
D: 0
E: 1
Answer: D

Question: Natural numbers may be referred to as what, especially in primary education settings?
A: Real numbers.
B: Imaginary numbers.
C: Counting numbers.
D: Integers.
E: Fractions.
Answer: C

Question: Which of the following sets includes negative numbers, zero, and positive numbers?
A: Natural numbers.
B: Real numbers.
C: Whole numbers.
D: Integers.
E: Irrational numbers.
Answer: D

Question: What is a hallmark characteristic of real numbers that distinguishes them from natural numbers in terms of continuity and discreteness?
A: Countability.
B: Measurement.
C: Positivity.
D: Finiteness.
E: Divisibility.
Answer: B
@
Two important generalizations of natural numbers arise from the two uses of counting and ordering: cardinal numbers and ordinal numbers.

A natural number can be used to express the size of a finite set; more precisely, a cardinal number is a measure for the size of a set, which is even suitable for infinite sets. This concept of "size" relies on maps between sets, such that two sets have the same size, exactly if there exists a bijection between them. The set of natural numbers itself, and any bijective image of it, is said to be countably infinite and to have cardinality aleph-null (ℵ0).
Natural numbers are also used as linguistic ordinal numbers: "first", "second", "third", and so forth. This way they can be assigned to the elements of a totally ordered finite set, and also to the elements of any well-ordered countably infinite set. This assignment can be generalized to general well-orderings with a cardinality beyond countability, to yield the ordinal numbers. An ordinal number may also be used to describe the notion of "size" for a well-ordered set, in a sense different from cardinality: if there is an order isomorphism (more than a bijection!) between two well-ordered sets, they have the same ordinal number. The first ordinal number that is not a natural number is expressed as ω; this is also the ordinal number of the set of natural numbers itself.
$
5
Question: Cardinal numbers are measures for the size of a set that are suitable even for what kind of sets?
A: Empty sets.
B: Finite sets.
C: Infinite sets.
D: Natural sets.
E: Integer sets.
Answer: C

Question: Which of the following numbers represents the cardinality of the set of natural numbers?
A: ω1
B: ℵ0
C: ℵ1
D: ℶ1
E: ℵ2
Answer: B

Question: The first ordinal number that is not a natural number is represented by which symbol?
A: ℵ1
B: ℵ0
C: ℶ1
D: ω
E: ℵ2
Answer: D

Question: When two sets have the same size, they can be related by what kind of function?
A: Additive function.
B: Continuous function.
C: Bijective function.
D: Derivative function.
E: Integrative function.
Answer: C

Question: Ordinal numbers can be assigned to elements of a well-ordered countably infinite set, with the assignment generalized to well-orderings with a cardinality beyond countability. This yields which kind of numbers?
A: Natural numbers.
B: Real numbers.
C: Cardinal numbers.
D: Integer numbers.
E: Ordinal numbers.
Answer: E
@
A thermodynamic system is a body of matter and/or radiation separate from its surroundings that can be studied using the laws of thermodynamics. A thermodynamic system may be an isolated system, a closed system, or an open system. An isolated system does not exchange matter or energy with its surroundings. A closed system may exchange heat, experience forces, and exert forces, but does not exchange matter. An open system can interact with its surroundings by exchanging both matter and energy.

The physical condition of a thermodynamic system at a given time is described by its state, which can be specified by the values of a set of thermodynamic state variables. A thermodynamic system is in thermodynamic equilibrium when there are no macroscopically apparent flows of matter or energy within it or between it and other systems.[1]
$
5
Question: Which system type does not exchange matter or energy with its surroundings?
A: Closed system
B: Open system
C: Thermodynamic system
D: Isolated system
E: Equilibrium system
Answer: D

Question: What is specified by the values of a set of thermodynamic state variables?
A: The equilibrium of the system
B: The energy flow within the system
C: The state of a thermodynamic system at a given time
D: The type of system
E: The heat capacity of the system
Answer: C

Question: In which type of system is there a potential exchange of both matter and energy with the surroundings?
A: Closed system
B: Isolated system
C: Thermodynamic system
D: Open system
E: Static system
Answer: D

Question: A thermodynamic system is in thermodynamic equilibrium when:
A: It is at its lowest possible energy state.
B: There are macroscopic flows of matter and energy.
C: It has the same temperature as its surroundings.
D: There are no macroscopically apparent flows of matter or energy within it or between it and other systems.
E: It can exchange matter with its surroundings.
Answer: D

Question: Which type of system does not allow for the exchange of matter but can exchange energy?
A: Closed system
B: Open system
C: Isolated system
D: Dynamic system
E: Static system
Answer: A

Subject2: Thermodynamic Equilibrium

Question: What type of thermodynamics deals with systems not in states of internal thermodynamic equilibrium?
A: Equilibrium thermodynamics
B: Static thermodynamics
C: Non-equilibrium thermodynamics
D: Transitional thermodynamics
E: Isolated
@
Thermodynamic equilibrium is characterized by absence of flow of mass or energy. Equilibrium thermodynamics, as a subject in physics, considers macroscopic bodies of matter and energy in states of internal thermodynamic equilibrium. It uses the concept of thermodynamic processes, by which bodies pass from one equilibrium state to another by transfer of matter and energy between them. The term 'thermodynamic system' is used to refer to bodies of matter and energy in the special context of thermodynamics. The possible equilibria between bodies are determined by the physical properties of the walls that separate the bodies. Equilibrium thermodynamics in general does not measure time. Equilibrium thermodynamics is a relatively simple and well settled subject. One reason for this is the existence of a well defined physical quantity called 'the entropy of a body'.

Non-equilibrium thermodynamics, as a subject in physics, considers bodies of matter and energy that are not in states of internal thermodynamic equilibrium, but are usually participating in processes of transfer that are slow enough to allow description in terms of quantities that are closely related to thermodynamic state variables. It is characterized by presence of flows of matter and energy. For this topic, very often the bodies considered have smooth spatial inhomogeneities, so that spatial gradients, for example a temperature gradient, are well enough defined. Thus the description of non-equilibrium thermodynamic systems is a field theory, more complicated than the theory of equilibrium thermodynamics. Non-equilibrium thermodynamics is a growing subject, not an established edifice. In general, it is not possible to find an exactly defined entropy for non-equilibrium problems. For many non-equilibrium thermodynamical problems, an approximately defined quantity called 'time rate of entropy production' is very useful. Non-equilibrium thermodynamics is mostly beyond the scope of the present article.

Another kind of thermodynamic system is considered in most engineering. It takes part in a flow process. The account is in terms that approximate, well enough in practice in many cases, equilibrium thermodynamical concepts. This is mostly beyond the scope of the present article, and is set out in other articles, for example the article Flow process.
$
5
Question: What type of thermodynamics deals with systems not in states of internal thermodynamic equilibrium?
A: Equilibrium thermodynamics
B: Static thermodynamics
C: Non-equilibrium thermodynamics
D: Transitional thermodynamics
E: Isolated thermodynamics
Answer: C

Question: In equilibrium thermodynamics, the focus is mainly on bodies that:
A: Are experiencing fast matter and energy flows.
B: Are not in a state of equilibrium.
C: Are in states of internal thermodynamic equilibrium.
D: Have inhomogeneities.
E: Are part of a flow process.
Answer: C

Question: Which term is used to describe the "size" or "amount of disorder" of a system in equilibrium thermodynamics?
A: Energy
B: Heat
C: Entropy
D: Enthalpy
E: Temperature
Answer: C

Question: Non-equilibrium thermodynamics is characterized by:
A: Absence of flow of mass or energy.
B: Strictly equilibrium states.
C: Flows of matter and energy.
D: Stagnant states with no changes.
E: Time rate of entropy reduction.
Answer: C

Question: Equilibrium thermodynamics generally does not measure which factor?
A: Heat
B: Temperature
C: Entropy
D: Energy
E: Time
Answer: E
@
In isolated systems it is consistently observed that as time goes on internal rearrangements diminish and stable conditions are approached. Pressures and temperatures tend to equalize, and matter arranges itself into one or a few relatively homogeneous phases. A system in which all processes of change have gone practically to completion is considered in a state of thermodynamic equilibrium.[2] The thermodynamic properties of a system in equilibrium are unchanging in time. Equilibrium system states are much easier to describe in a deterministic manner than non-equilibrium states. In some cases, when analyzing a thermodynamic process, one can assume that each intermediate state in the process is at equilibrium. Such a process is called quasistatic.[3]

For a process to be reversible, each step in the process must be reversible. For a step in a process to be reversible, the system must be in equilibrium throughout the step. That ideal cannot be accomplished in practice because no step can be taken without perturbing the system from equilibrium, but the ideal can be approached by making changes slowly.

The very existence of thermodynamic equilibrium, defining states of thermodynamic systems, is the essential, characteristic, and most fundamental postulate of thermodynamics, though it is only rarely cited as a numbered law.[4][5][6] According to Bailyn, the commonly rehearsed statement of the zeroth law of thermodynamics is a consequence of this fundamental postulate.[7] In reality, practically nothing in nature is in strict thermodynamic equilibrium, but the postulate of thermodynamic equilibrium often provides very useful idealizations or approximations, both theoretically and experimentally; experiments can provide scenarios of practical thermodynamic equilibrium.
$
5
Question: A process where each intermediate state in the process is assumed to be at equilibrium is called:
A: Dynamic
B: Reversible
C: Irreversible
D: Quasistatic
E: Static
Answer: D

Question: For a step in a process to be reversible, the system must:
A: Have a constant temperature.
B: Be in equilibrium throughout the step.
C: Transfer maximum energy.
D: Transfer maximum entropy.
E: Be isolated.
Answer: B

Question: The concept that systems tend to approach stable conditions and reach equilibrium is fundamental to which postulate?
A: First law of thermodynamics
B: Zeroth law of thermodynamics
C: Third law of thermodynamics
D: The postulate of thermodynamic equilibrium
E: Second law of thermodynamics
Answer: D

Question: In reality, most systems are in strict:
A: Thermodynamic equilibrium.
B: Non-equilibrium.
C: Isothermal conditions.
D: Transitional states.
E: Quasistatic states.
Answer: B

Question: A system where all processes of change have practically finished is considered in:
A: A state of maximal entropy.
B: A state of thermodynamic disequilibrium.
C: A state of thermodynamic equilibrium.
D: A state of maximum energy.
E: A state of zero entropy.
Answer: C
@
An isolated system is more restrictive than a closed system as it does not interact with its surroundings in any way. Mass and energy remains constant within the system, and no energy or mass transfer takes place across the boundary. As time passes in an isolated system, internal differences in the system tend to even out and pressures and temperatures tend to equalize, as do density differences. A system in which all equalizing processes have gone practically to completion is in a state of thermodynamic equilibrium.

Truly isolated physical systems do not exist in reality (except perhaps for the universe as a whole), because, for example, there is always gravity between a system with mass and masses elsewhere.[23][24][25][26][27] However, real systems may behave nearly as an isolated system for finite (possibly very long) times. The concept of an isolated system can serve as a useful model approximating many real-world situations. It is an acceptable idealization used in constructing mathematical models of certain natural phenomena.
$
5
Question: An isolated system differs from a closed system in that it:
A: Exchanges energy but not matter.
B: Does not interact with its surroundings in any way.
C: Exchanges matter but not energy.
D: Continuously exchanges both matter and energy.
E: Is always in a state of equilibrium.
Answer: B

Question: With time, what happens to the internal differences in an isolated system?
A: They increase dramatically.
B: They remain unchanged.
C: They tend to even out.
D: They oscillate.
E: They lead to a maximum entropy state.
Answer: C

Question: What can be said about the existence of truly isolated physical systems?
A: They are common in laboratory settings.
B: They exist only in theoretical constructs.
C: They exist for a limited duration.
D: They do not exist in reality.
E: They are found only in space.
Answer: D

Question: Isolated systems are models that can serve as useful:
A: Proxies for real-world situations.
B: Approximations for equilibrium states.
C: Examples for non-equilibrium thermodynamics.
D: Indicators for maximum entropy.
E: Representations for open systems.
Answer: A

Question: What remains constant within an isolated system?
A: Pressure
B: Temperature
C: Volume
D: Mass and energy
E: Entropy
Answer: D
@
A thermographic camera (also called an infrared camera or thermal imaging camera, thermal camera or thermal imager) is a device that creates an image using infrared (IR) radiation, similar to a normal camera that forms an image using visible light. Instead of the 400–700 nanometre (nm) range of the visible light camera, infrared cameras are sensitive to wavelengths from about 1,000 nm (1 micrometre or μm) to about 14,000 nm (14 μm). The practice of capturing and analyzing the data they provide is called thermography.

In 1929, Hungarian physicist Kálmán Tihanyi invented the infrared-sensitive (night vision) electronic television camera for anti-aircraft defense in Britain.[11] The first American thermographic camera developed was an infrared line scanner. This was created by the US military and Texas Instruments in 1947[12][failed verification] and took one hour to produce a single image. While several approaches were investigated to improve the speed and accuracy of the technology, one of the most crucial factors dealt with scanning an image, which the AGA company was able to commercialize using a cooled photoconductor.[13]

The first infrared linescan system was the British Yellow Duckling of the mid-1950s.[14] This used a continuously rotating mirror and detector, with Y-axis scanning by the motion of the carrier aircraft. Although unsuccessful in its intended application of submarine tracking by wake detection, it was applied to land-based surveillance and became the foundation of military IR linescan.
$
5
Question: Which of the following devices creates an image using infrared (IR) radiation?
A: Visible light camera
B: X-ray scanner
C: Thermographic camera
D: Radio receiver
E: Ultrasound machine
Answer: C

Question: In which year did Kálmán Tihanyi invent the infrared-sensitive electronic television camera?
A: 1919
B: 1929
C: 1939
D: 1949
E: 1959
Answer: B

Question: What was the main application for the British Yellow Duckling infrared linescan system?
A: Submarine tracking
B: Land-based surveillance
C: Night vision for cars
D: Aircraft navigation
E: Medical imaging
Answer: B

Question: How long did the first American thermographic camera developed take to produce a single image?
A: 10 minutes
B: 30 minutes
C: One hour
D: Two hours
E: Four hours
Answer: C

Question: The AGA company commercialized the scanning of an image using what?
A: A heated lens
B: An uncooled sensor
C: A cooled photoconductor
D: A rotating mirror
E: A gamma ray detector
Answer: C
@
Infrared energy is just one part of the electromagnetic spectrum, which encompasses radiation from gamma rays, x-rays, ultraviolet, a thin region of visible light, infrared, terahertz waves, microwaves, and radio waves. These are all related and differentiated in the length of their wave (wavelength). All objects emit a certain amount of black body radiation as a function of their temperature.

Generally speaking, the higher an object's temperature, the more infrared radiation is emitted as black-body radiation. A special camera can detect this radiation in a way similar to the way an ordinary camera detects visible light. It even works in total darkness because ambient light level does not matter. This makes it useful for rescue operations in smoke-filled buildings and underground.

A major difference with optical cameras is that the focusing lenses cannot be made of glass, as glass blocks long-wave infrared light. Typically the spectral range of thermal radiation is from 7 to 14 μm. Special materials such as Germanium, calcium fluoride, crystalline silicon or newly developed special type of chalcogenide glasses must be used. Except for calcium fluoride all these materials are quite hard and have high refractive index (for germanium n=4) which leads to very high Fresnel reflection from uncoated surfaces (up to more than 30%). For this reason most of the lenses for thermal cameras have antireflective coatings. The higher cost of these special lenses is one reason why thermographic cameras are more expensive.
$
5
Question: Which of the following materials is NOT typically used in the focusing lenses of infrared cameras?
A: Germanium
B: Crystalline silicon
C: Glass
D: Calcium fluoride
E: Chalcogenide glasses
Answer: C

Question: Infrared cameras detect radiation in the wavelength range of:
A: 400-700 nm
B: 1,000 nm to 14,000 nm
C: 50-500 nm
D: 14,000 nm to 28,000 nm
E: 200-500 nm
Answer: B

Question: Which of the following is a major difference between optical cameras and infrared cameras?
A: Type of battery used
B: Focusing lenses material
C: Color of the camera body
D: Weight
E: Zoom capability
Answer: B

Question: Infrared energy is a part of the:
A: Thermodynamic spectrum
B: Electromagnetic spectrum
C: Visible light spectrum
D: Sound wave spectrum
E: Thermal spectrum
Answer: B

Question: A special camera can detect infrared radiation even in total darkness because:
A: It uses flash
B: It requires sunlight
C: It uses ultraviolet light
D: Ambient light level does not matter
E: It uses a special filter
Answer: D
@
Cooled detectors are typically contained in a vacuum-sealed case or Dewar and cryogenically cooled. The cooling is necessary for the operation of the semiconductor materials used. Typical operating temperatures range from 4 K (−269 °C) to just below room temperature, depending on the detector technology. Most modern cooled detectors operate in the 60 Kelvin (K) to 100 K range (-213 to -173 °C), depending on type and performance level.[27]

Without cooling, these sensors (which detect and convert light in much the same way as common digital cameras, but are made of different materials) would be 'blinded' or flooded by their own radiation. The drawbacks of cooled infrared cameras are that they are expensive both to produce and to run. Cooling is both energy-intensive and time-consuming.

The camera may need several minutes to cool down before it can begin working. The most commonly used cooling systems are peltier coolers which, although inefficient and limited in cooling capacity, are relatively simple and compact. To obtain better image quality or for imaging low temperature objects Stirling engine cryocoolers are needed. Although the cooling apparatus may be comparatively bulky and expensive, cooled infrared cameras provide greatly superior image quality compared to uncooled ones, particularly of objects near or below room temperature. Additionally, the greater sensitivity of cooled cameras also allow the use of higher F-number lenses, making high performance long focal length lenses both smaller and cheaper for cooled detectors.
$
5
Question: Cooled detectors are typically operated at temperatures ranging from:
A: 4 K to 273 K
B: 60 K to 100 K
C: 100 K to 273 K
D: 0 K to 50 K
E: 200 K to 300 K
Answer: B

Question: Why are cooled infrared cameras generally more expensive?
A: They use visible light
B: They don't require lenses
C: They need cooling which is energy-intensive and time-consuming
D: They are made of plastic
E: They use a basic sensor
Answer: C

Question: Which cooling system is simple and compact, but relatively inefficient and limited in cooling capacity?
A: Radiative cooler
B: Stirling engine cryocooler
C: Peltier cooler
D: Helium cooler
E: Nitrogen cooler
Answer: C

Question: Cooled infrared cameras provide superior image quality compared to uncooled ones when imaging objects near or below:
A: Boiling point
B: Melting point
C: Freezing point
D: Room temperature
E: Absolute zero
Answer: D

Question: The higher sensitivity of cooled cameras allows the use of higher:
A: Battery life
B: F-number lenses
C: Image stabilization
D: Focus speed
E: Zoom range
Answer: B
@
Uncooled thermal cameras use a sensor operating at ambient temperature, or a sensor stabilized at a temperature close to ambient using small temperature control elements. Modern uncooled detectors all use sensors that work by the change of resistance, voltage or current when heated by infrared radiation. These changes are then measured and compared to the values at the operating temperature of the sensor.

Uncooled infrared sensors can be stabilized to an operating temperature to reduce image noise, but they are not cooled to low temperatures and do not require bulky, expensive, energy consuming cryogenic coolers. This makes infrared cameras smaller and less costly. However, their resolution and image quality tend to be lower than cooled detectors. This is due to differences in their fabrication processes, limited by currently available technology. An uncooled thermal camera also needs to deal with its own heat signature.

Uncooled detectors are mostly based on pyroelectric and ferroelectric materials or microbolometer technology.[28] The material are used to form pixels with highly temperature-dependent properties, which are thermally insulated from the environment and read electronically.

Ferroelectric detectors operate close to phase transition temperature of the sensor material; the pixel temperature is read as the highly temperature-dependent polarization charge. The achieved NETD of ferroelectric detectors with f/1 optics and 320x240 sensors is 70-80 mK. A possible sensor assembly consists of barium strontium titanate bump-bonded by polyimide thermally insulated connection.
$
5
Question: Which of the following is a drawback of uncooled thermal cameras compared to cooled ones?
A: Better resolution
B: Lower cost
C: Higher image quality
D: Lower resolution and image quality
E: Faster processing speed
Answer: D

Question: How do uncooled infrared sensors primarily operate?
A: By emitting visible light
B: By rotating quickly
C: By change of resistance, voltage or current when heated by infrared radiation
D: By cooling to absolute zero
E: By using gamma rays
Answer: C

Question: Uncooled detectors are mostly based on which of the following materials or technologies?
A: Ultraviolet materials
B: Radio wave materials
C: X-ray technology
D: Pyroelectric and ferroelectric materials or microbolometer technology
E: Optical fibers
Answer: D

Question: The NETD achieved by ferroelectric detectors with f/1 optics and 320x240 sensors is approximately:
A: 1000-1100 mK
B: 900-1000 mK
C: 500-600 mK
D: 70-80 mK
E: 30-40 mK
Answer: D

Question: In uncooled detectors, the temperature-dependent polarization charge is read to determine the pixel temperature in which type of detector?
A: Radioelectric
B: Ultraviolet
C: Pyroelectric
D: Ferroelectric
E: Gamma ray
Answer: D
@
Vanadium(V) oxide (vanadia) is the inorganic compound with the formula V2O5. Commonly known as vanadium pentoxide, it is a brown/yellow solid, although when freshly precipitated from aqueous solution, its colour is deep orange. Because of its high oxidation state, it is both an amphoteric oxide and an oxidizing agent. From the industrial perspective, it is the most important compound of vanadium, being the principal precursor to alloys of vanadium and is a widely used industrial catalyst.[8]

The mineral form of this compound, shcherbinaite, is extremely rare, almost always found among fumaroles. A mineral trihydrate, V2O5·3H2O, is also known under the name of navajoite.
$
5
Question: What is the common name for Vanadium(V) oxide?
A: Vanadium tetroxide
B: Vanadium pentoxide
C: Vanadium dioxide
D: Vanadium tetraoxide
E: Vanadium trioxide
Answer: B

Question: The mineral form of Vanadium(V) oxide is known as:
A: Vanadinite
B: Shcherbinaite
C: Navajoite
D: Vanadate
E: Navadinite
Answer: B

Question: Which of the following statements about Vanadium(V) oxide is correct?
A: It is a potent reducing agent.
B: It is primarily used as a food preservative.
C: It is a blue/green solid.
D: It is both an amphoteric oxide and an oxidizing agent.
E: Its main use is in cosmetics.
Answer: D

Question: Which compound is a trihydrate mineral form of Vanadium(V) oxide?
A: V2O5
B: V2O3
C: V2O5·3H2O
D: V2O7
E: V3O8
Answer: C

Question: From an industrial perspective, Vanadium(V) oxide is significant because:
A: It is a widely used natural dye.
B: It is the main precursor to alloys of vanadium.
C: It has a major role in the production of plastics.
D: It is used for water purification.
E: It's the main component in car batteries.
Answer: B
@
Reduction to lower oxides
Upon heating a mixture of vanadium(V) oxide and vanadium(III) oxide, comproportionation occurs to give vanadium(IV) oxide, as a deep-blue solid:[9]

V2O5 + V2O3 → 4 VO2
The reduction can also be effected by oxalic acid, carbon monoxide, and sulfur dioxide. Further reduction using hydrogen or excess CO can lead to complex mixtures of oxides such as V4O7 and V5O9 before black V2O3 is reached.

Acid-base reactions
V2O5 is an amphoteric oxide. Unlike most transition metal oxides, it dissolves slightly in water to give a pale yellow, acidic solution. Thus V2O5 reacts with strong non-reducing acids to form solutions containing the pale yellow salts containing dioxovanadium(V) centers:

V2O5 + 2 HNO3 → 2 VO2(NO3) + H2O
It also reacts with strong alkali to form polyoxovanadates, which have a complex structure that depends on pH.[10] If excess aqueous sodium hydroxide is used, the product is a colourless salt, sodium orthovanadate, Na3VO4. If acid is slowly added to a solution of Na3VO4, the colour gradually deepens through orange to red before brown hydrated V2O5 precipitates around pH 2. These solutions contain mainly the ions HVO42− and V2O74− between pH 9 and pH 13, but below pH 9 more exotic species such as V4O124− and HV10O285− (decavanadate) predominate.

Upon treatment with thionyl chloride, it converts to the volatile liquid vanadium oxychloride, VOCl3:[11]

V2O5 + 3 SOCl2 → 2 VOCl3 + 3 SO2
$
5
Question: Upon heating a mixture of V2O5 and V2O3, which oxide is produced?
A: V2O4
B: VO
C: VO2
D: V2O7
E: V3O8
Answer: C

Question: Which gas is evolved when V2O5 reacts with thionyl chloride?
A: Carbon dioxide
B: Hydrogen chloride
C: Chlorine
D: Sulfur dioxide
E: Oxygen
Answer: D

Question: In acidic solutions, which ion is present when sodium orthovanadate is slowly acidified?
A: HVO42−
B: V2O84−
C: V4O124−
D: HV10O285−
E: V3O10−
Answer: A

Question: The product formed when V2O5 reacts with a strong non-reducing acid is a pale yellow salt containing which center?
A: Monoxovanadium(III)
B: Trioxovanadium(III)
C: Dioxovanadium(V)
D: Monoxovanadium(V)
E: Dioxovanadium(III)
Answer: C

Question: Which gas can also reduce V2O5 besides oxalic acid and carbon monoxide?
A: Hydrogen
B: Nitrogen
C: Helium
D: Oxygen
E: Chlorine
Answer: A
@
In terms of quantity, the dominant use for vanadium(V) oxide is in the production of ferrovanadium (see above). The oxide is heated with scrap iron and ferrosilicon, with lime added to form a calcium silicate slag. Aluminium may also be used, producing the iron-vanadium alloy along with alumina as a byproduct.

Sulfuric acid production
Another important use of vanadium(V) oxide is in the manufacture of sulfuric acid, an important industrial chemical with an annual worldwide production of 165 million tonnes in 2001, with an approximate value of US$8 billion. Vanadium(V) oxide serves the crucial purpose of catalysing the mildly exothermic oxidation of sulfur dioxide to sulfur trioxide by air in the contact process:

2 SO2 + O2 ⇌ 2 SO3
The discovery of this simple reaction, for which V2O5 is the most effective catalyst, allowed sulfuric acid to become the cheap commodity chemical it is today. The reaction is performed between 400 and 620 °C; below 400 °C the V2O5 is inactive as a catalyst, and above 620 °C it begins to break down. Since it is known that V2O5 can be reduced to VO2 by SO2, one likely catalytic cycle is as follows:

SO2 + V2O5 → SO3 + 2VO2
followed by

2VO2 +½O2 → V2O5
It is also used as catalyst in the selective catalytic reduction (SCR) of NOx emissions in some power plants and diesel engines. Due to its effectiveness in converting sulfur dioxide into sulfur trioxide, and thereby sulfuric acid, special care must be taken with the operating temperatures and placement of a power plant's SCR unit when firing sulfur-containing fuels.
$
5
Question: In which process does vanadium(V) oxide act as a catalyst for the oxidation of sulfur dioxide to sulfur trioxide?
A: Ostwald process
B: Haber process
C: Solvay process
D: Contact process
E: Fischer-Tropsch process
Answer: D

Question: Which of these is NOT a product when V2O5 acts as a catalyst between 400°C and 620°C?
A: Sulfur dioxide
B: VO2
C: Sulfur trioxide
D: Oxygen
E: V2O5
Answer: A

Question: In the contact process, vanadium(V) oxide catalyzes the oxidation of:
A: Oxygen to ozone
B: Nitrogen to nitrogen dioxide
C: Hydrogen to water
D: Sulfur dioxide to sulfur trioxide
E: Carbon monoxide to carbon dioxide
Answer: D

Question: Which compound is released in the selective catalytic reduction (SCR) of NOx emissions?
A: Carbon dioxide
B: Sulfuric acid
C: Nitrogen
D: Sulfur dioxide
E: Sulfur trioxide
Answer: C

Question: Vanadium(V) oxide becomes inactive as a catalyst in the contact process below which temperature?
A: 300°C
B: 350°C
C: 400°C
D: 450°C
E: 500°C
Answer: C
@
Ferrovanadium (FeV) is an alloy formed by combining iron and vanadium with a vanadium content range of 35–85%. The production of this alloy results in a grayish silver crystalline solid that can be crushed into a powder called "ferrovanadium dust".[2] Ferrovanadium is a universal hardener, strengthener and anti-corrosive additive for steels like high-strength low-alloy steel, tool steels, as well as other ferrous-based products. It has significant advantages over both iron and vanadium individually. Ferrovanadium is used as an additive to improve the qualities of ferrous alloys. One such use is to improve corrosion resistance to alkaline reagents as well as sulfuric and hydrochloric acids. It is also used to improve the tensile strength to weight ratio of the material. One application of such steels is in the chemical processing industry for high pressure high throughput fluid handling systems dealing with industrial scale sulfuric acid production. It is also commonly used for hand tools e.g. spanners (wrenches), screwdrivers, ratchets, etc.
$
5
Question: What is the primary purpose of Ferrovanadium?
A: As a food preservative
B: To improve the properties of ferrous alloys
C: As a catalyst in the production of plastics
D: As a fuel additive
E: As a natural dye
Answer: B

Question: Which of the following is NOT an advantage of Ferrovanadium?
A: It's biodegradable
B: Strengthener for steels
C: Universal hardener
D: Anti-corrosive additive
E: Improves tensile strength to weight ratio
Answer: A

Question: Ferrovanadium is used to enhance resistance to:
A: UV radiation
B: Alkaline reagents
C: Friction
D: Oxidation at high altitudes
E: Radiation from nuclear materials
Answer: B

Question: One application of steels enhanced with Ferrovanadium is in:
A: Aerospace designs
B: Food processing
C: Solar panel production
D: High pressure high throughput fluid handling systems in sulfuric acid production
E: Deep-sea exploration
Answer: D

Question: Ferrovanadium has a content range of vanadium between:
A: 10-60%
B: 20-70%
C: 25-75%
D: 30-80%
E: 35-85%
Answer: E
@
Iron is a chemical element with the symbol Fe (from Latin ferrum 'iron') and atomic number 26. It is a metal that belongs to the first transition series and group 8 of the periodic table. It is, by mass, the most common element on Earth, just ahead of oxygen (32.1% and 30.1%, respectively), forming much of Earth's outer and inner core. It is the fourth most common element in the Earth's crust, being mainly deposited by meteorites in its metallic state, with its ores also being found there.

Extracting usable metal from iron ores requires kilns or furnaces capable of reaching 1,500 °C (2,730 °F) or higher, about 500 °C (932 °F) higher than that required to smelt copper. Humans started to master that process in Eurasia during the 2nd millennium BCE and the use of iron tools and weapons began to displace copper alloys—in some regions, only around 1200 BCE. That event is considered the transition from the Bronze Age to the Iron Age. In the modern world, iron alloys, such as steel, stainless steel, cast iron and special steels, are by far the most common industrial metals, due to their mechanical properties and low cost. The iron and steel industry is thus very important economically, and iron is the cheapest metal, with a price of a few dollars per kilogram or pound.
$
5
Question: What is the atomic number of iron?
A: 24
B: 25
C: 26
D: 27
E: 28
Answer: C

Question: Which two elements are the most common by mass on Earth?
A: Hydrogen and Helium
B: Carbon and Oxygen
C: Iron and Oxygen
D: Silicon and Aluminum
E: Nitrogen and Carbon
Answer: C

Question: In which millennium BCE did humans in Eurasia start mastering the process of extracting usable metal from iron ores?
A: 1st millennium BCE
B: 2nd millennium BCE
C: 3rd millennium BCE
D: 4th millennium BCE
E: 5th millennium BCE
Answer: B

Question: What event marked the transition from the Bronze Age to the Iron Age?
A: Introduction of steel
B: Discovery of copper
C: Use of iron tools and weapons displacing copper alloys
D: Introduction of stainless steel
E: The smelting of iron at lower temperatures
Answer: C

Question: Which of the following iron alloys is not a common industrial metal?
A: Steel
B: Stainless steel
C: Cast iron
D: Brass
E: Special steels
Answer: D
@
At least four allotropes of iron (differing atom arrangements in the solid) are known, conventionally denoted α, γ, δ, and ε.

The first three forms are observed at ordinary pressures. As molten iron cools past its freezing point of 1538 °C, it crystallizes into its δ allotrope, which has a body-centered cubic (bcc) crystal structure. As it cools further to 1394 °C, it changes to its γ-iron allotrope, a face-centered cubic (fcc) crystal structure, or austenite. At 912 °C and below, the crystal structure again becomes the bcc α-iron allotrope.[7]

The physical properties of iron at very high pressures and temperatures have also been studied extensively,[8][9] because of their relevance to theories about the cores of the Earth and other planets. Above approximately 10 GPa and temperatures of a few hundred kelvin or less, α-iron changes into another hexagonal close-packed (hcp) structure, which is also known as ε-iron. The higher-temperature γ-phase also changes into ε-iron, but does so at higher pressure.

Some controversial experimental evidence exists for a stable β phase at pressures above 50 GPa and temperatures of at least 1500 K. It is supposed to have an orthorhombic or a double hcp structure.[10] (Confusingly, the term "β-iron" is sometimes also used to refer to α-iron above its Curie point, when it changes from being ferromagnetic to paramagnetic, even though its crystal structure has not changed.[7])

The inner core of the Earth is generally presumed to consist of an iron-nickel alloy with ε (or β) structure.[11]
$
5
Question: Which allotrope of iron has a face-centered cubic (fcc) crystal structure?
A: α-iron
B: β-iron
C: γ-iron
D: δ-iron
E: ε-iron
Answer: C

Question: At what temperature does iron change to its γ-iron allotrope?
A: 1538 °C
B: 1394 °C
C: 912 °C
D: 770 °C
E: 1200 °C
Answer: B

Question: Which structure is α-iron expected to change into at high pressures and temperatures?
A: Body-centered cubic
B: Face-centered cubic
C: Hexagonal close-packed
D: Double hcp
E: Orthorhombic
Answer: C

Question: At pressures above 50 GPa and temperatures of at least 1500 K, what is the potential phase of iron?
A: α-phase
B: β-phase
C: γ-phase
D: δ-phase
E: ε-phase
Answer: B

Question: The inner core of the Earth is presumed to consist of an alloy with which structure?
A: α-iron
B: β-iron
C: γ-iron
D: δ-iron
E: ε-iron
Answer: E
@
Below its Curie point of 770 °C (1,420 °F; 1,040 K), α-iron changes from paramagnetic to ferromagnetic: the spins of the two unpaired electrons in each atom generally align with the spins of its neighbors, creating an overall magnetic field.[16] This happens because the orbitals of those two electrons (dz2 and dx2 −. y2) do not point toward neighboring atoms in the lattice, and therefore are not involved in metallic bonding.[7]

In the absence of an external source of magnetic field, the atoms get spontaneously partitioned into magnetic domains, about 10 micrometers across,[17] such that the atoms in each domain have parallel spins, but some domains have other orientations. Thus a macroscopic piece of iron will have a nearly zero overall magnetic field.

Application of an external magnetic field causes the domains that are magnetized in the same general direction to grow at the expense of adjacent ones that point in other directions, reinforcing the external field. This effect is exploited in devices that need to channel magnetic fields to fulfill design function, such as electrical transformers, magnetic recording heads, and electric motors. Impurities, lattice defects, or grain and particle boundaries can "pin" the domains in the new positions, so that the effect persists even after the external field is removed – thus turning the iron object into a (permanent) magnet.[16]
$
3
Question: Below which temperature does α-iron change from paramagnetic to ferromagnetic?
A: 1394 °C
B: 932 °F
C: 912 °C
D: 770 °C
E: 1538 °C
Answer: D

Question: What size are the magnetic domains in iron in the absence of an external magnetic field?
A: 1 micrometer
B: 10 micrometers
C: 100 micrometers
D: 1000 micrometers
E: 0.1 micrometers
Answer: B

Question: Which devices exploit the magnetic properties of iron?
A: Batteries
B: Electrical transformers
C: Transistors
D: Solar panels
E: Light bulbs
Answer: B
@
Wrought iron contains less than 0.25% carbon but large amounts of slag that give it a fibrous characteristic.[126] It is a tough, malleable product, but not as fusible as pig iron. If honed to an edge, it loses it quickly. Wrought iron is characterized by the presence of fine fibers of slag entrapped within the metal. Wrought iron is more corrosion resistant than steel. It has been almost completely replaced by mild steel for traditional "wrought iron" products and blacksmithing.

Mild steel corrodes more readily than wrought iron, but is cheaper and more widely available. Carbon steel contains 2.0% carbon or less,[128] with small amounts of manganese, sulfur, phosphorus, and silicon. Alloy steels contain varying amounts of carbon as well as other metals, such as chromium, vanadium, molybdenum, nickel, tungsten, etc. Their alloy content raises their cost, and so they are usually only employed for specialist uses. One common alloy steel, though, is stainless steel. Recent developments in ferrous metallurgy have produced a growing range of microalloyed steels, also termed 'HSLA' or high-strength, low alloy steels, containing tiny additions to produce high strengths and often spectacular toughness at minimal cost.[128][129][130]

Alloys with high purity elemental makeups (such as alloys of electrolytic iron) have specifically enhanced properties such as ductility, tensile strength, toughness, fatigue strength, heat resistance, and corrosion resistance.
$
5
Question: What characteristic does wrought iron notably have due to the presence of slag?
A: Brittle nature
B: Reddish hue
C: Fibrous characteristic
D: High conductivity
E: Lustrous sheen
Answer: C

Question: Which material corrodes more readily than wrought iron?
A: Gold
B: Silver
C: Mild steel
D: Copper
E: Brass
Answer: C

Question: Which of the following is not an alloy of iron?
A: Stainless steel
B: Mild steel
C: Brass
D: High-strength, low alloy steels
E: Ferrovanadium
Answer: C

Question: Which property is specifically enhanced in alloys with high purity elemental makeups?
A: Magnetism
B: Ductility
C: Reflectivity
D: Weight
E: Melting point
Answer: B

Question: For what specific use is wrought iron most recognized?
A: Electrical conductivity
B: Resistance to high temperatures
C: Corrosion resistance
D: Lustrous appearance
E: High tensile strength
Answer: C
@
Graphite (/ˈɡræfaɪt/) is a crystalline form of the element carbon. It consists of stacked layers of graphene. Graphite occurs naturally and is the most stable form of carbon under standard conditions. Synthetic and natural graphite are consumed on large scale (300 kton/year, in 1989) for uses in pencils, lubricants, and electrodes. Under high pressures and temperatures it converts to diamond. It is a good (but not excellent) conductor of both heat[6] and electricity.[7]
$
4
Question: What is graphite primarily made up of?
A: Stacked layers of graphene
B: Individual diamond particles
C: Isolated hexagonal layers
D: Compressed carbon dioxide
E: Flakes of pure carbon
Answer: A

Question: Which of the following is NOT a use for graphite?
A: Writing in pencils
B: As lubricants
C: In light bulb filaments
D: For electrodes
E: None of the above
Answer: C

Question: Under which conditions does graphite convert to diamond?
A: Low temperatures and high pressures
B: High temperatures and low pressures
C: High temperatures and high pressures
D: Normal room temperature and pressure
E: None of the above
Answer: C

Question: How does graphite conduct electricity and heat?
A: Poorly for both
B: Poorly for electricity, well for heat
C: Well for both
D: Well for electricity, poorly for heat
E: Neither conducts electricity nor heat
Answer: C
@
The principal types of natural graphite, each occurring in different types of ore deposits, are

Crystalline small flakes of graphite (or flake graphite) occurs as isolated, flat, plate-like particles with hexagonal edges if unbroken. When broken the edges can be irregular or angular;
Amorphous graphite: very fine flake graphite is sometimes called amorphous;[8]
Lump graphite (or vein graphite) occurs in fissure veins or fractures and appears as massive platy intergrowths of fibrous or acicular crystalline aggregates, and is probably hydrothermal in origin.[9]
Highly ordered pyrolytic graphite refers to graphite with an angular spread between the graphite sheets of less than 1°.[10]
The name "graphite fiber" is sometimes used to refer to carbon fibers or carbon fiber-reinforced polymer.
$
4
Question: Which type of graphite appears as massive platy intergrowths of fibrous crystalline aggregates?
A: Amorphous graphite
B: Lump graphite
C: Crystalline small flakes of graphite
D: Highly ordered pyrolytic graphite
E: None of the above
Answer: B

Question: What does the term "graphite fiber" commonly refer to?
A: Flakes of graphite
B: Carbon fiber-reinforced polymer
C: Pure graphite
D: Amorphous graphite
E: A type of graphite ore
Answer: B

Question: How are the edges of crystalline small flakes of graphite described when unbroken?
A: Irregular
B: Angular
C: Hexagonal
D: Circular
E: Jagged
Answer: C

Question: Which graphite type has an angular spread between sheets of less than 1°?
A: Lump graphite
B: Amorphous graphite
C: Crystalline small flakes of graphite
D: Highly ordered pyrolytic graphite
E: None of the above
Answer: D
@
Graphite consists of sheets of trigonal planar carbon.[16][17] The individual layers are called graphene. In each layer, the carbon atoms are arranged in a honeycomb lattice with a bond length of 0.142 nm, and the distance between planes is 0.335 nm.[18] Bonding between layers is relatively weak van der Waals bonds and are often occupied by gases, which allows the graphene-like layers to be easily separated and to glide past each other.[19]

Electrical conductivity perpendicular to the layers is consequently about 1000 times lower.[20]

The two forms of graphite are called alpha (hexagonal) and beta (rhombohedral). Their properties are very similar. They differ in terms of the stacking of the graphene layers: stacking in alpha graphite is ABA, as opposed to ABC stacking in energetically less stable and less common beta graphite.[21] The alpha form can be converted to the beta form through mechanical treatment and the beta form reverts to the alpha form when it is heated above 1300 °C.[22]
$
4
Question: How are carbon atoms in graphene layers arranged?
A: In a square lattice
B: In a honeycomb lattice
C: In a triangular grid
D: In a circular pattern
E: Randomly scattered
Answer: B

Question: The bond between different layers of graphite is of which type?
A: Covalent bonds
B: Hydrogen bonds
C: Ionic bonds
D: Metallic bonds
E: Van der Waals bonds
Answer: E

Question: How does the electrical conductivity perpendicular to the layers in graphite compare to its conductivity within the layers?
A: It's the same
B: About 100 times higher
C: About 1000 times higher
D: About 100 times lower
E: About 1000 times lower
Answer: E

Question: What stacking pattern is observed in alpha graphite?
A: AAA
B: ABC
C: ABA
D: BCA
E: CBA
Answer: C
@
The acoustic and thermal properties of graphite are highly anisotropic, since phonons propagate quickly along the tightly bound planes, but are slower to travel from one plane to another. Graphite's high thermal stability and electrical and thermal conductivity facilitate its widespread use as electrodes and refractories in high temperature material processing applications. However, in oxygen-containing atmospheres graphite readily oxidizes to form carbon dioxide at temperatures of 700 °C and above.[26]

Graphite is an electrical conductor, hence useful in such applications as arc lamp electrodes. It can conduct electricity due to the vast electron delocalization within the carbon layers (a phenomenon called aromaticity). These valence electrons are free to move, so are able to conduct electricity. However, the electricity is primarily conducted within the plane of the layers. The conductive properties of powdered graphite[27] allow its use as pressure sensor in carbon microphones.
$
5
Question: Which property of graphite is anisotropic?
A: Electrical conductivity
B: Acoustic and thermal properties
C: Mechanical strength
D: Solubility in water
E: Resistance to oxidation
Answer: B

Question: At what temperature does graphite readily oxidize to form carbon dioxide in oxygen-containing atmospheres?
A: 500 °C
B: 600 °C
C: 700 °C
D: 800 °C
E: 900 °C
Answer: C

Question: Why can graphite conduct electricity?
A: Due to its tightly bound planes
B: Due to vast electron delocalization within the carbon layers
C: Because of the hexagonal arrangement of its atoms
D: Due to its high temperature resistance
E: None of the above
Answer: B

Question: The conductive properties of which form of graphite allow its use as a pressure sensor in carbon microphones?
A: Graphite in solid blocks
B: Highly ordered pyrolytic graphite
C: Powdered graphite
D: Amorphous graphite
E: Graphite fiber
Answer: C
@
Anisotropy (/ˌænaɪˈsɒtrəpi, ˌænɪ-/) is the structural property of non-uniformity in different directions, as opposed to isotropy. An anisotropic object or pattern has properties that differ according to direction of measurement. For example many materials exhibit very different properties when measured along different axes: physical or mechanical properties (absorbance, refractive index, conductivity, tensile strength, etc.).

An example of anisotropy is light coming through a polarizer. Another is wood, which is easier to split along its grain than across it because of the directional non-uniformity of the grain (the grain is the same in one direction, not all directions).
$
4
Question: What does anisotropy refer to?
A: The uniformity of a structure in all directions
B: The variability of a structure in different directions
C: The strength of a structure
D: The symmetry of a structure
E: The age of a structure
Answer: B

Question: Which of the following can be an example of anisotropy?
A: Water's freezing point
B: Light's speed in a vacuum
C: Wood grain's directional strength
D: Metal's melting point
E: Glass transparency
Answer: C

Question: In what context is light coming through a polarizer considered anisotropic?
A: It changes the color of light
B: It increases the light's intensity
C: It filters out certain directions of light waves
D: It splits light into multiple beams
E: It absorbs all incoming light
Answer: C

Question: Why is wood considered anisotropic?
A: It has the same strength regardless of the direction
B: It has a uniform density throughout
C: It is easier to split along its grain than across it
D: It reflects light uniformly
E: All of the above
Answer: C
@
In physics and geometry, isotropy (from Ancient Greek ἴσος (ísos) 'equal', and τρόπος (trópos) 'turn, way') is uniformity in all orientations. Precise definitions depend on the subject area. Exceptions, or inequalities, are frequently indicated by the prefix a- or an-, hence anisotropy. Anisotropy is also used to describe situations where properties vary systematically, dependent on direction. Isotropic radiation has the same intensity regardless of the direction of measurement, and an isotropic field exerts the same action regardless of how the test particle is oriented.
$
4
Question: What does the term "isotropy" signify?
A: Difference in all directions
B: Uniformity in all orientations
C: Instability in structures
D: Reflectivity in specific directions
E: Rigidity in certain orientations
Answer: B

Question: Which prefix often indicates exceptions to isotropy?
A: Iso-
B: Tri-
C: Uni-
D: Multi-
E: An-
Answer: E

Question: What kind of radiation has the same intensity regardless of the direction of measurement?
A: Reflected radiation
B: Refracted radiation
C: Isotropic radiation
D: Anisotropic radiation
E: Polarized radiation
Answer: C

Question: In which context does isotropic field exert the same action regardless of the test particle's orientation?
A: Optical fields
B: Gravitational fields
C: Electromagnetic fields
D: Strong nuclear fields
E: Weak nuclear fields
Answer: C
@
A chemical anisotropic filter, as used to filter particles, is a filter with increasingly smaller interstitial spaces in the direction of filtration so that the proximal regions filter out larger particles and distal regions increasingly remove smaller particles, resulting in greater flow-through and more efficient filtration.

In fluorescence spectroscopy, the fluorescence anisotropy, calculated from the polarization properties of fluorescence from samples excited with plane-polarized light, is used, e.g., to determine the shape of a macromolecule. Anisotropy measurements reveal the average angular displacement of the fluorophore that occurs between absorption and subsequent emission of a photon.

In NMR spectroscopy, the orientation of nuclei with respect to the applied magnetic field determines their chemical shift. In this context, anisotropic systems refer to the electron distribution of molecules with abnormally high electron density, like the pi system of benzene. This abnormal electron density affects the applied magnetic field and causes the observed chemical shift to change.
$
4
Question: In the context of a chemical anisotropic filter, what results in greater flow-through and more efficient filtration?
A: Larger interstitial spaces throughout
B: Consistent interstitial spaces
C: Increasingly smaller interstitial spaces in the direction of filtration
D: Randomly placed interstitial spaces
E: The use of magnetically polarized materials
Answer: C

Question: In fluorescence spectroscopy, what does anisotropy measurement reveal?
A: The color of the fluorophore
B: The strength of the fluorescence
C: The average angular displacement of the fluorophore
D: The duration of fluorescence
E: The source of the fluorescence
Answer: C

Question: What refers to the electron distribution of molecules with abnormally high electron density in NMR spectroscopy?
A: Alpha systems
B: Beta systems
C: Isotropic systems
D: Anisotropic systems
E: Polar systems
Answer: D

Question: In the context of NMR spectroscopy, what determines the orientation of nuclei with respect to the applied magnetic field?
A: The chemical bond
B: The temperature of the sample
C: The electron distribution of the molecule
D: The chemical shift
E: The polarization of light applied
Answer: D
@
Quantum mechanics or particle physics
When a spinless particle (or even an unpolarized particle with spin) decays, the resulting decay distribution must be isotropic in the rest frame of the decaying particle regardless of the detailed physics of the decay. This follows from rotational invariance of the Hamiltonian, which in turn is guaranteed for a spherically symmetric potential.
Kinetic theory of gases is also an example of isotropy. It is assumed that the molecules move in random directions and as a consequence, there is an equal probability of a molecule moving in any direction. Thus when there are many molecules in the gas, with high probability there will be very similar numbers moving in one direction as any other, demonstrating approximate isotropy.
Fluid dynamics
Fluid flow is isotropic if there is no directional preference (e.g. in fully developed 3D turbulence). An example of anisotropy is in flows with a background density as gravity works in only one direction. The apparent surface separating two differing isotropic fluids would be referred to as an isotrope.
Thermal expansion
A solid is said to be isotropic if the expansion of solid is equal in all directions when thermal energy is provided to the solid.
Electromagnetics
An isotropic medium is one such that the permittivity, ε, and permeability, μ, of the medium are uniform in all directions of the medium, the simplest instance being free space.
Optics
Optical isotropy means having the same optical properties in all directions. The individual reflectance or transmittance of the domains is averaged for micro-heterogeneous samples if the macroscopic reflectance or transmittance is to be calculated. This can be verified simply by investigating, e.g., a polycrystalline material under a polarizing microscope having the polarizers crossed: If the crystallites are larger than the resolution limit, they will be visible.
Cosmology
The cosmological principle, which underpins much of modern cosmology including the Big Bang theory of the evolution of the observable universe, assumes that the Universe is both isotropic and homogeneous, meaning that the universe has no preferred location (is the same everywhere) and has no preferred direction.[2] Observations made in 2006 suggest that, on distance scales much larger than galaxies, galaxy clusters are "Great" features, but small compared to so-called multiverse scenarios.[citation needed]
$
4
Question: In quantum mechanics, what is expected of the decay distribution of a spinless particle?
A: It must be anisotropic
B: It must be isotropic
C: It has no specific orientation
D: It is always polarized
E: It is always dependent on external factors
Answer: B

Question: Which property describes a medium where the permittivity and permeability are uniform in all directions?
A: Anisotropic
B: Polarized
C: Isotropic
D: Reflective
E: Refractive
Answer: C

Question: Optical isotropy means that a material has the same optical properties in which manner?
A: Only when viewed under a microscope
B: When exposed to high temperatures
C: When placed in a magnetic field
D: In all directions
E: In alternate directions
Answer: D

Question: What does the cosmological principle in modern cosmology assume about the universe?
A: The universe has a preferred direction
B: The universe has a preferred location
C: The universe is both isotropic and homogeneous
D: The universe is constantly expanding in one direction
E: The universe has boundaries
Answer: C
@
Thermal expansion is the tendency of matter to change its shape, area, volume, and density in response to a change in temperature, usually not including phase transitions.[1]

Temperature is a monotonic function of the average molecular kinetic energy of a substance. When a substance is heated, molecules begin to vibrate and move more, usually creating more distance between themselves. Substances which contract with increasing temperature are unusual, and only occur within limited temperature ranges (see examples below). The relative expansion (also called strain) divided by the change in temperature is called the material's coefficient of linear thermal expansion and generally varies with temperature. As energy in particles increases, they start moving faster and faster, weakening the intermolecular forces between them and therefore expanding the substance.
$
5
Question: What is a primary reason for matter to change its shape and volume when heated?
A: Increased bond energy
B: Decrease in molecular kinetic energy
C: Molecules move less
D: Molecules vibrate and move more, creating more distance between themselves
E: Phase transitions
Answer: D

Question: What is the relative expansion divided by the change in temperature termed as?
A: Temperature quotient
B: Molecular expansion rate
C: Coefficient of linear thermal expansion
D: Heat ratio
E: Thermal quotient
Answer: C

Question: How does temperature relate to the average molecular kinetic energy of a substance?
A: Inversely proportional
B: Proportional in a non-monotonic manner
C: Monotonic function
D: No relation
E: Complex relation
Answer: C

Question: As energy in particles of a substance increases, what happens to the intermolecular forces between them?
A: They strengthen
B: They remain constant
C: They oscillate
D: They weaken
E: They are not affected by particle energy
Answer: D

Question: Which of the following is an unusual behavior of substances in response to temperature?
A: Expansion with increased temperature
B: Contraction with increasing temperature
C: Vibration of molecules
D: Decrease in density
E: Increase in bond energy
Answer: B
@
Unlike gases or liquids, solid materials tend to keep their shape when undergoing thermal expansion.

Thermal expansion generally decreases with increasing bond energy, which also has an effect on the melting point of solids, so high melting point materials are more likely to have lower thermal expansion. In general, liquids expand slightly more than solids. The thermal expansion of glasses is slightly higher compared to that of crystals.[4] At the glass transition temperature, rearrangements that occur in an amorphous material lead to characteristic discontinuities of coefficient of thermal expansion and specific heat. These discontinuities allow detection of the glass transition temperature where a supercooled liquid transforms to a glass.[5] An interesting "cooling-by-heating" effect occurs when a glass-forming liquid is heated from the outside, resulting in a temperature drop deep inside the liquid.[6]

Absorption or desorption of water (or other solvents) can change the size of many common materials; many organic materials change size much more due to this effect than due to thermal expansion. Common plastics exposed to water can, in the long term, expand by many percent.
$
5
Question: How do solid materials typically behave during thermal expansion in comparison to gases or liquids?
A: They expand more
B: They don’t change their shape
C: They undergo phase transitions
D: They contract
E: They liquefy
Answer: B

Question: What happens at the glass transition temperature of an amorphous material?
A: It melts completely
B: Rearrangements occur leading to discontinuities of coefficient of thermal expansion
C: It becomes isotropic
D: Its thermal conductivity increases drastically
E: It starts to emit light
Answer: B

Question: Which has a higher thermal expansion: glasses or crystals?
A: Crystals
B: Glasses
C: They have the same thermal expansion
D: It depends on the type of crystal or glass
E: Neither expands with heat
Answer: B

Question: What can lead to the expansion of many common organic materials more significantly than thermal expansion?
A: Electric charge
B: Compression
C: Absorption or desorption of water
D: Reflectivity
E: Magnetism
Answer: C

Question: What unusual effect occurs when a glass-forming liquid is heated from the outside?
A: It explodes
B: A temperature rise on the inside
C: A temperature drop deep inside the liquid
D: It turns into a solid
E: It becomes completely transparent
Answer: C
@
The coefficient of thermal expansion describes how the size of an object changes with a change in temperature. Specifically, it measures the fractional change in size per degree change in temperature at a constant pressure, such that lower coefficients describe lower propensity for change in size. Several types of coefficients have been developed: volumetric, area, and linear. The choice of coefficient depends on the particular application and which dimensions are considered important. For solids, one might only be concerned with the change along a length, or over some area.

The volumetric thermal expansion coefficient is the most basic thermal expansion coefficient, and the most relevant for fluids. In general, substances expand or contract when their temperature changes, with expansion or contraction occurring in all directions. Substances that expand at the same rate in every direction are called isotropic. For isotropic materials, the area and volumetric thermal expansion coefficient are, respectively, approximately twice and three times larger than the linear thermal expansion coefficient.

Mathematical definitions of these coefficients are defined below for solids, liquids, and gases.
$
5
Question: What does the coefficient of thermal expansion measure?
A: Heat capacity of an object
B: The absolute change in temperature
C: Intermolecular forces in an object
D: The fractional change in size per degree change in temperature
E: Thermal conductivity of an object
Answer: D

Question: Which coefficient is the most relevant for fluids?
A: Linear thermal expansion coefficient
B: Area thermal expansion coefficient
C: Volumetric thermal expansion coefficient
D: Isotropic expansion coefficient
E: Strain coefficient
Answer: C

Question: How many times larger than the linear thermal expansion coefficient is the volumetric thermal expansion coefficient for isotropic materials?
A: Once larger
B: Twice as large
C: Three times larger
D: Four times larger
E: Five times larger
Answer: C

Question: What do substances that expand at the same rate in every direction called?
A: Anisotropic
B: Isotropic
C: Linear
D: Volumetric
E: Invariable
Answer: B

Question: For which state of matter would one be primarily concerned with the change along a length?
A: Liquid
B: Gas
C: Plasma
D: Solid
E: Supercritical fluid
Answer: D
@
Material	Material type	Linear
coefficient CLTE α
at 20 °C
(x10−6 K−1)	Volumetric
coefficient αV
at 20 °C
(x10−6 K−1)	Notes
Aluminium	Metal	23.1	69	
Brass	Metal alloy	19	57	
Carbon steel	Metal alloy	10.8	32.4	
CFRP		–0.8[28]	Anisotropic	Fiber direction
Concrete	Aggregate	12	36	
Copper	Metal	17	51	
Diamond	Nonmetal	1	3	
Ethanol	Liquid	250	750[29]	
Gasoline	Liquid	317	950[27]	
Glass	Glass	8.5	25.5	
Borosilicate glass[30]	Glass	3.3[31]	9.9	matched sealing partner for tungsten, molybdenum and kovar.
Glycerine	Liquid		485[30]	
Gold	Metal	14	42	
Granite	Rock	35–43	105–129	
Ice	Nonmetal	51		
Invar		1.2	3.6	
Iron	Metal	11.8	35.4	
$
5
Question: Which metal has a linear coefficient (CLTE α) of 23.1 at 20°C?
A: Brass
B: Gold
C: Aluminium
D: Copper
E: Iron
Answer: C

Question: What is the volumetric coefficient (αV) of ethanol at 20°C?
A: 69 (x10−6 K−1)
B: 950 (x10−6 K−1)
C: 750 (x10−6 K−1)
D: 485 (x10−6 K−1)
E: 51 (x10−6 K−1)
Answer: C

Question: Which material has an anisotropic nature in terms of its linear thermal expansion coefficient?
A: Glass
B: Aluminium
C: CFRP
D: Concrete
E: Diamond
Answer: C

Question: Which nonmetal has a linear coefficient (CLTE α) of 1 at 20°C?
A: Glass
B: Ice
C: Diamond
D: Invar
E: Ethanol
Answer: C

Question: Which metal alloy has a linear coefficient (CLTE α) of 19 at 20°C?
A: Aluminium
B: Brass
C: Carbon steel
D: Gold
E: Iron
Answer: B
@
Ethanol (also called ethyl alcohol, grain alcohol, drinking alcohol, or simply alcohol) is an organic compound with the chemical formula CH3CH2OH. It is an alcohol, with its formula also written as C2H5OH, C2H6O or EtOH, where Et stands for ethyl. Ethanol is a volatile, flammable, colorless liquid with a characteristic wine-like odor and pungent taste.[11][12] It is a psychoactive recreational drug, and the active ingredient in alcoholic drinks.

Ethanol is naturally produced by the fermentation process of sugars by yeasts or via petrochemical processes such as ethylene hydration. Historically it was used as a general anesthetic, and has modern medical applications as an antiseptic, disinfectant, solvent for some medications, and antidote for methanol poisoning and ethylene glycol poisoning.[13][14] It is used as a chemical solvent and in the synthesis of organic compounds, and as a fuel source. Ethanol also can be dehydrated to make ethylene, an important chemical feedstock. As of 2006, world production of ethanol was 51 gigalitres (1.3×1010 US gallons), coming mostly from Brazil and the U.S.[15]
$
5
Question: Which of the following is NOT a synonym for ethanol?
A: Methane
B: Ethyl alcohol
C: Grain alcohol
D: Drinking alcohol
E: C2H6O
Answer: A

Question: What is the primary psychoactive component in alcoholic beverages?
A: Methanol
B: Ethylene
C: Propanol
D: Ethanol
E: Glycerol
Answer: D

Question: Ethanol is naturally produced through the fermentation of:
A: Fats
B: Proteins
C: Sugars
D: Amino acids
E: Starch
Answer: C

Question: Which medical condition can ethanol serve as an antidote for?
A: Cyanide poisoning
B: Carbon monoxide poisoning
C: Ethylene glycol poisoning
D: Arsenic poisoning
E: Snake venom
Answer: C

Question: As of 2006, the two primary producers of ethanol were:
A: China and India
B: Brazil and the U.S.
C: Germany and Russia
D: Australia and South Africa
E: UK and Canada
Answer: B
@
Ethanol is the systematic name defined by the International Union of Pure and Applied Chemistry for a compound consisting of an alkyl group with two carbon atoms (prefix "eth-"), having a single bond between them (infix "-an-") and an attached −OH functional group (suffix "-ol").[16]

The "eth-" prefix and the qualifier "ethyl" in "ethyl alcohol" originally came from the name "ethyl" assigned in 1834 to the group C
2H
5− by Justus Liebig. He coined the word from the German name Aether of the compound C
2H
5−O−C
2H
5 (commonly called "ether" in English, more specifically called "diethyl ether").[17] According to the Oxford English Dictionary, Ethyl is a contraction of the Ancient Greek αἰθήρ (aithḗr, "upper air") and the Greek word ὕλη (hýlē, "substance").[18]

The name ethanol was coined as a result of a resolution on naming alcohols and phenols that was adopted at the International Conference on Chemical Nomenclature that was held in April 1892 in Geneva, Switzerland.[19]

The term alcohol now refers to a wider class of substances in chemistry nomenclature, but in common parlance it remains the name of ethanol. It is a medieval loan from Arabic al-kuḥl, a powdered ore of antimony used since antiquity as a cosmetic, and retained that meaning in Middle Latin.[20] The use of 'alcohol' for ethanol (in full, "alcohol of wine") is modern and was first recorded in 1753. Before the late 18th century the term "alcohol" generally referred to any sublimated substance.[21]
$
5
Question: The "ethyl" in "ethyl alcohol" originates from which language?
A: Latin
B: French
C: German
D: Greek
E: Arabic
Answer: D

Question: When was the name "ethanol" coined?
A: 1834
B: 1753
C: 1892
D: 2006
E: 1965
Answer: C

Question: What did the term "alcohol" originally refer to?
A: Wine
B: A cosmetic powder
C: Beer
D: Ethanol
E: An ancient drink
Answer: B

Question: The modern use of the term "alcohol" to refer to ethanol was first recorded in:
A: 1892
B: 1753
C: 1834
D: 2006
E: 1910
Answer: B

Question: Which organization defined ethanol's systematic name?
A: World Health Organization
B: American Chemical Society
C: European Chemical Agency
D: International Union of Pure and Applied Chemistry
E: Global Alcohol Standards Board
Answer: D
@
Ethanol is a versatile solvent, miscible with water and with many organic solvents, including acetic acid, acetone, benzene, carbon tetrachloride, chloroform, diethyl ether, ethylene glycol, glycerol, nitromethane, pyridine, and toluene. Its main use as a solvent is in making tincture of iodine, cough syrups, etc.[71][73] It is also miscible with light aliphatic hydrocarbons, such as pentane and hexane, and with aliphatic chlorides such as trichloroethane and tetrachloroethylene.[73]

Ethanol's miscibility with water contrasts with the immiscibility of longer-chain alcohols (five or more carbon atoms), whose water miscibility decreases sharply as the number of carbons increases.[74] The miscibility of ethanol with alkanes is limited to alkanes up to undecane: mixtures with dodecane and higher alkanes show a miscibility gap below a certain temperature (about 13 °C for dodecane[75]). The miscibility gap tends to get wider with higher alkanes, and the temperature for complete miscibility increases.

Ethanol-water mixtures have less volume than the sum of their individual components at the given fractions. Mixing equal volumes of ethanol and water results in only 1.92 volumes of mixture.[71][76] Mixing ethanol and water is exothermic, with up to 777 J/mol[77] being released at 298 K.

Mixtures of ethanol and water form an azeotrope at about 89 mole-% ethanol and 11 mole-% water[78] or a mixture of 95.6% ethanol by mass (or about 97% alcohol by volume) at normal pressure, which boils at 351 K (78 °C). This azeotropic composition is strongly temperature- and pressure-dependent and vanishes at temperatures below 303 K.[79]
$
5
Question: Ethanol is miscible with all the following EXCEPT:
A: Water
B: Acetic acid
C: Pentane
D: Carbon tetrachloride
E: Benzene
Answer: C

Question: What happens when equal volumes of ethanol and water are mixed?
A: They form 2 volumes of mixture.
B: They form 2.08 volumes of mixture.
C: They form 1.92 volumes of mixture.
D: They form 3 volumes of mixture.
E: No change in volume occurs.
Answer: C

Question: The azeotropic composition of ethanol and water boils at approximately:
A: 100°C
B: 89°C
C: 50°C
D: 78°C
E: 60°C
Answer: D

Question: Mixing ethanol and water is:
A: Isothermic
B: Endothermic
C: Athermal
D: Exothermic
E: Thermoneutral
Answer: D

Question: As the number of carbons in alcohols increases, their water miscibility:
A: Increases
B: Decreases
C: Remains the same
D: Doubles
E: Is unpredictable
Answer: B
@
Distillation
Ethylene hydration or brewing produces an ethanol–water mixture. For most industrial and fuel uses, the ethanol must be purified. Fractional distillation at atmospheric pressure can concentrate ethanol to 95.6% by weight (89.5 mole%). This mixture is an azeotrope with a boiling point of 78.1 °C (172.6 °F), and cannot be further purified by distillation. Addition of an entraining agent, such as benzene, cyclohexane, or heptane, allows a new ternary azeotrope comprising the ethanol, water, and the entraining agent to be formed. This lower-boiling ternary azeotrope is removed preferentially, leading to water-free ethanol.[102]
$
5
Question: What is the maximum concentration of ethanol achievable by fractional distillation at atmospheric pressure?
A: 100%
B: 78.1%
C: 89.5%
D: 95.6%
E: 90%
Answer: D

Question: To further purify ethanol beyond the azeotropic mixture, you could add:
A: Water
B: Salt
C: Ethylene glycol
D: Cyclohexane
E: Methanol
Answer: D

Question: An azeotrope of ethanol and water boils at:
A: 100°C
B: 78.1°C
C: 89.5°C
D: 75°C
E: 90°C
Answer: B

Question: In distillation, when the concentration of ethanol reaches the azeotropic point:
A: It cannot be further concentrated by distillation.
B: It turns into ethylene.
C: It becomes solid.
D: Its boiling point decreases drastically.
E: It separates into different layers.
Answer: A

Question: The process of producing an ethanol-water mixture can be achieved by:
A: Electrolysis
B: Ethylene hydration
C: Osmosis
D: Hydrogenation
E: Methanation
Answer: B
@
The electrochemical reduction of carbon dioxide, also known as electrolysis of carbon dioxide, is the conversion of carbon dioxide (CO2) to more reduced chemical species using electrical energy. It is one possible step in the broad scheme of carbon capture and utilization, nevertheless it is deemed to be one of the most promising approaches.[1]

Electrochemical reduction of carbon dioxide represents a possible means of producing chemicals or fuels, converting carbon dioxide (CO2) to organic feedstocks such as formic acid (HCOOH),[2] carbon monoxide (CO), methane (CH4), ethylene (C2H4) and ethanol (C2H5OH).[3][4][5] Among the more selective metallic catalysts in this field are tin for formic acid, silver for carbon monoxide and copper for methane, ethylene or ethanol. Methanol, propanol and 1-butanol have also been produced via CO2 electrochemical reduction, albeit in small quantities.[6] The main challenges are the relatively high cost of electricity (vs petroleum) and the fact that most CO2 is diluted with O2 and thus is an unsuitable substrate.

The first examples of electrochemical reduction of carbon dioxide are from the 19th century, when carbon dioxide was reduced to carbon monoxide using a zinc cathode. Research in this field intensified in the 1980s following the oil embargoes of the 1970s. As of 2021, pilot-scale carbon dioxide electrochemical reduction is being developed by several companies, including Siemens,[7] Dioxide Materials,[8][9] Twelve and GIGKarasek. The techno-economic analysis was recently conducted to assess the key technical gaps and commercial potentials of the carbon dioxide electrolysis technology at near ambient conditions.[10][11]
$
5
Question: What is one possible outcome of the electrochemical reduction of carbon dioxide?
A: Creation of helium
B: Conversion to methane
C: Transformation to hydrogen
D: Production of oxygen
E: Formation of ozone
Answer: B

Question: Which metal is most selective as a catalyst for producing formic acid through CO2 electrochemical reduction?
A: Gold
B: Copper
C: Tin
D: Iron
E: Nickel
Answer: C

Question: When did the research intensity in the field of electrochemical reduction of carbon dioxide increase?
A: 19th century
B: 1960s
C: 1980s
D: 2000s
E: 2010s
Answer: C

Question: What poses a challenge in the electrochemical reduction of carbon dioxide?
A: Abundance of nitrogen in the atmosphere
B: Low cost of electricity
C: High concentration of CO2
D: CO2 diluted with O2
E: The absence of effective catalysts
Answer: D

Question: Which company is NOT mentioned as developing pilot-scale carbon dioxide electrochemical reduction?
A: Siemens
B: Twelve
C: Dioxide Materials
D: GIGKarasek
E: ElectroTech
Answer: E

Subject2: Carbon Dioxide Compound

Question: What happens when carbon dioxide dissolves in water?
A: It forms ozone
B: It creates carbon monoxide
C: It produces methane
D: It forms carbonate and bicarbonate
E: It leads to carbonic acid only
Answer: D
@
Carbon dioxide is a chemical compound with the chemical formula CO2. It is made up of molecules that each have one carbon atom covalently double bonded to two oxygen atoms. It is found in the gas state at room temperature, and as the source of available carbon in the carbon cycle, atmospheric CO2 is the primary carbon source for life on Earth. In the air, carbon dioxide is transparent to visible light but absorbs infrared radiation, acting as a greenhouse gas. Carbon dioxide is soluble in water and is found in groundwater, lakes, ice caps, and seawater. When carbon dioxide dissolves in water, it forms carbonate and mainly bicarbonate (HCO
−
3
), which causes ocean acidification as atmospheric CO2 levels increase.[9]

It is a trace gas in Earth's atmosphere at 421 parts per million (ppm)[note 1], or about 0.04% (as of May 2022) having risen from pre-industrial levels of 280 ppm or about 0.025%.[11][12] Burning fossil fuels is the primary cause of these increased CO2 concentrations and also the primary cause of climate change.[13]
$
5
Question: What happens when carbon dioxide dissolves in water?
A: It forms ozone
B: It creates carbon monoxide
C: It produces methane
D: It forms carbonate and bicarbonate
E: It leads to carbonic acid only
Answer: D

Question: As of May 2022, what was the concentration of carbon dioxide in Earth's atmosphere?
A: 0.04%
B: 1%
C: 0.1%
D: 0.025%
E: 10%
Answer: A

Question: What has been identified as the primary cause of the increased CO2 concentrations in the atmosphere?
A: Natural forest fires
B: Increased human population
C: Solar flares
D: Volcanic activity
E: Burning fossil fuels
Answer: E

Question: In what state is carbon dioxide found at room temperature?
A: Solid
B: Liquid
C: Gas
D: Plasma
E: None of the above
Answer: C

Question: Which of the following is NOT a property of carbon dioxide?
A: It is opaque to visible light
B: It absorbs infrared radiation
C: It is insoluble in water
D: Acts as a greenhouse gas
E: Primary carbon source for life on Earth
Answer: C
@
It is a trace gas in Earth's atmosphere at 421 parts per million (ppm)[note 1], or about 0.04% (as of May 2022) having risen from pre-industrial levels of 280 ppm or about 0.025%.[11][12] Burning fossil fuels is the primary cause of these increased CO2 concentrations and also the primary cause of climate change.[13]

Its concentration in Earth's pre-industrial atmosphere since late in the Precambrian was regulated by organisms and geological phenomena. Plants, algae and cyanobacteria use energy from sunlight to synthesize carbohydrates from carbon dioxide and water in a process called photosynthesis, which produces oxygen as a waste product.[14] In turn, oxygen is consumed and CO2 is released as waste by all aerobic organisms when they metabolize organic compounds to produce energy by respiration.[15] CO2 is released from organic materials when they decay or combust, such as in forest fires. Since plants require CO2 for photosynthesis, and humans and animals depend on plants for food, CO2 is necessary for the survival of life on earth.
$
5
Question: What process uses energy from sunlight to synthesize carbohydrates from CO2 and water?
A: Digestion
B: Respiration
C: Fermentation
D: Photosynthesis
E: Decomposition
Answer: D

Question: Which of the following is NOT a product of photosynthesis?
A: Oxygen
B: Carbonate
C: Water
D: Carbohydrates
E: Methane
Answer: E

Question: What process results in the consumption of oxygen and the release of CO2 as waste?
A: Photosynthesis
B: Digestion
C: Respiration
D: Fermentation
E: Transpiration
Answer: C

Question: In which event is CO2 released from organic materials?
A: Photosynthesis
B: Combustion
C: Oxygenation
D: Hydrolysis
E: Carbonation
Answer: B

Question: What is necessary for the survival of life on earth?
A: Helium
B: Methane
C: Carbon dioxide
D: Nitrogen
E: Hydrogen peroxide
Answer: C
@
Carbon dioxide is one of the most commonly used compressed gases for pneumatic (pressurized gas) systems in portable pressure tools. Carbon dioxide is also used as an atmosphere for welding, although in the welding arc, it reacts to oxidize most metals. Use in the automotive industry is common despite significant evidence that welds made in carbon dioxide are more brittle than those made in more inert atmospheres.[citation needed] When used for MIG welding, CO2 use is sometimes referred to as MAG welding, for Metal Active Gas, as CO2 can react at these high temperatures. It tends to produce a hotter puddle than truly inert atmospheres, improving the flow characteristics. Although, this may be due to atmospheric reactions occurring at the puddle site. This is usually the opposite of the desired effect when welding, as it tends to embrittle the site, but may not be a problem for general mild steel welding, where ultimate ductility is not a major concern.

Carbon dioxide is used in many consumer products that require pressurized gas because it is inexpensive and nonflammable, and because it undergoes a phase transition from gas to liquid at room temperature at an attainable pressure of approximately 60 bar (870 psi; 59 atm), allowing far more carbon dioxide to fit in a given container than otherwise would. Life jackets often contain canisters of pressured carbon dioxide for quick inflation. Aluminium capsules of CO2 are also sold as supplies of compressed gas for air guns, paintball markers/guns, inflating bicycle tires, and for making carbonated water. High concentrations of carbon dioxide can also be used to kill pests. Liquid carbon dioxide is used in supercritical drying of some food products and technological materials, in the preparation of specimens for scanning electron microscopy[126] and in the decaffeination of coffee beans.
$
5
Question: Why is CO2 sometimes referred to as MAG when used for MIG welding?
A: Metal Argon Gas
B: Metal Active Gas
C: Metal Alloyed Gas
D: Metal Atmospheric Gas
E: Metal Artificial Gas
Answer: B

Question: In which product is pressurized CO2 commonly used for quick inflation?
A: Balloons
B: Car tires
C: Life jackets
D: Air mattresses
E: Beach balls
Answer: C

Question: For what purpose is liquid carbon dioxide used in the preparation of specimens?
A: Light microscopy
B: DNA sequencing
C: Scanning electron microscopy
D: X-ray diffraction
E: Ultraviolet spectroscopy
Answer: C

Question: What is a notable property of CO2 when used for welding?
A: It makes welds more flexible
B: It reacts to oxidize most metals
C: It reduces the temperature of the weld
D: It acts as a coolant
E: It increases weld strength
Answer: B

Question: Which of the following is NOT a use of CO2 mentioned in the text?
A: Used in fire extinguishers
B: Used for making carbonated water
C: Used in supercritical drying of food products
D: Used in the creation of lasers
E: Used in inflating bicycle tires
Answer: D
@
Welding is a fabrication process that joins materials, usually metals or thermoplastics, by using high heat to melt the parts together and allowing them to cool, causing fusion. Welding is distinct from lower temperature techniques such as brazing and soldering, which do not melt the base metal (parent metal).

In addition to melting the base metal, a filler material is typically added to the joint to form a pool of molten material (the weld pool) that cools to form a joint that, based on weld configuration (butt, full penetration, fillet, etc.), can be stronger than the base material. Pressure may also be used in conjunction with heat or by itself to produce a weld. Welding also requires a form of shield to protect the filler metals or melted metals from being contaminated or oxidized.

Many different energy sources can be used for welding, including a gas flame (chemical), an electric arc (electrical), a laser, an electron beam, friction, and ultrasound. While often an industrial process, welding may be performed in many different environments, including in open air, under water, and in outer space. Welding is a hazardous undertaking and precautions are required to avoid burns, electric shock, vision damage, inhalation of poisonous gases and fumes, and exposure to intense ultraviolet radiation.
$
5
Question: Which welding technique is distinct from methods that do not melt the base metal?
A: Brazing
B: Oxy-fuel welding
C: Arc welding
D: Laser welding
E: Ultrasonic welding
Answer: A

Question: Which of the following can be used as an energy source for welding?
A: Ultrasound
B: Gas flame
C: Electric arc
D: Friction
E: All of the above
Answer: E

Question: In which location can welding potentially be performed?
A: Underwater
B: Outer space
C: In open air
D: In a vacuum
E: Both A and B
Answer: E

Question: Why is a shield required during the welding process?
A: To increase the temperature
B: To protect the filler metals from contamination
C: To provide light
D: To bind the metals
E: To melt the base metal
Answer: B

Question: Welding may cause exposure to what type of harmful radiation?
A: Gamma radiation
B: X-ray radiation
C: Radioactive radiation
D: Ultraviolet radiation
E: Microwave radiation
Answer: D
@
Oxy-fuel welding (commonly called oxyacetylene welding, oxy welding, or gas welding in the United States) and oxy-fuel cutting are processes that use fuel gases (or liquid fuels such as gasoline or petrol, diesel, bio diesel, kerosene, etc) and oxygen to weld or cut metals. French engineers Edmond Fouché and Charles Picard became the first to develop oxygen-acetylene welding in 1903.[1] Pure oxygen, instead of air, is used to increase the flame temperature to allow localised melting of the workpiece material (e.g. steel) in a room environment. A common propane/air flame burns at about 2,250 K (1,980 °C; 3,590 °F),[2] a propane/oxygen flame burns at about 2,526 K (2,253 °C; 4,087 °F),[3] an oxyhydrogen flame burns at 3,073 K (2,800 °C; 5,072 °F) and an acetylene/oxygen flame burns at about 3,773 K (3,500 °C; 6,332 °F).[4]
$
5
Question: Which gas is used along with fuel gases in oxy-fuel welding to increase flame temperature?
A: Hydrogen
B: Nitrogen
C: Oxygen
D: Carbon dioxide
E: Helium
Answer: C

Question: Which fuel does not commonly pair with oxygen in oxy-fuel welding?
A: Diesel
B: Acetylene
C: Methane
D: Hydrogen
E: Kerosene
Answer: C

Question: What temperature does a propane/oxygen flame burn at?
A: 2,250 K
B: 2,526 K
C: 3,073 K
D: 3,773 K
E: 2,800 K
Answer: B

Question: Who developed oxygen-acetylene welding?
A: Lewis and Clark
B: Edmond Fouché and Charles Picard
C: Isaac Newton
D: Max Planck
E: Albert Einstein
Answer: B

Question: The burning temperature of acetylene/oxygen flame is approximately?
A: 2,800 °C
B: 3,500 °C
C: 2,253 °C
D: 1,980 °C
E: 4,087 °F
Answer: B
@
Arc welding is a welding process that is used to join metal to metal by using electricity to create enough heat to melt metal, and the melted metals, when cool, result in a binding of the metals. It is a type of welding that uses a welding power supply to create an electric arc between a metal stick ("electrode") and the base material to melt the metals at the point of contact. Arc welding power supplies can deliver either direct (DC) or alternating (AC) current to the work, while consumable or non-consumable electrodes are used.

The welding area is usually protected by some type of shielding gas (e.g. an inert gas), vapor, or slag. Arc welding processes may be manual, semi-automatic, or fully automated. First developed in the late part of the 19th century, arc welding became commercially important in shipbuilding during the Second World War. Today it remains an important process for the fabrication of steel structures and vehicles.
$
5
Question: What creates the electric arc in arc welding?
A: Metal stick and base material
B: Shielding gas and base material
C: Filler material and shielding gas
D: Weld pool and shielding gas
E: Electrode and shielding gas
Answer: A

Question: Which welding process was commercially significant in shipbuilding during the Second World War?
A: Ultrasonic welding
B: Oxy-fuel welding
C: Forge welding
D: Arc welding
E: Solid-state welding
Answer: D

Question: Arc welding can be used to bind metals by using which form of energy?
A: Chemical
B: Mechanical
C: Thermal
D: Electricity
E: Vibrations
Answer: D

Question: What typically protects the welding area in arc welding?
A: Electric arc
B: Shielding gas or vapor
C: Base material
D: Filler material
E: Electric current
Answer: B

Question: Which of the following may be used in arc welding?
A: Direct current
B: Alternating current
C: Inert gas
D: Both A and B
E: All of the above
Answer: E
@
Solid-state welding

Solid-state welding processes classification chart[45]
Like the first welding process, forge welding, some modern welding methods do not involve the melting of the materials being joined. One of the most popular, ultrasonic welding, is used to connect thin sheets or wires made of metal or thermoplastic by vibrating them at high frequency and under high pressure.[46] The equipment and methods involved are similar to that of resistance welding, but instead of electric current, vibration provides energy input. Welding metals with this process does not involve melting the materials; instead, the weld is formed by introducing mechanical vibrations horizontally under pressure. When welding plastics, the materials should have similar melting temperatures, and the vibrations are introduced vertically. Ultrasonic welding is commonly used for making electrical connections out of aluminum or copper, and it is also a very common polymer welding process.[46]

Another common process, explosion welding, involves the joining of materials by pushing them together under extremely high pressure. The energy from the impact plasticizes the materials, forming a weld, even though only a limited amount of heat is generated. The process is commonly used for welding dissimilar materials, including bonding aluminum to carbon steel in ship hulls and stainless steel or titanium to carbon steel in petrochemical pressure vessels.[46]

Other solid-state welding processes include friction welding (including friction stir welding and friction stir spot welding),[47] magnetic pulse welding,[48] co-extrusion welding, cold welding, diffusion bonding, exothermic welding, high frequency welding, hot pressure welding, induction welding, and roll bonding.[46]
$
5
Question: Which welding process involves joining materials without melting them and using mechanical vibrations horizontally under pressure?
A: Friction stir welding
B: Explosion welding
C: Ultrasonic welding
D: Arc welding
E: Forge welding
Answer: C

Question: In which welding process are materials pushed together under high pressure, plasticizing them without generating much heat?
A: Ultrasonic welding
B: Friction welding
C: Magnetic pulse welding
D: Explosion welding
E: Diffusion bonding
Answer: D

Question: Which welding process is commonly used for making electrical connections out of aluminum or copper?
A: Friction stir welding
B: Roll bonding
C: Ultrasonic welding
D: Co-extrusion welding
E: Exothermic welding
Answer: C

Question: Which welding process was like the first welding process and did not involve the melting of materials being joined?
A: Forge welding
B: Arc welding
C: Oxy-fuel welding
D: Ultrasonic welding
E: Explosion welding
Answer: A

Question: Magnetic pulse welding is an example of:
A: Arc welding
B: Solid-state welding
C: Fusion welding
D: Gas welding
E: Resistance welding
Answer: B
@
Fluorescence spectroscopy (also known as fluorimetry or spectrofluorometry) is a type of electromagnetic spectroscopy that analyzes fluorescence from a sample. It involves using a beam of light, usually ultraviolet light, that excites the electrons in molecules of certain compounds and causes them to emit light; typically, but not necessarily, visible light. A complementary technique is absorption spectroscopy. In the special case of single molecule fluorescence spectroscopy, intensity fluctuations from the emitted light are measured from either single fluorophores, or pairs of fluorophores.

Devices that measure fluorescence are called fluorometers.
$
5
Question: What type of light typically excites electrons in molecules during fluorescence spectroscopy?
A: Infrared light
B: Visible light
C: Ultraviolet light
D: Gamma rays
E: X-rays
Answer: C

Question: What is the name of the device used to measure fluorescence?
A: Spectrometer
B: Fluorometer
C: Chromatograph
D: Centrifuge
E: Osmometer
Answer: B

Question: Which is a complementary technique to fluorescence spectroscopy?
A: Resonance
B: Diffraction
C: Absorption spectroscopy
D: Dispersion
E: Refraction
Answer: C

Question: What is emitted from certain compounds after being excited by a beam of light in fluorescence spectroscopy?
A: Sound waves
B: Radio waves
C: Heat
D: Light
E: Electrical current
Answer: D

Question: In single molecule fluorescence spectroscopy, intensity fluctuations are measured from either single or pairs of what?
A: Electrons
B: Protons
C: Neutrons
D: Photons
E: Fluorophores
Answer: E
@
Molecules have various states referred to as energy levels. Fluorescence spectroscopy is primarily concerned with electronic and vibrational states. Generally, the species being examined has a ground electronic state (a low energy state) of interest, and an excited electronic state of higher energy. Within each of these electronic states there are various vibrational states.[1]

In fluorescence, the species is first excited, by absorbing a photon, from its ground electronic state to one of the various vibrational states in the excited electronic state. Collisions with other molecules cause the excited molecule to lose vibrational energy until it reaches the lowest vibrational state from the excited electronic state. This process is often visualized with a Jablonski diagram.[1]

The molecule then drops down to one of the various vibrational levels of the ground electronic state again, emitting a photon in the process.[1] As molecules may drop down into any of several vibrational levels in the ground state, the emitted photons will have different energies, and thus frequencies. Therefore, by analysing the different frequencies of light emitted in fluorescent spectroscopy, along with their relative intensities, the structure of the different vibrational levels can be determined.

For atomic species, the process is similar; however, since atomic species do not have vibrational energy levels, the emitted photons are often at the same wavelength as the incident radiation. This process of re-emitting the absorbed photon is "resonance fluorescence" and while it is characteristic of atomic fluorescence, is seen in molecular fluorescence as well.[2]

In a typical fluorescence (emission) measurement, the excitation wavelength is fixed and the detection wavelength varies, while in a fluorescence excitation measurement the detection wavelength is fixed and the excitation wavelength is varied across a region of interest. An emission map is measured by recording the emission spectra resulting from a range of excitation wavelengths and combining them all together. This is a three dimensional surface data set: emission intensity as a function of excitation and emission wavelengths, and is typically depicted as a contour map.
$
5
Question: What is a visualization tool commonly used to represent energy level transitions in fluorescence spectroscopy?
A: Pie chart
B: Bar graph
C: Jablonski diagram
D: Flow chart
E: Venn diagram
Answer: C

Question: Which of the following is NOT a state for molecules in fluorescence spectroscopy?
A: Liquid state
B: Ground electronic state
C: Excited electronic state
D: Vibrational state
E: Resonance state
Answer: A

Question: After absorbing a photon and getting excited, a molecule loses vibrational energy until it reaches which state?
A: Highest vibrational state in the ground electronic state
B: Lowest vibrational state in the ground electronic state
C: Highest vibrational state in the excited electronic state
D: Middle vibrational state in the excited electronic state
E: Lowest vibrational state in the excited electronic state
Answer: E

Question: How does the emitted photon frequency vary in fluorescent spectroscopy?
A: It is always the same frequency as the absorbed photon
B: It varies based on the vibrational level to which the molecule drops
C: It is always of a lower frequency than the absorbed photon
D: It is always of a higher frequency than the absorbed photon
E: It is random and unpredictable
Answer: B

Question: Which form of fluorescence involves re-emitting the absorbed photon, often at the same wavelength?
A: Absorption fluorescence
B: Intrinsic fluorescence
C: Resonance fluorescence
D: Extrinsic fluorescence
E: Diffused fluorescence
Answer: C
@
Two general types of instruments exist: filter fluorometers that use filters to isolate the incident light and fluorescent light and spectrofluorometers that use diffraction grating monochromators to isolate the incident light and fluorescent light.

Both types use the following scheme: the light from an excitation source passes through a filter or monochromator, and strikes the sample. A proportion of the incident light is absorbed by the sample, and some of the molecules in the sample fluoresce. The fluorescent light is emitted in all directions. Some of this fluorescent light passes through a second filter or monochromator and reaches a detector, which is usually placed at 90° to the incident light beam to minimize the risk of transmitted or reflected incident light reaching the detector.

Various light sources may be used as excitation sources, including lasers, LED, and lamps; xenon arcs and mercury-vapor lamps in particular. A laser only emits light of high irradiance at a very narrow wavelength interval, typically under 0.01 nm, which makes an excitation monochromator or filter unnecessary. The disadvantage of this method is that the wavelength of a laser cannot be changed by much. A mercury vapor lamp is a line lamp, meaning it emits light near peak wavelengths. By contrast, a xenon arc has a continuous emission spectrum with nearly constant intensity in the range from 300-800 nm and a sufficient irradiance for measurements down to just above 200 nm.
$
5
Question: Which of the following light sources emits light of high irradiance at a very narrow wavelength interval?
A: LED
B: Xenon arc
C: Mercury-vapor lamp
D: Laser
E: Incandescent bulb
Answer: D

Question: In which direction is the fluorescent light usually emitted from the sample?
A: Only upwards
B: Only downwards
C: In all directions
D: Only forwards
E: Only backwards
Answer: C

Question: A detector in a typical fluorescence instrument is usually placed at what angle to the incident light beam?
A: 0°
B: 45°
C: 90°
D: 180°
E: 270°
Answer: C

Question: What is the primary disadvantage of using a laser as the excitation source in fluorescence measurements?
A: It can damage the sample
B: It emits light of varying wavelengths
C: Its wavelength cannot be changed much
D: It is not powerful enough
E: It requires frequent replacement
Answer: C

Question: Which lamp has a continuous emission spectrum from 300-800 nm?
A: LED lamp
B: Mercury-vapor lamp
C: Xenon arc
D: Laser light
E: Sodium-vapor lamp
Answer: C
@
Tryptophan fluorescence
The fluorescence of a folded protein is a mixture of the fluorescence from individual aromatic residues. Most of the intrinsic fluorescence emissions of a folded protein are due to excitation of tryptophan residues, with some emissions due to tyrosine and phenylalanine; but disulfide bonds also have appreciable absorption in this wavelength range. Typically, tryptophan has a wavelength of maximum absorption of 280 nm and an emission peak that is solvatochromic, ranging from ca. 300 to 350 nm depending in the polarity of the local environment [11] Hence, protein fluorescence may be used as a diagnostic of the conformational state of a protein.[12] Furthermore, tryptophan fluorescence is strongly influenced by the proximity of other residues (i.e., nearby protonated groups such as Asp or Glu can cause quenching of Trp fluorescence). Also, energy transfer between tryptophan and the other fluorescent amino acids is possible, which would affect the analysis, especially in cases where the Förster acidic approach is taken. In addition, tryptophan is a relatively rare amino acid; many proteins contain only one or a few tryptophan residues. Therefore, tryptophan fluorescence can be a very sensitive measurement of the conformational state of individual tryptophan residues. The advantage compared to extrinsic probes is that the protein itself is not changed. The use of intrinsic fluorescence for the study of protein conformation is in practice limited to cases with few (or perhaps only one) tryptophan residues, since each experiences a different local environment, which gives rise to different emission spectra.

Tryptophan is an important intrinsic fluorescent (amino acid), which can be used to estimate the nature of microenvironment of the tryptophan. When performing experiments with denaturants, surfactants or other amphiphilic molecules, the microenvironment of the tryptophan might change. For example, if a protein containing a single tryptophan in its 'hydrophobic' core is denatured with increasing temperature, a red-shifted emission spectrum will appear. This is due to the exposure of the tryptophan to an aqueous environment as opposed to a hydrophobic protein interior. In contrast, the addition of a surfactant to a protein which contains a tryptophan which is exposed to the aqueous solvent will cause a blue-shifted emission spectrum if the tryptophan is embedded in the surfactant vesicle or micelle.[13] Proteins that lack tryptophan may be coupled to a fluorophore.

With fluorescence excitation at 295 nm, the tryptophan emission spectrum is dominant over the weaker tyrosine and phenylalanine fluorescence.
$
5
Question: The fluorescence of a folded protein is primarily due to the excitation of which residue?
A: Tyrosine
B: Phenylalanine
C: Tryptophan
D: Aspartate
E: Glutamate
Answer: C

Question: A red-shifted emission spectrum in a protein after denaturation suggests what about tryptophan?
A: It remains in the hydrophobic core
B: It is fully removed from the protein
C: It has been destroyed
D: It is exposed to an aqueous environment
E: It has transformed into tyrosine
Answer: D

Question: When a protein contains a tryptophan that becomes embedded in a surfactant vesicle or micelle, what type of emission spectrum is observed?
A: No-shift
B: Yellow-shifted
C: Blue-shifted
D: Red-shifted
E: Green-shifted
Answer: C

Question: Intrinsic fluorescence from tryptophan can give insight into the ___________ state of a protein.
A: Chemical
B: Molecular
C: Conformational
D: Solid
E: Gaseous
Answer: C

Question: When tryptophan fluorescence is excited at 295 nm, which emitted spectrum becomes dominant?
A: Aspartate
B: Phenylalanine
C: Tyrosine
D: Tryptophan
E: Glutamate
Answer: D
@
Water is an inorganic compound with the chemical formula H2O. It is a transparent, tasteless, odorless,[c] and nearly colorless chemical substance, and it is the main constituent of Earth's hydrosphere and the fluids of all known living organisms (in which it acts as a solvent[19]). It is vital for all known forms of life, despite not providing food energy, or organic micronutrients. Its chemical formula, H2O, indicates that each of its molecules contains one oxygen and two hydrogen atoms, connected by covalent bonds. The hydrogen atoms are attached to the oxygen atom at an angle of 104.45°.[20] "Water" is also the name of the liquid state of H2O at standard temperature and pressure.

Because Earth's environment is relatively close to water's triple point, water exists on Earth as a solid, liquid, and gas.[21] It forms precipitation in the form of rain and aerosols in the form of fog. Clouds consist of suspended droplets of water and ice, its solid state. When finely divided, crystalline ice may precipitate in the form of snow. The gaseous state of water is steam or water vapor.
$
5
Question: Which state(s) can water exist in due to Earth's environment being close to its triple point?
A: Solid only
B: Liquid only
C: Gas only
D: Solid and liquid only
E: Solid, liquid, and gas
Answer: E

Question: What is the bond angle formed by the hydrogen atoms with the oxygen atom in a water molecule?
A: 90°
B: 109.5°
C: 180°
D: 104.5°
E: 45°
Answer: D

Question: Which of the following does NOT correctly describe water?
A: It provides food energy.
B: It acts as a solvent in organisms.
C: It's the main constituent of Earth's hydrosphere.
D: It's essential for all known forms of life.
E: It's a polar molecule.
Answer: A

Question: What type of bonds connect hydrogen and oxygen atoms in a water molecule?
A: Ionic bonds
B: Metallic bonds
C: Hydrogen bonds
D: Covalent bonds
E: Van der Waals forces
Answer: D

Question: What is the state of H2O at standard temperature and pressure?
A: Gas
B: Solid
C: Liquid
D: Plasma
E: None of the above
Answer: C
@
Water covers about 71% of the Earth's surface, with seas and oceans making up most of the water volume (about 96.5%).[22] Small portions of water occur as groundwater (1.7%), in the glaciers and the ice caps of Antarctica and Greenland (1.7%), and in the air as vapor, clouds (consisting of ice and liquid water suspended in air), and precipitation (0.001%).[23][24] Water moves continually through the water cycle of evaporation, transpiration (evapotranspiration), condensation, precipitation, and runoff, usually reaching the sea.

Water plays an important role in the world economy. Approximately 70% of the freshwater used by humans goes to agriculture.[25] Fishing in salt and fresh water bodies has been, and continues to be, a major source of food for many parts of the world, providing 6.5% of global protein.[26] Much of the long-distance trade of commodities (such as oil, natural gas, and manufactured products) is transported by boats through seas, rivers, lakes, and canals. Large quantities of water, ice, and steam are used for cooling and heating in industry and homes. Water is an excellent solvent for a wide variety of substances, both mineral and organic; as such, it is widely used in industrial processes and in cooking and washing. Water, ice, and snow are also central to many sports and other forms of entertainment, such as swimming, pleasure boating, boat racing, surfing, sport fishing, diving, ice skating, and skiing.
$
5
Question: Approximately how much of Earth's surface is covered by water?
A: 51%
B: 71%
C: 89%
D: 95%
E: 100%
Answer: B

Question: Which among the following constitutes the largest percentage of water on Earth?
A: Groundwater
B: Glaciers and ice caps
C: Clouds and precipitation
D: Seas and oceans
E: Rivers and lakes
Answer: D

Question: How much of the freshwater used by humans is directed towards agriculture?
A: 20%
B: 50%
C: 70%
D: 90%
E: 95%
Answer: C

Question: What is the process where water undergoes evaporation, condensation, and then falls back to Earth as precipitation?
A: Water displacement
B: Hydrolysis
C: Water cycle
D: Water transition
E: Hydrologic phase
Answer: C

Question: In the global economy, water plays a significant role in which of the following activities?
A: Conducting electricity
B: Reflecting sunlight
C: Absorbing heat
D: Transport of commodities
E: Producing light
Answer: D
@
In a water molecule, the hydrogen atoms form a 104.5° angle with the oxygen atom. The hydrogen atoms are close to two corners of a tetrahedron centered on the oxygen. At the other two corners are lone pairs of valence electrons that do not participate in the bonding. In a perfect tetrahedron, the atoms would form a 109.5° angle, but the repulsion between the lone pairs is greater than the repulsion between the hydrogen atoms.[89][90] The O–H bond length is about 0.096 nm.[91]

Other substances have a tetrahedral molecular structure, for example, methane (CH
4) and hydrogen sulfide (H
2S). However, oxygen is more electronegative than most other elements, so the oxygen atom retains a negative charge while the hydrogen atoms are positively charged. Along with the bent structure, this gives the molecule an electrical dipole moment and it is classified as a polar molecule.[92]

Water is a good polar solvent, dissolving many salts and hydrophilic organic molecules such as sugars and simple alcohols such as ethanol. Water also dissolves many gases, such as oxygen and carbon dioxide—the latter giving the fizz of carbonated beverages, sparkling wines and beers. In addition, many substances in living organisms, such as proteins, DNA and polysaccharides, are dissolved in water. The interactions between water and the subunits of these biomacromolecules shape protein folding, DNA base pairing, and other phenomena crucial to life (hydrophobic effect).

Many organic substances (such as fats and oils and alkanes) are hydrophobic, that is, insoluble in water. Many inorganic substances are insoluble too, including most metal oxides, sulfides, and silicates.
$
5
Question: In a perfect tetrahedral molecule, what would be the expected bond angle?
A: 90°
B: 104.5°
C: 109.5°
D: 180°
E: 120°
Answer: C

Question: Why is water classified as a polar molecule?
A: It has a linear structure.
B: It contains only nonmetals.
C: It has an electrical dipole moment.
D: It contains noble gases.
E: It is colorless.
Answer: C

Question: Which gas gives the fizz in carbonated beverages when dissolved in water?
A: Nitrogen
B: Helium
C: Methane
D: Oxygen
E: Carbon dioxide
Answer: E

Question: Which of the following is NOT soluble in water?
A: Sugars
B: Ethanol
C: Oxygen
D: Alkanes
E: Sodium chloride
Answer: D

Question: The interactions between water and biomacromolecules are responsible for phenomena crucial to life known as:
A: Hydrophilic effect
B: Hydrophobic effect
C: Hydrogen bonding
D: Dipole interactions
E: Ion-dipole interactions
Answer: B
@
From a biological standpoint, water has many distinct properties that are critical for the proliferation of life. It carries out this role by allowing organic compounds to react in ways that ultimately allow replication. All known forms of life depend on water. Water is vital both as a solvent in which many of the body's solutes dissolve and as an essential part of many metabolic processes within the body. Metabolism is the sum total of anabolism and catabolism. In anabolism, water is removed from molecules (through energy requiring enzymatic chemical reactions) in order to grow larger molecules (e.g., starches, triglycerides, and proteins for storage of fuels and information). In catabolism, water is used to break bonds in order to generate smaller molecules (e.g., glucose, fatty acids, and amino acids to be used for fuels for energy use or other purposes). Without water, these particular metabolic processes could not exist.

Water is fundamental to photosynthesis and respiration. Photosynthetic cells use the sun's energy to split off water's hydrogen from oxygen.[105] In the presence of sunlight, hydrogen is combined with CO
2 (absorbed from air or water) to form glucose and release oxygen.[106] All living cells use such fuels and oxidize the hydrogen and carbon to capture the sun's energy and reform water and CO
2 in the process (cellular respiration).

Water is also central to acid-base neutrality and enzyme function. An acid, a hydrogen ion (H+
, that is, a proton) donor, can be neutralized by a base, a proton acceptor such as a hydroxide ion (OH−
) to form water. Water is considered to be neutral, with a pH (the negative log of the hydrogen ion concentration) of 7. Acids have pH values less than 7 while bases have values greater than 7.
$
5
Question: Water is integral to which process that uses the sun's energy to split off water's hydrogen from oxygen?
A: Respiration
B: Digestion
C: Photosynthesis
D: Fermentation
E: Decomposition
Answer: C

Question: In the process of metabolism, when larger molecules are grown, water is:
A: Taken up
B: Remained unchanged
C: Removed
D: Used as a catalyst
E: Ionized
Answer: C

Question: Which ion is a donor in the context of acid in water?
A: OH−
B: O2−
C: HCO3−
D: CO32−
E: H+
Answer: E

Question: What is the pH of neutral water?
A: 0
B: 7
C: 14
D: 10
E: 5
Answer: B

Question: Which process involves the combination of hydrogen with CO2 to form glucose and release oxygen in the presence of sunlight?
A: Cellular respiration
B: Fermentation
C: Decomposition
D: Digestion
E: Photosynthesis
Answer: E
@
Distillation, or classical distillation, is the process of separating the components or substances from a liquid mixture by using selective boiling and condensation, usually inside an apparatus known as a still. Dry distillation is the heating of solid materials to produce gaseous products (which may condense into liquids or solids); this may involve chemical changes such as destructive distillation or cracking. Distillation may result in essentially complete separation (resulting in nearly pure components), or it may be a partial separation that increases the concentration of selected components; in either case, the process exploits differences in the relative volatility of the mixture's components. In industrial applications, distillation is a unit operation of practically universal importance, but is a physical separation process, not a chemical reaction. An installation used for distillation, especially of distilled beverages, is a distillery. Distillation includes the following applications:

The distillation of fermented products produces distilled beverages with a high alcohol content, or separates other fermentation products of commercial value.
Distillation is an effective and traditional method of desalination.
In the petroleum industry, oil stabilization is a form of partial distillation that reduces the vapor pressure of crude oil, thereby making it safe for storage and transport as well as reducing the atmospheric emissions of volatile hydrocarbons. In midstream operations at oil refineries, fractional distillation is a major class of operation for transforming crude oil into fuels and chemical feed stocks.[2][3][4]
Cryogenic distillation leads to the separation of air into its components – notably oxygen, nitrogen, and argon – for industrial use.
In the chemical industry, large amounts of crude liquid products of chemical synthesis are distilled to separate them, either from other products, from impurities, or from unreacted starting materials.
$
5
Question: What is the primary goal of distillation?
A: Mixing components
B: Replacing components
C: Synthesizing new compounds
D: Separating components
E: Heating components
Answer: D

Question: What does dry distillation produce from solid materials?
A: Alcohol
B: Gaseous products
C: Water
D: Crystals
E: Ash
Answer: B

Question: Distillation in the petroleum industry primarily helps in:
A: Increasing the oil's viscosity
B: Making oil glow in the dark
C: Reducing the vapor pressure of crude oil
D: Adding fragrance to the oil
E: Changing the color of the oil
Answer: C

Question: Which application of distillation involves separating air into its components?
A: Desalination
B: Cryogenic distillation
C: Oil stabilization
D: Fermentation
E: Perfumery
Answer: B

Question: In which industry is distillation used to produce beverages with a high alcohol content?
A: Dairy industry
B: Textile industry
C: Fermented products industry
D: Electronics industry
E: Plastics industry
Answer: C
@
Ancient Near East (3000–330 BCE)
Early evidence of distillation was found on Akkadian tablets dated c. 1200 BCE describing perfumery operations. The tablets provided textual evidence that an early, primitive form of distillation was known to the Babylonians of ancient Mesopotamia.[8]

Aristotle knew that water condensing from evaporating seawater is fresh.[9]

I have proved by experiment that salt water evaporated forms fresh, and the vapour does not, when it condenses, condense into sea water again.

— Aristotle, Meteorologica, Book II, Chapter III
Letting seawater evaporate and condense into freshwater is not distillation, for distillation involves boiling, but the experiment may have been an important step towards distillation.[10]

Ancient India and China (1–500 CE)
Distillation was practiced in the ancient Indian subcontinent, which is evident from baked clay retorts and receivers found at Taxila, Shaikhan Dheri, and Charsadda in Pakistan and Rang Mahal in India dating to the early centuries of the Common Era.[11][12][13] Frank Raymond Allchin says these terracotta distill tubes were "made to imitate bamboo".[14] These "Gandhara stills" were only capable of producing very weak liquor, as there was no efficient means of collecting the vapors at low heat.[15]

Distillation in China may have begun during the Eastern Han dynasty (1st–2nd century CE)
$
5
Question: Early evidence of distillation was found on tablets from which ancient civilization?
A: Roman
B: Egyptian
C: Greek
D: Akkadian
E: Mayan
Answer: D

Question: Aristotle made an observation about which substance turning fresh from evaporating seawater?
A: Salt
B: Sugar
C: Sand
D: Water
E: Oil
Answer: D

Question: In ancient India, the distillation devices found were made to resemble what?
A: Metal pipes
B: Bamboo
C: Leather pouches
D: Stone cylinders
E: Clay pots
Answer: B

Question: Distillation in China is believed to have started during which dynasty?
A: Ming dynasty
B: Qing dynasty
C: Tang dynasty
D: Eastern Han dynasty
E: Song dynasty
Answer: D

Question: Distillation tubes from ancient India were termed as what?
A: Indian stills
B: Bamboo replicas
C: Terracotta tubes
D: Gandhara stills
E: Taxila vessels
Answer: D
@
The application of distillation can roughly be divided into four groups: laboratory scale, industrial distillation, distillation of herbs for perfumery and medicinals (herbal distillate), and food processing. The latter two are distinctively different from the former two in that distillation is not used as a true purification method but more to transfer all volatiles from the source materials to the distillate in the processing of beverages and herbs.

The main difference between laboratory scale distillation and industrial distillation are that laboratory scale distillation is often performed on a batch basis, whereas industrial distillation often occurs continuously. In batch distillation, the composition of the source material, the vapors of the distilling compounds, and the distillate change during the distillation. In batch distillation, a still is charged (supplied) with a batch of feed mixture, which is then separated into its component fractions, which are collected sequentially from most volatile to less volatile, with the bottoms – remaining least or non-volatile fraction – removed at the end. The still can then be recharged and the process repeated.

In continuous distillation, the source materials, vapors, and distillate are kept at a constant composition by carefully replenishing the source material and removing fractions from both vapor and liquid in the system. This results in a more detailed control of the separation process.
$
5
Question: What is a key difference between laboratory scale distillation and industrial distillation?
A: Color of the distillate
B: Time taken for distillation
C: The scale of operation (batch vs. continuous)
D: The type of equipment used
E: The type of compound distilled
Answer: C

Question: In which type of distillation does the composition of the source material change during the process?
A: Continuous distillation
B: Cryogenic distillation
C: Laboratory scale distillation
D: Batch distillation
E: Herbal distillation
Answer: D

Question: In continuous distillation, what remains constant?
A: Temperature
B: Equipment size
C: Source material's composition
D: Distillate's color
E: Distillation speed
Answer: C

Question: What is NOT used as a true purification method in the processing of beverages and herbs?
A: Filtration
B: Evaporation
C: Distillation
D: Crystallization
E: Separation
Answer: C

Question: In batch distillation, what happens to the still after one process is complete?
A: It is discarded
B: It is recharged
C: It is melted
D: It is broken down
E: It is sold
Answer: B
@
Some compounds have high boiling points as well as being air sensitive. A simple vacuum distillation system as exemplified above can be used, whereby the vacuum is replaced with an inert gas after the distillation is complete. However, this is a less satisfactory system if one desires to collect fractions under a reduced pressure. To do this a "cow" or "pig" adaptor can be added to the end of the condenser, or for better results or for very air sensitive compounds a Perkin triangle apparatus can be used.

The Perkin triangle, has means via a series of glass or Teflon taps to allows fractions to be isolated from the rest of the still, without the main body of the distillation being removed from either the vacuum or heat source, and thus can remain in a state of reflux. To do this, the sample is first isolated from the vacuum by means of the taps, the vacuum over the sample is then replaced with an inert gas (such as nitrogen or argon) and can then be stoppered and removed. A fresh collection vessel can then be added to the system, evacuated and linked back into the distillation system via the taps to collect a second fraction, and so on, until all fractions have been collected.
$
5
Question: In vacuum distillation, what can be used to replace the vacuum after the distillation process?
A: Air
B: Water
C: Inert gas
D: Alcohol
E: Hydrogen
Answer: C

Question: Which apparatus allows fractions to be isolated without removing the main body of the distillation from the vacuum or heat source?
A: Batch funnel
B: Cryogenic tube
C: Perkin triangle
D: Distillation flask
E: Separatory funnel
Answer: C

Question: For very air-sensitive compounds, what can be added to the end of the condenser during vacuum distillation?
A: A "cow" or "pig" adapter
B: A "horse" adapter
C: A filtration unit
D: A crystallization unit
E: A "sheep" adapter
Answer: A

Question: What is the purpose of a "Perkin triangle" in distillation?
A: To heat the substance
B: To mix the substances
C: To isolate fractions during the process
D: To enhance the flavor of the distillate
E: To cool down the vapors
Answer: C

Question: Some compounds that have high boiling points and are air sensitive are distilled under reduced pressure to:
A: Increase their boiling point further
B: Make them more air-sensitive
C: Make them less volatile
D: Protect them from reacting with air
E: Change their color
Answer: D
@
A valve is a device or natural object that regulates, directs or controls the flow of a fluid (gases, liquids, fluidized solids, or slurries) by opening, closing, or partially obstructing various passageways. Valves are technically fittings, but are usually discussed as a separate category. In an open valve, fluid flows in a direction from higher pressure to lower pressure. The word is derived from the Latin valva, the moving part of a door, in turn from volvere, to turn, roll.

The simplest, and very ancient, valve is simply a freely hinged flap which swings down to obstruct fluid (gas or liquid) flow in one direction, but is pushed up by the flow itself when the flow is moving in the opposite direction. This is called a check valve, as it prevents or "checks" the flow in one direction. Modern control valves may regulate pressure or flow downstream and operate on sophisticated automation systems.

Valves have many uses, including controlling water for irrigation, industrial uses for controlling processes, residential uses such as on/off and pressure control to dish and clothes washers and taps in the home. Even aerosol spray cans have a tiny valve built in. Valves are also used in the military and transport sectors. In HVAC ductwork and other near-atmospheric air flows, valves are instead called dampers. In compressed air systems, however, valves are used with the most common type being ball valves.
$
5
Question: What is the primary purpose of a valve?
A: To generate energy
B: To create sound
C: To control flow of a fluid
D: To increase the temperature of a fluid
E: To blend two fluids
Answer: C

Question: What type of valve allows flow in one direction but obstructs in the opposite direction?
A: Ball valve
B: Gate valve
C: Check valve
D: Plug valve
E: Damper
Answer: C

Question: What is the component in an aerosol spray can that helps in its function?
A: Trigger
B: Nozzle
C: Piston
D: Valve
E: Pump
Answer: D

Question: In HVAC systems, what term is used instead of 'valves'?
A: Pumps
B: Compressors
C: Dampers
D: Pistons
E: Cylinders
Answer: C

Question: Which of the following is NOT a common type of valve?
A: Butterfly valve
B: Ball valve
C: Bucket valve
D: Check valve
E: Plug valve
Answer: C
@
The valve's body is the outer casing of most or all of the valve that contains the internal parts or trim. The bonnet is the part of the encasing through which the stem (see below) passes and that forms a guide and seal for the stem. The bonnet typically screws into or is bolted to the valve body.

Valve bodies are usually metallic or plastic. Brass, bronze, gunmetal, cast iron, steel, alloy steels and stainless steels are very common.[1] Seawater applications, like desalination plants, often use duplex valves, as well as super duplex valves, due to their corrosion resistant properties, particularly against warm seawater. Alloy 20 valves are typically used in sulphuric acid plants, whilst monel valves are used in hydrofluoric acid (HF Acid) plants. Hastelloy valves are often used in high temperature applications, such as nuclear plants, whilst inconel valves are often used in hydrogen applications. Plastic bodies are used for relatively low pressures and temperatures. PVC, PP, PVDF and glass-reinforced nylon are common plastics used for valve bodies.[citation needed]
$
5
Question: What is the role of the bonnet in a valve?
A: To control the flow
B: To act as a guide and seal for the stem
C: To connect to the fluid source
D: To provide structural support
E: To act as a visual indicator
Answer: B

Question: Which material is often used in valves due to its resistance against warm seawater?
A: Brass
B: PVC
C: Alloy 20
D: Super duplex
E: Hastelloy
Answer: D

Question: In which type of application are Alloy 20 valves typically used?
A: Hydrofluoric acid plants
B: Sulphuric acid plants
C: Nuclear plants
D: Seawater desalination
E: Hydrogen applications
Answer: B

Question: What common plastic is NOT mentioned as being used for valve bodies?
A: PVC
B: PP
C: PVDF
D: ABS
E: Glass-reinforced nylon
Answer: D

Question: Which type of valve body is considered more durable?
A: Soft seated
B: Hard seated
C: Alloy seated
D: PVC seated
E: Metallic seated
Answer: B
@
The seat is the interior surface of the body which contacts the disc to form a leak-tight seal. In discs that move linearly or swing on a hinge or trunnion, the disc comes into contact with the seat only when the valve is shut. In disks that rotate, the seat is always in contact with the disk, but the area of contact changes as the disc is turned. The seat always remains stationary relative to the body.

Seats are classified by whether they are cut directly into the body, or if they are made of a different material:

Hard seats are integral to the valve body. Nearly all hard seated metal valves have a small amount of leakage.
Soft seats are fitted to the valve body and made of softer materials such as PTFE or various elastomers such as NBR, EPDM, or FKM depending on the maximum operating temperature.
A closed soft seated valve is much less liable to leak when shut while hard seated valves are more durable. Gate, globe, and check valves are usually hard seated while butterfly, ball, plug, and diaphragm valves are usually soft seated.
$
5
Question: Which type of seat remains stationary relative to the valve body?
A: Rotating seat
B: Mobile seat
C: Transient seat
D: Hard seat
E: All of the above
Answer: E

Question: What type of seated valve is less likely to leak when shut?
A: Hard seated
B: Soft seated
C: Integral seated
D: Rotating seated
E: Fixed seated
Answer: B

Question: In which valve type is the seat always in contact with the disc, but the area of contact changes?
A: Swing valve
B: Linear valve
C: Rotating disk valve
D: Stationary disk valve
E: Hinged valve
Answer: C

Question: What type of valve typically has a soft seat?
A: Gate valve
B: Globe valve
C: Check valve
D: Ball valve
E: Butterfly valve
Answer: D

Question: Which material is NOT mentioned as a material for soft seats in valves?
A: PTFE
B: EPDM
C: NBR
D: PVC
E: FKM
Answer: D
@
Many valves are controlled manually with a handle attached to the stem. If the handle is turned ninety degrees between operating positions, the valve is called a quarter-turn valve. Butterfly, ball valves, and plug valves are often quarter-turn valves. If the handle is circular with the stem as the axis of rotation in the center of the circle, then the handle is called a handwheel. Valves can also be controlled by actuators attached to the stem. They can be electromechanical actuators such as an electric motor or solenoid, pneumatic actuators which are controlled by air pressure, or hydraulic actuators which are controlled by the pressure of a liquid such as oil or water. Actuators can be used for the purposes of automatic control such as in washing machine cycles, remote control such as the use of a centralised control room, or because manual control is too difficult such as when the valve is very large. Pneumatic actuators and hydraulic actuators need pressurised air or liquid lines to supply the actuator: an inlet line and an outlet line. Pilot valves are valves which are used to control other valves. Pilot valves in the actuator lines control the supply of air or liquid going to the actuators.

The fill valve in a toilet water tank is a liquid level-actuated valve. When a high water level is reached, a mechanism shuts the valve which fills the tank.

In some valve designs, the pressure of the flow fluid itself or pressure difference of the flow fluid between the ports automatically controls flow through the valve.
$
5
Question: If a valve's handle is turned ninety degrees between operating positions, what is it called?
A: Full-turn valve
B: Half-turn valve
C: Quarter-turn valve
D: Multi-turn valve
E: Single-turn valve
Answer: C

Question: What type of actuator is controlled by air pressure?
A: Electromechanical
B: Pneumatic
C: Hydraulic
D: Manual
E: Solenoid
Answer: B

Question: Which type of valve is used to control other valves?
A: Main valve
B: Actuator valve
C: Control valve
D: Pilot valve
E: Primary valve
Answer: D

Question: What type of valve is liquid level-actuated and found in a toilet water tank?
A: Flow valve
B: Pressure valve
C: Fill valve
D: Release valve
E: Flush valve
Answer: C

Question: In some valve designs, what automatically controls flow through the valve?
A: A manual handle
B: An external electric switch
C: The pressure or pressure difference of the flow fluid
D: A connected computer system
E: The temperature of the surrounding environment
Answer: C
@
Hydraulics (from Greek ὕδωρ (hydor) 'water', and αὐλός (aulos) 'pipe')[2] is a technology and applied science using engineering, chemistry, and other sciences involving the mechanical properties and use of liquids. At a very basic level, hydraulics is the liquid counterpart of pneumatics, which concerns gases. Fluid mechanics provides the theoretical foundation for hydraulics, which focuses on applied engineering using the properties of fluids. In its fluid power applications, hydraulics is used for the generation, control, and transmission of power by the use of pressurized liquids. Hydraulic topics range through some parts of science and most of engineering modules, and cover concepts such as pipe flow, dam design, fluidics and fluid control circuitry. The principles of hydraulics are in use naturally in the human body within the vascular system and erectile tissue.[3][4]

Free surface hydraulics is the branch of hydraulics dealing with free surface flow, such as occurring in rivers, canals, lakes, estuaries and seas. Its sub-field open-channel flow studies the flow in open channels.
$
5
Question: Which term refers to the technology and applied science using properties and uses of liquids for various applications?
A: Electromechanics
B: Thermodynamics
C: Hydraulics
D: Aerodynamics
E: Geophysics
Answer: C

Question: What does hydraulics focus on in fluid power applications?
A: Absorption
B: Filtration
C: Reflection
D: Generation, control, and transmission of power
E: Radiation
Answer: D

Question: Which is the branch of hydraulics that deals with free surface flow such as in rivers and seas?
A: Subterranean hydraulics
B: Closed-circuit hydraulics
C: Free surface hydraulics
D: Fluid dynamics
E: Pneumatics
Answer: C

Question: Pneumatics is the counterpart of hydraulics and concerns which of the following?
A: Solids
B: Liquids
C: Gases
D: Plasmas
E: Granules
Answer: C

Question: What is the primary science that provides the theoretical foundation for hydraulics?
A: Fluid mechanics
B: Electromagnetism
C: Thermodynamics
D: Quantum mechanics
E: Relativity
Answer: A
@
In Ancient Greece, the Greeks constructed sophisticated water and hydraulic power systems. An example is a construction by Eupalinos, under a public contract, of a watering channel for Samos, the Tunnel of Eupalinos. An early example of the usage of hydraulic wheel, probably the earliest in Europe, is the Perachora wheel (3rd century BC).[16]

In Greco-Roman Egypt, the construction of the first hydraulic machine automata by Ctesibius (flourished c. 270 BC) and Hero of Alexandria (c. 10 – 80 AD) is notable. Hero describes several working machines using hydraulic power, such as the force pump, which is known from many Roman sites as having been used for raising water and in fire engines.[17]


Aqueduct of Segovia, a 1st-century AD masterpiece
In the Roman Empire, different hydraulic applications were developed, including public water supplies, innumerable aqueducts, power using watermills and hydraulic mining. They were among the first to make use of the siphon to carry water across valleys, and used hushing on a large scale to prospect for and then extract metal ores. They used lead widely in plumbing systems for domestic and public supply, such as feeding thermae.[citation needed]

Hydraulic mining was used in the gold-fields of northern Spain, which was conquered by Augustus in 25 BC. The alluvial gold-mine of Las Medulas was one of the largest of their mines. At least seven long aqueducts worked it, and the water streams were used to erode the soft deposits, and then wash the tailings for the valuable gold content.[18][19]
$
5
Question: In which civilization were hydraulic wheels likely used the earliest in Europe?
A: Roman Empire
B: Ancient Greece
C: Ancient Egypt
D: Medieval England
E: Renaissance Italy
Answer: B

Question: Which hydraulic application did the Roman Empire NOT utilize?
A: Aqueducts
B: Hydraulic clocks
C: Hydraulic mining
D: Siphons across valleys
E: Watermills for power
Answer: B

Question: Hydraulic mining was extensively used in the gold-fields of which region?
A: Ancient Mesopotamia
B: Northern India
C: Northern Spain
D: Southern Egypt
E: Western China
Answer: C

Question: Who constructed a watering channel for Samos, known as the Tunnel of Eupalinos?
A: Hero of Alexandria
B: Augustus
C: Ctesibius
D: Eupalinos
E: Pliny the Elder
Answer: D

Question: What was the primary material widely used by the Roman Empire in their plumbing systems?
A: Copper
B: Gold
C: Iron
D: Lead
E: Bronze
Answer: D
@
Modern era (c. 1600–1870)
Benedetto Castelli
In 1619 Benedetto Castelli, a student of Galileo Galilei, published the book Della Misura dell'Acque Correnti or "On the Measurement of Running Waters," one of the foundations of modern hydrodynamics. He served as a chief consultant to the Pope on hydraulic projects, i.e., management of rivers in the Papal States, beginning in 1626.[28]

Blaise Pascal
Blaise Pascal (1623–1662) studied fluid hydrodynamics and hydrostatics, centered on the principles of hydraulic fluids. His discovery on the theory behind hydraulics led to his invention of the hydraulic press, which multiplied a smaller force acting on a smaller area into the application of a larger force totaled over a larger area, transmitted through the same pressure (or exact change of pressure) at both locations. Pascal's law or principle states that for an incompressible fluid at rest, the difference in pressure is proportional to the difference in height, and this difference remains the same whether or not the overall pressure of the fluid is changed by applying an external force. This implies that by increasing the pressure at any point in a confined fluid, there is an equal increase at every other end in the container, i.e., any change in pressure applied at any point of the liquid is transmitted undiminished throughout the fluids.

Jean Léonard Marie Poiseuille
A French physician, Poiseuille (1797–1869) researched the flow of blood through the body and discovered an important law governing the rate of flow with the diameter of the tube in which flow occurred.[29][citation needed]
$
5
Question: Who published the book "Della Misura dell'Acque Correnti" in 1619?
A: Jean Léonard Marie Poiseuille
B: Galileo Galilei
C: Blaise Pascal
D: Benedetto Castelli
E: Hero of Alexandria
Answer: D

Question: Which principle states that any change in pressure applied at any point of a liquid is transmitted undiminished throughout the fluid?
A: Galileo's principle
B: Poiseuille's equation
C: Newton's third law
D: Pascal's law
E: Bernoulli's principle
Answer: D

Question: Blaise Pascal's hydraulic discovery led to the invention of what device?
A: Hydraulic turbine
B: Hydraulic pump
C: Hydraulic press
D: Hydraulic brake
E: Hydraulic elevator
Answer: C

Question: Who researched blood flow in the human body and discovered a law regarding flow rate and tube diameter?
A: Jean Léonard Marie Poiseuille
B: Blaise Pascal
C: Benedetto Castelli
D: Galileo Galilei
E: Augustus
Answer: A

Question: What did Benedetto Castelli mainly consult the Pope on?
A: Astronomical phenomena
B: Hydraulic projects
C: Religious scriptures
D: Architectural designs
E: Military strategies
Answer: B
@
Hydraulic machines use liquid fluid power to perform work. Heavy construction vehicles are a common example. In this type of machine, hydraulic fluid is pumped to various hydraulic motors and hydraulic cylinders throughout the machine and becomes pressurized according to the resistance present. The fluid is controlled directly or automatically by control valves and distributed through hoses, tubes, or pipes.

Hydraulic systems, like pneumatic systems, are based on Pascal's law which states that any pressure applied to a fluid inside a closed system will transmit that pressure equally everywhere and in all directions. A hydraulic system uses an incompressible liquid as its fluid, rather than a compressible gas.

The popularity of hydraulic machinery is due to the very large amount of power that can be transferred through small tubes and flexible hoses, the high power density and a wide array of actuators that can make use of this power, and the huge multiplication of forces that can be achieved by applying pressures over relatively large areas. One drawback, compared to machines using gears and shafts, is that any transmission of power results in some losses due to resistance of fluid flow through the piping.
$
5
Question: What do heavy construction vehicles commonly utilize for their operations?
A: Pneumatic machinery
B: Electric machinery
C: Hydraulic machinery
D: Thermal machinery
E: Magnetic machinery
Answer: C

Question: What kind of fluid does a hydraulic system predominantly use?
A: Compressible gas
B: Incompressible liquid
C: Semi-compressible gel
D: Plasma
E: Granular solid
Answer: B

Question: Which law states that pressure applied to a fluid inside a closed system will transmit that pressure equally everywhere and in all directions?
A: Newton's third law
B: Bernoulli's principle
C: Pascal's law
D: Poiseuille's law
E: Boyle's law
Answer: C

Question: Which of the following is NOT a benefit of using hydraulic machinery?
A: Zero power loss in transmission
B: High power density
C: Ability to use a wide array of actuators
D: Transmission of a large amount of power through small tubes
E: Huge multiplication of forces
Answer: A

Question: Hydraulic and pneumatic systems are both based on whose law?
A: Galileo's principle
B: Newton's law
C: Bernoulli's principle
D: Pascal's law
E: Boyle's law
Answer: D
@
Pneumatics (from Greek πνεῦμα pneuma ‘wind, breath’) is a branch of engineering that makes use of gas or pressurized air.

Pneumatic systems used in industry are commonly powered by compressed air or compressed inert gases. A centrally located and electrically-powered compressor powers cylinders, air motors, pneumatic actuators, and other pneumatic devices. A pneumatic system controlled through manual or automatic solenoid valves is selected when it provides a lower cost, more flexible, or safer alternative to electric motors, and hydraulic actuators.

Pneumatics also has applications in dentistry, construction, mining, and other areas.
$
5
Question: What is the primary gas used in pneumatic systems in industry?
A: Oxygen
B: Nitrogen
C: Compressed air
D: Carbon dioxide
E: Hydrogen
Answer: C

Question: Pneumatics is derived from the Greek word meaning:
A: Pressure
B: Gas
C: Wind or breath
D: Movement
E: Machine
Answer: C

Question: Which sector often uses pneumatic applications?
A: Space exploration
B: Oceanography
C: Dentistry
D: Agriculture
E: Astronomy
Answer: C

Question: What powers pneumatic devices like cylinders and air motors in an industrial setting?
A: Hydraulic fluid
B: Compressed inert gases
C: Natural air
D: A centrally located and electrically-powered compressor
E: Battery power
Answer: D

Question: Which of the following is a reason for choosing pneumatics over electric motors?
A: Higher power density
B: More flammable
C: Safer alternative
D: Less flexibility
E: More expensive
Answer: C
@
Pneumatic systems in fixed installations, such as factories, use compressed air because a sustainable supply can be made by compressing atmospheric air. The air usually has moisture removed, and a small quantity of oil is added at the compressor to prevent corrosion and lubricate mechanical components.

Factory-plumbed pneumatic-power users need not worry about poisonous leakage, as the gas is usually just air. Any compressed gas other than air is an asphyxiation hazard—including nitrogen, which makes up 78% of air. Compressed oxygen (approx. 21% of air) would not asphyxiate, but is not used in pneumatically-powered devices because it is a fire hazard, more expensive, and offers no performance advantage over air. Smaller or stand-alone systems can use other compressed gases that present an asphyxiation hazard, such as nitrogen—often referred to as OFN (oxygen-free nitrogen) when supplied in cylinders.

Portable pneumatic tools and small vehicles, such as Robot Wars machines and other hobbyist applications are often powered by compressed carbon dioxide, because containers designed to hold it such as soda stream canisters and fire extinguishers are readily available, and the phase change between liquid and gas makes it possible to obtain a larger volume of compressed gas from a lighter container than compressed air requires. Carbon dioxide is an asphyxiant and can be a freezing hazard if vented improperly.
$
5
Question: In fixed installations, why is compressed air typically used in pneumatic systems?
A: It's flammable
B: A sustainable supply can be made by compressing atmospheric air
C: It's cheaper than other gases
D: It provides more power
E: It's less safe than other gases
Answer: B

Question: What is a potential danger of using compressed gases other than air?
A: Explosiveness
B: Too expensive
C: Asphyxiation hazard
D: Corrosion
E: Over-pressurization
Answer: C

Question: Why is compressed oxygen not used in pneumatically-powered devices?
A: It is an asphyxiant
B: It is too heavy
C: It's a fire hazard
D: It's not easily available
E: It causes rusting
Answer: C

Question: What advantage does compressed carbon dioxide offer for portable pneumatic tools?
A: It is cheaper than air
B: It is inflammable
C: The phase change allows more gas storage in a lighter container
D: It provides more power than compressed air
E: It is readily available everywhere
Answer: C

Question: Which gas is often used in smaller or stand-alone pneumatic systems due to the availability of containers?
A: Nitrogen
B: Helium
C: Carbon dioxide
D: Oxygen
E: Hydrogen
Answer: C
@
Both pneumatics and hydraulics are applications of fluid power. Pneumatics uses an easily compressible gas such as air or a suitable pure gas—while hydraulics uses relatively incompressible liquid media such as oil. Most industrial pneumatic applications use pressures of about 80 to 100 pounds per square inch (550 to 690 kPa). Hydraulics applications commonly use from 1,000 to 5,000 psi (6.9 to 34.5 MPa), but specialized applications may exceed 10,000 psi (69 MPa).[citation needed]

Advantages of pneumatics
Simplicity of design and control—Machines are easily designed using standard cylinders and other components, and operate via simple on-off control.
Reliability—Pneumatic systems generally have long operating lives and require little maintenance. Because gas is compressible, equipment is less subject to shock damage. Gas absorbs excessive force, whereas fluid in hydraulics directly transfers force. Compressed gas can be stored, so machines still run for a while if electrical power is lost.
Safety—There is a very low chance of fire compared to hydraulic oil. New machines are usually overload safe to a certain limit.
Advantages of hydraulics
Fluid does not absorb any of the supplied energy.
Capable of moving much higher loads and providing much lower forces due to the incompressibility.
The hydraulic working fluid is practically inincompressible, leading to a minimum of spring action. When hydraulic fluid flow is stopped, the slightest motion of the load releases the pressure on the load; there is no need to "bleed off" pressurized air to release the pressure on the load.
Highly responsive compared to pneumatics.
Supply more power than pneumatics.
Can also do many purposes at one time: lubrication, cooling and power transmission.
$
5
Question: Which system uses an easily compressible gas for its operations?
A: Hydraulics
B: Thermodynamics
C: Electromagnetism
D: Pneumatics
E: Kinetics
Answer: D

Question: What is the typical pressure range for most industrial pneumatic applications?
A: 1,000 to 5,000 psi
B: 80 to 100 pounds per square inch
C: 10 to 30 psi
D: 500 to 800 kPa
E: Above 10,000 psi
Answer: B

Question: What does NOT absorb any of the supplied energy in its system?
A: Gas in pneumatics
B: Fluid in hydraulics
C: Water in aquatics
D: Metal in mechanics
E: Light in optics
Answer: B

Question: Which system is highly responsive and can also lubricate, cool, and transmit power simultaneously?
A: Pneumatics
B: Electromechanics
C: Thermodynamics
D: Hydraulics
E: Kinematics
Answer: D

Question: In which system does gas absorb excessive force, reducing shock damage?
A: Hydraulics
B: Thermodynamics
C: Kinetics
D: Pneumatics
E: Electromagnetism
Answer: D
@
Air brakes are typically used on heavy trucks and buses. Typical operating pressure is approximately 100–120 psi or 690–830 kPa or 6.9–8.3 bar. A compressed-air-brake system is divided into a supply system and a control system.

The supply system compresses, stores and supplies high-pressure air to the control system as well as to additional air operated auxiliary truck systems (gearbox shift control, clutch pedal air assistance servo, etc.). The air compressor draws filtered air from the atmosphere and compresses it, storing the compressed air in high-pressure reservoirs.[4] Most heavy vehicles have a gauge within the driver's view, indicating the availability of air pressure for safe vehicle operation, often including warning tones or lights. A mechanical "wig wag" that automatically drops down into the driver's field of vision when the pressure drops below a certain point is also common.

The control system consists of service brakes, parking brakes, a control pedal, and an air storage tank. If the vehicle is towing a trailer, it often has a separate trailer-brake system that receives compressed air from the supply system.

The parking brakes use a disc or drum arrangement which is designed to be held in the 'applied' position by spring pressure. Air pressure must be produced to release these "spring brake" parking brakes. Setting the parking/emergency brake releases the pressurized air in the lines between the compressed air storage tank and the brakes, thus allowing the spring actuated parking brake to engage. A sudden loss of air pressure would result in full spring brake pressure immediately.

The service brakes are used while driving for slowing or stopping the vehicle. When the brake pedal is pushed to apply the service brakes, air is routed under pressure from a supply reservoir to the service brake chamber, causing the brake to be engaged. When the pedal is released, a return spring in the brake chamber disengages the brake, and the compressed air is exhausted to the atmosphere.[4] Most types of truck air brakes are drum brakes, though there is an increasing trend towards the use of disc brakes.
$
5
Question: Where are air brakes typically used?
A: Motorcycles
B: Passenger cars
C: Bicycles
D: Heavy trucks and buses
E: Airplanes
Answer: D

Question: What is the purpose of the supply system in a compressed-air-brake system?
A: Control the vehicle's speed
B: Activate the parking brakes
C: Store and supply high-pressure air to the control system
D: Monitor tire pressure
E: Cool the brake discs
Answer: C

Question: What happens when the parking/emergency brake is set in an air brake system?
A: Air pressure is increased
B: The vehicle accelerates
C: Pressurized air in the lines is released
D: More air is pumped into the storage tank
E: Brake fluid is added to the system
Answer: C

Question: Which type of brakes are most common in truck air brakes?
A: Disk brakes
B: Caliper brakes
C: Drum brakes
D: Hydraulic brakes
E: Regenerative brakes
Answer: C

Question: How do service brakes in an air brake system work?
A: They release air pressure when the brake pedal is pushed
B: They add air pressure to the storage tank
C: They use brake fluid to slow down the vehicle
D: They engage the brake when air is routed to the brake chamber
E: They operate on hydraulic principles
Answer: D
@
A disc brake is a type of brake that uses the calipers to squeeze pairs of pads against a disc or a "rotor"[1] to create friction.[2] This action slows the rotation of a shaft, such as a vehicle axle, either to reduce its rotational speed or to hold it stationary. The energy of motion is converted into waste heat which must be dispersed.

Hydraulically actuated disc brakes are the most commonly used form of brake for motor vehicles, but the principles of a disc brake are applicable to almost any rotating shaft. The components include the disc, master cylinder, and caliper, which contains at least one cylinder and two brake pads on both sides of the disc.
$
5
Question: Which component in a disc brake system creates friction to slow down the rotation?
A: Caliper
B: Rotor
C: Axle
D: Master cylinder
E: Brake pad
Answer: E

Question: What happens to the energy of motion in a disc brake system?
A: It is stored.
B: It is converted into waste heat.
C: It is released into the atmosphere.
D: It is converted into electrical energy.
E: It remains constant.
Answer: B

Question: Disc brakes are most commonly used for which type of vehicle?
A: Bicycles
B: Motor vehicles
C: Airplanes
D: Trains
E: Boats
Answer: B

Question: How many brake pads are typically present on both sides of a disc in a disc brake system?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: B

Question: The primary purpose of a disc brake is to:
A: Increase rotational speed.
B: Hold a shaft stationary.
C: Convert rotational speed into electrical energy.
D: Lubricate the rotating shaft.
E: Amplify the rotational speed.
Answer: B
@
The development of disc-type brakes began in England in the 1890s. In 1902, the Lanchester Motor Company designed brakes that looked and operated in a similar way to a modern disc-brake system even though the disc was thin and a cable activated the brake pad.[3] Other designs were not practical or widely available in cars for another 60 years. Successful application began in airplanes before World War II, and even the German Tiger tank was fitted with discs in 1942. After the war, technological progress began to arrive in 1949, with caliper-type four-wheel disc brakes on the Crosley line, and a Chrysler non-caliper type. In the 1950s, there was a critical demonstration of superiority at the 1953 24 Hours of Le Mans race, which required braking from high speeds several times per lap.[4] The Jaguar racing team won, using disc brake-equipped cars, with much of the credit being given to the brakes' superior performance over rivals equipped with drum brakes.[4] Mass production began with the 1949–1950 inclusion in all Crosley production, with sustained mass production beginning in 1955 Citroën DS.[3]

Compared to drum brakes, disc brakes offer better-stopping performance because the disc is more readily cooled. As a consequence discs are less prone to the brake fade caused when brake components overheat. Disc brakes also recover more quickly from immersion (wet brakes are less effective than dry ones).[4]
$
5
Question: Which company designed a brake system in 1902 that resembled a modern disc-brake system?
A: Jaguar
B: Porsche
C: Crosley
D: Lanchester Motor Company
E: Citroën
Answer: D

Question: What significant advantage do disc brakes have over drum brakes?
A: They are cheaper.
B: They are more complex.
C: They recover more slowly from immersion.
D: They offer better-stopping performance due to cooling.
E: They have higher maintenance needs.
Answer: D

Question: Disc brakes became more popular in mass production after their success in which racing event?
A: Formula One Racing
B: NASCAR
C: 1953 24 Hours of Le Mans
D: Grand Prix Motor Racing
E: The Indianapolis 500
Answer: C

Question: What is a key advantage of disc brakes when compared to drum brakes concerning heat?
A: They produce more heat.
B: They are less prone to brake fade caused by overheating.
C: They retain heat for longer durations.
D: They require external cooling systems.
E: They function optimally at higher temperatures.
Answer: B

Question: In the 1950s, which car brand began the sustained mass production of disc brakes?
A: Mercedes-Benz
B: Ford
C: BMW
D: Volkswagen
E: Citroën
Answer: E
@
Ceramic discs are used in some high-performance cars and heavy vehicles.

The first development of the modern ceramic brake was made by British engineers for TGV applications in 1988. The objective was to reduce weight, the number of brakes per axle, as well as provide stable friction from high speeds and all temperatures. The result was a carbon-fiber-reinforced ceramic process which is now used in various forms for automotive, railway, and aircraft brake applications.

Due to the high heat tolerance and mechanical strength of ceramic composite discs, they are often used on exotic vehicles where the cost is not prohibitive.[48] They are also found in industrial applications where the ceramic disc's lightweight and low-maintenance properties justify the cost. Composite brakes can withstand temperatures that would damage steel discs.

Porsche's Composite Ceramic Brakes (PCCB) are siliconized carbon fiber, with high-temperature capability, a 50% weight reduction over iron discs (hence reducing the vehicle's unsprung weight), a significant reduction in dust generation, substantially extended maintenance intervals, and enhanced durability in corrosive environments. Found on some of their more expensive models, it is also an optional brake for all street Porsches at added expense. They can be recognized by the bright yellow paintwork on the aluminum six-piston calipers. The discs are internally vented much like cast-iron ones, and cross-drilled.[citation needed]
$
5
Question: What was the main goal of the initial development of the modern ceramic brake in 1988?
A: Increase cost
B: Make it applicable only to sports cars
C: Reduce weight and provide stable friction at various speeds
D: Make it more visually appealing
E: Make the brakes more noisy
Answer: C

Question: Ceramic composite discs have a high tolerance to:
A: Magnetic fields
B: Sound waves
C: High heat
D: Water submersion
E: Electrical surges
Answer: C

Question: Which car brand offers Composite Ceramic Brakes (PCCB) with bright yellow paintwork on the aluminum six-piston calipers?
A: Mercedes-Benz
B: Audi
C: BMW
D: Porsche
E: Lamborghini
Answer: D

Question: Ceramic brakes offer an advantage over steel discs due to their ability to:
A: Rust easily
B: Absorb moisture
C: Withstand higher temperatures
D: Decrease a vehicle's speed
E: Increase a vehicle's weight
Answer: C

Question: Porsche's Composite Ceramic Brakes are reinforced with which material?
A: Iron
B: Gold
C: Copper
D: Siliconized carbon fiber
E: Platinum
Answer: D
@
The brake caliper is the assembly that houses the brake pads and pistons. The pistons are usually made of plastic, aluminium or chrome-plated steel.

Calipers are of two types, floating or fixed. A fixed caliper does not move relative to the disc and is thus less tolerant of disc imperfections. It uses one or more pairs of opposing pistons to clamp from each side of the disc, and is more complex and expensive than a floating caliper.

A floating caliper (also called a "sliding caliper") moves with respect to the disc, along a line parallel to the axis of rotation of the disc; a piston on one side of the disc pushes the inner brake pad until it makes contact with the braking surface, then pulls the caliper body with the outer brake pad so the pressure is applied to both sides of the disc. Floating caliper (single piston) designs are subject to sticking failure, caused by dirt or corrosion entering at least one mounting mechanism and stopping its normal movement. This can lead to the caliper's pads rubbing on the disc when the brake is not engaged or engaging it at an angle. Sticking can result from infrequent vehicle use, failure of a seal or rubber protection boot allowing debris entry, dry-out of the grease in the mounting mechanism, and subsequent moisture incursion leading to corrosion, or some combination of these factors. Consequences may include reduced fuel efficiency, extreme heating of the disc, or excessive wear on the affected pad. A sticking front caliper may also cause steering vibration.

Another type of floating caliper is a swinging caliper. Instead of a pair of horizontal bolts that allow the caliper to move straight in and out respective to the car body, a swinging caliper utilizes a single, vertical pivot bolt located somewhere behind the axle centerline. When the driver presses the brakes, the brake piston pushes on the inside piston and rotates the whole caliper inward, when viewed from the top. Because the swinging caliper's piston angle changes relative to the disc, this design uses wedge-shaped pads that are narrower in the rear on the outside and narrower in the front on the inside.
$
5
Question: The brake caliper houses which components?
A: Brake disc and rotor
B: Brake pads and pistons
C: Brake fluid and master cylinder
D: Brake cables and lines
E: Brake shoe and drum
Answer: B

Question: What is NOT a typical material for brake pistons?
A: Plastic
B: Aluminium
C: Chrome-plated steel
D: Rubber
E: Iron
Answer: D

Question: In a floating caliper design, what can be a consequence of sticking?
A: Increased fuel efficiency
B: Steering vibration
C: Enhanced braking power
D: Longer brake lifespan
E: Cooler brake temperatures
Answer: B

Question: A fixed caliper:
A: Moves relative to the disc.
B: Uses only one piston.
C: Is less complex and cheaper.
D: Does not move relative to the disc.
E: Always results in uneven braking.
Answer: D

Question: A swinging caliper differs from other floating calipers by:
A: Using a pair of horizontal bolts.
B: Moving perpendicular to the disc.
C: Having its piston angle change relative to the disc.
D: Using no pistons.
E: Being less responsive.
Answer: C
@
Friction is the force resisting the relative motion of solid surfaces, fluid layers, and material elements sliding against each other.[2] There are several types of friction:

Dry friction is a force that opposes the relative lateral motion of two solid surfaces in contact. Dry friction is subdivided into static friction ("stiction") between non-moving surfaces, and kinetic friction between moving surfaces. With the exception of atomic or molecular friction, dry friction generally arises from the interaction of surface features, known as asperities (see Figure 1).
Fluid friction describes the friction between layers of a viscous fluid that are moving relative to each other.[3][4]
Lubricated friction is a case of fluid friction where a lubricant fluid separates two solid surfaces.[5][6][7]
Skin friction is a component of drag, the force resisting the motion of a fluid across the surface of a body.
Internal friction is the force resisting motion between the elements making up a solid material while it undergoes deformation.[4]
$
5
Question: Which type of friction opposes the lateral motion of two solid surfaces in contact?
A: Fluid friction
B: Internal friction
C: Lubricated friction
D: Dry friction
E: Skin friction
Answer: D

Question: What term describes the friction between layers of a viscous fluid moving relative to each other?
A: Dry friction
B: Skin friction
C: Lubricated friction
D: Internal friction
E: Fluid friction
Answer: E

Question: Which type of friction occurs when a fluid separates two solid surfaces?
A: Skin friction
B: Dry friction
C: Internal friction
D: Lubricated friction
E: Fluid friction
Answer: D

Question: What component of drag resists the motion of a fluid across the surface of a body?
A: Lubricated friction
B: Dry friction
C: Internal friction
D: Fluid friction
E: Skin friction
Answer: E

Question: Which frictional type resists motion between elements of a solid material undergoing deformation?
A: Fluid friction
B: Dry friction
C: Internal friction
D: Lubricated friction
E: Skin friction
Answer: C
@
Friction can have dramatic consequences, as illustrated by the use of friction created by rubbing pieces of wood together to start a fire. Kinetic energy is converted to thermal energy whenever motion with friction occurs, for example when a viscous fluid is stirred. Another important consequence of many types of friction can be wear, which may lead to performance degradation or damage to components.

Friction is a non-conservative force – work done against friction is path dependent. In the presence of friction, some kinetic energy is always transformed to thermal energy, so mechanical energy is not conserved. Friction is not itself a fundamental force. Dry friction arises from a combination of inter-surface adhesion, surface roughness, surface deformation, and surface contamination. The complexity of these interactions makes the calculation of friction from first principles difficult and it is often easier to use empirical methods for analysis and the development of theory.
$
5
Question: What type of energy is converted to thermal energy when motion with friction occurs?
A: Potential energy
B: Chemical energy
C: Elastic energy
D: Kinetic energy
E: Electrical energy
Answer: D

Question: Which statement best describes friction as a force?
A: Friction is always conserved.
B: Friction is a fundamental force.
C: Friction is a conservative force.
D: Friction is path dependent.
E: Friction only arises from surface roughness.
Answer: D

Question: What can an increase in friction lead to in components?
A: Improved efficiency
B: Increased kinetic energy
C: Performance enhancement
D: Damage or performance degradation
E: Conservation of energy
Answer: D

Question: Friction is a combination of which factors?
A: Surface smoothness and contamination
B: Surface adhesion, roughness, contamination, and deformation
C: Surface adhesion and molecular bonding
D: Surface reflection and refraction
E: Surface tension and cohesion
Answer: B

Question: Friction is NOT considered which of the following?
A: A non-conservative force
B: A result of surface roughness
C: A fundamental force
D: A source of wear
E: A result of inter-surface adhesion
Answer: C
@
The elementary property of sliding (kinetic) friction were discovered by experiment in the 15th to 18th centuries and were expressed as three empirical laws:

Amontons' First Law: The force of friction is directly proportional to the applied load.
Amontons' Second Law: The force of friction is independent of the apparent area of contact.
Coulomb's Law of Friction: Kinetic friction is independent of the sliding velocity.
$
3
Question: According to Amontons' First Law, what is the relationship between the force of friction and the applied load?
A: Inversely proportional
B: Directly proportional
C: Independent
D: Quadratically proportional
E: Logarithmically proportional
Answer: B

Question: Which law states that the force of friction is independent of the sliding velocity?
A: Amontons' First Law
B: Amontons' Second Law
C: Coulomb's Law of Friction
D: Newton's Third Law
E: Pascal's Law
Answer: C

Question: Amontons' Second Law suggests that frictional force is not dependent on which factor?
A: Applied load
B: Sliding velocity
C: Temperature
D: Apparent area of contact
E: Surface roughness
Answer: D
@
The work done by friction can translate into deformation, wear, and heat that can affect the contact surface properties (even the coefficient of friction between the surfaces). This can be beneficial as in polishing. The work of friction is used to mix and join materials such as in the process of friction welding. Excessive erosion or wear of mating sliding surfaces occurs when work due to frictional forces rise to unacceptable levels. Harder corrosion particles caught between mating surfaces in relative motion (fretting) exacerbates wear of frictional forces. As surfaces are worn by work due to friction, fit and surface finish of an object may degrade until it no longer functions properly.[75] For example, bearing seizure or failure may result from excessive wear due to work of friction.

In the reference frame of the interface between two surfaces, static friction does no work, because there is never displacement between the surfaces. In the same reference frame, kinetic friction is always in the direction opposite the motion, and does negative work.[76] However, friction can do positive work in certain frames of reference. One can see this by placing a heavy box on a rug, then pulling on the rug quickly. In this case, the box slides backwards relative to the rug, but moves forward relative to the frame of reference in which the floor is stationary. Thus, the kinetic friction between the box and rug accelerates the box in the same direction that the box moves, doing positive work.[77]

When sliding takes place between two rough bodies in contact, the algebraic sum of the works done is different from zero, and the algebraic sum of the quantities of heat gained by the two bodies is equal to the quantity of work lost by friction, and the total quantity of heat gained is positive.[78][79] In a natural thermodynamic process, the work done by an agency in the surroundings of a thermodynamic system or working body is greater than the work received by the body, because of friction. Thermodynamic work is measured by changes in a body's state variables, sometimes called work-like variables, other than temperature and entropy. Examples of work-like variables, which are ordinary macroscopic physical variables and which occur in conjugate pairs, are pressure – volume, and electric field – electric polarization. Temperature and entropy are a specifically thermodynamic conjugate pair of state variables. They can be affected microscopically at an atomic level, by mechanisms such as friction, thermal conduction, and radiation. The part of the work done by an agency in the surroundings that does not change the volume of the working body but is dissipated in friction, is called isochoric work. It is received as heat, by the working body and sometimes partly by a body in the surroundings. It is not counted as thermodynamic work received by the working body.
$
5
Question: If two rough bodies are sliding against each other, the sum of the works done is:
A: Always zero
B: Different from zero
C: Equal to the kinetic energy
D: Equal to potential energy
E: Equal to thermal energy
Answer: B

Question: In what frame of reference does kinetic friction do negative work?
A: In the reference frame of the moving body
B: In the reference frame of the stationary body
C: In the reference frame of the interface between two surfaces
D: In the reference frame of the applied force
E: In the reference frame of the gravitational field
Answer: C

Question: How does static friction operate in terms of work done?
A: It does positive work
B: It does no work
C: It does negative work
D: It does work only in the presence of an external force
E: It does work proportional to the applied force
Answer: B

Question: Excessive wear due to work of friction can lead to:
A: Improved efficiency
B: Performance enhancement
C: Bearing seizure or failure
D: Increased lubrication
E: Increased surface roughness
Answer: C

Question: Which of the following is NOT counted as thermodynamic work received by the working body?
A: Volume change work
B: Pressure work
C: Isochoric work
D: Expansion work
E: Compression work
Answer: C
@
In physics and astronomy, a frame of reference (or reference frame) is an abstract coordinate system whose origin, orientation, and scale are specified by a set of reference points―geometric points whose position is identified both mathematically (with numerical coordinate values) and physically (signaled by conventional markers).[1]

For n dimensions, n + 1 reference points are sufficient to fully define a reference frame. Using rectangular Cartesian coordinates, a reference frame may be defined with a reference point at the origin and a reference point at one unit distance along each of the n coordinate axes.[citation needed]

In Einsteinian relativity, reference frames are used to specify the relationship between a moving observer and the phenomenon under observation. In this context, the term often becomes observational frame of reference (or observational reference frame), which implies that the observer is at rest in the frame, although not necessarily located at its origin. A relativistic reference frame includes (or implies) the coordinate time, which does not equate across different reference frames moving relatively to each other. The situation thus differs from Galilean relativity, in which all possible coordinate times are essentially equivalent.[citation needed]
$
5
Question: In a frame of reference, what defines the origin, orientation, and scale?
A: A set of measurements
B: A set of physical objects
C: A set of equations
D: A set of reference points
E: A set of celestial bodies
Answer: D

Question: How many reference points are required to fully define a reference frame in n dimensions using rectangular Cartesian coordinates?
A: n
B: n + 1
C: n - 1
D: 2n
E: 2n + 1
Answer: B

Question: What does a relativistic reference frame include that differs across different moving frames?
A: Coordinate density
B: Coordinate speed
C: Coordinate acceleration
D: Coordinate distance
E: Coordinate time
Answer: E

Question: In Galilean relativity, how are all possible coordinate times perceived?
A: As being subjective
B: As being relative
C: As being essentially equivalent
D: As being different across frames
E: As being nonlinear
Answer: C

Question: What term is used to specify the relationship between a moving observer and the phenomenon under observation in Einsteinian relativity?
A: Inertial frame of reference
B: Observational frame of reference
C: Cartesian frame of reference
D: Galilean frame of reference
E: Moving frame of reference
Answer: B
@
The need to distinguish between the various meanings of "frame of reference" has led to a variety of terms. For example, sometimes the type of coordinate system is attached as a modifier, as in Cartesian frame of reference. Sometimes the state of motion is emphasized, as in rotating frame of reference. Sometimes the way it transforms to frames considered as related is emphasized as in Galilean frame of reference. Sometimes frames are distinguished by the scale of their observations, as in macroscopic and microscopic frames of reference.[2]

In this article, the term observational frame of reference is used when emphasis is upon the state of motion rather than upon the coordinate choice or the character of the observations or observational apparatus. In this sense, an observational frame of reference allows study of the effect of motion upon an entire family of coordinate systems that could be attached to this frame. On the other hand, a coordinate system may be employed for many purposes where the state of motion is not the primary concern. For example, a coordinate system may be adopted to take advantage of the symmetry of a system. In a still broader perspective, the formulation of many problems in physics employs generalized coordinates, normal modes or eigenvectors, which are only indirectly related to space and time. It seems useful to divorce the various aspects of a reference frame for the discussion below. We therefore take observational frames of reference, coordinate systems, and observational equipment as independent concepts, separated as below:

An observational frame (such as an inertial frame or non-inertial frame of reference) is a physical concept related to state of motion.
A coordinate system is a mathematical concept, amounting to a choice of language used to describe observations.[3] Consequently, an observer in an observational frame of reference can choose to employ any coordinate system (Cartesian, polar, curvilinear, generalized, …) to describe observations made from that frame of reference. A change in the choice of this coordinate system does not change an observer's state of motion, and so does not entail a change in the observer's observational frame of reference. This viewpoint can be found elsewhere as well.[4] Which is not to dispute that some coordinate systems may be a better choice for some observations than are others.
Choice of what to measure and with what observational apparatus is a matter separate from the observer's state of motion and choice of coordinate system.
$
5
Question: What term is used when emphasis is on the state of motion rather than the coordinate choice or character of observations?
A: Cartesian frame of reference
B: Rotating frame of reference
C: Galilean frame of reference
D: Macroscopic frame of reference
E: Observational frame of reference
Answer: E

Question: What does a change in the choice of a coordinate system NOT affect?
A: Observer's measurements
B: Observer's calculations
C: Observer's state of motion
D: Observer's viewpoint
E: Observer's equipment
Answer: C

Question: A coordinate system is fundamentally a:
A: Physical concept
B: Linguistic concept
C: Mathematical concept
D: Astronomical concept
E: Philosophical concept
Answer: C

Question: Which of the following is NOT a primary concern when adopting a coordinate system?
A: Symmetry of a system
B: State of motion
C: Scale of observations
D: Nature of observations
E: Apparatus of observation
Answer: B

Question: What is separate from the observer's state of motion and choice of coordinate system?
A: Observational frame
B: Coordinate system choice
C: Measurement apparatus
D: State of acceleration
E: Coordinate transformation
Answer: C
@
A further aspect of a frame of reference is the role of the measurement apparatus (for example, clocks and rods) attached to the frame (see Norton quote above). This question is not addressed in this article, and is of particular interest in quantum mechanics, where the relation between observer and measurement is still under discussion (see measurement problem).

In physics experiments, the frame of reference in which the laboratory measurement devices are at rest is usually referred to as the laboratory frame or simply "lab frame." An example would be the frame in which the detectors for a particle accelerator are at rest. The lab frame in some experiments is an inertial frame, but it is not required to be (for example the laboratory on the surface of the Earth in many physics experiments is not inertial). In particle physics experiments, it is often useful to transform energies and momenta of particles from the lab frame where they are measured, to the center of momentum frame "COM frame" in which calculations are sometimes simplified, since potentially all kinetic energy still present in the COM frame may be used for making new particles.

In this connection it may be noted that the clocks and rods often used to describe observers' measurement equipment in thought, in practice are replaced by a much more complicated and indirect metrology that is connected to the nature of the vacuum, and uses atomic clocks that operate according to the standard model and that must be corrected for gravitational time dilation.[24] (See second, meter and kilogram).

In fact, Einstein felt that clocks and rods were merely expedient measuring devices and they should be replaced by more fundamental entities based upon, for example, atoms and molecules.[25]
$
5
Question: In which frame are the laboratory measurement devices usually at rest in physics experiments?
A: Observational frame
B: Inertial frame
C: Laboratory frame
D: COM frame
E: Microscopic frame
Answer: C

Question: What type of frame might be useful for simplifying calculations in particle physics experiments?
A: Laboratory frame
B: Galilean frame
C: Inertial frame
D: Observational frame
E: COM frame
Answer: E

Question: Which of the following has NOT been replaced by atomic clocks in practice?
A: Clocks
B: Rods
C: Meters
D: Kilograms
E: Seconds
Answer: D

Question: Who believed that clocks and rods should be replaced by more fundamental entities based on atoms and molecules?
A: Newton
B: Galileo
C: Kepler
D: Einstein
E: Planck
Answer: D

Question: The relation between observer and measurement in quantum mechanics is still under discussion due to:
A: The gravitational problem
B: The relativity problem
C: The measurement problem
D: The observational problem
E: The quantum entanglement problem
Answer: C
@
In classical physics and special relativity, an inertial frame of reference (also called inertial space, or Galilean reference frame) is a frame of reference not undergoing any acceleration. It is a frame in which an isolated physical object—an object with zero net force acting on it—is perceived to move with a constant velocity or, equivalently, it is a frame of reference in which Newton's first law of motion holds.[1] All inertial frames are in a state of constant, rectilinear motion with respect to one another; in other words, an accelerometer moving with any of them would detect zero acceleration.

It has been observed that celestial objects which are far away from other objects and which are in uniform motion with respect to the cosmic microwave background radiation maintain such uniform motion.[2]

Measurements in one inertial frame can be converted to measurements in another by a simple transformation - the Galilean transformation in Newtonian physics and the Lorentz transformation in special relativity.[3]

In analytical mechanics, an inertial frame of reference can be defined as a frame of reference that describes time and space homogeneously, isotropically, and in a time-independent manner.[4]
$
5
Question: How is an inertial frame of reference described with respect to motion?
A: Undergoing constant acceleration
B: Not undergoing any acceleration
C: Rotating constantly
D: Fluctuating between velocities
E: Always in a state of rest
Answer: B

Question: Measurements in one inertial frame can be converted to another using which transformation in Newtonian physics?
A: Einstein transformation
B: Kepler transformation
C: Lorentz transformation
D: Galilean transformation
E: Quantum transformation
Answer: D

Question: How do celestial objects in uniform motion with respect to the cosmic microwave background radiation behave?
A: They undergo acceleration
B: They change direction frequently
C: They maintain such uniform motion
D: They exhibit quantum fluctuations
E: They revolve around a central point
Answer: C

Question: What describes an inertial frame of reference in terms of an isolated physical object?
A: An object under constant force
B: An object with zero net force moving with variable velocity
C: An object with zero net force moving with a constant velocity
D: An object in constant rotation
E: An object under gravitational influence
Answer: C

Question: In analytical mechanics, an inertial frame of reference describes space and time in a manner that is:
A: Non-uniform, isotropic, and time-dependent
B: Homogeneous, anisotropic, and time-dependent
C: Homogeneous, isotropic, and time-independent
D: Non-uniform, anisotropic, and time-independent
E: Random, chaotic, and time-variant
Answer: C
@
General relativity is a metric theory of gravitation. At its core are Einstein's equations, which describe the relation between the geometry of a four-dimensional pseudo-Riemannian manifold representing spacetime, and the energy–momentum contained in that spacetime.[46] Phenomena that in classical mechanics are ascribed to the action of the force of gravity (such as free-fall, orbital motion, and spacecraft trajectories), correspond to inertial motion within a curved geometry of spacetime in general relativity; there is no gravitational force deflecting objects from their natural, straight paths. Instead, gravity corresponds to changes in the properties of space and time, which in turn changes the straightest-possible paths that objects will naturally follow.[47] The curvature is, in turn, caused by the energy–momentum of matter. Paraphrasing the relativist John Archibald Wheeler, spacetime tells matter how to move; matter tells spacetime how to curve.[48]

While general relativity replaces the scalar gravitational potential of classical physics by a symmetric rank-two tensor, the latter reduces to the former in certain limiting cases. For weak gravitational fields and slow speed relative to the speed of light, the theory's predictions converge on those of Newton's law of universal gravitation.[49]

As it is constructed using tensors, general relativity exhibits general covariance: its laws—and further laws formulated within the general relativistic framework—take on the same form in all coordinate systems.[50] Furthermore, the theory does not contain any invariant geometric background structures, i.e. it is background independent. It thus satisfies a more stringent general principle of relativity, namely that the laws of physics are the same for all observers.[51] Locally, as expressed in the equivalence principle, spacetime is Minkowskian, and the laws of physics exhibit local Lorentz invariance.[52]
$
5
Question: Which theory describes the relationship between spacetime's geometry and the energy-momentum it contains?
A: Quantum mechanics
B: Newton's theory of gravitation
C: General relativity
D: Electromagnetic theory
E: Thermodynamics
Answer: C

Question: In general relativity, what is the concept that replaces the force of gravity from classical mechanics?
A: Magnetic force
B: Inertial motion within a curved spacetime
C: Electromagnetic radiation
D: Quantum field fluctuations
E: Wave-particle duality
Answer: B

Question: What tells spacetime how to curve according to John Archibald Wheeler's paraphrasing?
A: Light
B: Electric charge
C: Magnetic fields
D: Matter
E: Quantum fluctuations
Answer: D

Question: For weak gravitational fields, general relativity predictions align with which law?
A: Ohm's Law
B: Maxwell's Equations
C: Newton's law of universal gravitation
D: Lenz's Law
E: Boyle's Law
Answer: C

Question: General relativity satisfies a principle stating that the laws of physics are the same for all observers. What is this principle called?
A: Hubble's principle
B: Uncertainty principle
C: Law of inertia
D: General principle of relativity
E: Principle of least action
Answer: D
@
The core concept of general-relativistic model-building is that of a solution of Einstein's equations. Given both Einstein's equations and suitable equations for the properties of matter, such a solution consists of a specific semi-Riemannian manifold (usually defined by giving the metric in specific coordinates), and specific matter fields defined on that manifold. Matter and geometry must satisfy Einstein's equations, so in particular, the matter's energy–momentum tensor must be divergence-free. The matter must, of course, also satisfy whatever additional equations were imposed on its properties. In short, such a solution is a model universe that satisfies the laws of general relativity, and possibly additional laws governing whatever matter might be present.[53]

Einstein's equations are nonlinear partial differential equations and, as such, difficult to solve exactly.[54] Nevertheless, a number of exact solutions are known, although only a few have direct physical applications.[55] The best-known exact solutions, and also those most interesting from a physics point of view, are the Schwarzschild solution, the Reissner–Nordström solution and the Kerr metric, each corresponding to a certain type of black hole in an otherwise empty universe,[56] and the Friedmann–Lemaître–Robertson–Walker and de Sitter universes, each describing an expanding cosmos.[57] Exact solutions of great theoretical interest include the Gödel universe (which opens up the intriguing possibility of time travel in curved spacetimes), the Taub–NUT solution (a model universe that is homogeneous, but anisotropic), and anti-de Sitter space (which has recently come to prominence in the context of what is called the Maldacena conjecture).[58]
$
5
Question: What is at the heart of building general-relativistic models?
A: Quantum equations
B: Thermodynamic cycles
C: Solution of Einstein's equations
D: Maxwell's equations
E: Newton's laws
Answer: C

Question: Which solution corresponds to a black hole in an otherwise empty universe?
A: Gödel universe
B: Maldacena conjecture
C: Schwarzschild solution
D: Friedmann–Lemaître–Robertson–Walker universe
E: Taub–NUT solution
Answer: C

Question: Einstein's equations are what type of differential equations?
A: Linear
B: Intermittent
C: Iterative
D: Nonlinear
E: Differential-integral
Answer: D

Question: What does the Friedmann–Lemaître–Robertson–Walker universe describe?
A: An oscillating universe
B: A static universe
C: A contracting universe
D: An expanding universe
E: A toroidal universe
Answer: D

Question: Which universe offers the possibility of time travel in curved spacetimes?
A: de Sitter universe
B: Schwarzschild universe
C: Gödel universe
D: Taub–NUT solution
E: Friedmann–Lemaître–Robertson–Walker universe
Answer: C
@
According to general relativity, a binary system will emit gravitational waves, thereby losing energy. Due to this loss, the distance between the two orbiting bodies decreases, and so does their orbital period. Within the Solar System or for ordinary double stars, the effect is too small to be observable. This is not the case for a close binary pulsar, a system of two orbiting neutron stars, one of which is a pulsar: from the pulsar, observers on Earth receive a regular series of radio pulses that can serve as a highly accurate clock, which allows precise measurements of the orbital period. Because neutron stars are immensely compact, significant amounts of energy are emitted in the form of gravitational radiation.[97]

The first observation of a decrease in orbital period due to the emission of gravitational waves was made by Hulse and Taylor, using the binary pulsar PSR1913+16 they had discovered in 1974. This was the first detection of gravitational waves, albeit indirect, for which they were awarded the 1993 Nobel Prize in physics.[98] Since then, several other binary pulsars have been found, in particular the double pulsar PSR J0737−3039, where both stars are pulsars[99] and which was last reported to also be in agreement with general relativity in 2021 after 16 years of observations.[96]
$
5
Question: In general relativity, what do close binary pulsars emit?
A: Electromagnetic radiation
B: Gravitational waves
C: Quantum fluctuations
D: Thermal radiation
E: Neutrinos
Answer: B

Question: The detection of what phenomenon led to Hulse and Taylor winning the Nobel Prize in physics in 1993?
A: Time dilation
B: Dark matter
C: Gravitational waves
D: Quantum entanglement
E: Black hole radiation
Answer: C

Question: What happens to the distance between two bodies in a binary system due to the emission of gravitational waves?
A: It oscillates periodically
B: It increases
C: It remains constant
D: It decreases
E: It becomes unpredictable
Answer: D

Question: The binary pulsar PSR1913+16, discovered in 1974, was used to make the first indirect detection of what?
A: Gamma rays
B: Dark energy
C: Neutrinos
D: Gravitational waves
E: Cosmic background radiation
Answer: D

Question: Neutron stars in a close binary pulsar system are significant because of their:
A: Reflective surfaces
B: High rotation speeds
C: Immense compactness
D: Positive charge
E: Emission of visible light
Answer: C
@
As intriguing as geometric Newtonian gravity may be, its basis, classical mechanics, is merely a limiting case of (special) relativistic mechanics.[32] In the language of symmetry: where gravity can be neglected, physics is Lorentz invariant as in special relativity rather than Galilei invariant as in classical mechanics. (The defining symmetry of special relativity is the Poincaré group, which includes translations, rotations, boosts and reflections.) The differences between the two become significant when dealing with speeds approaching the speed of light, and with high-energy phenomena.[33]

With Lorentz symmetry, additional structures come into play. They are defined by the set of light cones (see image). The light-cones define a causal structure: for each event A, there is a set of events that can, in principle, either influence or be influenced by A via signals or interactions that do not need to travel faster than light (such as event B in the image), and a set of events for which such an influence is impossible (such as event C in the image). These sets are observer-independent.[34] In conjunction with the world-lines of freely falling particles, the light-cones can be used to reconstruct the spacetime's semi-Riemannian metric, at least up to a positive scalar factor. In mathematical terms, this defines a conformal structure[35] or conformal geometry.

Special relativity is defined in the absence of gravity. For practical applications, it is a suitable model whenever gravity can be neglected. Bringing gravity into play, and assuming the universality of free fall motion, an analogous reasoning as in the previous section applies: there are no global inertial frames. Instead there are approximate inertial frames moving alongside freely falling particles. Translated into the language of spacetime: the straight time-like lines that define a gravity-free inertial frame are deformed to lines that are curved relative to each other, suggesting that the inclusion of gravity necessitates a change in spacetime geometry.[36]
$
4
Question: Which theory is considered a limiting case of (special) relativistic mechanics?
A: Electromagnetism
B: Quantum mechanics
C: Classical mechanics
D: Thermodynamics
E: Fluid dynamics
Answer: C

Question: What defines the causal structure in Lorentz symmetry?
A: Sound waves
B: Gravitational waves
C: Light cones
D: Quantum fields
E: Electromagnetic fields
Answer: C

Question: In the absence of gravity, which symmetry is relevant?
A: Galilei invariance
B: Lorentz invariance
C: Quantum symmetry
D: Thermal symmetry
E: Rotational symmetry
Answer: B

Question: Special relativity becomes inadequate and needs a change in spacetime geometry when:
A: Gravity can be neglected
B: Objects move at very slow speeds
C: Electromagnetic fields are absent
D: Gravity is factored in and universality of free fall motion is assumed
E: Observations are
@
Euclidean geometry is a mathematical system attributed to ancient Greek mathematician Euclid, which he described in his textbook on geometry, Elements. Euclid's approach consists in assuming a small set of intuitively appealing axioms (postulates) and deducing many other propositions (theorems) from these. Although many of Euclid's results had been stated earlier,[1] Euclid was the first to organize these propositions into a logical system in which each result is proved from axioms and previously proved theorems.[2]

The Elements begins with plane geometry, still taught in secondary school (high school) as the first axiomatic system and the first examples of mathematical proofs. It goes on to the solid geometry of three dimensions. Much of the Elements states results of what are now called algebra and number theory, explained in geometrical language.[1]

Euclidean geometry is an example of synthetic geometry, in that it proceeds logically from axioms describing basic properties of geometric objects such as points and lines, to propositions about those objects. This is in contrast to analytic geometry, introduced almost 2,000 years later by René Descartes, which uses coordinates to express geometric properties by means of algebraic formulas.
$
5
Question: Which ancient Greek mathematician is attributed with the development of Euclidean geometry?
A: Pythagoras
B: Socrates
C: Archimedes
D: Euclid
E: Aristotle
Answer: D

Question: How is Euclidean geometry different from analytic geometry?
A: It uses coordinates to express properties.
B: It is introduced by René Descartes.
C: It proceeds logically from axioms about geometric objects.
D: It was developed in the modern era.
E: It deals only with three dimensions.
Answer: C

Question: What did Euclid use to deduce many propositions in Euclidean geometry?
A: Calculations
B: Intuitive algorithms
C: Axioms (postulates)
D: Modern mathematical tools
E: Coordinate systems
Answer: C

Question: The "Elements" begins with which type of geometry?
A: Solid
B: Analytic
C: Coordinate
D: Non-Euclidean
E: Plane
Answer: E

Question: What did René Descartes introduce almost 2,000 years after Euclidean geometry?
A: Plane geometry
B: Solid geometry
C: Synthetic geometry
D: Analytic geometry
E: Triangular geometry
Answer: D
@
Euclidean Geometry is constructive. Postulates 1, 2, 3, and 5 assert the existence and uniqueness of certain geometric figures, and these assertions are of a constructive nature: that is, we are not only told that certain things exist, but are also given methods for creating them with no more than a compass and an unmarked straightedge.[8] In this sense, Euclidean geometry is more concrete than many modern axiomatic systems such as set theory, which often assert the existence of objects without saying how to construct them, or even assert the existence of objects that cannot be constructed within the theory.[9] Strictly speaking, the lines on paper are models of the objects defined within the formal system, rather than instances of those objects. For example, a Euclidean straight line has no width, but any real drawn line will. Though nearly all modern mathematicians consider nonconstructive methods just as sound as constructive ones, Euclid's constructive proofs often supplanted fallacious nonconstructive ones—e.g., some of the Pythagoreans' proofs that involved irrational numbers, which usually required a statement such as "Find the greatest common measure of ..."[10]

Euclid often used proof by contradiction. Euclidean geometry also allows the method of superposition, in which a figure is transferred to another point in space. For example, proposition I.4, side–angle–side congruence of triangles, is proved by moving one of the two triangles so that one of its sides coincides with the other triangle's equal side, and then proving that the other sides coincide as well. Some modern treatments add a sixth postulate, the rigidity of the triangle, which can be used as an alternative to superposition.[11]
$
5
Question: What is asserted by the postulates in Euclidean geometry?
A: The non-existence of geometric figures
B: The existence and uniqueness of certain geometric figures
C: The redundancy of geometric figures
D: The relativity of geometric figures
E: The multiplicity of geometric figures
Answer: B

Question: Which of the following tools are not needed to construct figures in Euclidean geometry as described by its postulates?
A: Compass
B: Calculator
C: Unmarked straightedge
D: Ruler with measurements
E: Protractor
Answer: B

Question: What method did Euclid often use in his proofs?
A: Proof by association
B: Proof by conjecture
C: Proof by superposition
D: Proof by repetition
E: Proof by assumption
Answer: C

Question: What does the rigidity of a triangle pertain to in some modern treatments of Euclidean geometry?
A: A sixth postulate
B: A method to measure its angles
C: Its adaptability in different spaces
D: Its ability to be split into smaller triangles
E: Its tendency to change shape under pressure
Answer: A

Question: Which type of proofs often supplanted fallacious nonconstructive ones in Euclidean geometry?
A: Inconclusive proofs
B: Non-Euclidean proofs
C: Constructive proofs
D: Abstract proofs
E: Theoretical proofs
Answer: C
@
Euclidean geometry has two fundamental types of measurements: angle and distance. The angle scale is absolute, and Euclid uses the right angle as his basic unit, so that, for example, a 45-degree angle would be referred to as half of a right angle. The distance scale is relative; one arbitrarily picks a line segment with a certain nonzero length as the unit, and other distances are expressed in relation to it. Addition of distances is represented by a construction in which one line segment is copied onto the end of another line segment to extend its length, and similarly for subtraction.

Measurements of area and volume are derived from distances. For example, a rectangle with a width of 3 and a length of 4 has an area that represents the product, 12. Because this geometrical interpretation of multiplication was limited to three dimensions, there was no direct way of interpreting the product of four or more numbers, and Euclid avoided such products, although they are implied, for example in the proof of book IX, proposition 20.

Euclid refers to a pair of lines, or a pair of planar or solid figures, as "equal" (ἴσος) if their lengths, areas, or volumes are equal respectively, and similarly for angles. The stronger term "congruent" refers to the idea that an entire figure is the same size and shape as another figure. Alternatively, two figures are congruent if one can be moved on top of the other so that it matches up with it exactly. (Flipping it over is allowed.) Thus, for example, a 2x6 rectangle and a 3x4 rectangle are equal but not congruent, and the letter R is congruent to its mirror image. Figures that would be congruent except for their differing sizes are referred to as similar. Corresponding angles in a pair of similar shapes are congruent and corresponding sides are in proportion to each other.
$
5
Question: In Euclidean geometry, which of the following is used as the basic unit for measuring angles?
A: 90-degree angle
B: 180-degree angle
C: 360-degree angle
D: 45-degree angle
E: Right angle
Answer: E

Question: How is distance measurement considered in Euclidean geometry?
A: Absolute
B: Relative
C: In terms of square units
D: Based on time
E: Independent of angles
Answer: B

Question: Two figures that match up exactly when one is moved on top of the other, even after flipping, are considered:
A: Equal
B: Proportional
C: Similar
D: Congruent
E: Symmetrical
Answer: D

Question: Which term refers to figures that are the same size and shape as each other in Euclidean geometry?
A: Equal
B: Congruent
C: Similar
D: Proportional
E: Parallel
Answer: B

Question: If two shapes have corresponding angles that are congruent and sides that are proportional, they are:
A: Equal
B: Congruent
C: Symmetrical
D: Parallel
E: Similar
Answer: E
@
In mathematics, the Pythagorean theorem or Pythagoras' theorem is a fundamental relation in Euclidean geometry between the three sides of a right triangle. It states that the area of the square whose side is the hypotenuse (the side opposite the right angle) is equal to the sum of the areas of the squares on the other two sides.

The theorem can be written as an equation relating the lengths of the sides a, b and the hypotenuse c, sometimes called the Pythagorean equation:[1]

�
2
+
�
2
=
�
2
.
{\displaystyle a^{2}+b^{2}=c^{2}.}
The theorem is named for the Greek philosopher Pythagoras, born around 570 BC. The theorem has been proved numerous times by many different methods – possibly the most for any mathematical theorem. The proofs are diverse, including both geometric proofs and algebraic proofs, with some dating back thousands of years.

When Euclidean space is represented by a Cartesian coordinate system in analytic geometry, Euclidean distance satisfies the Pythagorean relation: the squared distance between two points equals the sum of squares of the difference in each coordinate between the points.

The theorem can be generalized in various ways: to higher-dimensional spaces, to spaces that are not Euclidean, to objects that are not right triangles, and to objects that are not triangles at all but n-dimensional solids. The Pythagorean theorem has attracted interest outside mathematics as a symbol of mathematical abstruseness, mystique, or intellectual power; popular references in literature, plays, musicals, songs, stamps, and cartoons abound.[citation needed]
$
5
Question: The Pythagorean theorem relates to which type of triangle?
A: Equilateral
B: Isosceles
C: Obtuse
D: Right
E: Scalene
Answer: D

Question: According to the Pythagorean theorem, which of the following represents the hypotenuse's squared length in a right triangle?
A: a^2 - b^2
B: a^2 + b^2
C: a^2 * b^2
D: a^2 / b^2
E: (a+b)^2
Answer: B

Question: Who is the Pythagorean theorem named after?
A: Archimedes
B: Socrates
C: Pythagoras
D: Euclid
E: Aristotle
Answer: C

Question: How is the Euclidean distance represented in a Cartesian coordinate system in analytic geometry?
A: By the Pythagorean relation
B: By the distance formula
C: By the angle of intersection
D: By the slope of the line
E: By the area of the enclosed region
Answer: A

Question: The Pythagorean theorem can be generalized to:
A: Only two-dimensional spaces
B: Only spaces that are not Euclidean
C: Objects that are not triangles at all
D: Objects that are only right triangles
E: Only to four-dimensional spaces
Answer: C
@
In geometry, a Cartesian coordinate system (UK: /kɑːrˈtiːzjən/, US: /kɑːrˈtiʒən/) in a plane is a coordinate system that specifies each point uniquely by a pair of real numbers called coordinates, which are the signed distances to the point from two fixed perpendicular oriented lines, called coordinate lines, coordinate axes or just axes (plural of axis) of the system. The point where they meet is called the origin and has (0, 0) as coordinates.

Similarly, the position of any point in three-dimensional space can be specified by three Cartesian coordinates, which are the signed distances from the point to three mutually perpendicular planes. More generally, n Cartesian coordinates specify the point in an n-dimensional Euclidean space for any dimension n. These coordinates are the signed distances from the point to n mutually perpendicular fixed hyperplanes.
$
5
Question 1: In a Cartesian coordinate system in a plane, how many real numbers specify each point?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: B

Question 2: In the Cartesian coordinate system, what is the name of the point where the coordinate axes meet?
A: Plane
B: Hyperplane
C: Vertex
D: Center
E: Origin
Answer: E

Question 3: If a point is defined in a three-dimensional space using a Cartesian coordinate system, how many coordinates are used to specify its position?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: C

Question 4: In an n-dimensional Euclidean space, how many Cartesian coordinates are needed to specify a point?
A: One
B: n-1
C: n
D: n+1
E: 2n
Answer: C

Question 5: In the Cartesian coordinate system, what is the coordinate of the origin?
A: (1, 1)
B: (0, 1)
C: (-1, -1)
D: (1, 0)
E: (0, 0)
Answer: E
@
Cartesian coordinates are named for René Descartes, whose invention of them in the 17th century revolutionized mathematics by providing the first systematic link between geometry and algebra. Using the Cartesian coordinate system, geometric shapes (such as curves) can be described by equations involving the coordinates of points of the shape. For example, a circle of radius 2, centered at the origin of the plane, may be described as the set of all points whose coordinates x and y satisfy the equation x2 + y2 = 4.

Cartesian coordinates are the foundation of analytic geometry, and provide enlightening geometric interpretations for many other branches of mathematics, such as linear algebra, complex analysis, differential geometry, multivariate calculus, group theory and more. A familiar example is the concept of the graph of a function. Cartesian coordinates are also essential tools for most applied disciplines that deal with geometry, including astronomy, physics, engineering and many more. They are the most common coordinate system used in computer graphics, computer-aided geometric design and other geometry-related data processing.
$
5
Question 6: Who is credited with the invention of the Cartesian coordinate system?
A: Isaac Newton
B: Albert Einstein
C: René Descartes
D: Pythagoras
E: Euclid
Answer: C

Question 7: Using Cartesian coordinates, a circle of radius 2 centered at the origin can be described by which equation?
A: x + y = 2
B: x2 - y2 = 4
C: x + y2 = 4
D: x2 + y = 4
E: x2 + y2 = 4
Answer: E

Question 8: What did the invention of Cartesian coordinates provide a link between?
A: Algebra and Calculus
B: Geometry and Algebra
C: Trigonometry and Algebra
D: Geometry and Trigonometry
E: Calculus and Trigonometry
Answer: B

Question 9: The graph of a function is an example of a concept from:
A: Multivariate calculus
B: Differential geometry
C: Group theory
D: Analytic geometry
E: Complex analysis
Answer: D

Question 10: In which of the following disciplines are Cartesian coordinates NOT an essential tool?
A: Astronomy
B: Engineering
C: Poetry
D: Physics
E: Computer-aided geometric design
Answer: C
@
The Cartesian coordinates of a point are usually written in parentheses and separated by commas, as in (10, 5) or (3, 5, 7). The origin is often labelled with the capital letter O. In analytic geometry, unknown or generic coordinates are often denoted by the letters (x, y) in the plane, and (x, y, z) in three-dimensional space. This custom comes from a convention of algebra, which uses letters near the end of the alphabet for unknown values (such as the coordinates of points in many geometric problems), and letters near the beginning for given quantities.

These conventional names are often used in other domains, such as physics and engineering, although other letters may be used. For example, in a graph showing how a pressure varies with time, the graph coordinates may be denoted p and t. Each axis is usually named after the coordinate which is measured along it; so one says the x-axis, the y-axis, the t-axis, etc.

Another common convention for coordinate naming is to use subscripts, as (x1, x2, ..., xn) for the n coordinates in an n-dimensional space, especially when n is greater than 3 or unspecified. Some authors prefer the numbering (x0, x1, ..., xn−1). These notations are especially advantageous in computer programming: by storing the coordinates of a point as an array, instead of a record, the subscript can serve to index the coordinates.
$
5
Question 11: In analytic geometry, what letters are typically used to denote unknown or generic coordinates in the plane?
A: (a, b)
B: (x, y)
C: (m, n)
D: (u, v)
E: (l, m)
Answer: B

Question 12: If you're showing how pressure varies with time on a graph, which coordinate might represent pressure?
A: x
B: y
C: z
D: p
E: t
Answer: D

Question 13: Which letter often labels the origin in Cartesian coordinates?
A: P
B: C
C: M
D: O
E: X
Answer: D

Question 14: How are the n coordinates in an n-dimensional space commonly named?
A: (x, y, z, ...)
B: (a, b, c, ...)
C: (n1, n2, n3, ...)
D: (x1, x2, x3, ...)
E: (d1, d2, d3, ...)
Answer: D

Question 15: In algebra, which section of the alphabet is conventionally used for unknown values?
A: Beginning
B: Middle
C: End
D: Anywhere
E: Only vowels
Answer: C
@
Cartesian coordinates are an abstraction that have a multitude of possible applications in the real world. However, three constructive steps are involved in superimposing coordinates on a problem application.

Units of distance must be decided defining the spatial size represented by the numbers used as coordinates.
An origin must be assigned to a specific spatial location or landmark, and
the orientation of the axes must be defined using available directional cues for all but one axis.
Consider as an example superimposing 3D Cartesian coordinates over all points on the Earth (that is, geospatial 3D). Kilometers are a good choice of units, since the original definition of the kilometer was geospatial, with 10,000 km equaling the surface distance from the equator to the North Pole. Based on symmetry, the gravitational center of the Earth suggests a natural placement of the origin (which can be sensed via satellite orbits). The axis of Earth's rotation provides a natural orientation for the X, Y, and Z axes, strongly associated with "up vs. down", so positive Z can adopt the direction from the geocenter to the North Pole. A location on the equator is needed to define the X-axis, and the prime meridian stands out as a reference orientation, so the X-axis takes the orientation from the geocenter out to 0 degrees longitude, 0 degrees latitude. With three dimensions, and two perpendicular axes orientations pinned down for X and Z, the Y-axis is determined by the first two choices. In order to obey the right-hand rule, the Y-axis must point out from the geocenter to 90 degrees longitude, 0 degrees latitude. From a longitude of −73.985656 degrees, a latitude 40.748433 degrees, and Earth radius of 
40,000
/
2π
 km, and transforming from spherical to Cartesian coordinates, one can estimate the geocentric coordinates of the Empire State Building, (x, y, z) = (1,330.53 km, 4,635.75 km, 4,155.46 km). GPS navigation relies on such geocentric coordinates.
$
5
Question 16: What is the first step involved in superimposing coordinates on a real-world problem?
A: Deciding the origin
B: Orienting the axes
C: Choosing units of distance
D: Defining the spatial size
E: Picking a landmark
Answer: C

Question 17: In a geospatial 3D Cartesian system, where does the positive Z axis direction point towards?
A: Equator
B: South Pole
C: Gravitational center of the Earth
D: North Pole
E: Prime Meridian
Answer: D

Question 18: What defines the orientation of the X-axis in a geospatial 3D Cartesian system?
A: 90 degrees longitude, 0 degrees latitude
B: Geocenter to the equator
C: 0 degrees longitude, 0 degrees latitude
D: Geocenter to the South Pole
E: 45 degrees longitude, 45 degrees latitude
Answer: C

Question 19: The Y-axis orientation in a geospatial 3D Cartesian system points out from the geocenter to:
A: 0 degrees longitude, 90 degrees latitude
B: 90 degrees longitude, 0 degrees latitude
C: 45 degrees longitude, 0 degrees latitude
D: 0 degrees longitude, 45 degrees latitude
E: 180 degrees longitude, 0 degrees latitude
Answer: B

Question 20: What does GPS navigation rely on?
A: Geotemporal coordinates
B: Polar coordinates
C: Spherical coordinates
D: Geocentric coordinates
E: Astrological coordinates
Answer: D
@
Stellar evolution is the process by which a star changes over the course of time. Depending on the mass of the star, its lifetime can range from a few million years for the most massive to trillions of years for the least massive, which is considerably longer than the current age of the universe. The table shows the lifetimes of stars as a function of their masses.[1] All stars are formed from collapsing clouds of gas and dust, often called nebulae or molecular clouds. Over the course of millions of years, these protostars settle down into a state of equilibrium, becoming what is known as a main-sequence star.

Nuclear fusion powers a star for most of its existence. Initially the energy is generated by the fusion of hydrogen atoms at the core of the main-sequence star. Later, as the preponderance of atoms at the core becomes helium, stars like the Sun begin to fuse hydrogen along a spherical shell surrounding the core. This process causes the star to gradually grow in size, passing through the subgiant stage until it reaches the red-giant phase. Stars with at least half the mass of the Sun can also begin to generate energy through the fusion of helium at their core, whereas more-massive stars can fuse heavier elements along a series of concentric shells. Once a star like the Sun has exhausted its nuclear fuel, its core collapses into a dense white dwarf and the outer layers are expelled as a planetary nebula. Stars with around ten or more times the mass of the Sun can explode in a supernova as their inert iron cores collapse into an extremely dense neutron star or black hole. Although the universe is not old enough for any of the smallest red dwarfs to have reached the end of their existence, stellar models suggest they will slowly become brighter and hotter before running out of hydrogen fuel and becoming low-mass white dwarfs.[2]
$
5
Question 1: Which process provides energy to a star for the majority of its existence?
A: Nuclear fission
B: Solar flares
C: Nuclear fusion
D: Gamma radiation
E: Gravitational contraction
Answer: C

Question 2: Once a star like the Sun exhausts its nuclear fuel, what does its core collapse into?
A: Red giant
B: Neutron star
C: Supernova
D: Black hole
E: White dwarf
Answer: E

Question 3: What is the fate of stars with at least ten times the mass of the Sun?
A: They become planetary nebulas.
B: They turn into white dwarfs.
C: They transform into brown dwarfs.
D: They explode in a supernova.
E: They remain as main-sequence stars.
Answer: D

Question 4: What initial stage do all stars undergo?
A: Supernova
B: Red giant
C: Formation from nebulae or molecular clouds
D: Fusion of hydrogen
E: Formation of white dwarf
Answer: C

Question 5: What happens when the core atoms of a main-sequence star become helium?
A: The star shrinks.
B: Hydrogen fusion stops.
C: The star fuses helium in concentric shells.
D: The star becomes a white dwarf.
E: The star explodes as a supernova.
Answer: C
@
A protostar is a very young star that is still gathering mass from its parent molecular cloud. The protostellar phase is the earliest one in the process of stellar evolution.[1] For a low-mass star (i.e. that of the Sun or lower), it lasts about 500,000 years.[2] The phase begins when a molecular cloud fragment first collapses under the force of self-gravity and an opaque, pressure supported core forms inside the collapsing fragment. It ends when the infalling gas is depleted, leaving a pre-main-sequence star, which contracts to later become a main-sequence star at the onset of hydrogen fusion producing helium.
$
5
Question 6: At what stage does a star transition from a protostar?
A: After a supernova
B: When hydrogen fusion begins
C: When its mass exceeds 0.08 M☉
D: When the infalling gas is depleted
E: At the onset of the carbon-nitrogen-oxygen fusion reaction
Answer: D

Question 7: Approximately how long does the protostellar phase last for a star like the Sun?
A: 5 million years
B: 500,000 years
C: 50,000 years
D: 50 million years
E: 5 billion years
Answer: B

Question 8: The protostellar phase starts when a molecular cloud fragment...
A: Expands due to external pressures.
B: Initiates hydrogen fusion.
C: Collapses under self-gravity.
D: Reaches equilibrium with its surroundings.
E: Begins emitting visible light.
Answer: C

Question 9: What stage follows the protostellar phase?
A: Supernova
B: Brown dwarf
C: Neutron star
D: Pre-main-sequence star
E: White dwarf
Answer: D

Question 10: A protostar is still accumulating mass from its...
A: Gravitational pull.
B: Nuclear fusion reactions.
C: Parent molecular cloud.
D: Previous supernova.
E: Helium burning.
Answer: C
@
Protostars with masses less than roughly 0.08 M☉ (1.6×1029 kg) never reach temperatures high enough for nuclear fusion of hydrogen to begin. These are known as brown dwarfs. The International Astronomical Union defines brown dwarfs as stars massive enough to fuse deuterium at some point in their lives (13 Jupiter masses (MJ), 2.5 × 1028 kg, or 0.0125 M☉). Objects smaller than 13 MJ are classified as sub-brown dwarfs (but if they orbit around another stellar object they are classified as planets).[7] Both types, deuterium-burning and not, shine dimly and fade away slowly, cooling gradually over hundreds of millions of years.

For a more-massive protostar, the core temperature will eventually reach 10 million kelvin, initiating the proton–proton chain reaction and allowing hydrogen to fuse, first to deuterium and then to helium. In stars of slightly over 1 M☉ (2.0×1030 kg), the carbon–nitrogen–oxygen fusion reaction (CNO cycle) contributes a large portion of the energy generation. The onset of nuclear fusion leads relatively quickly to a hydrostatic equilibrium in which energy released by the core maintains a high gas pressure, balancing the weight of the star's matter and preventing further gravitational collapse. The star thus evolves rapidly to a stable state, beginning the main-sequence phase of its evolution.
$
5
Question 11: What is the fate of protostars that don't reach temperatures sufficient for hydrogen fusion?
A: Neutron stars
B: White dwarfs
C: Supernovae
D: Main-sequence stars
E: Brown dwarfs
Answer: E

Question 12: When a protostar reaches a core temperature of 10 million kelvin, it initiates the...
A: CNO cycle.
B: Helium fusion.
C: Proton–proton chain reaction.
D: Deuterium fusion.
E: Carbon fusion.
Answer: C

Question 13: In stars slightly over 1 M☉, which fusion reaction contributes significantly to energy generation?
A: Proton–proton chain reaction
B: Helium fusion
C: Deuterium fusion
D: Carbon–nitrogen–oxygen fusion reaction
E: Hydrogen fusion
Answer: D

Question 14: The onset of nuclear fusion in a protostar leads to...
A: Supernova explosion.
B: Formation of brown dwarfs.
C: Contraction into white dwarfs.
D: Hydrostatic equilibrium.
E: Expansion into red giants.
Answer: D

Question 15: Objects smaller than 13 Jupiter masses are classified as...
A: Neutron stars.
B: Sub-brown dwarfs.
C: Planets.
D: A and C.
E: B and C.
Answer: E
@
What happens after a low-mass star ceases to produce energy through fusion has not been directly observed; the universe is around 13.8 billion years old, which is less time (by several orders of magnitude, in some cases) than it takes for fusion to cease in such stars.

Recent astrophysical models suggest that red dwarfs of 0.1 M☉ may stay on the main sequence for some six to twelve trillion years, gradually increasing in both temperature and luminosity, and take several hundred billion years more to collapse, slowly, into a white dwarf.[9][10] Such stars will not become red giants as the whole star is a convection zone and it will not develop a degenerate helium core with a shell burning hydrogen. Instead, hydrogen fusion will proceed until almost the whole star is helium.

Slightly more massive stars do expand into red giants, but their helium cores are not massive enough to reach the temperatures required for helium fusion so they never reach the tip of the red-giant branch. When hydrogen shell burning finishes, these stars move directly off the red-giant branch like a post-asymptotic-giant-branch (AGB) star, but at lower luminosity, to become a white dwarf.[2] A star with an initial mass about 0.6 M☉ will be able to reach temperatures high enough to fuse helium, and these "mid-sized" stars go on to further stages of evolution beyond the red-giant branch.[11]
$
5
Question 16: How long may a red dwarf of 0.1 M☉ stay on the main sequence according to recent astrophysical models?
A: 6 to 12 trillion years
B: 6 to 12 billion years
C: 600 to 1200 years
D: 60 to 120 million years
E: 6 to 12 million years
Answer: A

Question 17: More massive stars expand into red giants but never reach the tip of the red-giant branch because...
A: They run out of hydrogen fuel.
B: Their helium cores aren't massive enough for helium fusion.
C: They are surrounded by brown dwarfs.
D: Their nuclear fusion is too rapid.
E: They are influenced by external gravitational forces.
Answer: B

Question 18: What happens to stars that don't become red giants and have their whole star as a convection zone?
A: Hydrogen fusion will proceed until almost the whole star is helium.
B: They explode as supernovae.
C: They collapse into black holes.
D: They convert all their helium into hydrogen.
E: They become neutron stars.
Answer: A

Question 19: Red dwarfs with 0.1 M☉ do not become red giants because...
A: They are too massive.
B: They run out of helium.
C: They are surrounded by a shell of hydrogen.
D: They do not have a degenerate helium core.
E: Their nuclear fusion is unstable.
Answer: D

Question 20: A star with an initial mass about 0.6 M☉ will eventually...
A: Expand into a supergiant.
B: Become a brown dwarf.
C: Collapse into a neutron star.
D: Fuse helium and evolve beyond the red-giant branch.
E: Stay on the main sequence indefinitely.
Answer: D
@
As non-fusing helium ash accumulates in the core of a main-sequence star, the reduction in the abundance of hydrogen per unit mass results in a gradual lowering of the fusion rate within that mass. Since it is the outflow of fusion-supplied energy that supports the higher layers of the star, the core is compressed, producing higher temperatures and pressures. Both factors increase the rate of fusion thus moving the equilibrium towards a smaller, denser, hotter core producing more energy whose increased outflow pushes the higher layers further out. Thus there is a steady increase in the luminosity and radius of the star over time.[16] For example, the luminosity of the early Sun was only about 70% of its current value.[39] As a star ages this luminosity increase changes its position on the HR diagram. This effect results in a broadening of the main sequence band because stars are observed at random stages in their lifetime. That is, the main sequence band develops a thickness on the HR diagram; it is not simply a narrow line.[40]

Other factors that broaden the main sequence band on the HR diagram include uncertainty in the distance to stars and the presence of unresolved binary stars that can alter the observed stellar parameters. However, even perfect observation would show a fuzzy main sequence because mass is not the only parameter that affects a star's color and luminosity. Variations in chemical composition caused by the initial abundances, the star's evolutionary status,[41] interaction with a close companion,[42] rapid rotation,[43] or a magnetic field can all slightly change a main-sequence star's HR diagram position, to name just a few factors. As an example, there are metal-poor stars (with a very low abundance of elements with higher atomic numbers than helium) that lie just below the main sequence and are known as subdwarfs. These stars are fusing hydrogen in their cores and so they mark the lower edge of the main sequence fuzziness caused by variance in chemical composition.[44]
$
5
Question: How does a star's position on the HR diagram change due to its luminosity increase as it ages?
A: It moves downward on the HR diagram.
B: It moves upward on the HR diagram.
C: It moves to the left on the HR diagram.
D: It broadens the main sequence band.
E: It narrows the main sequence band.
Answer: D

Question: What is a subdwarf's relationship to the main sequence?
A: It lies above the main sequence.
B: It is the same as a main sequence star.
C: It lies just below the main sequence.
D: It is a result of binary stars.
E: It is unaffected by chemical composition.
Answer: C

Question: Which factor does NOT contribute to the broadening of the main sequence band on the HR diagram?
A: Distance uncertainties to stars.
B: Presence of unresolved binary stars.
C: Rapid rotation of stars.
D: Interaction with a close companion.
E: Stellar black holes.
Answer: E

Question: Why does a star's core compress as non-fusing helium ash accumulates?
A: Expansion of the star's outer layers.
B: Decreased gravitational pull.
C: Lowering of the fusion rate due to reduced hydrogen.
D: Increased magnetic fields.
E: Helium ash causes expansion of the core.
Answer: C

Question: Which of the following does NOT cause a main-sequence star's HR diagram position to change?
A: Its evolutionary status.
B: Rapid rotation.
C: Interaction with a close companion.
D: Its distance from Earth.
E: Variations in chemical composition.
Answer: D
@
The total amount of energy that a star can generate through nuclear fusion of hydrogen is limited by the amount of hydrogen fuel that can be consumed at the core. For a star in equilibrium, the thermal energy generated at the core must be at least equal to the energy radiated at the surface. Since the luminosity gives the amount of energy radiated per unit time, the total life span can be estimated, to first approximation, as the total energy produced divided by the star's luminosity.[46]

For a star with at least 0.5 M☉, when the hydrogen supply in its core is exhausted and it expands to become a red giant, it can start to fuse helium atoms to form carbon. The energy output of the helium fusion process per unit mass is only about a tenth the energy output of the hydrogen process, and the luminosity of the star increases.[47] This results in a much shorter length of time in this stage compared to the main-sequence lifetime. (For example, the Sun is predicted to spend 130 million years burning helium, compared to about 12 billion years burning hydrogen.)[48] Thus, about 90% of the observed stars above 0.5 M☉ will be on the main sequence.[49] On average, main-sequence stars are known to follow an empirical mass–luminosity relationship.[50] The luminosity (L) of the star is roughly proportional to the total mass (M) as the following power law:

�
 
∝
 
�
3.5
{\displaystyle L\ \propto \ M^{3.5}}
This relationship applies to main-sequence stars in the range 0.1–50 M☉.[51]
$
5
Question: What determines the total amount of energy a star can generate through nuclear fusion?
A: The star's size.
B: The star's distance from Earth.
C: The amount of hydrogen fuel in its core.
D: The star's age.
E: The star's position on the HR diagram.
Answer: C

Question: Why is the helium fusion process's energy output per unit mass lower than that of hydrogen?
A: Helium fusion requires higher temperatures.
B: Helium atoms are smaller than hydrogen atoms.
C: Helium fusion produces more waste products.
D: The energy output of helium fusion is only about a tenth of the hydrogen process.
E: Helium fusion only occurs in the star's outer layers.
Answer: D

Question: Roughly what proportion of observed stars above 0.5 M☉ will be on the main sequence?
A: 10%
B: 50%
C: 75%
D: 90%
E: 95%
Answer: D

Question: In the given mass–luminosity relationship, the luminosity (L) of the star is proportional to the total mass (M) as which power law?
A: L ∝ M
B: L ∝ M^2
C: L ∝ M^3
D: L ∝ M^3.5
E: L ∝ M^4
Answer: D

Question: Which phase of a star's life lasts shorter, the main-sequence lifetime or the time it spends burning helium?
A: The main-sequence lifetime.
B: The time it spends burning helium.
C: Both last roughly the same time.
D: It depends on the star's initial mass.
E: Neither, as stars do not burn helium.
Answer: B
@
Although more massive stars have more fuel to burn and might intuitively be expected to last longer, they also radiate a proportionately greater amount with increased mass. This is required by the stellar equation of state; for a massive star to maintain equilibrium, the outward pressure of radiated energy generated in the core not only must but will rise to match the titanic inward gravitational pressure of its envelope. Thus, the most massive stars may remain on the main sequence for only a few million years, while stars with less than a tenth of a solar mass may last for over a trillion years.[54]

The exact mass-luminosity relationship depends on how efficiently energy can be transported from the core to the surface. A higher opacity has an insulating effect that retains more energy at the core, so the star does not need to produce as much energy to remain in hydrostatic equilibrium. By contrast, a lower opacity means energy escapes more rapidly and the star must burn more fuel to remain in equilibrium.[55] A sufficiently high opacity can result in energy transport via convection, which changes the conditions needed to remain in equilibrium.[16]

In high-mass main-sequence stars, the opacity is dominated by electron scattering, which is nearly constant with increasing temperature. Thus the luminosity only increases as the cube of the star's mass.[47] For stars below 10 M☉, the opacity becomes dependent on temperature, resulting in the luminosity varying approximately as the fourth power of the star's mass.[51] For very low-mass stars, molecules in the atmosphere also contribute to the opacity. Below about 0.5 M☉, the luminosity of the star varies as the mass to the power of 2.3, producing a flattening of the slope on a graph of mass versus luminosity. Even these refinements are only an approximation, however, and the mass-luminosity relation can vary depending on a star's composition.[12]
$
5
Question: Why do massive stars have shorter main sequence lifetimes despite having more fuel?
A: They have weaker gravitational forces.
B: They radiate energy proportionately greater with increased mass.
C: They undergo less nuclear fusion.
D: They are cooler than smaller stars.
E: Their cores are less dense.
Answer: B

Question: What dominates the opacity in high-mass main-sequence stars?
A: Hydrogen scattering.
B: Helium burning.
C: Molecular interaction.
D: Electron scattering.
E: Radiation pressure.
Answer: D

Question: In which type of stars does the luminosity vary approximately as the fourth power of the star's mass?
A: Stars above 10 M☉.
B: Very low-mass stars.
C: Stars below 10 M☉.
D: Main-sequence stars.
E: Red giants.
Answer: C

Question: Which mode of energy transport is dominant in regions with a low temperature gradient and a low opacity?
A: Convection.
B: Radiation.
C: Thermal conduction.
D: Electron scattering.
E: Opacity transport.
Answer: B

Question: How does the opacity in stars below 10 M☉ behave?
A: It remains constant.
B: It decreases with increasing temperature.
C: It increases with decreasing mass.
D: It becomes dependent on temperature.
E: It is dominated by hydrogen scattering.
Answer: D
@
Stellar structure models describe the internal structure of a star in detail and make predictions about the luminosity, the color and the future evolution of the star. Different classes and ages of stars have different internal structures, reflecting their elemental makeup and energy transport mechanisms.

Different layers of the stars transport heat up and outwards in different ways, primarily convection and radiative transfer, but thermal conduction is important in white dwarfs.

Convection is the dominant mode of energy transport when the temperature gradient is steep enough so that a given parcel of gas within the star will continue to rise if it rises slightly via an adiabatic process. In this case, the rising parcel is buoyant and continues to rise if it is warmer than the surrounding gas; if the rising parcel is cooler than the surrounding gas, it will fall back to its original height.[1] In regions with a low temperature gradient and a low enough opacity to allow energy transport via radiation, radiation is the dominant mode of energy transport.

The internal structure of a main sequence star depends upon the mass of the star.

In stars with masses of 0.3–1.5 solar masses (M☉), including the Sun, hydrogen-to-helium fusion occurs primarily via proton–proton chains, which do not establish a steep temperature gradient. Thus, radiation dominates in the inner portion of solar mass stars. The outer portion of solar mass stars is cool enough that hydrogen is neutral and thus opaque to ultraviolet photons, so convection dominates. Therefore, solar mass stars have radiative cores with convective envelopes in the outer portion of the star.
$
5
Question: What determines the mode of energy transport within a star?
A: Its position in the galaxy.
B: The temperature gradient and opacity.
C: Its spectral type.
D: The type of fusion it undergoes.
E: Its orbital period around the galaxy.
Answer: B

Question: Which is NOT a method of heat transport in stars?
A: Convection.
B: Radiative transfer.
C: Thermal conduction.
D: Electron scattering.
E: Gravitational pull.
Answer: E

Question: What happens in stars with masses of 0.3–1.5 solar masses in terms of hydrogen-to-helium fusion?
A: They undergo fusion primarily via electron scattering.
B: They undergo fusion primarily via the CNO cycle.
C: They undergo fusion primarily via proton–proton chains.
D: They do not undergo hydrogen-to-helium fusion.
E: They undergo fusion via neutron capture.
Answer: C

Question: How is energy primarily transported in the inner portion of solar mass stars?
A: By convection.
B: By radiation.
C: By thermal conduction.
D: By electron scattering.
E: By opacity mechanisms.
Answer: B

Question: In which type of star does thermal conduction play an important role in heat transport?
A: Red giants.
B: Main-sequence stars.
C: Brown dwarfs.
D: Neutron stars.
E: White dwarfs.
Answer: E
@
Convection is single or multiphase fluid flow that occurs spontaneously due to the combined effects of material property heterogeneity and body forces on a fluid, most commonly density and gravity (see buoyancy). When the cause of the convection is unspecified, convection due to the effects of thermal expansion and buoyancy can be assumed. Convection may also take place in soft solids or mixtures where particles can flow.

Convective flow may be transient (such as when a multiphase mixture of oil and water separates) or steady state (see Convection cell). The convection may be due to gravitational, electromagnetic or fictitious body forces. Heat transfer by natural convection plays a role in the structure of Earth's atmosphere, its oceans, and its mantle. Discrete convective cells in the atmosphere can be identified by clouds, with stronger convection resulting in thunderstorms. Natural convection also plays a role in stellar physics. Convection is often categorised or described by the main effect causing the convective flow, e.g. Thermal convection.

Convection cannot take place in most solids because neither bulk current flows nor significant diffusion of matter can take place. Granular convection is a similar phenomenon in granular material instead of fluids. Advection is fluid motion created by velocity instead of thermal gradients. Convective heat transfer is the intentional use of convection as a method for heat transfer. Convection is a process in which heat is carried from place to place by the bulk movement of a fluid and gases.
$
5
Question: What often assumes the cause of convection when unspecified?
A: Thermal expansion.
B: Velocity gradients.
C: Electromagnetic force.
D: Gravitational pull.
E: Soft solids movement.
Answer: A

Question: Which of the following is NOT a cause for convection?
A: Electromagnetic forces.
B: Thermal gradients.
C: Soft solid state.
D: Gravitational forces.
E: Particle charge.
Answer: E

Question: In which of the following does convection typically NOT occur?
A: Fluids.
B: Granular materials.
C: Gases.
D: Soft solids.
E: Liquids.
Answer: D

Question: What can discrete convective cells in the atmosphere be identified by?
A: Temperature changes.
B: Strong winds.
C: Clouds.
D: Stellar activity.
E: Air pressure.
Answer: C

Question: Advection is fluid motion created by what?
A: Thermal expansion.
B: Gravitational pull.
C: Velocity gradients.
D: Soft solid movement.
E: Diffusion of matter.
Answer: C
@
Gravitational convection is a type of natural convection induced by buoyancy variations resulting from material properties other than temperature. Typically this is caused by a variable composition of the fluid. If the varying property is a concentration gradient, it is known as solutal convection.[4] For example, gravitational convection can be seen in the diffusion of a source of dry salt downward into wet soil due to the buoyancy of fresh water in saline.[5]

Variable salinity in water and variable water content in air masses are frequent causes of convection in the oceans and atmosphere which do not involve heat, or else involve additional compositional density factors other than the density changes from thermal expansion (see thermohaline circulation). Similarly, variable composition within the Earth's interior which has not yet achieved maximal stability and minimal energy (in other words, with densest parts deepest) continues to cause a fraction of the convection of fluid rock and molten metal within the Earth's interior (see below).

Gravitational convection, like natural thermal convection, also requires a g-force environment in order to occur.
$
3
Question: Gravitational convection can be observed in the diffusion of what into wet soil?
A: Air.
B: Sugar.
C: Fresh water.
D: Dry salt.
E: Molten metal.
Answer: D

Question: What is NOT a frequent cause of convection in the oceans and atmosphere?
A: Thermal expansion.
B: Variable water content.
C: Variable salinity.
D: Density changes due to heat.
E: Light refraction.
Answer: E

Question: What is necessary for gravitational convection to occur?
A: Radiation.
B: Heat source.
C: G-force environment.
D: External fluid movement.
E: Presence of solutes.
Answer: C
@
Natural convection is a type of flow, of motion of a liquid such as water or a gas such as air, in which the fluid motion is not generated by any external source (like a pump, fan, suction device, etc.) but by some parts of the fluid being heavier than other parts. In most cases this leads to natural circulation, the ability of a fluid in a system to circulate continuously, with gravity and possible changes in heat energy. The driving force for natural convection is gravity. For example if there is a layer of cold dense air on top of hotter less dense air, gravity pulls more strongly on the denser layer on top, so it falls while the hotter less dense air rises to take its place. This creates circulating flow: convection. As it relies on gravity, there is no convection in free-fall (inertial) environments, such as that of the orbiting International Space Station. Natural convection can occur when there are hot and cold regions of either air or water, because both water and air become less dense as they are heated. But, for example, in the world's oceans it also occurs due to salt water being heavier than fresh water, so a layer of salt water on top of a layer of fresher water will also cause convection.
$
3
Question: What is the driving force behind natural convection?
A: Electromagnetic force.
B: Heat energy.
C: Gravity.
D: External pump.
E: Fluid composition.
Answer: C

Question: Why doesn't convection occur in free-fall environments?
A: Absence of temperature difference.
B: Absence of gravity.
C: Absence of fluid.
D: Absence of heat source.
E: Presence of electromagnetic force.
Answer: B

Question: In the world's oceans, what besides temperature differences can lead to convection?
A: Depth differences.
B: Tidal forces.
C: Chemical composition.
D: Salt water being heavier than fresh water.
E: Electromagnetic activity.
Answer: D
@
Convection, especially Rayleigh–Bénard convection, where the convecting fluid is contained by two rigid horizontal plates, is a convenient example of a pattern-forming system.

When heat is fed into the system from one direction (usually below), at small values it merely diffuses (conducts) from below upward, without causing fluid flow. As the heat flow is increased, above a critical value of the Rayleigh number, the system undergoes a bifurcation from the stable conducting state to the convecting state, where bulk motion of the fluid due to heat begins. If fluid parameters other than density do not depend significantly on temperature, the flow profile is symmetric, with the same volume of fluid rising as falling. This is known as Boussinesq convection.

As the temperature difference between the top and bottom of the fluid becomes higher, significant differences in fluid parameters other than density may develop in the fluid due to temperature. An example of such a parameter is viscosity, which may begin to significantly vary horizontally across layers of fluid. This breaks the symmetry of the system, and generally changes the pattern of up- and down-moving fluid from stripes to hexagons, as seen at right. Such hexagons are one example of a convection cell.

As the Rayleigh number is increased even further above the value where convection cells first appear, the system may undergo other bifurcations, and other more complex patterns, such as spirals, may begin to appear.
$
5
Question: Which form of convection is contained between two rigid horizontal plates?
A: Advection.
B: Gravitational convection.
C: Natural convection.
D: Rayleigh–Bénard convection.
E: Granular convection.
Answer: D

Question: At low heat values, what is the dominant method of heat transfer in Rayleigh–Bénard convection?
A: Radiation.
B: Conduction.
C: Natural convection.
D: Advection.
E: Thermal expansion.
Answer: B

Question: In the Boussinesq convection, what happens when the temperature difference increases significantly?
A: Viscosity remains constant.
B: Flow profile becomes asymmetrical.
C: Flow volume decreases.
D: Gravitational forces decrease.
E: Advection takes over.
Answer: B

Question: In Rayleigh–Bénard convection, what pattern appears as temperature differences grow significantly?
A: Spirals.
B: Vertical stripes.
C: Random clusters.
D: Hexagons.
E: Circles.
Answer: D

Question: As the Rayleigh number increases further above where convection cells first appear, what pattern may begin to appear?
A: Hexagons.
B: Vertical stripes.
C: Spirals.
D: Circles.
E: Random clusters.
Answer: C
@
In fluid thermodynamics, Rayleigh–Bénard convection is a type of natural convection, occurring in a planar horizontal layer of fluid heated from below, in which the fluid develops a regular pattern of convection cells known as Bénard cells. Bénard–Rayleigh convection is one of the most commonly studied convection phenomena because of its analytical and experimental accessibility.[1] The convection patterns are the most carefully examined example of self-organizing nonlinear systems.[1][2]

Buoyancy, and hence gravity, are responsible for the appearance of convection cells. The initial movement is the upwelling of less-dense fluid from the warmer bottom layer.[3] This upwelling spontaneously organizes into a regular pattern of cells.
$
5
Question: Which type of convection results in the formation of Bénard cells?
A: Forced convection
B: Advection
C: Rayleigh–Bénard convection
D: Granular convection
E: Solutal convection
Answer: C

Question: The self-organizing nature of which system makes it one of the most studied nonlinear phenomena?
A: Thermodynamic systems
B: Oscillatory systems
C: Resonance systems
D: Rayleigh–Bénard convection patterns
E: Electromagnetic systems
Answer: D

Question: What causes the initial movement in Rayleigh–Bénard convection?
A: Rapid cooling from the top
B: Mechanical stirring
C: Upwelling of less-dense fluid from the bottom
D: Random particle collisions
E: External fan or pump
Answer: C

Question: What force plays a key role in the appearance of convection cells?
A: Electromagnetic
B: Centripetal
C: Elastic
D: Frictional
E: Gravity
Answer: E

Question: Which cells are a direct result of natural convection in a horizontal layer of fluid heated from below?
A: Thomson cells
B: Bénard cells
C: Maxwell cells
D: Pascal cells
E: Newton cells
Answer: B
@
The features of Bénard convection can be obtained by a simple experiment first conducted by Henri Bénard, a French physicist, in 1900.

The experimental set-up uses a layer of liquid, e.g. water, between two parallel planes. The height of the layer is small compared to the horizontal dimension. At first, the temperature of the bottom plane is the same as the top plane. The liquid will then tend towards an equilibrium, where its temperature is the same as its surroundings. (Once there, the liquid is perfectly uniform: to an observer it would appear the same from any position. This equilibrium is also asymptotically stable: after a local, temporary perturbation of the outside temperature, it will go back to its uniform state, in line with the second law of thermodynamics).

Then, the temperature of the bottom plane is increased slightly yielding a flow of thermal energy conducted through the liquid. The system will begin to have a structure of thermal conductivity: the temperature, and the density and pressure with it, will vary linearly between the bottom and top plane. A uniform linear gradient of temperature will be established. (This system may be modelled by statistical mechanics).

Once conduction is established, the microscopic random movement spontaneously becomes ordered on a macroscopic level, forming Benard convection cells, with a characteristic correlation length.
$
5
Question: Who first conducted the simple experiment to study the features of Bénard convection?
A: Lord Rayleigh
B: Henri Bénard
C: Isaac Newton
D: Albert Einstein
E: Marie Curie
Answer: B

Question: If the temperature of the bottom plane matches the top plane, how does the liquid appear to an observer?
A: Randomly convecting
B: Uniform throughout
C: Oscillating in waves
D: Rotating
E: Chaotic with unpredictable patterns
Answer: B

Question: What spontaneous behavior is observed once thermal conduction is established in the liquid during the Bénard convection experiment?
A: Rapid cooling
B: Oscillation
C: Bénard convection cell formation
D: Particle sedimentation
E: Evaporation
Answer: C

Question: In the initial stages of the experiment, the liquid approaches an equilibrium state due to:
A: Electromagnetic forces
B: Mechanical stirring
C: Uniform temperature of the surroundings
D: Chemical reactions
E: Rapid evaporation
Answer: C

Question: When the temperature of the bottom plane is slightly increased, what type of energy flow is observed?
A: Advection
B: Magnetic
C: Thermal conduction
D: Elastic wave
E: Light radiation
Answer: C
@
The rotation of the cells is stable and will alternate from clock-wise to counter-clockwise horizontally; this is an example of spontaneous symmetry breaking. Bénard cells are metastable. This means that a small perturbation will not be able to change the rotation of the cells, but a larger one could affect the rotation; they exhibit a form of hysteresis.

Moreover, the deterministic law at the microscopic level produces a non-deterministic arrangement of the cells: if the experiment is repeated, a particular position in the experiment will be in a clockwise cell in some cases, and a counter-clockwise cell in others. Microscopic perturbations of the initial conditions are enough to produce a non-deterministic macroscopic effect. That is, in principle, there is no way to calculate the macroscopic effect of a microscopic perturbation. This inability to predict long-range conditions and sensitivity to initial-conditions are characteristics of chaotic or complex systems (i.e., the butterfly effect).

If the temperature of the bottom plane was to be further increased, the structure would become more complex in space and time; the turbulent flow would become chaotic.
$
5
Question: What does the spontaneous symmetry breaking in Bénard cells refer to?
A: Random cell division
B: Alternating rotation directions
C: Cells becoming square-shaped
D: All cells rotating clockwise
E: All cells rotating counter-clockwise
Answer: B

Question: Which term describes the unpredictability of cell rotation patterns upon repeated experiments?
A: Thermal equilibrium
B: Hysteresis
C: Advection
D: Chaotic or complex systems
E: Viscosity
Answer: D

Question: Bénard cells are:
A: In a constant state of flux
B: Metastable
C: Always rotating clockwise
D: Unaffected by perturbations
E: Deterministic in nature
Answer: B

Question: If the temperature of the bottom plane is increased further, the flow within the Bénard cells becomes:
A: Laminar
B: Inert
C: Symmetric
D: Chaotic
E: Equilibrium
Answer: D

Question: Which term describes the sensitivity of a system to its initial conditions, often used in the context of chaos theory?
A: Centrifugal force
B: Elasticity
C: Hysteresis
D: Buoyancy
E: Butterfly effect
Answer: E
@
Since there is a density gradient between the top and the bottom plate, gravity acts trying to pull the cooler, denser liquid from the top to the bottom. This gravitational force is opposed by the viscous damping force in the fluid. The balance of these two forces is expressed by a non-dimensional parameter called the Rayleigh number. The Rayleigh number is defined as:

R
a
�
=
�
�
�
�
(
�
�
−
�
�
)
�
3
\mathrm{Ra}_{L} = \frac{g \beta} {\nu \alpha} (T_b - T_u) L^3
where

Tu is the temperature of the top plate
Tb is the temperature of the bottom plate
L is the height of the container
g is the acceleration due to gravity
ν is the kinematic viscosity
α is the thermal diffusivity
β is the thermal expansion coefficient.
As the Rayleigh number increases, the gravitational forces become more dominant. At a critical Rayleigh number of 1708,[2] instability sets in and convection cells appear.

The critical Rayleigh number can be obtained analytically for a number of different boundary conditions by doing a perturbation analysis on the linearized equations in the stable state.[10] The simplest case is that of two free boundaries, which Lord Rayleigh solved in 1916, obtaining Ra = 27⁄4 π4 ≈ 657.51.[11] In the case of a rigid boundary at the bottom and a free boundary at the top (as in the case of a kettle without a lid), the critical Rayleigh number comes out as Ra = 1,100.65.[12]
$
5
Question: What non-dimensional parameter describes the balance between gravitational and viscous damping forces?
A: Gravitational constant
B: Thermal coefficient
C: Reynolds number
D: Rayleigh number
E: Bénard number
Answer: D

Question: What does the Rayleigh number become more dominated by as it increases?
A: Viscous damping forces
B: Elastic forces
C: Gravitational forces
D: Electromagnetic forces
E: Buoyant forces
Answer: C

Question: At what critical Rayleigh number do convection cells typically appear?
A: 100
B: 657.51
C: 1708
D: 1100.65
E: 2500
Answer: C

Question: The variable "L" in the Rayleigh number equation represents:
A: Luminosity
B: Length of fluid column
C: Latent heat
D: Liquid density
E: Liquid viscosity
Answer: B

Question: Which scientist solved the critical Rayleigh number for two free boundaries in 1916?
A: Albert Einstein
B: Henri Bénard
C: Lord Rayleigh
D: Sir Isaac Newton
E: Marie Curie
Answer: C
@
In heat transfer analysis, thermal diffusivity is the thermal conductivity divided by density and specific heat capacity at constant pressure.[1] It is a measure of the rate of heat transfer inside a material. It has the SI derived unit of m2/s. Thermal diffusivity is usually denoted by lowercase alpha (α), but a, h, κ (kappa),[2] K,[3] and D are also used.

The formula is:[4]

�
=
�
�
�
�
{\displaystyle \alpha ={\frac {k}{\rho c_{p}}}}
where

k is thermal conductivity (W/(m·K))
cp is specific heat capacity (J/(kg·K))
ρ is density (kg/m3)
Together, ρcp can be considered the volumetric heat capacity (J/(m3·K)).

As seen in the heat equation,[5]

∂
�
∂
�
=
�
∇
2
�
,
{\displaystyle {\frac {\partial T}{\partial t}}=\alpha \nabla ^{2}T,}
one way to view thermal diffusivity is as the ratio of the time derivative of temperature to its curvature, quantifying the rate at which temperature concavity is "smoothed out". Thermal diffusivity is a contrasting measure to thermal effusivity.[6][7] In a substance with high thermal diffusivity, heat moves rapidly through it because the substance conducts heat quickly relative to its volumetric heat capacity or 'thermal bulk'.

Thermal diffusivity is often measured with the flash method.[8][9] It involves heating a strip or cylindrical sample with a short energy pulse at one end and analyzing the temperature change (reduction in amplitude and phase shift of the pulse) a short distance away.[10][11]
$
5
Question: What is the unit of thermal diffusivity in the SI system?
A: W/m·K
B: J/(m^3·K)
C: m2/s
D: J/(kg·K)
E: kg/m^3
Answer: C

Question: Which symbol commonly represents thermal diffusivity?
A: k
B: κ
C: T
D: ρ
E: q
Answer: B

Question: The volumetric heat capacity is a combination of which two parameters?
A: Thermal conductivity and density
B: Specific heat capacity and density
C: Thermal diffusivity and thermal conductivity
D: Thermal conductivity and specific heat capacity
E: Specific heat capacity and thermal diffusivity
Answer: B

Question: Which method is often used to measure thermal diffusivity?
A: Fourier's method
B: Convective method
C: Gradient method
D: Flash method
E: Heat transfer method
Answer: D

Question: Thermal diffusivity provides insight into the rate at which what property is "smoothed out"?
A: Heat
B: Pressure
C: Density
D: Temperature concavity
E: Thermal gradient
Answer: D
@
The thermal conductivity of a material is a measure of its ability to conduct heat. It is commonly denoted by 
�
k, 
�\lambda , or 
�\kappa  and is measured in W·m−1·K−1.

Heat transfer occurs at a lower rate in materials of low thermal conductivity than in materials of high thermal conductivity. For instance, metals typically have high thermal conductivity and are very efficient at conducting heat, while the opposite is true for insulating materials like mineral wool or Styrofoam. Correspondingly, materials of high thermal conductivity are widely used in heat sink applications, and materials of low thermal conductivity are used as thermal insulation. The reciprocal of thermal conductivity is called thermal resistivity.

The defining equation for thermal conductivity is 
�
=
−
�
∇
�
{\displaystyle \mathbf {q} =-k\nabla T}, where 
�
\mathbf {q}  is the heat flux, 
�
k is the thermal conductivity, and 
∇
�
{\displaystyle \nabla T} is the temperature gradient. This is known as Fourier's Law for heat conduction. Although commonly expressed as a scalar, the most general form of thermal conductivity is a second-rank tensor. However, the tensorial description only becomes necessary in materials which are anisotropic.
$
5
Question: What is the reciprocal of thermal conductivity called?
A: Thermal induction
B: Thermal resistance
C: Thermal gradient
D: Thermal resistivity
E: Thermal diffusivity
Answer: D

Question: Which of the following materials is typically known for high thermal conductivity?
A: Styrofoam
B: Mineral wool
C: Metals
D: Wood
E: Rubber
Answer: C

Question: Fourier's Law for heat conduction relates heat flux to which two parameters?
A: Thermal conductivity and density
B: Thermal conductivity and temperature gradient
C: Temperature gradient and specific heat capacity
D: Thermal conductivity and specific heat capacity
E: Thermal conductivity and thermal diffusivity
Answer: B

Question: In which type of material is the tensorial description of thermal conductivity necessary?
A: Isotropic
B: Homogeneous
C: Anisotropic
D: Transparent
E: Metallic
Answer: C

Question: Which is the typical unit of thermal conductivity?
A: m2/s
B: W·m−1·K−1
C: J/(m^3·K)
D: J/(kg·K)
E: kg/m^3
Answer: B
@
Conduction is the process by which heat is transferred from the hotter end to the colder end of an object. The ability of the object to conduct heat is known as its thermal conductivity, and is denoted k.

Heat spontaneously flows along a temperature gradient (i.e. from a hotter body to a colder body). For example, heat is conducted from the hotplate of an electric stove to the bottom of a saucepan in contact with it. In the absence of an opposing external driving energy source, within a body or between bodies, temperature differences decay over time, and thermal equilibrium is approached, temperature becoming more uniform.

In conduction, the heat flow is within and through the body itself. In contrast, in heat transfer by thermal radiation, the transfer is often between bodies, which may be separated spatially. Heat can also be transferred by a combination of conduction and radiation. In solids, conduction is mediated by the combination of vibrations and collisions of molecules, propagation and collisions of phonons, and diffusion and collisions of free electrons. In gases and liquids, conduction is due to the collisions and diffusion of molecules during their random motion. Photons in this context do not collide with one another, and so heat transport by electromagnetic radiation is conceptually distinct from heat conduction by microscopic diffusion and collisions of material particles and phonons. But the distinction is often not easily observed unless the material is semi-transparent.

In the engineering sciences, heat transfer includes the processes of thermal radiation, convection, and sometimes mass transfer.[further explanation needed] Usually, more than one of these processes occurs in a given situation.
$
5
Question: By which process does heat transfer from a hotplate of an electric stove to a saucepan in contact with it?
A: Radiation
B: Conduction
C: Convection
D: Reflection
E: Refraction
Answer: B

Question: During conduction in solids, which are the primary carriers of heat?
A: Photons
B: Protons
C: Electrons
D: Phonons
E: Neutrons
Answer: D

Question: In the absence of an external energy source, what does temperature tend to become over time due to conduction?
A: Increasing
B: Decreasing
C: Oscillating
D: More uniform
E: Less uniform
Answer: D

Question: Heat can be transferred by conduction and which other method, especially in semi-transparent materials?
A: Reflection
B: Absorption
C: Radiation
D: Refraction
E: Translucence
Answer: C

Question: In gases and liquids, conduction primarily occurs due to what?
A: Vibration of molecules
B: Reflection of photons
C: Emission of radiation
D: Collisions and diffusion of molecules
E: Transfer of electrons
Answer: D
@
Thermal diffusivity of selected materials and substances[12]
Material	Thermal diffusivity (mm2/s)	References
Pyrolytic graphite, parallel to layers	1,220	
Diamond	1,060 - 1,160	
Carbon/carbon composite at 25 °C	216.5	[13]
Helium (300 K, 1 atm)	190	[14]
Silver, pure (99.9%)	165.63	
Hydrogen (300 K, 1 atm)	160	[14]
Gold	127	[15]
Copper at 25 °C	111	[13]
Aluminium	97	[15]
Silicon	88	[15]
Al-10Si-Mn-Mg (Silafont 36) at 20 °C	74.2	[16]
Aluminium 6061-T6 Alloy	64	[15]
Molybdenum (99.95%) at 25 °C	54.3	[17]
Al-5Mg-2Si-Mn (Magsimal-59) at 20 °C	44.0	[18]
Tin	40	[15]
Water vapor (1 atm, 400 K)	23.38	
Iron	23	[15]
Argon (300 K, 1 atm)	22	[14]
Nitrogen (300 K, 1 atm)	22	[14]
Air (300 K)	19	[15]
Steel, AISI 1010 (0.1% carbon)	18.8	[19]
Aluminium oxide (polycrystalline)	12.0	
Steel, 1% carbon	11.72	
Si3N4 with CNTs 26 °C	9.142	[20]
Si3N4 without CNTs 26 °C	8.605	[20]
Steel, stainless 304A at 27 °C	4.2	[15]
Pyrolytic graphite, normal to layers	3.6	
Steel, stainless 310 at 25 °C	3.352	[21]
Inconel 600 at 25 °C	3.428	[22]
Quartz	1.4	[15]
Sandstone	1.15	
Ice at 0 °C	1.02	
Silicon dioxide (polycrystalline)	0.83	[15]
Brick, common	0.52	
Glass, window	0.34	
Brick, adobe	0.27	
PC (polycarbonate) at 25 °C	0.144	[23]
Water at 25 °C	0.143	[23]
PTFE (Polytetrafluorethylene) at 25 °C	0.124	[24]
PP (polypropylene) at 25 °C	0.096	[23]
Nylon	0.09	
Rubber	0.089 - 0.13	[3]
Wood (yellow pine)	0.082	
Paraffin at 25 °C	0.081	[23]
PVC (polyvinyl chloride)	0.08	[15]
Oil, engine (saturated liquid, 100 °C)	0.0738	
Alcohol	0.07	[15]
$
5
Question: Which material has the highest thermal diffusivity among the ones listed?
A: Diamond
B: Gold
C: Silicon
D: Iron
E: Water at 25 °C
Answer: A

Question: Which of the following materials is closer in thermal diffusivity to water at 25°C?
A: Glass, window
B: Steel, AISI 1010
C: PC (polycarbonate) at 25 °C
D: Oil, engine (saturated liquid, 100 °C)
E: Aluminium
Answer: C

Question: How does the thermal diffusivity of Diamond compare to that of Pyrolytic graphite parallel to its layers?
A: Diamond has higher thermal diffusivity
B: Diamond has lower thermal diffusivity
C: They have approximately the same thermal diffusivity
D: Diamond's thermal diffusivity is exactly half of that of Pyrolytic graphite
E: Pyrolytic graphite's thermal diffusivity is ten times that of Diamond
Answer: B

Question: What is the thermal diffusivity of air at 300 K?
A: 190 mm2/s
B: 64 mm2/s
C: 19 mm2/s
D: 0.143 mm2/s
E: 0.08 mm2/s
Answer: C

Question: Between Pyrolytic graphite (parallel to layers) and Rubber, which material has a significantly lower thermal diffusivity?
A: Pyrolytic graphite
B: Rubber
C: Both have similar thermal diffusivity
D: Pyrolytic graphite, but only slightly lower
E: Rubber, but only slightly lower
Answer: B
@
In mathematics and physics, the heat equation is a certain partial differential equation. Solutions of the heat equation are sometimes known as caloric functions. The theory of the heat equation was first developed by Joseph Fourier in 1822 for the purpose of modeling how a quantity such as heat diffuses through a given region.

As the prototypical parabolic partial differential equation, the heat equation is among the most widely studied topics in pure mathematics, and its analysis is regarded as fundamental to the broader field of partial differential equations. The heat equation can also be considered on Riemannian manifolds, leading to many geometric applications. Following work of Subbaramiah Minakshisundaram and Åke Pleijel, the heat equation is closely related with spectral geometry. A seminal nonlinear variant of the heat equation was introduced to differential geometry by James Eells and Joseph Sampson in 1964, inspiring the introduction of the Ricci flow by Richard Hamilton in 1982 and culminating in the proof of the Poincaré conjecture by Grigori Perelman in 2003. Certain solutions of the heat equation known as heat kernels provide subtle information about the region on which they are defined, as exemplified through their application to the Atiyah–Singer index theorem.[1]

The heat equation, along with variants thereof, is also important in many fields of science and applied mathematics. In probability theory, the heat equation is connected with the study of random walks and Brownian motion via the Fokker–Planck equation. The Black–Scholes equation of financial mathematics is a small variant of the heat equation, and the Schrödinger equation of quantum mechanics can be regarded as a heat equation in imaginary time. In image analysis, the heat equation is sometimes used to resolve pixelation and to identify edges. Following Robert Richtmyer and John von Neumann's introduction of "artificial viscosity" methods, solutions of heat equations have been useful in the mathematical formulation of hydrodynamical shocks. Solutions of the heat equation have also been given much attention in the numerical analysis literature, beginning in the 1950s with work of Jim Douglas, D.W. Peaceman, and Henry Rachford Jr.
$
5
Question: Who first developed the theory of the heat equation?
A: Joseph Fourier
B: Richard Hamilton
C: Grigori Perelman
D: James Eells
E: Robert Richtmyer
Answer: A

Question: The heat equation was developed by Fourier in which year?
A: 1700
B: 1775
C: 1822
D: 1964
E: 2003
Answer: C

Question: The Ricci flow, which played a role in the proof of the Poincaré conjecture, was introduced by whom?
A: Richard Hamilton
B: Joseph Fourier
C: Åke Pleijel
D: Joseph Sampson
E: Robert Richtmyer
Answer: A

Question: The Black–Scholes equation of financial mathematics can be considered a variant of which equation?
A: Fourier's equation
B: Quantum mechanics equation
C: Schrödinger equation
D: Heat equation
E: Fokker–Planck equation
Answer: D

Question: In which field is the heat equation used to resolve pixelation and identify edges?
A: Spectral geometry
B: Probability theory
C: Financial mathematics
D: Image analysis
E: Hydrodynamics
Answer: D
@
In mathematics, if given an open subset U of Rn and a subinterval I of R, one says that a function u : U × I → R is a solution of the heat equation if

∂
�
∂
�
=
∂
2
�
∂
�
1
2
+
⋯
+
∂
2
�
∂
�
�
2
,
{\displaystyle {\frac {\partial u}{\partial t}}={\frac {\partial ^{2}u}{\partial x_{1}^{2}}}+\cdots +{\frac {\partial ^{2}u}{\partial x_{n}^{2}}},}
where (x1, …, xn, t) denotes a general point of the domain. It is typical to refer to t as "time" and x1, …, xn as "spatial variables," even in abstract contexts where these phrases fail to have their intuitive meaning. The collection of spatial variables is often referred to simply as x. For any given value of t, the right-hand side of the equation is the Laplacian of the function u(⋅, t) : U → R. As such, the heat equation is often written more compactly as

∂
�
∂
�
=
Δ
�
{\displaystyle {\frac {\partial u}{\partial t}}=\Delta u}.

In physics and engineering contexts, especially in the context of diffusion through a medium, it is more common to fix a Cartesian coordinate system and then to consider the specific case of a function u(x, y, z, t) of three spatial variables (x, y, z) and time variable t. One then says that u is a solution of the heat equation if

∂
�
∂
�
=
�
(
∂
2
�
∂
�
2
+
∂
2
�
∂
�
2
+
∂
2
�
∂
�
2
)
{\displaystyle {\frac {\partial u}{\partial t}}=\alpha \left({\frac {\partial ^{2}u}{\partial x^{2}}}+{\frac {\partial ^{2}u}{\partial y^{2}}}+{\frac {\partial ^{2}u}{\partial z^{2}}}\right)}
in which α is a positive coefficient called the thermal diffusivity of the medium. In addition to other physical phenomena, this equation describes the flow of heat in a homogeneous and isotropic medium, with u(x, y, z, t) being the temperature at the point (x, y, z) and time t. If the medium is not homogeneous and isotropic, then α would not be a fixed coefficient, and would instead depend on (x, y, z); the equation would also have a slightly different form. In the physics and engineering literature, it is common to use ∇2 to denote the Laplacian, rather than ∆.
$
5
Question: In the given mathematical description, what is typically referred to as "time"?
A: x
B: Δ
C: t
D: u
E: α
Answer: C

Question: What does the Laplacian operator represent in the heat equation?
A: Time derivative
B: Spatial variables
C: Right-hand side of the equation
D: Spatial difference in temperature
E: Thermal diffusivity
Answer: C

Question: Which coefficient represents the thermal diffusivity of the medium in the equation?
A: Δ
B: t
C: x
D: α
E: u
Answer: D

Question: In contexts of diffusion through a medium, which of the following is commonly used instead of ∆ to denote the Laplacian?
A: ∇^2
B: ∂
C: π
D: γ
E: λ
Answer: A

Question: The function u(x, y, z, t) describes the temperature at a given point in what?
A: Time only
B: One-dimensional space
C: Two-dimensional space
D: Three-dimensional space
E: Four-dimensional space
Answer: D
@
Physical interpretation of the equation
Informally, the Laplacian operator ∆ gives the difference between the average value of a function in the neighborhood of a point, and its value at that point. Thus, if u is the temperature, ∆ tells whether (and by how much) the material surrounding each point is hotter or colder, on the average, than the material at that point.

By the second law of thermodynamics, heat will flow from hotter bodies to adjacent colder bodies, in proportion to the difference of temperature and of the thermal conductivity of the material between them. When heat flows into (respectively, out of) a material, its temperature increases (respectively, decreases), in proportion to the amount of heat divided by the amount (mass) of material, with a proportionality factor called the specific heat capacity of the material.

By the combination of these observations, the heat equation says the rate 
�
˙{\displaystyle {\dot {u}}} at which the material at a point will heat up (or cool down) is proportional to how much hotter (or cooler) the surrounding material is. The coefficient α in the equation takes into account the thermal conductivity, specific heat, and density of the material.
$
3
Question: What does the Laplacian operator informally give regarding a function at a point?
A: Rate of heat flow
B: Maximum temperature
C: Thermal diffusivity
D: Difference between the average value of the function in the neighborhood and its value at that point
E: Thermal conductivity
Answer: D

Question: Heat will flow from hotter bodies to colder bodies in proportion to what?
A: Specific heat capacity
B: Difference of temperature and thermal conductivity
C: Rate of heat transfer
D: Thermal diffusivity
E: Laplacian of the function
Answer: B

Question: The coefficient α in the heat equation considers what properties of the material?
A: Thermal conductivity, specific heat, and density
B: Temperature, Laplacian, and time
C: Heat flow, specific heat, and conductivity
D: Temperature, heat flow, and specific heat
E: Density, heat flow, and temperature
Answer: A
@
Further applications
The heat equation arises in the modeling of a number of phenomena and is often used in financial mathematics in the modeling of options. The Black–Scholes option pricing model's differential equation can be transformed into the heat equation allowing relatively easy solutions from a familiar body of mathematics. Many of the extensions to the simple option models do not have closed form solutions and thus must be solved numerically to obtain a modeled option price. The equation describing pressure diffusion in a porous medium is identical in form with the heat equation. Diffusion problems dealing with Dirichlet, Neumann and Robin boundary conditions have closed form analytic solutions (Thambynayagam 2011). The heat equation is also widely used in image analysis (Perona & Malik 1990) and in machine-learning as the driving theory behind scale-space or graph Laplacian methods. The heat equation can be efficiently solved numerically using the implicit Crank–Nicolson method of (Crank & Nicolson 1947). This method can be extended to many of the models with no closed form solution, see for instance (Wilmott, Howison & Dewynne 1995).

An abstract form of heat equation on manifolds provides a major approach to the Atiyah–Singer index theorem, and has led to much further work on heat equations in Riemannian geometry.
$
5
Question: In which field of mathematics is the heat equation often used for modeling options?
A: Quantum mechanics
B: Image analysis
C: Financial mathematics
D: Riemannian geometry
E: Probability theory
Answer: C

Question: The equation that describes pressure diffusion in a porous medium is identical in form to which equation?
A: Black–Scholes equation
B: Schrödinger equation
C: Heat equation
D: Fokker–Planck equation
E: Fourier's equation
Answer: C

Question: Which numerical method is efficient for solving the heat equation?
A: Perelman method
B: Fourier's method
C: Crank–Nicolson method
D: Poincaré method
E: Atiyah–Singer method
Answer: C

Question: In which field has an abstract form of the heat equation played a significant role in the Atiyah–Singer index theorem?
A: Financial mathematics
B: Image analysis
C: Probability theory
D: Quantum mechanics
E: Riemannian geometry
Answer: E

Question: The heat equation in image analysis is primarily used for which purpose?
A: Color correction
B: Edge identification and resolving pixelation
C: Enhancing brightness
D: Reducing noise
E: Image compression
Answer: B
@
Mathematical finance, also known as quantitative finance and financial mathematics, is a field of applied mathematics, concerned with mathematical modeling of financial markets.

In general, there exist two separate branches of finance that require advanced quantitative techniques: derivatives pricing on the one hand, and risk and portfolio management on the other.[1] Mathematical finance overlaps heavily with the fields of computational finance and financial engineering. The latter focuses on applications and modeling, often by help of stochastic asset models, while the former focuses, in addition to analysis, on building tools of implementation for the models. Also related is quantitative investing, which relies on statistical and numerical models (and lately machine learning) as opposed to traditional fundamental analysis when managing portfolios.
$
5
Question: Which field of applied mathematics focuses on the modeling of financial markets?
A: Algebraic geometry
B: Topology
C: Mathematical finance
D: Combinatorics
E: Graph theory
Answer: C

Question: What is NOT a branch of finance that requires advanced quantitative techniques?
A: Derivatives pricing
B: Risk management
C: Tax evasion
D: Portfolio management
E: Quantitative investing
Answer: C

Question: Which field focuses on building tools of implementation for financial models?
A: Financial engineering
B: Computational finance
C: Mathematical finance
D: Quantitative investing
E: Fundamental analysis
Answer: A

Question: Quantitative investing relies on which of the following instead of traditional fundamental analysis?
A: Public opinion
B: Historical trends
C: Machine learning and statistical models
D: CEO interviews
E: Stock market gossip
Answer: C

Question: Which of the following is NOT a related field to mathematical finance?
A: Computational finance
B: Financial engineering
C: Architecture
D: Quantitative investing
E: Risk management
Answer: C
@
French mathematician Louis Bachelier's doctoral thesis, defended in 1900, is considered the first scholarly work on mathematical finance. But mathematical finance emerged as a discipline in the 1970s, following the work of Fischer Black, Myron Scholes and Robert Merton on option pricing theory. Mathematical investing originated from the research of mathematician Edward Thorp who used statistical methods to first invent card counting in blackjack and then applied its principles to modern systematic investing.[2]

The subject has a close relationship with the discipline of financial economics, which is concerned with much of the underlying theory that is involved in financial mathematics. While trained economists use complex economic models that are built on observed empirical relationships, in contrast, mathematical finance analysis will derive and extend the mathematical or numerical models without necessarily establishing a link to financial theory, taking observed market prices as input. See: Valuation of options; Financial modeling; Asset pricing. The fundamental theorem of arbitrage-free pricing is one of the key theorems in mathematical finance, while the Black–Scholes equation and formula are amongst the key results.[3]

Today many universities offer degree and research programs in mathematical finance.
$
5
Question: Who defended the first scholarly work on mathematical finance in 1900?
A: Myron Scholes
B: Robert Merton
C: Louis Bachelier
D: Fischer Black
E: Edward Thorp
Answer: C

Question: What did Edward Thorp invent using statistical methods?
A: Probability theory
B: Card counting in blackjack
C: The Black-Scholes equation
D: Risk-neutral probability
E: Derivative pricing
Answer: B

Question: Which of these is a key theorem in mathematical finance?
A: Pythagorean theorem
B: Euler's theorem
C: Fundamental theorem of arbitrage-free pricing
D: Bayes' theorem
E: Modus Ponens
Answer: C

Question: Mathematical finance has a close relationship with which discipline that focuses on the underlying theory?
A: Political science
B: Environmental science
C: Physics
D: Financial economics
E: Anthropology
Answer: D

Question: Which of the following results is central to mathematical finance?
A: Euler's formula
B: Pythagoras' theorem
C: The Black–Scholes equation and formula
D: Law of large numbers
E: Newton's third law
Answer: C
@
Risk and portfolio management aims at modeling the statistically derived probability distribution of the market prices of all the securities at a given future investment horizon.
This "real" probability distribution of the market prices is typically denoted by the blackboard font letter "
�
\mathbb {P} ", as opposed to the "risk-neutral" probability "
�
\mathbb {Q} " used in derivatives pricing. Based on the P distribution, the buy-side community takes decisions on which securities to purchase in order to improve the prospective profit-and-loss profile of their positions considered as a portfolio. Increasingly, elements of this process are automated; see Outline of finance § Quantitative investing for a listing of relevant articles.

For their pioneering work, Markowitz and Sharpe, along with Merton Miller, shared the 1990 Nobel Memorial Prize in Economic Sciences, for the first time ever awarded for a work in finance.

The portfolio-selection work of Markowitz and Sharpe introduced mathematics to investment management. With time, the mathematics has become more sophisticated. Thanks to Robert Merton and Paul Samuelson, one-period models were replaced by continuous time, Brownian-motion models, and the quadratic utility function implicit in mean–variance optimization was replaced by more general increasing, concave utility functions.[11] Furthermore, in recent years the focus shifted toward estimation risk, i.e., the dangers of incorrectly assuming that advanced time series analysis alone can provide completely accurate estimates of the market parameters.[12] See Financial risk management § Investment management.

Much effort has gone into the study of financial markets and how prices vary with time. Charles Dow, one of the founders of Dow Jones & Company and The Wall Street Journal, enunciated a set of ideas on the subject which are now called Dow Theory. This is the basis of the so-called technical analysis method of attempting to predict future changes. One of the tenets of "technical analysis" is that market trends give an indication of the future, at least in the short term. The claims of the technical analysts are disputed by many academics.[citation needed]

$
5
Question: The "real" probability distribution of market prices is typically denoted by what symbol?
A: 
�
Q
B: 
�
P
C: 
�
R
D: 
�
S
E: 
�
T
Answer: B

Question: Which probability distribution is used in derivatives pricing?
A: 
�
Q
B: 
�
P
C: 
�
R
D: 
�
S
E: 
�
T
Answer: A

Question: Who were awarded the 1990 Nobel Memorial Prize in Economic Sciences for their work in finance?
A: Dow and Jones
B: Black and Scholes
C: Markowitz, Sharpe, and Miller
D: Merton and Samuelson
E: Taleb and Wilmott
Answer: C

Question: The portfolio-selection work introduced what to investment management?
A: Risk analysis
B: Mathematics
C: Human psychology
D: Stock market trends
E: Technical analysis
Answer: B

Question: Which theory suggests that market trends can predict the future at least in the short term?
A: Arbitrage-free pricing theorem
B: The Black-Scholes equation
C: Dow Theory
D: Lucas critique
E: Merton's continuous time theory
Answer: C
@
Over the years, increasingly sophisticated mathematical models and derivative pricing strategies have been developed, but their credibility was damaged by the financial crisis of 2007–2010. Contemporary practice of mathematical finance has been subjected to criticism from figures within the field notably by Paul Wilmott, and by Nassim Nicholas Taleb, in his book The Black Swan.[14] Taleb claims that the prices of financial assets cannot be characterized by the simple models currently in use, rendering much of current practice at best irrelevant, and, at worst, dangerously misleading. Wilmott and Emanuel Derman published the Financial Modelers' Manifesto in January 2009[15] which addresses some of the most serious concerns. Bodies such as the Institute for New Economic Thinking are now attempting to develop new theories and methods.[16]

In general, modeling the changes by distributions with finite variance is, increasingly, said to be inappropriate.[17] In the 1960s it was discovered by Benoit Mandelbrot that changes in prices do not follow a Gaussian distribution, but are rather modeled better by Lévy alpha-stable distributions.[18] The scale of change, or volatility, depends on the length of the time interval to a power a bit more than 1/2. Large changes up or down are more likely than what one would calculate using a Gaussian distribution with an estimated standard deviation. But the problem is that it does not solve the problem as it makes parametrization much harder and risk control less reliable.[14]

Perhaps more fundamental: though mathematical finance models may generate a profit in the short-run, this type of modeling is often in conflict with a central tenet of modern macroeconomics, the Lucas critique - or rational expectations - which states that observed relationships may not be structural in nature and thus may not be possible to exploit for public policy or for profit unless we have identified relationships using causal analysis and econometrics.[19] Mathematical finance models do not, therefore, incorporate complex elements of human psychology that are critical to modeling modern macroeconomic movements such as the self-fulfilling panic that motivates bank runs.
$
5
Question: The credibility of sophisticated mathematical models in finance was damaged by which event?
A: The dot-com bubble
B: World War II
C: The global financial crisis of 2007-2010
D: The launch of Bitcoin
E: The Great Depression
Answer: C

Question: Who criticized current financial models as being too simplistic in his book "The Black Swan"?
A: Paul Wilmott
B: Charles Dow
C: Benoit Mandelbrot
D: Louis Bachelier
E: Nassim Nicholas Taleb
Answer: E

Question: Which distribution was discovered to model changes in prices better than a Gaussian distribution?
A: Poisson distribution
B: Bernoulli distribution
C: Lévy alpha-stable distributions
D: Uniform distribution
E: Exponential distribution
Answer: C

Question: The Lucas critique suggests observed relationships might not be what?
A: Profitable
B: Replicable
C: Predictable
D: Structural
E: Quantifiable
Answer: D

Question: Which aspect is often not incorporated in mathematical finance models, making them less applicable to modeling macroeconomic movements?
A: Advanced time series analysis
B: Human psychology
C: Risk-neutral probability
D: Derivative pricing
E: Financial econometrics
Answer: B
@
The human heart is situated in the mediastinum, at the level of thoracic vertebrae T5-T8. A double-membraned sac called the pericardium surrounds the heart and attaches to the mediastinum.[15] The back surface of the heart lies near the vertebral column, and the front surface known as the sternocostal surface sits behind the sternum and rib cartilages.[7] The upper part of the heart is the attachment point for several large blood vessels—the venae cavae, aorta and pulmonary trunk. The upper part of the heart is located at the level of the third costal cartilage.[7] The lower tip of the heart, the apex, lies to the left of the sternum (8 to 9 cm from the midsternal line) between the junction of the fourth and fifth ribs near their articulation with the costal cartilages.[7]

The largest part of the heart is usually slightly offset to the left side of the chest (though occasionally it may be offset to the right) and is felt to be on the left because the left heart is stronger and larger, since it pumps to all body parts. Because the heart is between the lungs, the left lung is smaller than the right lung and has a cardiac notch in its border to accommodate the heart.[7] The heart is cone-shaped, with its base positioned upwards and tapering down to the apex.[7] An adult heart has a mass of 250–350 grams (9–12 oz).[16] The heart is often described as the size of a fist: 12 cm (5 in) in length, 8 cm (3.5 in) wide, and 6 cm (2.5 in) in thickness,[7] although this description is disputed, as the heart is likely to be slightly larger.[17] Well-trained athletes can have much larger hearts due to the effects of exercise on the heart muscle, similar to the response of skeletal muscle.[7]
$
5
Question: Which vertebrae does the human heart sit between?
A: T1-T4
B: T5-T8
C: L1-L5
D: T9-T12
E: C1-C5
Answer: B

Question: What surrounds the heart and attaches to the mediastinum?
A: Myocardium
B: Endocardium
C: Pleura
D: Pericardium
E: Periosteum
Answer: D

Question: At what level is the upper part of the heart located?
A: First costal cartilage
B: Second costal cartilage
C: Third costal cartilage
D: Fourth costal cartilage
E: Fifth costal cartilage
Answer: C

Question: Where does the apex of the heart lie?
A: Right of the sternum
B: Behind the sternum
C: Above the sternum
D: Left of the sternum
E: Below the sternum
Answer: D

Question: What is the average mass of an adult heart?
A: 150-250 grams
B: 100-200 grams
C: 250–350 grams
D: 350–450 grams
E: 200-300 grams
Answer: C
@
The heart has four chambers, two upper atria, the receiving chambers, and two lower ventricles, the discharging chambers. The atria open into the ventricles via the atrioventricular valves, present in the atrioventricular septum. This distinction is visible also on the surface of the heart as the coronary sulcus.[18] There is an ear-shaped structure in the upper right atrium called the right atrial appendage, or auricle, and another in the upper left atrium, the left atrial appendage.[19] The right atrium and the right ventricle together are sometimes referred to as the right heart. Similarly, the left atrium and the left ventricle together are sometimes referred to as the left heart.[6] The ventricles are separated from each other by the interventricular septum, visible on the surface of the heart as the anterior longitudinal sulcus and the posterior interventricular sulcus.[18]

The fibrous cardiac skeleton gives structure to the heart. It forms the atrioventricular septum, which separates the atria from the ventricles, and the fibrous rings, which serve as bases for the four heart valves.[20] The cardiac skeleton also provides an important boundary in the heart's electrical conduction system since collagen cannot conduct electricity. The interatrial septum separates the atria, and the interventricular septum separates the ventricles.[7] The interventricular septum is much thicker than the interatrial septum since the ventricles need to generate greater pressure when they contract.[7]
$
5
Question: How many chambers does the heart have?
A: Two
B: Three
C: Four
D: Five
E: Six
Answer: C

Question: What separates the atria from the ventricles?
A: Coronary sulcus
B: Atrioventricular valves
C: Interventricular septum
D: Atrioventricular septum
E: Fibrous cardiac skeleton
Answer: D

Question: Which structure is ear-shaped and found in the upper right atrium?
A: Left atrial appendage
B: Right atrial appendage
C: Ventricular appendage
D: Septal appendage
E: Sulcus appendage
Answer: B

Question: Which heart structure provides structure and cannot conduct electricity?
A: Myocardium
B: Pericardium
C: Fibrous cardiac skeleton
D: Atrioventricular valves
E: Aortic valve
Answer: C

Question: What is thicker: the interatrial septum or the interventricular septum?
A: They are the same thickness
B: Interventricular septum
C: Intercoronary septum
D: Intercardiac septum
E: Interatrial septum
Answer: B
@
The normal sinus rhythm of the heart, giving the resting heart rate, is influenced by a number of factors. The cardiovascular centres in the brainstem control the sympathetic and parasympathetic influences to the heart through the vagus nerve and sympathetic trunk.[50] These cardiovascular centres receive input from a series of receptors including baroreceptors, sensing the stretching of blood vessels and chemoreceptors, sensing the amount of oxygen and carbon dioxide in the blood and its pH. Through a series of reflexes these help regulate and sustain blood flow.[7]

Baroreceptors are stretch receptors located in the aortic sinus, carotid bodies, the venae cavae, and other locations, including pulmonary vessels and the right side of the heart itself. Baroreceptors fire at a rate determined by how much they are stretched,[51] which is influenced by blood pressure, level of physical activity, and the relative distribution of blood. With increased pressure and stretch, the rate of baroreceptor firing increases, and the cardiac centers decrease sympathetic stimulation and increase parasympathetic stimulation. As pressure and stretch decrease, the rate of baroreceptor firing decreases, and the cardiac centers increase sympathetic stimulation and decrease parasympathetic stimulation.[7] There is a similar reflex, called the atrial reflex or Bainbridge reflex, associated with varying rates of blood flow to the atria. Increased venous return stretches the walls of the atria where specialized baroreceptors are located. However, as the atrial baroreceptors increase their rate of firing and as they stretch due to the increased blood pressure, the cardiac center responds by increasing sympathetic stimulation and inhibiting parasympathetic stimulation to increase heart rate. The opposite is also true.[7] Chemoreceptors present in the carotid body or adjacent to the aorta in an aortic body respond to the blood's oxygen, carbon dioxide levels. Low oxygen or high carbon dioxide will stimulate firing of the receptors.[52]

Exercise and fitness levels, age, body temperature, basal metabolic rate, and even a person's emotional state can all affect the heart rate. High levels of the hormones epinephrine, norepinephrine, and thyroid hormones can increase the heart rate. The levels of electrolytes including calcium, potassium, and sodium can also influence the speed and regularity of the heart rate; low blood oxygen, low blood pressure and dehydration may increase it.[7]
$
5
Question: What receptors sense the stretching of blood vessels?
A: Chemoreceptors
B: Thermoreceptors
C: Photoreceptors
D: Baroreceptors
E: Nociceptors
Answer: D

Question: Which reflex is associated with varying rates of blood flow to the atria?
A: Sinus reflex
B: Vagus reflex
C: Bainbridge reflex
D: Baroreceptor reflex
E: Aortic reflex
Answer: C

Question: Which of the following factors can influence the heart rate?
A: Diet
B: Basal metabolic rate
C: Blood type
D: Lung capacity
E: Height
Answer: B

Question: Chemoreceptors in the carotid body respond to which elements in the blood?
A: Glucose and insulin levels
B: Oxygen and carbon dioxide levels
C: Platelet count
D: Sodium and potassium levels
E: Cholesterol levels
Answer: B

Question: What can high levels of electrolytes including calcium, potassium, and sodium influence in the heart?
A: Size
B: Shape
C: Regeneration
D: Speed and regularity of heart rate
E: Color
Answer: D
@
Typically, healthy hearts have only two audible heart sounds, called S1 and S2. The first heart sound S1, is the sound created by the closing of the atrioventricular valves during ventricular contraction and is normally described as "lub". The second heart sound, S2, is the sound of the semilunar valves closing during ventricular diastole and is described as "dub".[7] Each sound consists of two components, reflecting the slight difference in time as the two valves close.[78] S2 may split into two distinct sounds, either as a result of inspiration or different valvular or cardiac problems.[78] Additional heart sounds may also be present and these give rise to gallop rhythms. A third heart sound, S3 usually indicates an increase in ventricular blood volume. A fourth heart sound S4 is referred to as an atrial gallop and is produced by the sound of blood being forced into a stiff ventricle. The combined presence of S3 and S4 give a quadruple gallop.[7] Heart murmurs are abnormal heart sounds which can be either related to disease or benign, and there are several kinds.[79] There are normally two heart sounds, and abnormal heart sounds can either be extra sounds, or "murmurs" related to the flow of blood between the sounds. Murmurs are graded by volume, from 1 (the quietest), to 6 (the loudest), and evaluated by their relationship to the heart sounds, position in the cardiac cycle, and additional features such as their radiation to other sites, changes with a person's position, the frequency of the sound as determined by the side of the stethoscope by which they are heard, and site at which they are heard loudest.[79] Murmurs may be caused by damaged heart valves or congenital heart disease such as ventricular septal defects, or may be heard in normal hearts. A different type of sound, a pericardial friction rub can be heard in cases of pericarditis where the inflamed membranes can rub together.
$
5
Question: What is the sound created by the closing of the atrioventricular valves during ventricular contraction described as?
A: Dub
B: Tack
C: Click
D: Whistle
E: Lub
Answer: E

Question: Which heart sound indicates an increase in ventricular blood volume?
A: S1
B: S2
C: S3
D: S4
E: S5
Answer: C

Question: Heart murmurs are graded by volume from:
A: 1 to 4
B: 1 to 5
C: 1 to 6
D: 1 to 7
E: 1 to 8
Answer: C

Question: A pericardial friction rub sound can be heard in cases of:
A: Myocardial infarction
B: Ventricular fibrillation
C: Angina pectoris
D: Pericarditis
E: Atrial flutter
Answer: D

Question: What sound is produced by the semilunar valves closing during ventricular diastole?
A: S1
B: S2
C: S3
D: S4
E: Murmur
Answer: B
@
Electrocardiography is the process of producing an electrocardiogram (ECG or EKG[a]), a recording of the heart's electrical activity through repeated cardiac cycles.[4] It is an electrogram of the heart which is a graph of voltage versus time of the electrical activity of the heart[5] using electrodes placed on the skin. These electrodes detect the small electrical changes that are a consequence of cardiac muscle depolarization followed by repolarization during each cardiac cycle (heartbeat). Changes in the normal ECG pattern occur in numerous cardiac abnormalities, including cardiac rhythm disturbances (such as atrial fibrillation[6] and ventricular tachycardia[7]), inadequate coronary artery blood flow (such as myocardial ischemia[8] and myocardial infarction[9]), and electrolyte disturbances (such as hypokalemia[10] and hyperkalemia[11]).

Traditionally, "ECG" usually means a 12-lead ECG taken while lying down as discussed below. However, other devices can record the electrical activity of the heart such as a Holter monitor but also some models of smartwatch are capable of recording an ECG. ECG signals can be recorded in other contexts with other devices.

In a conventional 12-lead ECG, ten electrodes are placed on the patient's limbs and on the surface of the chest. The overall magnitude of the heart's electrical potential is then measured from twelve different angles ("leads") and is recorded over a period of time (usually ten seconds). In this way, the overall magnitude and direction of the heart's electrical depolarization is captured at each moment throughout the cardiac cycle.[12]
$
5
Question 1: Which process is used to record the heart's electrical activity?
A: Echocardiogram
B: Electroencephalogram
C: Electromyography
D: Electrocardiogram
E: Electrocorticography
Answer: D

Question 2: What can changes in the normal ECG pattern indicate?
A: Liver dysfunction
B: Kidney failure
C: Cardiac abnormalities
D: Brain activity
E: Muscle tension
Answer: C

Question 3: Where are electrodes for ECG traditionally placed?
A: Inside the heart
B: On the brain
C: On the skin
D: In the spinal cord
E: On the muscles
Answer: C

Question 4: How long is the recording duration for a typical 12-lead ECG?
A: One minute
B: Five seconds
C: Half a minute
D: Twenty seconds
E: Ten seconds
Answer: E

Question 5: Which device is also capable of recording the electrical activity of the heart, besides an ECG machine?
A: Blood pressure monitor
B: Smartwatch
C: Thermometer
D: Spirometer
E: Treadmill
Answer: B
@
There are three main components to an ECG: the P wave, which represents depolarization of the atria; the QRS complex, which represents depolarization of the ventricles; and the T wave, which represents repolarization of the ventricles.[13]

During each heartbeat, a healthy heart has an orderly progression of depolarization that starts with pacemaker cells in the sinoatrial node, spreads throughout the atrium, and passes through the atrioventricular node down into the bundle of His and into the Purkinje fibers, spreading down and to the left throughout the ventricles.[13] This orderly pattern of depolarization gives rise to the characteristic ECG tracing. To the trained clinician, an ECG conveys a large amount of information about the structure of the heart and the function of its electrical conduction system.[14] Among other things, an ECG can be used to measure the rate and rhythm of heartbeats, the size and position of the heart chambers, the presence of any damage to the heart's muscle cells or conduction system, the effects of heart drugs, and the function of implanted pacemakers.[15]
$
5
Question 6: What does the P wave in an ECG represent?
A: Repolarization of ventricles
B: Depolarization of the atria
C: Damage to the heart's muscle cells
D: Ventricular tachycardia
E: Size of heart chambers
Answer: B

Question 7: Which component of an ECG represents repolarization of the ventricles?
A: P wave
B: QRS complex
C: T wave
D: U wave
E: R wave
Answer: C

Question 8: What starts the orderly progression of depolarization in a healthy heart?
A: Atrioventricular node
B: Bundle of His
C: Purkinje fibers
D: Sinoatrial node
E: Ventricles
Answer: D

Question 9: What does a characteristic ECG tracing provide information about?
A: Blood pressure
B: Breathing pattern
C: The structure and function of the heart
D: Liver function
E: Kidney function
Answer: C

Question 10: Among the things ECG can measure is:
A: Oxygen level in the blood
B: Amount of glucose in the blood
C: The function of implanted pacemakers
D: The lung's capacity
E: The brain's electrical activity
Answer: C
@
The overall goal of performing an ECG is to obtain information about the electrical functioning of the heart. Medical uses for this information are varied and often need to be combined with knowledge of the structure of the heart and physical examination signs to be interpreted. Some indications for performing an ECG include the following:

Chest pain or suspected myocardial infarction (heart attack), such as ST elevated myocardial infarction (STEMI)[16] or non-ST elevated myocardial infarction (NSTEMI)[17]
Symptoms such as shortness of breath, murmurs,[18] fainting, seizures, funny turns, or arrhythmias including new onset palpitations or monitoring of known cardiac arrhythmias
Medication monitoring (e.g., drug-induced QT prolongation, Digoxin toxicity) and management of overdose (e.g., tricyclic overdose)
Electrolyte abnormalities, such as hyperkalemia
Perioperative monitoring in which any form of anesthesia is involved (e.g., monitored anesthesia care, general anesthesia). This includes preoperative assessment and intraoperative and postoperative monitoring.
Cardiac stress testing
Computed tomography angiography (CTA) and magnetic resonance angiography (MRA) of the heart (ECG is used to "gate" the scanning so that the anatomical position of the heart is steady)
Clinical cardiac electrophysiology, in which a catheter is inserted through the femoral vein and can have several electrodes along its length to record the direction of electrical activity from within the heart.
ECGs can be recorded as short intermittent tracings or continuous ECG monitoring. Continuous monitoring is used for critically ill patients, patients undergoing general anesthesia,[19][18] and patients who have an infrequently occurring cardiac arrhythmia that would unlikely be seen on a conventional ten-second ECG. Continuous monitoring can be conducted by using Holter monitors, internal and external defibrillators and pacemakers, and/or biotelemetry.[20]
$
5
Question 11: An ECG can be used to detect which of the following cardiac issues?
A: Bone fractures
B: Lung infections
C: ST elevated myocardial infarction
D: High blood sugar
E: Asthma
Answer: C

Question 12: What might be an indication for performing an ECG?
A: Headache
B: Fever
C: Murmurs
D: Joint pain
E: Skin rash
Answer: C

Question 13: For which purpose is continuous ECG monitoring used?
A: For patients with frequently occurring cardiac arrhythmias
B: For patients with digestive problems
C: For patients with broken bones
D: For patients with respiratory infections
E: For patients with skin allergies
Answer: A

Question 14: In what scenario is an ECG used to "gate" the scanning?
A: X-ray
B: Ultrasound
C: CT angiography of the heart
D: Dental check-up
E: Bone density test
Answer: C

Question 15: Which device can be used for continuous ECG monitoring?
A: MRI machine
B: Nebulizer
C: Holter monitor
D: Pulse oximeter
E: Sphygmomanometer
Answer: C
@
Electrodes are the actual conductive pads attached to the body surface.[32] Any pair of electrodes can measure the electrical potential difference between the two corresponding locations of attachment. Such a pair forms a lead. However, "leads" can also be formed between a physical electrode and a virtual electrode, known as Wilson's central terminal (WCT), whose potential is defined as the average potential measured by three limb electrodes that are attached to the right arm, the left arm, and the left foot, respectively.[33]

Commonly, 10 electrodes attached to the body are used to form 12 ECG leads, with each lead measuring a specific electrical potential difference (as listed in the table below).[34]

Leads are broken down into three types: limb; augmented limb; and precordial or chest. The 12-lead ECG has a total of three limb leads and three augmented limb leads arranged like spokes of a wheel in the coronal plane (vertical), and six precordial leads or chest leads that lie on the perpendicular transverse plane (horizontal).[35]

In medical settings, the term leads is also sometimes used to refer to the electrodes themselves, although this is technically incorrect.[36]

The 10 electrodes in a 12-lead ECG are listed below.[37]

Electrode name	Electrode placement
RA	On the right arm, avoiding thick muscle.
LA	In the same location where RA was placed, but on the left arm.
RL	On the right leg, lower end of inner aspect of calf muscle. (Avoid bony prominences)
LL	In the same location where RL was placed, but on the left leg.
V1	In the fourth intercostal space (between ribs 4 and 5) just to the right of the sternum (breastbone)
V2	In the fourth intercostal space (between ribs 4 and 5) just to the left of the sternum.
V3	Between leads V2 and V4.
V4	In the fifth intercostal space (between ribs 5 and 6) in the mid-clavicular line.
V5	Horizontally even with V4, in the left anterior axillary line.
V6	Horizontally even with V4 and V5 in the mid-axillary line.
$
5
Question 16: How many electrodes are typically used in a 12-lead ECG?
A: 6
B: 8
C: 10
D: 12
E: 14
Answer: C

Question 17: What is the correct placement for the V1 electrode?
A: In the mid-clavicular line
B: In the left anterior axillary line
C: On the left leg
D: In the fourth intercostal space just to the right of the sternum
E: Horizontally even with V4 and V5 in the mid-axillary line
Answer: D

Question 18: Which lead measures the electrical potential between the right arm and Wilson's central terminal?
A: Limb lead
B: Augmented limb lead
C: Precordial lead
D: Chest lead
E: Transverse plane lead
Answer: B

Question 19: Where is the RA electrode placed for an ECG?
A: On the left arm
B: On the right leg
C: On the right arm
D: In the left anterior axillary line
E: In the mid-clavicular line
Answer: C

Question 20: How are the 12 leads of an ECG divided in terms of types?
A: 2 limb leads; 3 augmented limb leads; 7 precordial
B: 4 limb leads; 4 augmented limb leads; 4 precordial
C: 3 limb leads; 3 augmented limb leads; 6 precordial
D: 5 limb leads; 2 augmented limb leads; 5 precordial
E: 6 limb leads; 3 augmented limb leads; 3 precordial
Answer: C
@
An artificial cardiac pacemaker (artificial pacemaker, and sometimes just pacemaker, although the term is also used to refer to the body's natural cardiac pacemaker) is a medical device, nowadays always implanted, that generates electrical pulses delivered by electrodes to one or more of the chambers of the heart, the upper atria or lower ventricles. Each pulse causes the targeted chamber(s) to contract and pump blood,[3] thus regulating the function of the electrical conduction system of the heart.

The primary purpose of a pacemaker is to maintain an adequate heart rate, either because the heart's natural pacemaker is not fast enough, or because there is a block in the heart's electrical conduction system. Modern pacemakers are externally programmable and allow a cardiologist, particularly a cardiac electrophysiologist, to select the optimal pacing modes for individual patients. Most pacemakers are on demand, in which the stimulation of the heart is based on the dynamic demand of the circulatory system. Others send out a fixed rate of impulses.[4]
$
5
Question: Which medical device generates electrical pulses to regulate the function of the heart's electrical conduction system?
A: Cardiac ultrasound
B: Electrocardiogram
C: Artificial cardiac pacemaker
D: Heart rate monitor
E: Blood pressure machine
Answer: C

Question: What is the primary function of a pacemaker?
A: Measuring blood pressure
B: Visualizing internal cardiac structures
C: Monitoring cholesterol levels
D: Maintaining an adequate heart rate
E: Regulating body temperature
Answer: D

Question: Who usually programs modern pacemakers for optimal pacing modes?
A: General practitioner
B: Radiologist
C: Cardiac electrophysiologist
D: Neurologist
E: Pulmonologist
Answer: C

Question: How does an "on demand" pacemaker function?
A: It stimulates the heart at a fixed rate.
B: It adjusts based on the patient's mood.
C: It stimulates the heart based on the dynamic demand of the circulatory system.
D: It stimulates the heart only during physical activity.
E: It monitors and regulates body temperature.
Answer: C

Question: Which of the following is NOT a purpose of a pacemaker?
A: Assisting in heart valve function
B: Regulating the function of the electrical conduction system of the heart
C: Maintaining an adequate heart rate
D: Compensating for a block in the heart's electrical conduction system
E: Delivering electrical pulses to the heart's chambers
Answer: A
@
Permanent pacing with an implantable pacemaker involves transvenous placement of one or more pacing electrodes within a chamber, or chambers, of the heart, while the pacemaker is implanted under the skin below the clavicle. The procedure is performed by incision of a suitable vein into which the electrode lead is inserted and passed along the vein, through the valve of the heart, until positioned in the chamber. The procedure is facilitated by fluoroscopy which enables the physician to view the passage of the electrode lead. After satisfactory lodgement of the electrode is confirmed, the opposite end of the electrode lead is connected to the pacemaker generator.

There are three basic types of permanent pacemakers, classified according to the number of chambers involved and their basic operating mechanism:[10]

Single-chamber pacemaker. In this type, only one pacing lead is placed into a chamber of the heart, either the atrium or the ventricle.[10]
Dual-chamber pacemaker. Here, wires are placed in two chambers of the heart. One lead paces the atrium and one paces the ventricle. This type more closely resembles the natural pacing of the heart by assisting the heart in coordinating the function between the atria and ventricles.[10]
Biventricular pacemaker. This pacemaker has three wires placed in three chambers of the heart. One in the atrium and two in either ventricle. It is more complicated to implant.[10]
Rate-responsive pacemaker. This pacemaker has sensors that detect changes in the patient's physical activity and automatically adjust the pacing rate to fulfill the body's metabolic needs.[10]
The pacemaker generator is a hermetically sealed device containing a power source, usually a lithium battery, a sensing amplifier which processes the electrical manifestation of naturally occurring heart beats as sensed by the heart electrodes, the computer logic for the pacemaker and the output circuitry which delivers the pacing impulse to the electrodes.

Most commonly, the generator is placed below the subcutaneous fat of the chest wall, above the muscles and bones of the chest. However, the placement may vary on a case-by-case basis.

The outer casing of pacemakers is so designed that it will rarely be rejected by the body's immune system. It is usually made of titanium, which is inert in the body.
$
5
Question: Where is the pacemaker usually implanted?
A: Within the heart
B: Under the skin below the clavicle
C: Within the brain
D: In the abdominal region
E: At the base of the spine
Answer: B

Question: What type of pacemaker uses wires placed in two chambers of the heart?
A: Single-chamber pacemaker
B: Dual-chamber pacemaker
C: Biventricular pacemaker
D: Rate-responsive pacemaker
E: Dynamic pacemaker
Answer: B

Question: The outer casing of most pacemakers is typically made of which material?
A: Aluminum
B: Plastic
C: Stainless steel
D: Titanium
E: Copper
Answer: D

Question: What helps the physician view the passage of the electrode lead during implantation?
A: MRI
B: Ultrasound
C: Stethoscope
D: Fluoroscopy
E: X-ray
Answer: D

Question: Which pacemaker adjusts the pacing rate based on changes in the patient's physical activity?
A: Single-chamber pacemaker
B: Dual-chamber pacemaker
C: Biventricular pacemaker
D: Rate-responsive pacemaker
E: Dynamic pacemaker
Answer: D
@
A major step forward in pacemaker function has been to attempt to mimic nature by utilizing various inputs to produce a rate-responsive pacemaker using parameters such as the QT interval, pO2 – pCO2 (dissolved oxygen or carbon dioxide levels) in the arterial-venous system, physical activity as determined by an accelerometer, body temperature, ATP levels, adrenaline, etc. Instead of producing a static, predetermined heart rate, or intermittent control, such a pacemaker, a 'Dynamic Pacemaker', could compensate for both actual respiratory loading and potentially anticipated respiratory loading. The first dynamic pacemaker was invented by Anthony Rickards of the National Heart Hospital, London, UK, in 1982.[23]

Dynamic pacemaking technology could also be applied to future artificial hearts. Advances in transitional tissue welding would support this and other artificial organ/joint/tissue replacement efforts. Stem cells may be of interest in transitional tissue welding.[citation needed]

Many advancements have been made to improve the control of the pacemaker once implanted. Many of these have been made possible by the transition to microprocessor controlled pacemakers. Pacemakers that control not only the ventricles but the atria as well have become common. Pacemakers that control both the atria and ventricles are called dual-chamber pacemakers. Although these dual-chamber models are usually more expensive, timing the contractions of the atria to precede that of the ventricles improves the pumping efficiency of the heart and can be useful in congestive heart failure.
$
5
Question: Who invented the first dynamic pacemaker?
A: Michael Faraday
B: Marie Curie
C: Nikola Tesla
D: Benjamin Franklin
E: Anthony Rickards
Answer: E

Question: Which pacemaker can compensate for both actual and potentially anticipated respiratory loading?
A: Dual-chamber pacemaker
B: Static pacemaker
C: Dynamic pacemaker
D: Biventricular pacemaker
E: Rate-responsive pacemaker
Answer: C

Question: Dual-chamber pacemakers control both the _____ and ______.
A: Atria, lungs
B: Ventricles, lungs
C: Atria, ventricles
D: Arteries, veins
E: Left atrium, right ventricle
Answer: C

Question: What has allowed for improvements in pacemaker control once implanted?
A: Larger size
B: Transition to microprocessor control
C: Use of stronger metals
D: Use of radioactive materials
E: Manual adjustments
Answer: B

Question: What type of pacemaker helps improve the pumping efficiency of the heart in cases like congestive heart failure?
A: Single-chamber pacemaker
B: Static pacemaker
C: Rate-responsive pacemaker
D: Biventricular pacemaker
E: Dual-chamber pacemaker
Answer: E
@
A pacemaker may be implanted whilst a person is awake using local anesthetic to numb the skin with or without sedation, or asleep using a general anesthetic.[25] An antibiotic is usually given to reduce the risk of infection.[25] Pacemakers are generally implanted in the front of the chest in the region of the left or right shoulder. The skin is prepared by clipping or shaving any hair over the implant site before cleaning the skin with a disinfectant such as chlorhexidine. An incision is made below the collar bone and a space or pocket is created under the skin to house the pacemaker generator. This pocket is usually created just above the pectoralis major muscle (prepectoral), but in some cases the device may be inserted beneath the muscle (submuscular).[26] The lead or leads are fed into the heart through a large vein guided by X-ray imaging (fluoroscopy). The tips of the leads may be positioned within the right ventricle, the right atrium, or the coronary sinus, depending on the type of pacemaker required.[25] Surgery is typically completed within 30 to 90 minutes. Following implantation, the surgical wound should be kept clean and dry until it has healed. Some movements of the shoulder within a few weeks of insertion carry a risk of dislodging the pacemaker leads.[25]

The batteries within a pacemaker generator typically last 5 to 10 years. When the batteries are nearing the end of life, the generator is replaced in a procedure that is usually simpler than a new implant. Replacement involves making an incision to remove the existing device, disconnecting the leads from the old device and reconnecting them to a new generator, reinserting the new device and closing the skin.[25]
$
5
Question: How long do the batteries in a pacemaker generator typically last?
A: 6 months to 1 year
B: 1 to 3 years
C: 5 to 10 years
D: 15 to 20 years
E: Over 25 years
Answer: C

Question: During pacemaker implantation, where is the lead or leads typically positioned?
A: Within the left ventricle
B: Within the coronary sinus
C: Outside the heart muscle
D: Within the pulmonary artery
E: On the outer surface of the heart
Answer: B

Question: What is used to reduce the risk of infection during pacemaker implantation?
A: Radio waves
B: Fluoroscopy
C: Chlorhexidine
D: General anesthesia
E: Lithium batteries
Answer: C

Question: How long does a typical pacemaker implantation surgery last?
A: Less than 10 minutes
B: 30 to 90 minutes
C: 3 to 4 hours
D: 5 to 6 hours
E: Over 8 hours
Answer: B

Question: Where is the pacemaker generator typically implanted?
A: Just above the pectoralis major muscle
B: Within the heart itself
C: In the abdominal region
D: Under the skin of the forearm
E: At the base of the neck
Answer: A
@
An accelerometer is a tool that measures proper acceleration.[1] Proper acceleration is the acceleration (the rate of change of velocity) of a body in its own instantaneous rest frame;[2] this is different from coordinate acceleration, which is acceleration in a fixed coordinate system. For example, an accelerometer at rest on the surface of the Earth will measure an acceleration due to Earth's gravity, straight upwards[3] (by definition) of g ≈ 9.81 m/s2. By contrast, accelerometers in free fall (falling toward the center of the Earth at a rate of about 9.81 m/s2) will measure zero.

Accelerometers have many uses in industry and science. Highly sensitive accelerometers are used in inertial navigation systems for aircraft and missiles. Vibration in rotating machines is monitored by accelerometers. They are used in tablet computers and digital cameras so that images on screens are always displayed upright. In unmanned aerial vehicles, accelerometers help to stabilise flight.
$
5
Question: What does an accelerometer measure?
A: Velocity
B: Direction of wind
C: Gravitational force
D: Proper acceleration
E: Mass
Answer: D

Question: What acceleration reading would an accelerometer in free fall display?
A: 9.81 m/s^2
B: 0 m/s^2
C: 1 m/s^2
D: 5.5 m/s^2
E: 2.5 m/s^2
Answer: B

Question: Which of the following applications does NOT utilize accelerometers?
A: Digital cameras
B: Tablet computers
C: Unmanned aerial vehicles
D: Radio frequency systems
E: Inertial navigation systems
Answer: D

Question: In which direction will an accelerometer at rest on the Earth's surface measure acceleration due to gravity?
A: Downwards
B: Sideways
C: In a circular path
D: Upwards
E: No direction
Answer: D

Question: What is the acceleration due to Earth's gravity that an accelerometer at rest on its surface would measure?
A: Approximately 4.91 m/s^2
B: Approximately 0 m/s^2
C: Approximately 9.81 m/s^2
D: Approximately 12.5 m/s^2
E: Approximately 7.2 m/s^2
Answer: C
@
When two or more accelerometers are coordinated with one another, they can measure differences in proper acceleration, particularly gravity, over their separation in space—that is, the gradient of the gravitational field. Gravity gradiometry is useful because absolute gravity is a weak effect and depends on the local density of the Earth, which is quite variable.

Single- and multi-axis accelerometers can detect both the magnitude and the direction of the proper acceleration, as a vector quantity, and can be used to sense orientation (because the direction of weight changes), coordinate acceleration, vibration, shock, and falling in a resistive medium (a case in which the proper acceleration changes, increasing from zero). Micromachined microelectromechanical systems (MEMS) accelerometers are increasingly present in portable electronic devices and video-game controllers, to detect changes in the positions of these devices.
$
5
Question: When coordinated accelerometers measure differences in proper acceleration, they particularly focus on what field gradient?
A: Electromagnetic
B: Thermal
C: Gravitational
D: Kinetic
E: Nuclear
Answer: C

Question: What do single- and multi-axis accelerometers detect?
A: Only the magnitude of proper acceleration
B: Only the direction of proper acceleration
C: Both magnitude and direction of proper acceleration
D: The rate of change of magnitude
E: The temperature change due to acceleration
Answer: C

Question: Which devices increasingly contain micromachined MEMS accelerometers?
A: Mechanical clocks
B: Radios
C: Portable electronic devices
D: Gas stoves
E: Washing machines
Answer: C

Question: How does gravity gradiometry become useful?
A: Because gravity is a strong effect
B: Because it depends on the atmosphere
C: Because it relies on the local density of the Moon
D: Because absolute gravity is a weak effect and depends on the local density of the Earth
E: None of the above
Answer: D

Question: Micromachined microelectromechanical systems (MEMS) accelerometers are increasingly found in which gaming tool?
A: Dice
B: Board games
C: Video-game controllers
D: Card games
E: Joysticks without sensors
Answer: C
@
A basic mechanical accelerometer is a damped proof mass on a spring. When the accelerometer experiences an acceleration, Newton's third law causes the spring's compression to adjust to exert an equivalent force on the mass to counteract the acceleration. Since the spring's force scales linearly with amount of compression (according to Hooke's law) and because the spring constant and mass are known constants, a measurement of the spring's compression is also a measurement of acceleration. The system is damped to prevent oscillations of the mass and spring interfering with measurements. However, the damping causes accelerometers to have a frequency response.

Many animals have sensory organs to detect acceleration, especially gravity. In these, the proof mass is usually one or more crystals of calcium carbonate otoliths (Latin for "ear stone") or statoconia, acting against a bed of hairs connected to neurons. The hairs form the springs, with the neurons as sensors. The damping is usually by a fluid. Many vertebrates, including humans, have these structures in their inner ears. Most invertebrates have similar organs, but not as part of their hearing organs. These are called statocysts.

Mechanical accelerometers are often designed so that an electronic circuit senses a small amount of motion, then pushes on the proof mass with some type of linear motor to keep the proof mass from moving far. The motor might be an electromagnet or in very small accelerometers, electrostatic. Since the circuit's electronic behavior can be carefully designed, and the proof mass does not move far, these designs can be very stable (i.e. they do not oscillate), very linear with a controlled frequency response. (This is called servo mode design.)
$
5
Question: What causes the spring's compression to adjust in a mechanical accelerometer?
A: Rotation of the device
B: External pressure
C: Internal vibrations
D: An experienced acceleration
E: Electrostatic force
Answer: D

Question: Which law states that the spring's force scales linearly with the amount of compression?
A: Newton's first law
B: Newton's second law
C: Hooke's law
D: Law of thermodynamics
E: Faraday's law
Answer: C

Question: In many animals, which crystals act against a bed of hairs connected to neurons for detecting acceleration?
A: Quartz
B: Otoliths
C: Rubies
D: Diamonds
E: Sapphires
Answer: B

Question: Mechanical accelerometers are designed so that an electronic circuit senses motion and does what to the proof mass?
A: Heats it up
B: Detaches it from the device
C: Pushes on it to prevent it from moving far
D: Reduces its weight
E: Cools it down
Answer: C

Question: The control mechanism in mechanical accelerometers that can be very stable and linear with a controlled frequency response is termed what?
A: Servo mode design
B: Dynamic response mode
C: Oscillation mode
D: Acceleration feedback loop
E: Spring mode design
Answer: A
@
Another MEMS-based accelerometer is a thermal (or convective) accelerometer.[7] It contains a small heater in a very small dome. This heats the air or other fluid inside the dome. The thermal bubble acts as the proof mass. An accompanying temperature sensor (like a thermistor; or thermopile) in the dome measures the temperature in one location of the dome. This measures the location of the heated bubble within the dome. When the dome is accelerated, the colder, higher density fluid pushes the heated bubble. The measured temperature changes. The temperature measurement is interpreted as acceleration. The fluid provides the damping. Gravity acting on the fluid provides the spring. Since the proof mass is very lightweight gas, and not held by a beam or lever, thermal accelerometers can survive high shocks. Another variation uses a wire to both heat the gas and detect the change in temperature. The change of temperature changes the resistance of the wire. A two dimensional accelerometer can be economically constructed with one dome, one bubble and two measurement devices.

Most micromechanical accelerometers operate in-plane, that is, they are designed to be sensitive only to a direction in the plane of the die. By integrating two devices perpendicularly on a single die a two-axis accelerometer can be made. By adding another out-of-plane device, three axes can be measured. Such a combination may have much lower misalignment error than three discrete models combined after packaging.

Micromechanical accelerometers are available in a wide variety of measuring ranges, reaching up to thousands of g's. The designer must compromise between sensitivity and the maximum acceleration that can be measured.
$
5
Question: In a thermal accelerometer, what acts as the proof mass?
A: A metal bead
B: A liquid droplet
C: A thermal bubble
D: A microchip
E: A quartz crystal
Answer: C

Question: A temperature sensor in the dome of a thermal accelerometer measures the location of what?
A: The heated fluid
B: The thermal bubble
C: The electronic circuit
D: The dome wall
E: The thermal plate
Answer: B

Question: How do most micromechanical accelerometers operate?
A: In a circular pattern
B: Only vertically
C: In-plane
D: Randomly
E: In three-dimensional space without any plane restrictions
Answer: C

Question: Micromechanical accelerometers can measure up to how many g's?
A: Hundreds
B: Tens
C: Thousands
D: Single-digit g's
E: Negligible g's
Answer: C

Question: The position of the batteries in micromechanical accelerometers is mostly determined by:
A: The cost of the device
B: The type of metal used in construction
C: The manufacturer's brand
D: The required measuring range
E: The type of liquid inside the accelerometer
Answer: D
@
An instrument used to measure gravity is known as a gravimeter. For a small body, general relativity predicts gravitational effects indistinguishable from the effects of acceleration by the equivalence principle. Thus, gravimeters can be regarded as special-purpose accelerometers. Many weighing scales may be regarded as simple gravimeters. In one common form, a spring is used to counteract the force of gravity pulling on an object. The change in length of the spring may be calibrated to the force required to balance the gravitational pull. The resulting measurement may be made in units of force (such as the newton), but is more commonly made in units of gals or cm/s2.

Researchers use more sophisticated gravimeters when precise measurements are needed. When measuring the Earth's gravitational field, measurements are made to the precision of microgals to find density variations in the rocks making up the Earth. Several types of gravimeters exist for making these measurements, including some that are essentially refined versions of the spring scale described above. These measurements are used to define gravity anomalies.
$
6
Question: What is the equivalence principle in general relativity related to in gravimeters?
A: Acceleration effects
B: Gravitational waves
C: Time dilation
D: Black holes
E: Quantum entanglement
Answer: A

Question: Which instrument is specifically designed to measure gravity?
A: Barometer
B: Accelerometer
C: Gravimeter
D: Seismometer
E: Thermometer
Answer: C

Question: In the context of small bodies, general relativity makes gravitational effects similar to which of the following?
A: Magnetism
B: Electricity
C: Acceleration
D: Friction
E: Light
Answer: C

Question: Many weighing scales can be considered as:
A: Thermometers
B: Simple gravimeters
C: Complex gravimeters
D: Inclinometers
E: Tachometers
Answer: B

Question: In a common type of gravimeter, which object is used to counteract the force of gravity?
A: Liquid
B: Magnet
C: Spring
D: Lever
E: Pulley
Answer: C

Question: Gravimeters that are specifically designed for research and are highly precise measure the Earth's gravitational field to the precision of:
A: Kilogals
B: Decimals
C: Microgals
D: Megagals
E: Macrogals
Answer: C
@
The majority of modern gravimeters use specially-designed metal or quartz zero-length springs to support the test mass. Zero-length springs do not follow Hooke's law; instead they have a force proportional to their length. The special property of these springs is that the natural resonant period of oscillation of the spring–mass system can be made very long – approaching a thousand seconds. This detunes the test mass from most local vibration and mechanical noise, increasing the sensitivity and utility of the gravimeter. Quartz and metal springs are chosen for different reasons; quartz springs are less affected by magnetic and electric fields while metal springs have a much lower drift (elongation) with time. The test mass is sealed in an air-tight container so that tiny changes of barometric pressure from blowing wind and other weather do not change the buoyancy of the test mass in air.

Spring gravimeters are, in practice, relative instruments which measure the difference in gravity between different locations. A relative instrument also requires calibration by comparing instrument readings taken at locations with known complete or absolute values of gravity. Absolute gravimeters provide such measurements by determining the gravitational acceleration of a test mass in vacuum. A test mass is allowed to fall freely inside a vacuum chamber and its position is measured with a laser interferometer and timed with an atomic clock. The laser wavelength is known to ±0.025 ppb and the clock is stable to ±0.03 ppb as well. Great care must be taken to minimize the effects of perturbing forces such as residual air resistance (even in vacuum), vibration, and magnetic forces. Such instruments are capable of an accuracy of about two parts per billion or 0.002 mGal[1] and reference their measurement to atomic standards of length and time. Their primary use is for calibrating relative instruments, monitoring crustal deformation, and in geophysical studies requiring high accuracy and stability. However, absolute instruments are somewhat larger and significantly more expensive than relative spring gravimeters, and are thus relatively rare.
$
5
Question: What do zero-length springs not adhere to?
A: Newton's First Law
B: Pascal's Principle
C: Hooke's Law
D: Bernoulli's Principle
E: Avogadro's Law
Answer: C

Question: Why are quartz springs preferred in some gravimeters?
A: They are magnetic
B: They are more affected by electric fields
C: They are less affected by magnetic and electric fields
D: They have a higher drift with time
E: They are more flexible
Answer: C

Question: Spring gravimeters are essentially what type of instruments?
A: Absolute
B: Relative
C: Qualitative
D: Quantitative
E: Derivative
Answer: B

Question: In absolute gravimeters, what measures the position of a test mass?
A: Ultrasound
B: X-rays
C: Laser interferometer
D: Thermometer
E: Atomic scale
Answer: C

Question: Absolute gravimeters primarily serve all the following purposes EXCEPT:
A: Monitoring crustal deformation
B: Calibrating relative instruments
C: Powering electronic devices
D: Geophysical studies requiring high accuracy and stability
E: Referencing their measurement to atomic standards
Answer: C
@
The most common gravimeters are spring-based. They are used in gravity surveys over large areas for establishing the figure of the geoid over those areas. They are basically a weight on a spring, and by measuring the amount by which the weight stretches the spring, local gravity can be measured. However, the strength of the spring must be calibrated by placing the instrument in a location with a known gravitational acceleration.[5]

The current standard for sensitive gravimeters are the superconducting gravimeters, which operate by suspending a superconducting niobium sphere in an extremely stable magnetic field; the current required to generate the magnetic field that suspends the niobium sphere is proportional to the strength of the Earth's gravitational acceleration.[6] The superconducting gravimeter achieves sensitivities of 10–11 m·s−2 (one nanogal), approximately one trillionth (10−12) of the Earth surface gravity. In a demonstration of the sensitivity of the superconducting gravimeter, Virtanen (2006),[7] describes how an instrument at Metsähovi, Finland, detected the gradual increase in surface gravity as workmen cleared snow from its laboratory roof.

The largest component of the signal recorded by a superconducting gravimeter is the tidal gravity of the sun and moon acting at the station. This is roughly ±1000 
nm
/
s2
 (nanometers per second squared) at most locations. "SGs", as they are called, can detect and characterize earth tides, changes in the density of the atmosphere, the effect of changes in the shape of the surface of the ocean, the effect of the atmosphere's pressure on the earth, changes in the rate of rotation of the earth, oscillations of the earth's core, distant and nearby seismic events, and more.
$
5
Question: Which gravimeters are considered the most common?
A: Satellite-based
B: Spring-based
C: Superconducting
D: Thermal
E: Magnetic
Answer: B

Question: How does the superconducting gravimeter determine the strength of the Earth's gravitational acceleration?
A: By suspending a superconducting niobium sphere in a stable magnetic field
B: Using a series of springs and levers
C: Through laser measurements
D: Based on satellite imagery
E: Using a combination of thermistors
Answer: A

Question: Approximately what sensitivity does a superconducting gravimeter achieve?
A: 10 nm·s−2
B: 10–11 m·s−2
C: 10 μm·s−2
D: 10–9 m·s−2
E: 10 mm·s−2
Answer: B

Question: What is the largest component of the signal recorded by a superconducting gravimeter?
A: Earth's magnetic field
B: Solar wind
C: Tidal gravity of the sun and moon
D: Atmospheric pressure
E: Seismic activities
Answer: C

Question: "SGs" are able to detect all of the following EXCEPT:
A: Earth tides
B: Oscillations of the earth's core
C: Distant seismic events
D: Changes in the Earth's magnetic field
E: Changes in the density of the atmosphere
Answer: D
@
Currently, the static and time-variable Earth's gravity field parameters are being determined using modern satellite missions, such as GOCE, CHAMP, Swarm, GRACE and GRACE-FO.[10][11] The lowest-degree parameters, including the Earth's oblateness and geocenter motion are best determined from Satellite laser ranging.[12]

Large-scale gravity anomalies can be detected from space, as a by-product of satellite gravity missions, e.g., GOCE. These satellite missions aim at the recovery of a detailed gravity field model of the Earth, typically presented in the form of a spherical-harmonic expansion of the Earth's gravitational potential, but alternative presentations, such as maps of geoid undulations or gravity anomalies, are also produced.

The Gravity Recovery and Climate Experiment (GRACE) consists of two satellites that can detect gravitational changes across the Earth. Also these changes can be presented as gravity anomaly temporal variations. The Gravity Recovery and Interior Laboratory (GRAIL) also consisted of two spacecraft orbiting the Moon, which orbited for three years before their deorbit in 2015.
$
5
Question: Which satellite mission aims at determining the Earth's gravity field parameters by focusing on its oblateness and geocenter motion?
A: GOCE
B: CHAMP
C: GRACE
D: GRAIL
E: Satellite laser ranging
Answer: E

Question: What is one of the primary products of satellite gravity missions like GOCE?
A: Detailed wind patterns
B: Maps of geoid undulations or gravity anomalies
C: Measurement of the ozone layer
D: High-resolution images of the Earth's surface
E: Deep space communication signals
Answer: B

Question: The Gravity Recovery and Climate Experiment (GRACE) consists of how many satellites?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: B

Question: GRAIL was a mission concerning which celestial body?
A: Mars
B: Venus
C: Jupiter
D: The Moon
E: The Sun
Answer: D

Question: What is the purpose of satellite missions such as GOCE, CHAMP, and Swarm?
A: To study space weather phenomena
B: To determine the Earth's gravity field parameters
C: To monitor global temperature changes
D: To study extraterrestrial life
E: To map the surface of other planets
Answer: B
@
A heart valve is a one-way valve that allows blood to flow in one direction through the chambers of the heart. Four valves are usually present in a mammalian heart and together they determine the pathway of blood flow through the heart. A heart valve opens or closes according to differential blood pressure on each side.[1][2][3]

The heart valves and the chambers are lined with endocardium. Heart valves separate the atria from the ventricles, or the ventricles from a blood vessel. Heart valves are situated around the fibrous rings of the cardiac skeleton. The valves incorporate flaps called leaflets or cusps, similar to a duckbill valve or flutter valve, which are pushed open to allow blood flow and which then close together to seal and prevent backflow. The mitral valve has two cusps, whereas the others have three. There are nodules at the tips of the cusps that make the seal tighter.

The pulmonary valve has left, right, and anterior cusps.[4] The aortic valve has left, right, and posterior cusps.[5] The tricuspid valve has anterior, posterior, and septal cusps; and the mitral valve has just anterior and posterior cusps.

The valves of the human heart can be grouped in two sets:[6]

Two atrioventricular valves to prevent backflow of blood from the ventricles into the atria:
Tricuspid valve or right atrioventricular valve, between the right atrium and right ventricle
Mitral valve or bicuspid valve, between the left atrium and left ventricle
Two semilunar valves to prevent the backflow of blood into the ventricle:
Pulmonary valve, located at the opening between the right ventricle and the pulmonary trunk
Aortic valve, located at the opening between the left ventricle and the aorta.
$
5
Question: What is the primary function of a heart valve in the chambers of the heart?
A: To prevent backflow of blood.
B: To produce blood cells.
C: To supply oxygen to the heart tissue.
D: To store blood for future use.
E: To regulate the heart rate.
Answer: A

Question: What are the flaps on heart valves called?
A: Arteries
B: Vessels
C: Capillaries
D: Leaflets or cusps
E: Nodes
Answer: D

Question: How many cusps does the mitral valve have?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: B

Question: Which heart valve is situated between the right atrium and right ventricle?
A: Aortic valve
B: Pulmonary valve
C: Tricuspid valve
D: Mitral valve
E: Septal valve
Answer: C

Question: What prevents the backflow of blood into the left ventricle from the aorta?
A: Mitral valve
B: Pulmonary valve
C: Aortic valve
D: Tricuspid valve
E: Septal valve
Answer: C
@
In general, the motion of the heart valves is determined using the Navier–Stokes equation, using boundary conditions of the blood pressures, pericardial fluid, and external loading as the constraints. The motion of the heart valves is used as a boundary condition in the Navier–Stokes equation in determining the fluid dynamics of blood ejection from the left and right ventricles into the aorta and the lung.

Relationship between pressure and flow in open valves
The pressure drop, 
Δ
�
 {\Delta}p , across an open heart valve relates to the flow rate, Q, through the valve:

�
∂
�
∂
�
+
�
�
2
=
Δ
�
 a{{\partial}Q\over{\partial}t} + bQ^2 = {\Delta}p 
If:

Inflow energy conserved
Stagnant region behind leaflets
Outflow momentum conserved
Flat velocity profile

Valves with a single degree of freedom
Usually, the aortic and mitral valves are incorporated in valve studies within a single degree of freedom. These relationships are based on the idea of the valve being a structure with a single degree of freedom. These relationships are based on the Euler equations.

Equations for the aortic valve in this case:

�
(
∂
�
∂
�
+
�
∂
�
∂
�
)
+
∂
�
∂
�
=
0
 {\rho}\left({{\partial}u\over{\partial}t} + {u{\partial}u\over{\partial}x}\right) + {{\partial}p\over{\partial}x} = 0
∂
�
∂
�
+
∂
∂
�
(
�
�
)
=
0
 {{\partial}A\over{\partial}t} + {{\partial}\over{\partial}x}(Au) = 0
�
(
�
,
�
)
=
�
0
(
1
−
[
1
−
Λ
(
�
)
]
�
�
)
2
 A(x,t) = A_0 \left(1-[1-{\Lambda}(t)]{x\over{L}}\right)^2 
∫
0
�
�
(
�
,
�
)
∂
�
∂
�
�
�
=
[
�
0
−
�
(
�
,
�
)
]
�
(
�
,
�
)
 \int_{0}^{L} p(x,t) {{\partial}A\over{\partial}x}\, dx = [A_0 - A(L,t)] \, p(L,t) 
where:

u = axial velocity
p = pressure
A = cross sectional area of valve
L = axial length of valve
Λ(t) = single degree of freedom; when
Λ
2
(
�
)
=
�
(
�
,
�
)
�
0
 \Lambda^2 (t) = {A(L,t)\over A_0} 
Atrioventricular valve
$
5
Question: What equation determines the motion of the heart valves?
A: Einstein's equation
B: Heisenberg's uncertainty principle
C: Navier–Stokes equation
D: Pythagoras theorem
E: Newton's third law
Answer: C

Question: What does the equation Δp represent in relation to an open heart valve?
A: Heart rate
B: Blood volume
C: Pressure drop
D: Valve diameter
E: Blood viscosity
Answer: C

Question: What is conserved in the outflow behind leaflets?
A: Blood volume
B: Momentum
C: Energy
D: Valve integrity
E: Heat
Answer: B

Question: Which heart valves are often represented with a single degree of freedom in valve studies?
A: Pulmonary and Aortic
B: Tricuspid and Pulmonary
C: Mitral and Aortic
D: Tricuspid and Mitral
E: Aortic and Septal
Answer: C

Question: In the provided equations, what does the symbol "u" represent?
A: Pressure
B: Axial length of valve
C: Cross-sectional area of valve
D: Axial velocity
E: Time
Answer: D
@
Valvular heart disease is a general term referring to dysfunction of the valves, and is primarily in two forms, either regurgitation, (also insufficiency, or incompetence) where a dysfunctional valve lets blood flow in the wrong direction,[10] or stenosis, when a valve is narrow.[11]

Regurgitation occurs when a valve becomes insufficient and malfunctions, allowing some blood to flow in the wrong direction. This insufficiency can affect any of the valves as in aortic insufficiency, mitral insufficiency, pulmonary insufficiency and tricuspid insufficiency. The other form of valvular heart disease is stenosis, a narrowing of the valve. This is a result of the valve becoming thickened and any of the heart valves can be affected, as in mitral valve stenosis, tricuspid valve stenosis, pulmonary valve stenosis and aortic valve stenosis. Stenosis of the mitral valve is a common complication of rheumatic fever. Inflammation of the valves can be caused by infective endocarditis, usually a bacterial infection but can sometimes be caused by other organisms. Bacteria can more readily attach to damaged valves.[12] Another type of endocarditis which doesn't provoke an inflammatory response, is nonbacterial thrombotic endocarditis. This is commonly found on previously undamaged valves.[12] A major valvular heart disease is mitral valve prolapse, which is a weakening of connective tissue called myxomatous degeneration of the valve. This sees the displacement of a thickened mitral valve cusp into the left atrium during systole.[11]
$
5
Question: What is the consequence of regurgitation in heart valves?
A: Blood flows faster.
B: Blood flows in the correct direction.
C: Blood flows in the wrong direction.
D: Blood pressure decreases.
E: Blood volume decreases.
Answer: C

Question: Which disease is a common complication of rheumatic fever?
A: Mitral valve prolapse
B: Aortic insufficiency
C: Pulmonary insufficiency
D: Mitral valve stenosis
E: Tricuspid insufficiency
Answer: D

Question: What is the result when bacteria attach to damaged valves?
A: Bacterial endocarditis
B: Nonbacterial thrombotic endocarditis
C: Valvular heart disease
D: Mitral valve prolapse
E: Rheumatic fever
Answer: A

Question: What causes the thickened mitral valve cusp to displace into the left atrium during systole?
A: Aortic stenosis
B: Bicuspid aortic valve
C: Myxomatous degeneration of the valve
D: Infective endocarditis
E: Rheumatic fever
Answer: C

Question: Which condition doesn't provoke an inflammatory response on the heart valves?
A: Bacterial endocarditis
B: Aortic stenosis
C: Mitral valve prolapse
D: Nonbacterial thrombotic endocarditis
E: Rheumatic fever
Answer: D
@
Bicuspid aortic valve (BAV) is a form of heart disease in which two of the leaflets of the aortic valve fuse during development in the womb resulting in a two-leaflet (bicuspid) valve instead of the normal three-leaflet (tricuspid) valve. BAV is the most common cause of heart disease present at birth and affects approximately 1.3% of adults.[2] Normally, the mitral valve is the only bicuspid valve and this is situated between the heart's left atrium and left ventricle. Heart valves play a crucial role in ensuring the unidirectional flow of blood from the atrium to the ventricles, or from the ventricle to the aorta or pulmonary trunk. BAV is normally inherited.

In many cases, a bicuspid aortic valve will cause no problems.[3] People with BAV may become tired more easily than those with normal valvular function and have difficulty maintaining stamina for cardio-intensive activities due to poor heart performance caused by stress on the aortic wall.[4][5]
$
5
Question: What happens when two leaflets of the aortic valve fuse during development in the womb?
A: Tetralogy of Fallot
B: Coronary artery disease
C: Bicuspid aortic valve
D: Ventricular septal defect
E: Aortic stenosis
Answer: C

Question: How many adults approximately are affected by Bicuspid aortic valve?
A: 0.5%
B: 1.3%
C: 2.8%
D: 4.0%
E: 5.5%
Answer: B

Question: Which valve is normally bicuspid in the heart?
A: Aortic valve
B: Pulmonary valve
C: Tricuspid valve
D: Mitral valve
E: Septal valve
Answer: D

Question: What is a potential symptom of Bicuspid aortic valve?
A: Increased stamina for cardio-intensive activities
B: Difficulty in breathing
C: Enhanced heart performance
D: Tiredness and decreased stamina for cardio-intensive activities
E: Irregular heartbeat
Answer: D

Question: Is Bicuspid aortic valve a congenital condition?
A: Yes, it develops in childhood.
B: No, it develops during adulthood.
C: Yes, it is present at birth.
D: No, it is caused by bacterial infections.
E: No, it results from physical injuries.
Answer: C
@
An antibiotic is a type of antimicrobial substance active against bacteria. It is the most important type of antibacterial agent for fighting bacterial infections, and antibiotic medications are widely used in the treatment and prevention of such infections.[1][2] They may either kill or inhibit the growth of bacteria. A limited number of antibiotics also possess antiprotozoal activity.[3][4] Antibiotics are not effective against viruses such as the common cold or influenza;[5] drugs which inhibit growth of viruses are termed antiviral drugs or antivirals rather than antibiotics. They are also not effective against fungi; drugs which inhibit growth of fungi are called antifungal drugs.

Antibiotics have been used since ancient times. Many civilizations used topical application of moldy bread, with many references to its beneficial effects arising from ancient Egypt, Nubia, China, Serbia, Greece, and Rome.[7] The first person to directly document the use of molds to treat infections was John Parkinson (1567–1650). Antibiotics revolutionized medicine in the 20th century. Alexander Fleming (1881–1955) discovered modern day penicillin in 1928, the widespread use of which proved significantly beneficial during wartime. However, the effectiveness and easy access to antibiotics have also led to their overuse[8] and some bacteria have evolved resistance to them.[1][9][10][11] The World Health Organization has classified antimicrobial resistance as a widespread "serious threat [that] is no longer a prediction for the future, it is happening right now in every region of the world and has the potential to affect anyone, of any age, in any country".[12] Global deaths attributable to antimicrobial resistance numbered 1.27 million in 2019.[13]
$
5
Question: What is the primary purpose of antibiotics?
A: Inhibiting the growth of viruses.
B: Treating bacterial infections.
C: Preventing fungal growth.
D: Treating protozoal infections.
E: Inhibiting the growth of mitochondria.
Answer: B

Question: Which of the following is NOT treated by antibiotics?
A: Common cold
B: Bacterial infections
C: Protozoal infections
D: Influenza
E: Skin infections caused by bacteria
Answer: A

Question: Who is credited with the discovery of modern day penicillin?
A: John Parkinson
B: Alexander Graham Bell
C: Alexander Fleming
D: Albert Einstein
E: Isaac Newton
Answer: C

Question: Why is antimicrobial resistance considered a significant threat?
A: It increases the cost of medication.
B: It causes antibiotics to have more side effects.
C: Bacteria have evolved and are no longer affected by some antibiotics.
D: It increases the shelf life of drugs.
E: It enhances the potency of drugs.
Answer: C

Question: As of 2019, approximately how many global deaths were attributed to antimicrobial resistance?
A: 120,000
B: 1.27 million
C: 500,000
D: 3 million
E: 750,000
Answer: B
@
Antibiotics are used to treat or prevent bacterial infections,[29] and sometimes protozoan infections. (Metronidazole is effective against a number of parasitic diseases). When an infection is suspected of being responsible for an illness but the responsible pathogen has not been identified, an empiric therapy is adopted.[30] This involves the administration of a broad-spectrum antibiotic based on the signs and symptoms presented and is initiated pending laboratory results that can take several days.[29][30]

When the responsible pathogenic microorganism is already known or has been identified, definitive therapy can be started. This will usually involve the use of a narrow-spectrum antibiotic. The choice of antibiotic given will also be based on its cost. Identification is critically important as it can reduce the cost and toxicity of the antibiotic therapy and also reduce the possibility of the emergence of antimicrobial resistance.[30] To avoid surgery, antibiotics may be given for non-complicated acute appendicitis.[31]

Antibiotics may be given as a preventive measure and this is usually limited to at-risk populations such as those with a weakened immune system (particularly in HIV cases to prevent pneumonia), those taking immunosuppressive drugs, cancer patients, and those having surgery.[29] Their use in surgical procedures is to help prevent infection of incisions. They have an important role in dental antibiotic prophylaxis where their use may prevent bacteremia and consequent infective endocarditis. Antibiotics are also used to prevent infection in cases of neutropenia particularly cancer-related.[32][33]

The use of antibiotics for secondary prevention of coronary heart disease is not supported by current scientific evidence, and may actually increase cardiovascular mortality, all-cause mortality and the occurrence of stroke.[34]
$
5
Question: When is empiric therapy used?
A: When the responsible pathogen is known.
B: After definitive therapy fails.
C: When the responsible pathogen is not yet identified.
D: After surgery to prevent infection.
E: To prevent pneumonia in HIV cases.
Answer: C

Question: What determines the choice of antibiotic for definitive therapy?
A: The color of the pill
B: The age of the patient
C: The specific known pathogenic microorganism
D: The patient's weight
E: The patient's blood type
Answer: C

Question: Why might antibiotics be given as a preventive measure?
A: To boost energy levels.
B: To enhance cognitive abilities.
C: To those with weakened immune systems.
D: For weight loss.
E: To cure chronic diseases.
Answer: C

Question: Which of the following is NOT a supported use of antibiotics?
A: Preventing pneumonia in HIV cases.
B: Treating bacterial infections.
C: Preventing infection in cases of neutropenia.
D: Secondary prevention of coronary heart disease.
E: Dental antibiotic prophylaxis.
Answer: D

Question: What is the primary role of antibiotics in surgical procedures?
A: To sedate the patient
B: To increase the speed of recovery
C: To help prevent infection of incisions
D: To numb the surgical area
E: To prevent scarring
Answer: C
@
Antibiotics are screened for any negative effects before their approval for clinical use, and are usually considered safe and well tolerated. However, some antibiotics have been associated with a wide extent of adverse side effects ranging from mild to very severe depending on the type of antibiotic used, the microbes targeted, and the individual patient.[40][41] Side effects may reflect the pharmacological or toxicological properties of the antibiotic or may involve hypersensitivity or allergic reactions.[4] Adverse effects range from fever and nausea to major allergic reactions, including photodermatitis and anaphylaxis.[42]

Common side effects of oral antibiotics include diarrhea, resulting from disruption of the species composition in the intestinal flora, resulting, for example, in overgrowth of pathogenic bacteria, such as Clostridium difficile.[43] Taking probiotics during the course of antibiotic treatment can help prevent antibiotic-associated diarrhea.[44] Antibacterials can also affect the vaginal flora, and may lead to overgrowth of yeast species of the genus Candida in the vulvo-vaginal area.[45] Additional side effects can result from interaction with other drugs, such as the possibility of tendon damage from the administration of a quinolone antibiotic with a systemic corticosteroid.[46]

Some antibiotics may also damage the mitochondrion, a bacteria-derived organelle found in eukaryotic, including human, cells.[citation needed] Mitochondrial damage cause oxidative stress in cells and has been suggested as a mechanism for side effects from fluoroquinolones.[47] They are also known to affect chloroplasts.[48]
$
5
Question: What might cause diarrhea when taking oral antibiotics?
A: Disruption of the intestinal flora
B: Direct side effect of the antibiotic
C: An overdose of the drug
D: Combination with other drugs
E: Allergic reaction to the pill coating
Answer: A

Question: Which genus of yeast might overgrow in the vulvo-vaginal area due to antibiotics?
A: Saccharomyces
B: Cladosporium
C: Candida
D: Penicillium
E: Aspergillus
Answer: C

Question: What might help prevent antibiotic-associated diarrhea?
A: Increasing the dose of the antibiotic
B: Avoiding all solid foods
C: Drinking only water
D: Taking painkillers
E: Taking probiotics during the antibiotic treatment
Answer: E

Question: Quinolone antibiotics combined with which type of drugs may cause tendon damage?
A: Antiviral drugs
B: Systemic corticosteroids
C: Probiotics
D: Painkillers
E: Blood pressure medications
Answer: B

Question: Which organelle in eukaryotic cells can be damaged by some antibiotics?
A: Ribosomes
B: Lysosomes
C: Golgi apparatus
D: Mitochondrion
E: Endoplasmic reticulum
Answer: D
@
Antibiotics are commonly classified based on their mechanism of action, chemical structure, or spectrum of activity. Most target bacterial functions or growth processes.[14] Those that target the bacterial cell wall (penicillins and cephalosporins) or the cell membrane (polymyxins), or interfere with essential bacterial enzymes (rifamycins, lipiarmycins, quinolones, and sulfonamides) have bactericidal activities, killing the bacteria. Protein synthesis inhibitors (macrolides, lincosamides, and tetracyclines) are usually bacteriostatic, inhibiting further growth (with the exception of bactericidal aminoglycosides).[69] Further categorization is based on their target specificity. "Narrow-spectrum" antibiotics target specific types of bacteria, such as gram-negative or gram-positive, whereas broad-spectrum antibiotics affect a wide range of bacteria. Following a 40-year break in discovering classes of antibacterial compounds, four new classes of antibiotics were introduced to clinical use in the late 2000s and early 2010s: cyclic lipopeptides (such as daptomycin), glycylcyclines (such as tigecycline), oxazolidinones (such as linezolid), and lipiarmycins (such as fidaxomicin).[70][71]
$
5
Question: Which class of antibiotics targets the bacterial cell wall?
A: Macrolides
B: Quinolones
C: Penicillins
D: Aminoglycosides
E: Lipiarmycins
Answer: C

Question: Antibiotics that inhibit further growth but don't necessarily kill bacteria are termed:
A: Bactericidal
B: Bacteriophagic
C: Bacteriostatic
D: Bacterialytic
E: Bacteriogenic
Answer: C

Question: What does a "narrow-spectrum" antibiotic target?
A: All microbes in the body
B: Only bacteria, not fungi or viruses
C: Specific types of bacteria, such as gram-negative or gram-positive
D: Only antibiotic-resistant bacteria
E: All types of bacteria equally
Answer: C

Question: After a significant gap, which of these classes of antibiotics was NOT introduced in the late 2000s and early 2010s?
A: Macrolides
B: Cyclic lipopeptides
C: Glycylcyclines
D: Oxazolidinones
E: Lipiarmycins
Answer: A

Question: Which antibiotics are usually bacteriostatic, inhibiting further growth?
A: Penicillins and cephalosporins
B: Polymyxins
C: Rifamycins
D: Macrolides, lincosamides, and tetracyclines
E: Quinolones and sulfonamides
Answer: D
@
Protein biosynthesis (or protein synthesis) is a core biological process, occurring inside cells, balancing the loss of cellular proteins (via degradation or export) through the production of new proteins. Proteins perform a number of critical functions as enzymes, structural proteins or hormones. Protein synthesis is a very similar process for both prokaryotes and eukaryotes but there are some distinct differences.[1]

Protein synthesis can be divided broadly into two phases—transcription and translation. During transcription, a section of DNA encoding a protein, known as a gene, is converted into a template molecule called messenger RNA (mRNA). This conversion is carried out by enzymes, known as RNA polymerases, in the nucleus of the cell.[2] In eukaryotes, this mRNA is initially produced in a premature form (pre-mRNA) which undergoes post-transcriptional modifications to produce mature mRNA. The mature mRNA is exported from the cell nucleus via nuclear pores to the cytoplasm of the cell for translation to occur. During translation, the mRNA is read by ribosomes which use the nucleotide sequence of the mRNA to determine the sequence of amino acids. The ribosomes catalyze the formation of covalent peptide bonds between the encoded amino acids to form a polypeptide chain.

Following translation the polypeptide chain must fold to form a functional protein; for example, to function as an enzyme the polypeptide chain must fold correctly to produce a functional active site. To adopt a functional three-dimensional (3D) shape, the polypeptide chain must first form a series of smaller underlying structures called secondary structures. The polypeptide chain in these secondary structures then folds to produce the overall 3D tertiary structure. Once correctly folded, the protein can undergo further maturation through different post-translational modifications. Post-translational modifications can alter the protein's ability to function, where it is located within the cell (e.g. cytoplasm or nucleus) and the protein's ability to interact with other proteins.[3]
$
5
Question 1: What is the main function of RNA polymerases during protein biosynthesis?
A: Replication of DNA
B: Translation of mRNA
C: Degradation of DNA
D: Conversion of DNA into mRNA
E: Production of tRNA
Answer: D

Question 2: In which cellular structure does transcription primarily occur in eukaryotes?
A: Cytoplasm
B: Ribosomes
C: Mitochondria
D: Nucleus
E: Endoplasmic reticulum
Answer: D

Question 3: What molecule is read by ribosomes to determine the sequence of amino acids during translation?
A: tRNA
B: DNA
C: rRNA
D: mRNA
E: Enzymes
Answer: D

Question 4: What is the function of the ribosome in protein synthesis?
A: DNA replication
B: mRNA replication
C: Catalyzing the formation of covalent peptide bonds
D: Post-transcriptional modification
E: mRNA degradation
Answer: C

Question 5: Which process involves the polypeptide chain forming a functional three-dimensional shape after translation?
A: Transcription
B: Replication
C: Folding
D: Degradation
E: Glycosylation
Answer: C
@
Once synthesis of the polypeptide chain is complete, the polypeptide chain folds to adopt a specific structure which enables the protein to carry out its functions. The basic form of protein structure is known as the primary structure, which is simply the polypeptide chain i.e. a sequence of covalently bonded amino acids. The primary structure of a protein is encoded by a gene. Therefore, any changes to the sequence of the gene can alter the primary structure of the protein and all subsequent levels of protein structure, ultimately changing the overall structure and function.[citation needed]

The primary structure of a protein (the polypeptide chain) can then fold or coil to form the secondary structure of the protein. The most common types of secondary structure are known as an alpha helix or beta sheet, these are small structures produced by hydrogen bonds forming within the polypeptide chain. This secondary structure then folds to produce the tertiary structure of the protein. The tertiary structure is the proteins overall 3D structure which is made of different secondary structures folding together. In the tertiary structure, key protein features e.g. the active site, are folded and formed enabling the protein to function. Finally, some proteins may adopt a complex quaternary structure. Most proteins are made of a single polypeptide chain, however, some proteins are composed of multiple polypeptide chains (known as subunits) which fold and interact to form the quaternary structure. Hence, the overall protein is a multi-subunit complex composed of multiple folded, polypeptide chain subunits e.g. haemoglobin.[13]
$
5
Question 6: Which structure refers to the sequence of covalently bonded amino acids in a protein?
A: Secondary structure
B: Tertiary structure
C: Quaternary structure
D: Primary structure
E: Complex structure
Answer: D

Question 7: What common secondary structures are formed due to hydrogen bonds within the polypeptide chain?
A: Primary and tertiary
B: Helix and sheet
C: Alpha helix and beta sheet
D: Quaternary and alpha sheet
E: Beta coil and alpha twist
Answer: C

Question 8: How can the overall structure and function of a protein change?
A: Changing the cell type
B: Adding water molecules
C: Altering the sequence of its gene
D: Changing the pH level
E: Increasing the temperature
Answer: C

Question 9: What describes a protein made of multiple polypeptide chains?
A: Primary structure
B: Secondary structure
C: Quaternary structure
D: Tertiary structure
E: Alpha structure
Answer: C

Question 10: Which structure of a protein forms its overall 3D shape?
A: Primary structure
B: Secondary structure
C: Quaternary structure
D: Tertiary structure
E: Pre-structure
Answer: D
@
Post-translational modifications can incorporate more complex, large molecules into the folded protein structure. One common example of this is glycosylation, the addition of a polysaccharide molecule, which is widely considered to be most common post-translational modification.[16]

In glycosylation, a polysaccharide molecule (known as a glycan) is covalently added to the target protein by glycosyltransferases enzymes and modified by glycosidases in the endoplasmic reticulum and Golgi apparatus. Glycosylation can have a critical role in determining the final, folded 3D structure of the target protein. In some cases glycosylation is necessary for correct folding. N-linked glycosylation promotes protein folding by increasing solubility and mediates the protein binding to protein chaperones. Chaperones are proteins responsible for folding and maintaining the structure of other proteins.[1]

There are broadly two types of glycosylation, N-linked glycosylation and O-linked glycosylation. N-linked glycosylation starts in the endoplasmic reticulum with the addition of a precursor glycan. The precursor glycan is modified in the Golgi apparatus to produce complex glycan bound covalently to the nitrogen in an asparagine amino acid. In contrast, O-linked glycosylation is the sequential covalent addition of individual sugars onto the oxygen in the amino acids serine and threonine within the mature protein structure.[1]
$
5
Question 11: What does glycosylation involve the addition of to a protein?
A: Nucleotide
B: Polysaccharide molecule
C: Lipid molecule
D: Amino acid
E: Ribosome
Answer: B

Question 12: Which organelles are primarily involved in glycosylation?
A: Mitochondria and Nucleus
B: Cytoplasm and Ribosomes
C: Lysosomes and Peroxisomes
D: Endoplasmic reticulum and Golgi apparatus
E: Vacuole and Plasma membrane
Answer: D

Question 13: What type of glycosylation involves the addition of individual sugars to the amino acids serine and threonine?
A: N-linked glycosylation
B: A-linked glycosylation
C: O-linked glycosylation
D: P-linked glycosylation
E: G-linked glycosylation
Answer: C

Question 14: Chaperones play a role in which of the following?
A: mRNA degradation
B: Protein folding and maintaining structure
C: Protein degradation
D: DNA replication
E: Glycolysis
Answer: B

Question 15: N-linked glycosylation primarily modifies which amino acid in the protein?
A: Serine
B: Threonine
C: Glycine
D: Asparagine
E: Valine
Answer: D
@
Sickle cell disease is a group of diseases caused by a mutation in a subunit of hemoglobin, a protein found in red blood cells responsible for transporting oxygen. The most dangerous of the sickle cell diseases is known as sickle cell anemia. Sickle cell anemia is the most common homozygous recessive single gene disorder, meaning the affected individual must carry a mutation in both copies of the affected gene (one inherited from each parent) to experience the disease. Hemoglobin has a complex quaternary structure and is composed of four polypeptide subunits - two A subunits and two B subunits.[22] Patients with sickle cell anemia have a missense or substitution mutation in the gene encoding the hemoglobin B subunit polypeptide chain. A missense mutation means the nucleotide mutation alters the overall codon triplet such that a different amino acid is paired with the new codon. In the case of sickle cell anemia, the most common missense mutation is a single nucleotide mutation from thymine to adenine in the hemoglobin B subunit gene.[23] This changes codon 6 from encoding the amino acid glutamic acid to encoding valine.[22]

This change in the primary structure of the hemoglobin B subunit polypeptide chain alters the functionality of the hemoglobin multi-subunit complex in low oxygen conditions. When red blood cells unload oxygen into the tissues of the body, the mutated haemoglobin protein starts to stick together to form a semi-solid structure within the red blood cell. This distorts the shape of the red blood cell, resulting in the characteristic "sickle" shape, and reduces cell flexibility. This rigid, distorted red blood cell can accumulate in blood vessels creating a blockage. The blockage prevents blood flow to tissues and can lead to tissue death which causes great pain to the individual.[24]
$
5
Question 16: What is primarily affected by the mutation causing sickle cell anemia?
A: Hemoglobin A subunit
B: Hemoglobin B subunit
C: DNA structure
D: tRNA molecule
E: Ribosome structure
Answer: B

Question 17: A missense mutation leads to which type of change?
A: A deleted amino acid
B: A premature stop codon
C: A different amino acid encoded
D: A frameshift in the reading frame
E: Replication of an amino acid sequence
Answer: C

Question 18: The most common mutation causing sickle cell anemia involves a nucleotide change from which to which?
A: Adenine to Cytosine
B: Cytosine to Guanine
C: Guanine to Adenine
D: Thymine to Adenine
E: Thymine to Cytosine
Answer: D

Question 19: The mutated form of hemoglobin in sickle cell anemia causes red blood cells to adopt what kind of shape?
A: Square
B: Round
C: "Sickle"
D: Spiral
E: Triangular
Answer: C

Question 20: Blockages caused by the "sickle" shaped red blood cells can lead to which consequence?
A: Increased oxygen transport
B: Enhanced blood clotting
C: Reduced pain sensations
D: Tissue death
E: Increased red blood cell production
Answer: D
@
Redox (/ˈrɛdɒks/ RED-oks, /ˈriːdɒks/ REE-doks, reduction–oxidation[2] or oxidation–reduction[3]) is a type of chemical reaction in which the oxidation states of substrate change.[4] Oxidation is the loss of electrons or an increase in the oxidation state, while reduction is the gain of electrons or a decrease in the oxidation state.

There are two classes of redox reactions:

Electron-transfer – Only one (usually) electron flows from the atom being oxidized to the atom that is reduced. This type of redox reaction is often discussed in terms of redox couples and electrode potentials.
Atom transfer – An atom transfers from one substrate to another. For example, in the rusting of iron, the oxidation state of iron atoms increases as the iron converts to an oxide, and simultaneously the oxidation state of oxygen decreases as it accepts electrons released by the iron. Although oxidation reactions are commonly associated with the formation of oxides, other chemical species can serve the same function.[5] In hydrogenation, C=C (and other) bonds are reduced by transfer of hydrogen atoms.
$
5
Question: Which of the following describes the oxidation process?
A: Gain of electrons
B: Decrease in oxidation state
C: Loss of electrons
D: Atom transfer
E: Formation of hydrogen bonds
Answer: C

Question: In electron-transfer type of redox reactions, how many electrons typically flow from the atom being oxidized to the one being reduced?
A: Two
B: Zero
C: One
D: Four
E: Three
Answer: C

Question: In atom transfer redox reactions, what happens during rusting of iron?
A: Iron loses hydrogen atoms
B: Oxygen gains hydrogen atoms
C: Iron's oxidation state decreases
D: Oxygen's oxidation state increases
E: Iron's oxidation state increases
Answer: E

Question: Which of the following is reduced by transfer of hydrogen atoms?
A: Rusting
B: Oxidation
C: Hydrogenation
D: Electron-transfer
E: Atom-transfer
Answer: C

Question: What is the result of oxidation?
A: Loss of protons
B: Gain of electrons
C: Gain of protons
D: Increase in oxidation state
E: Decrease in oxidation state
Answer: D
@
Each half-reaction has a standard electrode potential (Eo
cell), which is equal to the potential difference or voltage at equilibrium under standard conditions of an electrochemical cell in which the cathode reaction is the half-reaction considered, and the anode is a standard hydrogen electrode where hydrogen is oxidized:

1⁄2 H2 → H+ + e−
The electrode potential of each half-reaction is also known as its reduction potential Eo
red, or potential when the half-reaction takes place at a cathode. The reduction potential is a measure of the tendency of the oxidizing agent to be reduced. Its value is zero for H+ + e− → 1⁄2 H2 by definition, positive for oxidizing agents stronger than H+ (e.g., +2.866 V for F2) and negative for oxidizing agents that are weaker than H+ (e.g., −0.763 V for Zn2+).[8]: 873 

For a redox reaction that takes place in a cell, the potential difference is:

Eo
cell = Eo
cathode – Eo
anode
However, the potential of the reaction at the anode is sometimes expressed as an oxidation potential:

Eo
ox = –Eo
red
The oxidation potential is a measure of the tendency of the reducing agent to be oxidized but does not represent the physical potential at an electrode. With this notation, the cell voltage equation is written with a plus sign

Eo
cell = Eo
red(cathode) + Eo
ox(anode)
$
5
Question: What is the standard electrode potential for H+ + e− → 1⁄2 H2?
A: +2.866 V
B: −0.763 V
C: 0 V
D: +1.5 V
E: -1.5 V
Answer: C

Question: How is the potential difference for a redox reaction in a cell determined?
A: Eo cell = Eo anode – Eo cathode
B: Eo cell = Eo red(cathode) – Eo ox(anode)
C: Eo cell = Eo cathode + Eo anode
D: Eo cell = Eo cathode – Eo anode
E: Eo cell = Eo anode + Eo cathode
Answer: D

Question: What does a positive reduction potential indicate about an oxidizing agent?
A: Weaker than H+
B: Equivalent to H+
C: Stronger than H+
D: Not related to H+
E: Neutral compared to H+
Answer: C

Question: Which statement about oxidation potential is correct?
A: It represents the physical potential at an electrode.
B: It is a measure of the tendency of the oxidizing agent to be reduced.
C: It is the same as the reduction potential.
D: It is a measure of the tendency of the reducing agent to be oxidized.
E: It is always positive.
Answer: D

Question: What is the oxidation potential of a reaction in terms of its reduction potential?
A: Eo ox = Eo red
B: Eo ox = –Eo red
C: Eo ox = Eo red + 1
D: Eo ox = Eo red – 1
E: Eo ox = 2 * Eo red
Answer: B
@
In chemistry, peroxides are a group of compounds with the structure R−O−O−R, where R is any element.[1][2] The O−O group in a peroxide is called the peroxide group or peroxy group (sometimes called peroxo group or peroxyl group). The nomenclature is somewhat variable,[3] and the term was introduced by Thomas Thomson in 1804 for an oxide with the greatest quantity of oxygen.[4]

The most common peroxide is hydrogen peroxide (H2O2), colloquially known simply as "peroxide". It is marketed as solutions in water at various concentrations. Many organic peroxides are known as well.

In addition to hydrogen peroxide, some other major classes of peroxides are:

Peroxy acids, the peroxy derivatives of many familiar acids, examples being peroxymonosulfuric acid and peracetic acid, and their salts, one example of which is potassium peroxydisulfate.
Main group peroxides, compounds with the linkage E−O−O−E (E = main group element).
Metal peroxides, examples being barium peroxide (BaO2), sodium peroxide (Na2O2) and zinc peroxide (ZnO2).
Organic peroxides, compounds with the linkage C−O−O−C or C−O−O−H. One example is tert-butylhydroperoxide.
$
5
Question: Which of the following is the most common peroxide?
A: Sodium peroxide
B: Barium peroxide
C: Tert-butylhydroperoxide
D: Hydrogen peroxide
E: Zinc peroxide
Answer: D

Question: What is the structure of peroxides?
A: R−O−R
B: R−O−H
C: R−O−O−H
D: R−O−O−R
E: H−O−H
Answer: D

Question: In which category do barium peroxide and zinc peroxide fall?
A: Organic peroxides
B: Peroxy acids
C: Main group peroxides
D: Metal peroxides
E: Hydrogen peroxides
Answer: D

Question: Which peroxide class contains compounds with C−O−O−C or C−O−O−H linkages?
A: Organic peroxides
B: Peroxy acids
C: Main group peroxides
D: Metal peroxides
E: Hydrogen peroxides
Answer: A

Question: What is the name of the group in peroxides with the structure O−O?
A: Peroxide group
B: Oxygen group
C: Oxide group
D: Hydroxyl group
E: Oxy group
Answer: A
@
Cathodic protection is a technique used to control the corrosion of a metal surface by making it the cathode of an electrochemical cell. A simple method of protection connects protected metal to a more easily corroded "sacrificial anode" to act as the anode. The sacrificial metal instead of the protected metal, then, corrodes. A common application of cathodic protection is in galvanized steel, in which a sacrificial coating of zinc on steel parts protects them from rust.[citation needed]

Oxidation is used in a wide variety of industries such as in the production of cleaning products and oxidizing ammonia to produce nitric acid.

Redox reactions are the foundation of electrochemical cells, which can generate electrical energy or support electrosynthesis. Metal ores often contain metals in oxidized states such as oxides or sulfides, from which the pure metals are extracted by smelting at high temperature in the presence of a reducing agent. The process of electroplating uses redox reactions to coat objects with a thin layer of a material, as in chrome-plated automotive parts, silver plating cutlery, galvanization and gold-plated jewelry.[citation needed]
$
5
Question: What technique is used to control the corrosion of a metal surface by making it the cathode of an electrochemical cell?
A: Oxidation protection
B: Hydrogenation
C: Cathodic protection
D: Anodic protection
E: Electroplating
Answer: C

Question: In cathodic protection, which metal corrodes?
A: The protected metal
B: Both protected and sacrificial metals
C: The sacrificial anode
D: Neither of the metals
E: The surrounding metals
Answer: C

Question: What process uses redox reactions to coat objects with a thin layer of material?
A: Oxidation
B: Hydrogenation
C: Smelting
D: Electroplating
E: Cathodic protection
Answer: D

Question: Metal ores often contain metals in which state?
A: Pure metals
B: Reducing agents
C: Oxidized states such as oxides or sulfides
D: Reduced states such as hydroxides
E: Electroplated states
Answer: C

Question: In which industry is oxidation used to produce nitric acid?
A: Oxidizing ammonia
B: Electroplating
C: Cathodic protection
D: Producing cleaning products
E: Galvanization
Answer: A
@
An electrochemical cell is a device that generates electrical energy from chemical reactions. Electrical energy can also be applied to these cells to cause chemical reactions to occur.[1] Electrochemical cells which generate an electric current are called voltaic or galvanic cells and those that generate chemical reactions, via electrolysis for example, are called electrolytic cells.[2]

Both galvanic and electrolytic cells can be thought of as having two half-cells: consisting of separate oxidation and reduction reactions.

When one or more electrochemical cells are connected in parallel or series they make a battery. Primary cells are single use batteries.
$
5
Question: What is the function of an electrochemical cell?
A: To convert heat into electricity
B: To generate electrical energy from chemical reactions
C: To convert sunlight into electricity
D: To store mechanical energy
E: To amplify electrical signals
Answer: B

Question: What type of electrochemical cell uses electrical energy to generate chemical reactions?
A: Galvanic cell
B: Electrolytic cell
C: Solar cell
D: Primary cell
E: Fuel cell
Answer: B

Question: What do you call multiple electrochemical cells connected in series or parallel?
A: An electrolyte
B: A diode
C: A resistor
D: A transformer
E: A battery
Answer: E

Question: In a voltaic cell, what is being generated?
A: Heat energy
B: Chemical energy
C: Electrical energy
D: Mechanical energy
E: Light energy
Answer: C

Question: Which of the following cells are single-use batteries?
A: Electrolytic cells
B: Voltaic cells
C: Primary cells
D: Secondary cells
E: Fuel cells
Answer: C
@
Competing half-reactions in solution electrolysis
Using a cell containing inert platinum electrodes, electrolysis of aqueous solutions of some salts leads to the reduction of the cations (such as metal deposition with, for example, zinc salts) and oxidation of the anions (such as the evolution of bromine with bromides). However, with salts of some metals (such as sodium) hydrogen is evolved at the cathode, and for salts containing some anions (such as sulfate SO2−
4) oxygen is evolved at the anode. In both cases, this is due to water being reduced to form hydrogen or oxidized to form oxygen. In principle, the voltage required to electrolyze a salt solution can be derived from the standard electrode potential for the reactions at the anode and cathode. The standard electrode potential is directly related to the Gibbs free energy, ΔG, for the reactions at each electrode and refers to an electrode with no current flowing. An extract from the table of standard electrode potentials is shown below.

Half-reaction	E° (V)	Ref.
Na+ + e− ⇌ Na(s)	−2.71	[24]
Zn2+ + 2 e− ⇌ Zn(s)	−0.7618	[25]
2 H+ + 2 e− ⇌ H2(g)	≡ 0	[25]
Br2(aq) + 2 e− ⇌ 2 Br−	+1.0873	[25]
O2(g) + 4 H+ + 4 e− ⇌ 2 H2O	+1.23	[24]
Cl2(g) + 2 e− ⇌ 2 Cl−	+1.36	[24]
S
2O2−
8 + 2 e− ⇌ 2 SO2−
4	+2.07	[24]
In terms of electrolysis, this table should be interpreted as follows:

Moving down the table, E° becomes more positive, and species on the left are more likely to be reduced: for example, zinc ions are more likely to be reduced to zinc metal than sodium ions are to be reduced to sodium metal.
Moving up the table, E° becomes more negative, and species on the right are more likely to be oxidized: for example, sodium metal is more likely to be oxidized to sodium ions than zinc metal is to be oxidized to zinc ions.
Using the Nernst equation the electrode potential can be calculated for a specific concentration of ions, temperature and the number of electrons involved. For pure water (pH 7):

the electrode potential for the reduction producing hydrogen is −0.41 V,
the electrode potential for the oxidation producing oxygen is +0.82 V.
$
5

Question: What process uses direct electric current to cause non-spontaneous chemical reactions?
A: Oxidation
B: Combustion
C: Reduction
D: Electrolysis
E: Evaporation
Answer: D

Question: Which component of the electrolysis setup carries the electric current?
A: Electrode
B: Power source
C: Electrolyte
D: Partition
E: Resistor
Answer: C

Question: Which of the following cannot undergo electrolysis due to its immobility?
A: Molten salt
B: Salt solution in water
C: Ion-conducting polymer
D: Solid salt
E: Ionic liquid compound
Answer: D

Question: What historical material was used as a non-reactive anode for electrolysis?
A: Aluminum
B: Gold
C: Graphite
D: Iron
E: Copper
Answer: C

Question: Which electrode in electrolysis tends to have greater wear due to oxidation?
A: Cathode
B: Anode
C: Neither, they wear out at the same rate
D: It depends on the electrolyte
E: It depends on the power source
Answer: B
@
Electrolysis of Iron Ore
The current method of producing steel from iron ore is very carbon intensive, in part to the direct release of CO2 in the blast furnace. A study of steel making in Germany found that producing 1 ton of steel emitted 2.1 tons of CO2e with 22% of that being direct emissions from the blast furnace.[42] As of 2022, steel production contributes 7–9% of global emissions.[43] Electrolysis of iron can eliminate direct emissions and further reduce emissions if the electricity is created from green energy.

The small-scale electrolysis of iron has been successfully reported by dissolving it in molten oxide salts and using a platinum anode.[44] Oxygen anions form oxygen gas and electrons at the anode. Iron cations consume electrons and form iron metal at the cathode. This method was performed a temperature of 1550 °C which presents a significant challenge to maintaining the reaction. Particularly, anode corrosion is a concern at these temperatures.

Additionally, the low temperature reduction of iron oxide by dissolving it in alkaline water has been reported.[45] The temperature is much lower than traditional iron production at 114 °C. The low temperatures also tend to correlate with higher current efficiencies, with an efficiency of 95% being reported. While these methods are promising, they struggle to be cost competitive because of the large economies of scale keeping the price of blast furnace iron low.
$
5
Question: What percentage of global emissions was contributed by steel production as of 2022?
A: 3-5%
B: 5-7%
C: 7-9%
D: 10-12%
E: 12-15%
Answer: C

Question: What presents a significant challenge in the electrolysis of iron at 1550 °C?
A: High electrical resistance
B: Anode corrosion
C: Rapid evaporation of the electrolyte
D: Formation of unwanted metal alloys
E: Insufficient supply of electrical current
Answer: B

Question: At what temperature does the low-temperature reduction of iron oxide in alkaline water occur?
A: 50 °C
B: 90 °C
C: 114 °C
D: 200 °C
E: 300 °C
Answer: C

Question: Why are new methods of producing iron through electrolysis not yet cost-competitive?
A: They are too energy-intensive
B: There's a lack of skilled workers to implement them
C: The electrolytes used are rare and expensive
D: Large economies of scale keep the price of blast furnace iron low
E: The machinery required is still under patent
Answer: D

Question: In the small-scale electrolysis of iron, what forms at the cathode?
A: Oxygen gas
B: Iron cations
C: Hydrogen gas
D: Iron metal
E: Iron oxide
Answer: D
@
Electrometallurgy is a method in metallurgy that uses electrical energy to produce metals by electrolysis. It is usually the last stage in metal production and is therefore preceded by pyrometallurgical or hydrometallurgical operations.[1] The electrolysis can be done on a molten metal oxide (smelt electrolysis) which is used for example to produce aluminium from aluminium oxide via the Hall-Hérault process. Electrolysis can be used as a final refining stage in pyrometallurgical metal production (electrorefining) and it is also used for reduction of a metal from an aqueous metal salt solution produced by hydrometallurgy (electrowinning).

Electrometallurgy is the field concerned with the processes of metal electrodeposition. There are four categories of these processes:

Electrolysis
Electrowinning, the extraction of metal from ores[2]
Electrorefining, the purification of metals.[2] Metal powder production by electrodeposition is included in this category, or sometimes electrowinning, or a separate category depending on application.[2]
Electroplating, the deposition of a layer of one metal on another[2]
Electroforming, the manufacture of, usually thin, metal parts through electroplating[2]
Electropolishing, the removal of material from a metallic workpiece
Etching, industrially known to Wikipedia as chemical milling
$
5
Question: What is the primary purpose of electrometallurgy?
A: To use electrical energy to produce metals by electrolysis
B: To purify metals using heat
C: To coat metals with a thin layer of plastic
D: To produce electrical energy from metals
E: To study the magnetic properties of metals
Answer: A

Question: Which process involves the purification of metals?
A: Electrolysis
B: Electrowinning
C: Electrorefining
D: Electroplating
E: Electroforming
Answer: C

Question: What is the objective of electroplating?
A: To extract metals from ores
B: To manufacture metal parts
C: To deposit a layer of one metal on another
D: To remove material from a metallic workpiece
E: To produce metals by electrolysis
Answer: C

Question: Which method in metallurgy is typically the last stage in metal production?
A: Pyrometallurgy
B: Hydrometallurgy
C: Electrometallurgy
D: Thermometallurgy
E: Aerometallurgy
Answer: C

Question: In the context of electrometallurgy, what is the function of electroforming?
A: Extraction of metal from ores
B: Manufacture of metal parts through electroplating
C: Purification of metals
D: Deposition of a layer of one metal on another
E: Removal of material from a metallic workpiece
Answer: B
@
Electroplating, also known as electrochemical deposition or electrodeposition, is a process for producing a metal coating on a solid substrate through the reduction of cations of that metal by means of a direct electric current. The part to be coated acts as the cathode (negative electrode) of an electrolytic cell; the electrolyte is a solution of a salt of the metal to be coated; and the anode (positive electrode) is usually either a block of that metal, or of some inert conductive material. The current is provided by an external power supply.

Electroplating is widely used in industry and decorative arts to improve the surface qualities of objects—such as resistance to abrasion and corrosion, lubricity, reflectivity, electrical conductivity, or appearance. It is used to build up thickness on undersized or worn-out parts, or to manufacture metal plates with complex shape, a process called electroforming. It is used to deposit copper and other conductors in forming printed circuit boards, and copper interconnects in integrated circuits. It is also used to purify metals such as copper.
$
5
Question: What is the primary purpose of electroplating?
A: To produce electricity by chemical reactions.
B: To convert chemical energy to electrical energy.
C: To improve the surface qualities of objects.
D: To dissolve metals for recycling purposes.
E: To extract metals from ores.
Answer: C

Question: Which electrode acts as the part to be coated in electroplating?
A: Anode
B: Electrolyte
C: Power supply
D: Cathode
E: Conductor
Answer: D

Question: In the process of electroplating, the electrolyte solution contains a salt of which metal?
A: The one to be disposed of.
B: The one that is an inert conductor.
C: The one to be coated onto the substrate.
D: The one used in the external power supply.
E: The one that acts as a resistor.
Answer: C

Question: What application of electroplating involves depositing copper in forming printed circuit boards?
A: Electroforming
B: Electrorefining
C: Electrotyping
D: Electrogravimetry
E: None of the above
Answer: E

Question: Which of the following is a use of electroplating?
A: Purification of metals
B: Decreasing reflectivity
C: Increasing abrasion
D: Reducing conductivity
E: Reducing lubricity
Answer: A

Subject 2: Electrolyte and Anode Function
Question: The electrolyte in the electrolytic plating cell contains which kind of ions for deposition?
A: Negative ions
B: Neutral ions
C: Metal cations
D: Anode ions
E: Cathode ions
Answer: C
@
The electrolyte in the electrolytic plating cell should contain positive ions (cations) of the metal to be deposited. These cations are reduced at the cathode to the metal in the zero valence state. For example, the electrolyte for copper plating can be a solution of copper(II) sulfate, which dissociates into Cu2+ cations and SO2−
4 anions. At the cathode, the Cu2+ is reduced to metallic copper by gaining two electrons.

When the anode is made of the metal that is intended for coating onto the cathode, the opposite reaction may occur at the anode, turning it into dissolved cations. For example, copper would be oxidized at the anode to Cu2+ by losing two electrons. In this case, the rate at which the anode is dissolved will be equal to the rate at which the cathode is plated and thus the ions in the electrolyte bath are continuously replenished by the anode. The net result is the effective transfer of metal from the anode to the cathode.[3]

The anode may instead be made of a material that resists electrochemical oxidation, such as lead or carbon. Oxygen, hydrogen peroxide, and some other byproducts are then produced at the anode instead. In this case, ions of the metal to be plated must be replenished (continuously or periodically) in the bath as they are drawn out of the solution.[4]
$
5
Question: The electrolyte in the electrolytic plating cell contains which kind of ions for deposition?
A: Negative ions
B: Neutral ions
C: Metal cations
D: Anode ions
E: Cathode ions
Answer: C

Question: In the electrolyte for copper plating, which cation is present?
A: Cu+
B: Cu2+
C: SO2−4
D: CuSO4
E: Cu3+
Answer: B

Question: If the anode is made of the same metal intended for coating, what happens to the anode during electroplating?
A: It gets coated with metal.
B: It gets dissolved into cations.
C: It remains unchanged.
D: It acts as a power supply.
E: It attracts anions.
Answer: B

Question: What byproduct can be produced at an anode made of a material that resists electrochemical oxidation?
A: Copper
B: Hydrogen peroxide
C: Electrons
D: Metal cations
E: Cathode ions
Answer: B

Question: When the anode is of a material that resists oxidation, what must be done to the bath?
A: The pH must be increased.
B: Metal ions must be periodically replenished.
C: It should be heated periodically.
D: The conductivity must be increased.
E: Electrons must be added.
Answer: B
@
A closely related process is brush electroplating, in which localized areas or entire items are plated using a brush saturated with plating solution. The brush, typically a stainless steel body wrapped with an absorbent cloth material that both holds the plating solution and prevents direct contact with the item being plated, is connected to the anode of a low voltage direct current power source, and the item to be plated connected to the cathode. The operator dips the brush in plating solution then applies it to the item, moving the brush continually to get an even distribution of the plating material.

Brush electroplating has several advantages over tank plating, including portability, ability to plate items that for some reason cannot be tank plated (one application was the plating of portions of very large decorative support columns in a building restoration), low or no masking requirements, and comparatively low plating solution volume requirements. Disadvantages compared to tank plating can include greater operator involvement (tank plating can frequently be done with minimal attention), and inability to achieve as great a plate thickness.
$
5
Question: What type of electroplating involves localized areas or entire items plated using a brush?
A: Tank Electroplating
B: Brush Electroplating
C: Column Electroplating
D: Anodic Electroplating
E: Cathodic Electroplating
Answer: B

Question: In brush electroplating, what prevents the brush from directly contacting the item?
A: Steel casing
B: A plastic barrier
C: An absorbent cloth material
D: A layer of electrolyte
E: An insulating layer
Answer: C

Question: One advantage of brush electroplating is:
A: Achieving a greater plate thickness.
B: Requires extensive masking.
C: Requires a large volume of plating solution.
D: Ability to plate items that cannot be tank plated.
E: Needs constant power supply.
Answer: D

Question: What is a potential drawback of brush electroplating compared to tank plating?
A: It is less portable.
B: It requires less operator involvement.
C: It cannot achieve as great a plate thickness.
D: It uses more plating solution.
E: It is more corrosive.
Answer: C

Question: The brush in brush electroplating is connected to:
A: The cathode of the power source.
B: The neutral terminal.
C: The anode of the power source.
D: The external power supply.
E: The electrolyte.
Answer: C
@
Throwing power (or macro throwing power) is an important parameter that provides a measure of the uniformity of electroplating current, and consequently the uniformity of the electroplated metal thickness, on regions of the part that are near to the anode compared to regions that are far from it. It depends mostly on the composition and temperature of the electroplating solution.[2] Micro throwing power refers to the extent to which a process can fill coat small recesses such as through-holes.[9] Quantitative measures of throwing power are specified by various conventions. These measures are derived from two ratios: the ratio M = m1 / m2 of the plating thickness of a specified region of the cathode "close" to the anode to the thickness of a region "far" from the cathode; and the ratio L = x2 / x1 of the distances of these regions through the electrolyte to the anode. In a Haring-Blum cell, for example, L = 5 for its two independent cathodes, and a cell yielding plating thickness ratio of M = 6 has Harring-Blum throwing power 100% × (L − M) / L = −20%.[9] Other conventions include the Heatley throwing power 100% × (L − M) / (L − 1), and Field throwing power 100% × (L − M) / (L + M − 2).[10] More uniform thickness is obtained by making the throwing power larger (less negative) according to any of these definitions.

Parameters that describe cell performance such as throwing power are measured in small test cells of various designs that aim to reproduce conditions similar to those found in the production plating bath.[9]
$
5
Question: Throwing power provides a measure of:
A: Temperature of the electroplating solution.
B: Concentration of the electrolyte.
C: Uniformity of the electroplated metal thickness.
D: Speed of the electroplating process.
E: Voltage of the external power supply.
Answer: C

Question: Micro throwing power is related to:
A: The overall size of the object being plated.
B: The color of the plating.
C: Filling coat small recesses.
D: The speed of the electroplating.
E: The weight of the plating.
Answer: C

Question: In a Haring-Blum cell, if L = 5 and the plating thickness ratio of M = 6, what is the Haring-Blum throwing power?
A: 10%
B: 0%
C: -20%
D: 20%
E: -10%
Answer: C

Question: A larger (less negative) throwing power according to the given definitions leads to:
A: More uneven thickness.
B: Increased time of plating.
C: More uniform thickness.
D: Lesser plate thickness.
E: Lower quality of metal coat.
Answer: C

Question: Test cells used to measure parameters like throwing power aim to:
A: Increase the cost of production.
B: Reduce the quality of plating.
C: Reproduce conditions similar to the production plating bath.
D: Test new types of electrolytes.
E: Modify the electroplating process.
Answer: C
@
In materials science, hardness (antonym: softness) is a measure of the resistance to localized plastic deformation induced by either mechanical indentation or abrasion. In general, different materials differ in their hardness; for example hard metals such as titanium and beryllium are harder than soft metals such as sodium and metallic tin, or wood and common plastics. Macroscopic hardness is generally characterized by strong intermolecular bonds, but the behavior of solid materials under force is complex; therefore, hardness can be measured in different ways, such as scratch hardness, indentation hardness, and rebound hardness. Hardness is dependent on ductility, elastic stiffness, plasticity, strain, strength, toughness, viscoelasticity, and viscosity. Common examples of hard matter are ceramics, concrete, certain metals, and superhard materials, which can be contrasted with soft matter.
$
5
Question: What is the antonym of hardness in materials science?
A: Roughness
B: Fragility
C: Softness
D: Porosity
E: Elasticity
Answer: C

Question: Which metal is used as an example of a soft metal?
A: Titanium
B: Copper
C: Sodium
D: Iron
E: Beryllium
Answer: C

Question: Which of the following is NOT a method to measure hardness?
A: Scratch hardness
B: Indentation hardness
C: Rebound hardness
D: Reflectivity hardness
E: Ductility measurement
Answer: D

Question: Hardness is dependent on all EXCEPT:
A: Ductility
B: Elastic stiffness
C: Reflectivity
D: Toughness
E: Viscosity
Answer: C

Question: Which of the following is a common example of soft matter?
A: Ceramics
B: Concrete
C: Superhard materials
D: Wood
E: Diamond
Answer: D
@
There are three main types of hardness measurements: scratch, indentation, and rebound. Within each of these classes of measurement there are individual measurement scales. For practical reasons conversion tables are used to convert between one scale and another.

Scratch hardness is the measure of how resistant a sample is to fracture or permanent plastic deformation due to friction from a sharp object.[1] The principle is that an object made of a harder material will scratch an object made of a softer material. When testing coatings, scratch hardness refers to the force necessary to cut through the film to the substrate. The most common test is Mohs scale, which is used in mineralogy. One tool to make this measurement is the sclerometer.

Indentation hardness measures the resistance of a sample to material deformation due to a constant compression load from a sharp object. Tests for indentation hardness are primarily used in engineering and metallurgy. The tests work on the basic premise of measuring the critical dimensions of an indentation left by a specifically dimensioned and loaded indenter. Common indentation hardness scales are Rockwell, Vickers, Shore, and Brinell, amongst others.

Rebound hardness, also known as dynamic hardness, measures the height of the "bounce" of a diamond-tipped hammer dropped from a fixed height onto a material. This type of hardness is related to elasticity. The device used to take this measurement is known as a scleroscope.[3] Two scales that measures rebound hardness are the Leeb rebound hardness test and Bennett hardness scale. Ultrasonic Contact Impedance (UCI) method determines hardness by measuring the frequency of an oscillating rod. The rod consists of a metal shaft with vibrating element and a pyramid-shaped diamond mounted on one end.[4]
$
5
Question: What does scratch hardness measure?
A: Resistance to elastic deformation
B: Resistance to localized plastic deformation
C: Height of a bounce of a hammer
D: Resistance to fracture due to friction
E: Resistance to tension
Answer: D

Question: Which hardness scale is commonly used in mineralogy?
A: Rockwell
B: Shore
C: Brinell
D: Vickers
E: Mohs
Answer: E

Question: In the rebound hardness test, what measures the height of the "bounce"?
A: Sclerometer
B: Mohs device
C: Scleroscope
D: Indentometer
E: Durometer
Answer: C

Question: Which method measures the resistance of a sample to material deformation due to a constant compression load?
A: Scratch hardness
B: Rebound hardness
C: Indentation hardness
D: Elastic hardness
E: Tensile hardness
Answer: C

Question: The Ultrasonic Contact Impedance method measures hardness by:
A: Applying a constant load
B: Scratching with a sharp object
C: Measuring the height of a bounce
D: Dropping a diamond-tipped hammer
E: Measuring the frequency of an oscillating rod
Answer: E
@
The key to understanding the mechanism behind hardness is understanding the metallic microstructure, or the structure and arrangement of the atoms at the atomic level. In fact, most important metallic properties critical to the manufacturing of today’s goods are determined by the microstructure of a material.[7] At the atomic level, the atoms in a metal are arranged in an orderly three-dimensional array called a crystal lattice. In reality, however, a given specimen of a metal likely never contains a consistent single crystal lattice. A given sample of metal will contain many grains, with each grain having a fairly consistent array pattern. At an even smaller scale, each grain contains irregularities.

There are two types of irregularities at the grain level of the microstructure that are responsible for the hardness of the material. These irregularities are point defects and line defects. A point defect is an irregularity located at a single lattice site inside of the overall three-dimensional lattice of the grain. There are three main point defects. If there is an atom missing from the array, a vacancy defect is formed. If there is a different type of atom at the lattice site that should normally be occupied by a metal atom, a substitutional defect is formed. If there exists an atom in a site where there should normally not be, an interstitial defect is formed. This is possible because space exists between atoms in a crystal lattice. While point defects are irregularities at a single site in the crystal lattice, line defects are irregularities on a plane of atoms. Dislocations are a type of line defect involving the misalignment of these planes. In the case of an edge dislocation, a half plane of atoms is wedged between two planes of atoms. In the case of a screw dislocation two planes of atoms are offset with a helical array running between them.[8]
$
5
Question: At what level are atoms in a metal arranged in an orderly three-dimensional array?
A: Microscopic
B: Mesoscopic
C: Atomic
D: Grain
E: Molecular
Answer: C

Question: What kind of defect involves an atom being present in a location where there normally wouldn't be an atom?
A: Vacancy defect
B: Substitutional defect
C: Interstitial defect
D: Dislocation defect
E: Edge dislocation
Answer: C

Question: What type of defect involves the misalignment of planes of atoms?
A: Point defect
B: Edge dislocation
C: Screw dislocation
D: Vacancy defect
E: Substitutional defect
Answer: B

Question: In the context of metallic microstructure, what is a grain?
A: A type of defect
B: A measure of hardness
C: An irregularity in the lattice
D: A consistent single crystal lattice
E: A unit of weight
Answer: D

Question: Which defect is formed when there's a different type of atom at the lattice site than the metal atom?
A: Interstitial defect
B: Dislocation
C: Vacancy defect
D: Substitutional defect
E: Edge dislocation
Answer: D
@
Careful note should be taken of the relationship between a hardness number and the stress-strain curve exhibited by the material. The latter, which is conventionally obtained via tensile testing, captures the full plasticity response of the material (which is in most cases a metal). It is in fact a dependence of the (true) von Mises plastic strain on the (true) von Mises stress, but this is readily obtained from a nominal stress – nominal strain curve (in the pre-necking regime), which is the immediate outcome of a tensile test. This relationship can be used to describe how the material will respond to almost any loading situation, often by using the Finite Element Method (FEM). This applies to the outcome of an indentation test (with a given size and shape of indenter, and a given applied load).

However, while a hardness number thus depends on the stress-strain relationship, inferring the latter from the former is far from simple and is not attempted in any rigorous way during conventional hardness testing. (In fact, the Indentation Plastometry technique, which involves iterative FEM modelling of an indentation test, does allow a stress-strain curve to be obtained via indentation, but this is outside the scope of conventional hardness testing.) A hardness number is just a semi-quantitative indicator of the resistance to plastic deformation. Although hardness is defined in a similar way for most types of test – usually as the load divided by the contact area – the numbers obtained for a particular material are different for different types of test, and even for the same test with different applied loads. Attempts are sometimes made[11][12][13][14][15] to identify simple analytical expressions that allow features of the stress-strain curve, particularly the yield stress and Ultimate Tensile Stress (UTS), to be obtained from a particular type of hardness number. However, these are all based on empirical correlations, often specific to particular types of alloy: even with such a limitation, the values obtained are often quite unreliable. The underlying problem is that metals with a range of combinations of yield stress and work hardening characteristics can exhibit the same hardness number. The use of hardness numbers for any quantitative purpose should, at best, be approached with considerable caution.
$
5
Question: What captures the full plasticity response of a material, typically obtained via tensile testing?
A: Hardness number
B: Elastic modulus
C: Stress-strain curve
D: Fracture toughness
E: Density
Answer: C

Question: Hardness numbers are different for different tests and even for the same test with different applied loads because hardness is usually defined as:
A: The material's ductility
B: The load divided by the contact area
C: The elasticity of the material
D: The metal's yield stress
E: The fracture point
Answer: B

Question: Indentation Plastometry technique allows what to be obtained via indentation?
A: Elasticity
B: Ductility
C: Stress-strain curve
D: Hardness number
E: Yield stress
Answer: C

Question: Which of the following is NOT a type of hardness number?
A: Rockwell
B: Mohs
C: Ultimate Tensile Stress (UTS)
D: Brinell
E: Vickers
Answer: C

Question: What is a potential problem when trying to correlate hardness numbers to features of the stress-strain curve?
A: Metals are all identical in structure
B: Hardness numbers are always accurate
C: Metals with different yield stress and work hardening can have the same hardness number
D: Stress-strain curves are not useful
E: Hardness numbers are independent of stress-strain curves
Answer: C
@
The field of strength of materials (also called mechanics of materials) typically refers to various methods of calculating the stresses and strains in structural members, such as beams, columns, and shafts. The methods employed to predict the response of a structure under loading and its susceptibility to various failure modes takes into account the properties of the materials such as its yield strength, ultimate strength, Young's modulus, and Poisson's ratio. In addition, the mechanical element's macroscopic properties (geometric properties) such as its length, width, thickness, boundary constraints and abrupt changes in geometry such as holes are considered.

The theory began with the consideration of the behavior of one and two dimensional members of structures, whose states of stress can be approximated as two dimensional, and was then generalized to three dimensions to develop a more complete theory of the elastic and plastic behavior of materials. An important founding pioneer in mechanics of materials was Stephen Timoshenko.
$
5
Question: Which property of a material describes its ability to return to its original shape after being deformed?
A: Ultimate strength
B: Yield strength
C: Young's modulus
D: Poisson's ratio
E: Length
Answer: C

Question: Which factor is NOT typically taken into consideration when predicting the response of a structural member under loading?
A: Young's modulus
B: Ultimate strength
C: Color
D: Yield strength
E: Thickness
Answer: C

Question: Abrupt changes in geometry such as holes in a material primarily affect its:
A: Young's modulus
B: Poisson's ratio
C: Yield strength
D: Ultimate strength
E: Mechanical element's macroscopic properties
Answer: E

Question: Who was a founding pioneer in the mechanics of materials?
A: Isaac Newton
B: Albert Einstein
C: Stephen Timoshenko
D: Richard Feynman
E: Niels Bohr
Answer: C

Question: The initial theory in mechanics of materials began with consideration of:
A: Three-dimensional members
B: The behavior of gases
C: One and two-dimensional members
D: The behavior of liquids
E: The effects of temperature on materials
Answer: C
@
In the mechanics of materials, the strength of a material is its ability to withstand an applied load without failure or plastic deformation. The field of strength of materials deals with forces and deformations that result from their acting on a material. A load applied to a mechanical member will induce internal forces within the member called stresses when those forces are expressed on a unit basis. The stresses acting on the material cause deformation of the material in various manners including breaking them completely. Deformation of the material is called strain when those deformations too are placed on a unit basis.

The stresses and strains that develop within a mechanical member must be calculated in order to assess the load capacity of that member. This requires a complete description of the geometry of the member, its constraints, the loads applied to the member and the properties of the material of which the member is composed. The applied loads may be axial (tensile or compressive), or rotational (strength shear). With a complete description of the loading and the geometry of the member, the state of stress and state of strain at any point within the member can be calculated. Once the state of stress and strain within the member is known, the strength (load carrying capacity) of that member, its deformations (stiffness qualities), and its stability (ability to maintain its original configuration) can be calculated.

The calculated stresses may then be compared to some measure of the strength of the member such as its material yield or ultimate strength. The calculated deflection of the member may be compared to deflection criteria that are based on the member's use. The calculated buckling load of the member may be compared to the applied load. The calculated stiffness and mass distribution of the member may be used to calculate the member's dynamic response and then compared to the acoustic environment in which it will be used.

Material strength refers to the point on the engineering stress–strain curve (yield stress) beyond which the material experiences deformations that will not be completely reversed upon removal of the loading and as a result, the member will have a permanent deflection. The ultimate strength of the material refers to the maximum value of stress reached. The fracture strength is the stress value at fracture (the last stress value recorded).
$
5
Question: When forces are expressed on a unit basis within a mechanical member, they are termed as:
A: Loads
B: Stresses
C: Strains
D: Deformations
E: Energies
Answer: B

Question: Deformation of material on a unit basis is termed as:
A: Stiffness
B: Load
C: Strain
D: Stress
E: Stability
Answer: C

Question: What kind of load induces rotational forces within a member?
A: Compressive
B: Tensile
C: Shear
D: Axial
E: Normal
Answer: C

Question: The point on the engineering stress–strain curve where the material experiences permanent deformation is called:
A: Ultimate strength
B: Material strength
C: Fracture strength
D: Yield stress
E: Deformation point
Answer: D

Question: Which strength of a material refers to the maximum value of stress achieved?
A: Yield strength
B: Fracture strength
C: Elastic strength
D: Ultimate strength
E: Deformation strength
Answer: D
@
There are four failure theories: maximum shear stress theory, maximum normal stress theory, maximum strain energy theory, and maximum distortion energy theory. Out of these four theories of failure, the maximum normal stress theory is only applicable for brittle materials, and the remaining three theories are applicable for ductile materials. Of the latter three, the distortion energy theory provides most accurate results in a majority of the stress conditions. The strain energy theory needs the value of Poisson's ratio of the part material, which is often not readily available. The maximum shear stress theory is conservative. For simple unidirectional normal stresses all theories are equivalent, which means all theories will give the same result.

Maximum Shear Stress Theory – This theory postulates that failure will occur if the magnitude of the maximum shear stress in the part exceeds the shear strength of the material determined from uniaxial testing.
Maximum Normal Stress Theory – This theory postulates that failure will occur if the maximum normal stress in the part exceeds the ultimate tensile stress of the material as determined from uniaxial testing. This theory deals with brittle materials only. The maximum tensile stress should be less than or equal to ultimate tensile stress divided by factor of safety. The magnitude of the maximum compressive stress should be less than ultimate compressive stress divided by factor of safety.
Maximum Strain Energy Theory – This theory postulates that failure will occur when the strain energy per unit volume due to the applied stresses in a part equals the strain energy per unit volume at the yield point in uniaxial testing.
Maximum Distortion Energy Theory – This theory is also known as shear energy theory or von Mises-Hencky theory. This theory postulates that failure will occur when the distortion energy per unit volume due to the applied stresses in a part equals the distortion energy per unit volume at the yield point in uniaxial testing. The total elastic energy due to strain can be divided into two parts: one part causes change in volume, and the other part causes change in shape. Distortion energy is the amount of energy that is needed to change the shape.
Fracture mechanics was established by Alan Arnold Griffith and George Rankine Irwin. This important theory is also known as numeric conversion of toughness of material in the case of crack existence.
$
5
Question: Which failure theory is applicable only for brittle materials?
A: Maximum shear stress theory
B: Maximum normal stress theory
C: Maximum strain energy theory
D: Maximum distortion energy theory
E: None of the above
Answer: B

Question: The distortion energy theory is also known as:
A: Maximum strain energy theory
B: Shear energy theory
C: Von Mises-Hencky theory
D: Maximum shear stress theory
E: Maximum normal stress theory
Answer: C

Question: Fracture mechanics was largely developed by:
A: Stephen Timoshenko
B: Isaac Newton
C: Alan Arnold Griffith and George Rankine Irwin
D: Richard Feynman
E: Albert Einstein
Answer: C

Question: The theory that postulates failure will occur when the strain energy per unit volume equals the strain energy at the yield point in uniaxial testing is the:
A: Maximum shear stress theory
B: Maximum normal stress theory
C: Maximum strain energy theory
D: Maximum distortion energy theory
E: Von Mises-Hencky theory
Answer: C

Question: For simple unidirectional normal stresses, how many of the listed theories provide equivalent results?
A: One
B: Two
C: Three
D: Four
E: None of the above
Answer: D
@
Ultimate strength is an attribute related to a material, rather than just a specific specimen made of the material, and as such it is quoted as the force per unit of cross section area (N/m2). The ultimate strength is the maximum stress that a material can withstand before it breaks or weakens.[12] For example, the ultimate tensile strength (UTS) of AISI 1018 Steel is 440 MPa. In Imperial units, the unit of stress is given as lbf/in² or pounds-force per square inch. This unit is often abbreviated as psi. One thousand psi is abbreviated ksi.

A factor of safety is a design criteria that an engineered component or structure must achieve. 
�
�
=
�
�
�
/
�
FS = UTS/R, where FS: the factor of safety, R: The applied stress, and UTS: ultimate stress (psi or N/m2)[13]

Margin of Safety is also sometimes used to as design criteria. It is defined MS = Failure Load/(Factor of Safety × Predicted Load) − 1.

For example, to achieve a factor of safety of 4, the allowable stress in an AISI 1018 steel component can be calculated to be 
�
=
�
�
�
/
�
�
R = UTS/FS = 440/4 = 110 MPa, or 
�
R = 110×106 N/m2. Such allowable stresses are also known as "design stresses" or "working stresses".
$
5
Question: What does the Ultimate Tensile Strength (UTS) of a material represent?
A: The maximum compressive strength of a material
B: The maximum stress a material can withstand before breaking
C: The point at which a material will deform plastically
D: The yield point of a material
E: The elasticity of a material
Answer: B

Question: The unit of stress in the Imperial system is often abbreviated as:
A: MPa
B: N/m2
C: lbf
D: ksi
E: psi
Answer: E

Question: How is the "Factor of Safety" defined in relation to Ultimate Stress (UTS) and Applied Stress (R)?
A: FS = UTS + R
B: FS = R/UTS
C: FS = UTS/R
D: FS = R x UTS
E: FS = R - UTS
Answer: C

Question: If the Ultimate Tensile Strength (UTS) of AISI 1018 Steel is 440 MPa, what would be the allowable stress to achieve a factor of safety of 4?
A: 55 MPa
B: 880 MPa
C: 110 MPa
D: 220 MPa
E: 1760 MPa
Answer: C

Question: The term used to describe the calculated allowable stresses based on factors of safety is:
A: Yield stresses
B: Ultimate stresses
C: Deformation stresses
D: Design stresses
E: Fracture stresses
Answer: D
@
Ultimate tensile strength (also called UTS, tensile strength, TS, ultimate strength or 
�
tu
{\displaystyle F_{\text{tu}}} in notation)[1] is the maximum stress that a material can withstand while being stretched or pulled before breaking. In brittle materials the ultimate tensile strength is close to the yield point, whereas in ductile materials the ultimate tensile strength can be higher.

The ultimate tensile strength is usually found by performing a tensile test and recording the engineering stress versus strain. The highest point of the stress–strain curve is the ultimate tensile strength and has units of stress. The equivalent point for the case of compression, instead of tension, is called the compressive strength.

Tensile strengths are rarely of any consequence in the design of ductile members, but they are important with brittle members. They are tabulated for common materials such as alloys, composite materials, ceramics, plastics, and wood.
$
5
Question: What is the maximum stress that a material can withstand while being stretched or pulled before it breaks?
A: Compressive strength
B: Yield point
C: Ultimate tensile strength
D: Engineering strain
E: Ductile strength
Answer: C

Question: In which type of material is the ultimate tensile strength close to the yield point?
A: Ductile
B: Malleable
C: Brittle
D: Elastic
E: Composite
Answer: C

Question: What is the point of maximum stress in compression called?
A: Ultimate compression strength
B: Compressive strength
C: Yield point
D: Tensile strength
E: Ductile strength
Answer: B

Question: In which type of material members is tensile strength rarely considered during design?
A: Brittle
B: Ductile
C: Composite
D: Plastic
E: Wood
Answer: B

Question: Which testing method is commonly used to determine the ultimate tensile strength of a material?
A: Compression test
B: Torsion test
C: Hardness test
D: Tensile test
E: Impact test
Answer: D
@
The ultimate tensile strength of a material is an intensive property; therefore its value does not depend on the size of the test specimen. However, depending on the material, it may be dependent on other factors, such as the preparation of the specimen, the presence or otherwise of surface defects, and the temperature of the test environment and material.

Some materials break very sharply, without plastic deformation, in what is called a brittle failure. Others, which are more ductile, including most metals, experience some plastic deformation and possibly necking before fracture.

Tensile strength is defined as a stress, which is measured as force per unit area. For some non-homogeneous materials (or for assembled components) it can be reported just as a force or as a force per unit width. In the International System of Units (SI), the unit is the pascal (Pa) (or a multiple thereof, often megapascals (MPa), using the SI prefix mega); or, equivalently to pascals, newtons per square metre (N/m2). A United States customary unit is pounds per square inch (lb/in2 or psi). Kilopounds per square inch (ksi, or sometimes kpsi) is equal to 1000 psi, and is commonly used in the United States, when measuring tensile strengths.
$
5
Question: What determines the value of the ultimate tensile strength of a material?
A: Size of the test specimen
B: Preparation of the specimen
C: Thickness of the specimen
D: Color of the specimen
E: Shape of the specimen
Answer: B

Question: What type of failure is characterized by materials breaking without plastic deformation?
A: Ductile failure
B: Malleable failure
C: Brittle failure
D: Elastic failure
E: Yield failure
Answer: C

Question: What is the SI unit for tensile strength?
A: Newton
B: Megapascal
C: Joule
D: Kelvin
E: Ampere
Answer: B

Question: How is tensile strength defined in terms of stress?
A: Force multiplied by area
B: Force per unit length
C: Force per unit area
D: Force per unit volume
E: Area per unit force
Answer: C

Question: Which unit is commonly used in the United States when measuring tensile strengths?
A: Pascal
B: Megapascal
C: Newton per square meter
D: Pounds per square inch
E: Kilopascal
Answer: D
@
Many materials can display linear elastic behavior, defined by a linear stress–strain relationship, as shown in figure 1 up to point 3. The elastic behavior of materials often extends into a non-linear region, represented in figure 1 by point 2 (the "yield strength"), up to which deformations are completely recoverable upon removal of the load; that is, a specimen loaded elastically in tension will elongate, but will return to its original shape and size when unloaded. Beyond this elastic region, for ductile materials, such as steel, deformations are plastic. A plastically deformed specimen does not completely return to its original size and shape when unloaded. For many applications, plastic deformation is unacceptable, and is used as the design limitation.

After the yield point, ductile metals undergo a period of strain hardening, in which the stress increases again with increasing strain, and they begin to neck, as the cross-sectional area of the specimen decreases due to plastic flow. In a sufficiently ductile material, when necking becomes substantial, it causes a reversal of the engineering stress–strain curve (curve A, figure 2); this is because the engineering stress is calculated assuming the original cross-sectional area before necking. The reversal point is the maximum stress on the engineering stress–strain curve, and the engineering stress coordinate of this point is the ultimate tensile strength, given by point 1.

Ultimate tensile strength is not used in the design of ductile static members because design practices dictate the use of the yield stress. It is, however, used for quality control, because of the ease of testing. It is also used to roughly determine material types for unknown samples.[2]

The ultimate tensile strength is a common engineering parameter to design members made of brittle material because such materials have no yield point.[2]
$
5
Question: Up to which point are deformations in materials completely recoverable upon removal of the load?
A: Ultimate tensile strength
B: Elastic limit
C: Ductile limit
D: Yield strength
E: Necking point
Answer: D

Question: In what type of deformation does a specimen not completely return to its original size and shape when unloaded?
A: Elastic deformation
B: Plastic deformation
C: Ductile deformation
D: Brittle deformation
E: Linear deformation
Answer: B

Question: What occurs in a material after the yield point and before necking?
A: Plastic flow
B: Strain hardening
C: Compressive strain
D: Elastic recovery
E: Ultimate stress
Answer: B

Question: Why is ultimate tensile strength often not used in the design of ductile static members?
A: It is too expensive to test.
B: Ductile materials do not have a tensile strength.
C: Design practices dictate the use of the yield stress.
D: It provides inaccurate results.
E: It is only relevant for brittle materials.
Answer: C

Question: In which type of material members is the ultimate tensile strength a common engineering parameter for design?
A: Ductile
B: Malleable
C: Brittle
D: Elastic
E: Composite
Answer: C
@
Typical tensile strengths of some materials
Material	Yield strength
(MPa)	Ultimate tensile strength
(MPa)	Density
(g/cm3)
Steel, structural ASTM A36 steel	250	400–550	7.8
Steel, 1090 mild	247	841	7.58
Chromium-vanadium steel AISI 6150	620	940	7.8
Steel, 2800 Maraging steel[4]	2,617	2,693	8.00
Steel, AerMet 340[5]	2,160	2,430	7.86
Steel, Sandvik Sanicro 36Mo logging cable precision wire[6]	1,758	2,070	8.00
Steel, AISI 4130,
water quenched 855 °C (1,570 °F), 480 °C (900 °F) temper[7]	951	1,110	7.85
Steel, API 5L X65[8]	448	531	7.8
Steel, high strength alloy ASTM A514	690	760	7.8
Acrylic, clear cast sheet (PMMA)[9]	72	87[10]	1.16
High-density polyethylene (HDPE)	26–33	37	0.85
Polypropylene	12–43	19.7–80	0.91
Steel, stainless AISI 302[11]	275	620	7.86
Cast iron 4.5% C, ASTM A-48	130	200	7.3
"Liquidmetal" alloy[citation needed]	1,723	550–1,600	6.1
Beryllium[12] 99.9% Be	345	448	1.84
Aluminium alloy[13] 2014-T6	414	483	2.8
Polyester resin (unreinforced)[14]	55	55	 
Polyester and chopped strand mat laminate 30% E-glass[14]	100	100	 
S-Glass epoxy composite[15]	2,358	2,358	 
Aluminium alloy 6061-T6	241	300	2.7
Copper 99.9% Cu	70	220[citation needed]	8.92
Cupronickel 10% Ni, 1.6% Fe, 1% Mn, balance Cu	130	350	8.94
Brass	200 +	500	8.73
Tungsten	941	1,510	19.25
Glass	 	33[16]	2.53
E-Glass	—	1,500 for laminates,
3,450 for fibers alone	2.57
S-Glass	—	4,710	2.48
Basalt fiber[17]	—	4,840	2.7
Marble	—	15	2.6
Concrete	—	2–5	2.7
Carbon fiber	—	1,600 for laminates,
4,137 for fibers alone	1.75
Carbon fiber (Toray T1100G)[18]
(the strongest human-made fibres)	 	7,000 fibre alone	1.79
Human hair	140–160	200–250[19]	 
Bamboo fiber	 	350–500	0.4–0.8
Spider silk (see note below)		1,000	1.3
Spider silk, Darwin's bark spider[20]	1,652		
Silkworm silk	500	 	1.3
Aramid (Kevlar or Twaron)	3,620	3,757	1.44
UHMWPE[21]	24	52	0.97
UHMWPE fibers[22][23] (Dyneema or Spectra)		2,300–3,500	0.97
Vectran	 	2,850–3,340	1.4
Polybenzoxazole (Zylon)[24]	2,700	5,800	1.56
Wood, pine (parallel to grain)	 	40	 
Bone (limb)	104–121	130	1.6
Nylon, molded, 6PLA/6M [25]		75-85	1.15
Nylon fiber, drawn[26]		900[27]	1.13
Epoxy adhesive	—	12–30[28]	—
Rubber	—	16	 
Boron	—	3,100	2.46
Silicon, monocrystalline (m-Si)	—	7,000	2.33
Ultra-pure silica glass fiber-optic strands[29]		4,100	
Sapphire (Al2O3)	400 at 25 °C,
275 at 500 °C,
345 at 1,000 °C	1,900	3.9–4.1
Boron nitride nanotube	—	33,000	2.62[30]
Diamond	1,600	2,800
~80–90 GPa at microscale[31]	3.5
Graphene	—	intrinsic 130,000;[32]
engineering 50,000–60,000[33]	1.0
First carbon nanotube ropes	?	3,600	1.3
Carbon nanotube (see note below)	—	11,000–63,000	0.037–1.34
Carbon nanotube composites	—	1,200[34]	—
High-strength carbon nanotube film	—	9,600[35]	—
Iron (pure mono-crystal)		3	7.874
Limpet Patella vulgata teeth (goethite whisker nanocomposite)		4,900
3,000–6,500[36]	
$
5
Question: Which of the following materials has an ultimate tensile strength range of 400-550 MPa?
A: Steel, structural ASTM A36 steel
B: Acrylic, clear cast sheet
C: Copper 99.9% Cu
D: Marble
E: Human hair
Answer: A

Question: What is the density of Chromium-vanadium steel AISI 6150 in g/cm3?
A: 6.1
B: 7.58
C: 7.8
D: 8.00
E: 8.92
Answer: C

Question: Which material has an ultimate tensile strength of approximately 7,000 when measured for fibre alone?
A: Carbon fiber
B: Bamboo fiber
C: Spider silk, Darwin's bark spider
D: Carbon nanotube
E: Carbon fiber (Toray T1100G)
Answer: E

Question: What is the density of Brass in g/cm3?
A: 7.3
B: 7.8
C: 8.00
D: 8.73
E: 8.92
Answer: D

Question: Which of the following materials does not have its ultimate tensile strength value mentioned?
A: Steel, 1090 mild
B: Aluminium alloy 6061-T6
C: Silkworm silk
D: E-Glass
E: Aramid (Kevlar or Twaron)
Answer: D
@
A carbon nanotube (CNT) is a tube made of carbon with a diameter in the nanometer range (nanoscale). They are one of the allotropes of carbon.

Single-walled carbon nanotubes (SWCNTs) have diameters around 0.5–2.0 nanometers, about 100,000 times smaller than the width of a human hair. They can be idealized as cutouts from a two-dimensional graphene sheet rolled up to form a hollow cylinder.[1]

Multi-walled carbon nanotubes (MWCNTs) consist of nested single-wall carbon nanotubes[1] in a nested, tube-in-tube structure.[2] Double- and triple-walled carbon nanotubes are special cases of MWCNT.

Carbon nanotubes can exhibit remarkable properties, such as exceptional tensile strength[3] and thermal conductivity[4][5][6] because of their nanostructure and strength of the bonds between carbon atoms. Some SWCNT structures exhibit high electrical conductivity[7][8] while others are semiconductors.[9][10] In addition, carbon nanotubes can be chemically modified.[11] These properties are expected to be valuable in many areas of technology, such as electronics, optics, composite materials (replacing or complementing carbon fibers), nanotechnology, and other applications of materials science.
$
5
Question: Which structure of carbon is described as a hollow cylinder derived from a graphene sheet?
A: Fullerene
B: Diamond
C: Carbon fiber
D: Carbon nanotube
E: Graphite
Answer: D

Question: What describes the diameter of Single-walled carbon nanotubes (SWCNTs)?
A: 0.5–2.0 meters
B: 5-10 nanometers
C: 50-100 nanometers
D: 0.5–2.0 nanometers
E: 5-10 millimeters
Answer: D

Question: What is the structure of Multi-walled carbon nanotubes (MWCNTs)?
A: Single layer tubes
B: Nested single-wall carbon nanotubes
C: Carbon tubes without hollow center
D: Tubes with irregular structures
E: Multiple interconnected graphene sheets
Answer: B

Question: Which property of carbon nanotubes can be chemically modified?
A: Tensile strength
B: Thermal conductivity
C: Electrical conductivity
D: Allotropy
E: Size
Answer: C

Question: In what fields are the properties of carbon nanotubes expected to be valuable?
A: Genetics
B: Geology
C: Electronics and optics
D: Astronomy
E: Oceanography
Answer: C
@
The structure of an ideal (infinitely long) single-walled carbon nanotube is that of a regular hexagonal lattice drawn on an infinite cylindrical surface, whose vertices are the positions of the carbon atoms. Since the length of the carbon-carbon bonds is fairly fixed, there are constraints on the diameter of the cylinder and the arrangement of the atoms on it.[15]

In the study of nanotubes, one defines a zigzag path on a graphene-like lattice as a path that turns 60 degrees, alternating left and right, after stepping through each bond. It is also conventional to define an armchair path as one that makes two left turns of 60 degrees followed by two right turns every four steps. On some carbon nanotubes, there is a closed zigzag path that goes around the tube. One says that the tube is of the zigzag type or configuration, or simply is a zigzag nanotube. If the tube is instead encircled by a closed armchair path, it is said to be of the armchair type, or an armchair nanotube. An infinite nanotube that is of the zigzag (or armchair) type consists entirely of closed zigzag (or armchair) paths, connected to each other.

The zigzag and armchair configurations are not the only structures that a single-walled nanotube can have. To describe the structure of a general infinitely long tube, one should imagine it being sliced open by a cut parallel to its axis, that goes through some atom A, and then unrolled flat on the plane, so that its atoms and bonds coincide with those of an imaginary graphene sheet—more precisely, with an infinitely long strip of that sheet. The two halves of the atom A will end up on opposite edges of the strip, over two atoms A1 and A2 of the graphene. The line from A1 to A2 will correspond to the circumference of the cylinder that went through the atom A, and will be perpendicular to the edges of the strip. In the graphene lattice, the atoms can be split into two classes, depending on the directions of their three bonds. Half the atoms have their three bonds directed the same way, and half have their three bonds rotated 180 degrees relative to the first half. The atoms A1 and A2, which correspond to the same atom A on the cylinder, must be in the same class. It follows that the circumference of the tube and the angle of the strip are not arbitrary, because they are constrained to the lengths and directions of the lines that connect pairs of graphene atoms in the same class.
$
5
Question: What pattern turns 60 degrees alternating left and right after stepping through each bond?
A: Spiral
B: Armchair
C: Zigzag
D: Lattice
E: Helical
Answer: C

Question: If there's a closed armchair path that goes around the tube, what type of nanotube is it?
A: Spiral nanotube
B: Lattice nanotube
C: Zigzag nanotube
D: Armchair nanotube
E: Hexagonal nanotube
Answer: D

Question: What structure is formed by a regular hexagonal lattice drawn on an infinite cylindrical surface?
A: Diamond structure
B: Armchair nanotube
C: Fullerene
D: Zigzag nanotube
E: Single-walled carbon nanotube
Answer: E

Question: How can one describe the structure of a general infinitely long tube?
A: By its chirality
B: By unrolling it flat on the plane after slicing it open
C: By its molecular weight
D: By the number of carbon atoms
E: By its electrical properties
Answer: B

Question: On a graphene lattice, how many directions do the bonds of atoms have?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: C
@
Unlike graphene, which is a two-dimensional semimetal, carbon nanotubes are either metallic or semiconducting along the tubular axis. For a given (n,m) nanotube, if n = m, the nanotube is metallic; if n − m is a multiple of 3 and n ≠ m, then the nanotube is quasi-metallic with a very small band gap, otherwise the nanotube is a moderate semiconductor.[56] Thus, all armchair (n = m) nanotubes are metallic, and nanotubes (6,4), (9,1), etc. are semiconducting.[57] Carbon nanotubes are not semimetallic because the degenerate point (the point where the π [bonding] band meets the π* [anti-bonding] band, at which the energy goes to zero) is slightly shifted away from the K point in the Brillouin zone because of the curvature of the tube surface, causing hybridization between the σ* and π* anti-bonding bands, modifying the band dispersion.

The rule regarding metallic versus semiconductor behavior has exceptions because curvature effects in small-diameter tubes can strongly influence electrical properties. Thus, a (5,0) SWCNT that should be semiconducting in fact is metallic according to the calculations. Likewise, zigzag and chiral SWCNTs with small diameters that should be metallic have a finite gap (armchair nanotubes remain metallic).[57] In theory, metallic nanotubes can carry an electric current density of 4 × 109 A/cm2, which is more than 1,000 times greater than those of metals such as copper,[58] where for copper interconnects, current densities are limited by electromigration. Carbon nanotubes are thus being explored as interconnects and conductivity-enhancing components in composite materials, and many groups are attempting to commercialize highly conducting electrical wire assembled from individual carbon nanotubes. There are significant challenges to be overcome however, such as undesired current saturation under voltage,[59] and the much more resistive nanotube-to-nanotube junctions and impurities, all of which lower the electrical conductivity of the macroscopic nanotube wires by orders of magnitude, as compared to the conductivity of the individual nanotubes.

Because of its nanoscale cross-section, electrons propagate only along the tube's axis. As a result, carbon nanotubes are frequently referred to as one-dimensional conductors. The maximum electrical conductance of a single-walled carbon nanotube is 2G0, where G0 = 2e2/h is the conductance of a single ballistic quantum channel.[60]
$
5
Question: What determines if a nanotube is metallic or semiconducting?
A: Length of the tube
B: The angle between the carbon atoms
C: The (n,m) value
D: The type of carbon
E: The radius of the tube
Answer: C

Question: Which type of nanotube is always metallic?
A: Zigzag
B: Chiral
C: Helical
D: Armchair
E: Spiral
Answer: D

Question: In theory, what electric current density can metallic nanotubes carry?
A: 4 × 107 A/cm2
B: 4 × 108 A/cm2
C: 4 × 106 A/cm2
D: 4 × 109 A/cm2
E: 4 × 105 A/cm2
Answer: D

Question: Carbon nanotubes are described as:
A: Two-dimensional conductors
B: Three-dimensional conductors
C: One-dimensional conductors
D: Zero-dimensional conductors
E: Semi-dimensional conductors
Answer: C

Question: The maximum electrical conductance of a single-walled carbon nanotube is defined by the value:
A: G0 = e2/h
B: G0 = 2e2/h
C: G0 = e3/h
D: G0 = e/h
E: G0 = 3e2/h
Answer: B
@
Carbon nanotubes are the strongest and stiffest materials yet discovered in terms of tensile strength and elastic modulus. This strength results from the covalent sp2 bonds formed between the individual carbon atoms. In 2000, a multiwalled carbon nanotube was tested to have a tensile strength of 63 gigapascals (9,100,000 psi).[3] (For illustration, this translates into the ability to endure tension of a weight equivalent to 6,422 kilograms-force (62,980 N; 14,160 lbf) on a cable with cross-section of 1 square millimetre (0.0016 sq in)). Further studies, such as one conducted in 2008, revealed that individual CNT shells have strengths of up to ≈100 gigapascals (15,000,000 psi), which is in agreement with quantum/atomistic models.[51] Because carbon nanotubes have a low density for a solid of 1.3 to 1.4 g/cm3,[52] its specific strength of up to 48,000 kN·m·kg−1 is the best of known materials, compared to high-carbon steel's 154 kN·m·kg−1.

Although the strength of individual CNT shells is extremely high, weak shear interactions between adjacent shells and tubes lead to significant reduction in the effective strength of multiwalled carbon nanotubes and carbon nanotube bundles down to only a few GPa.[53] This limitation has been recently addressed by applying high-energy electron irradiation, which crosslinks inner shells and tubes, and effectively increases the strength of these materials to ≈60 GPa for multiwalled carbon nanotubes[51] and ≈17 GPa for double-walled carbon nanotube bundles.[53] CNTs are not nearly as strong under compression. Because of their hollow structure and high aspect ratio, they tend to undergo buckling when placed under compressive, torsional, or bending stress.[54]

On the other hand, there was evidence that in the radial direction they are rather soft. The first transmission electron microscope observation of radial elasticity suggested that even van der Waals forces can deform two adjacent nanotubes. Later, nanoindentations with an atomic force microscope were performed by several groups to quantitatively measure radial elasticity of multiwalled carbon nanotubes and tapping/contact mode atomic force microscopy was also performed on single-walled carbon nanotubes. Young's modulus of on the order of several GPa showed that CNTs are in fact very soft in the radial direction.[citation needed]
$
5
Question: What makes carbon nanotubes the strongest and stiffest materials?
A: Hydrogen bonds
B: Ionic bonds
C: Van der Waals forces
D: Covalent sp2 bonds
E: Metallic bonds
Answer: D

Question: The tensile strength of a multiwalled carbon nanotube tested in 2000 was approximately:
A: 10 gigapascals
B: 63 gigapascals
C: 100 gigapascals
D: 45 gigapascals
E: 80 gigapascals
Answer: B

Question: What happens to multiwalled carbon nanotubes under compressive, torsional, or bending stress?
A: They become more resilient
B: They undergo elongation
C: They undergo buckling
D: They form crosslinks
E: They become more dense
Answer: C

Question: Which force was initially thought to be able to deform two adjacent nanotubes?
A: Ionic forces
B: Covalent forces
C: Electrostatic forces
D: Metallic forces
E: Van der Waals forces
Answer: E

Question: How are carbon nanotubes in the radial direction?
A: Extremely stiff
B: Elastic
C: Brittle
D: Soft
E: Indeterminate
Answer: D
@
Arc discharge
Nanotubes were observed in 1991 in the carbon soot of graphite electrodes during an arc discharge, by using a current of 100 amps, that was intended to produce fullerenes.[2] However the first macroscopic production of carbon nanotubes was made in 1992 by two researchers at NEC's Fundamental Research Laboratory.[3] The method used was the same as in 1991. During this process, the carbon contained in the negative electrode sublimates because of the high-discharge temperatures.

The yield for this method is up to 30% by weight and it produces both single- and multi-walled nanotubes with lengths of up to 50 micrometers with few structural defects.[4] Arc-discharge technique uses higher temperatures (above 1,700 °C) for CNT synthesis which typically causes the expansion of CNTs with fewer structural defects in comparison with other methods.[5]
$
5
Question: When were nanotubes first observed in the carbon soot of graphite electrodes?
A: 1981
B: 1991
C: 1995
D: 2001
E: 2010
Answer: B

Question: Which researchers were responsible for the first macroscopic production of carbon nanotubes?
A: Researchers at IBM
B: Researchers at Google
C: Researchers at MIT
D: Researchers at NEC's Fundamental Research Laboratory
E: Researchers at Harvard
Answer: D

Question: What is the typical temperature used in the arc-discharge technique for CNT synthesis?
A: 700 °C
B: 1,000 °C
C: 1,500 °C
D: 1,700 °C
E: 2,000 °C
Answer: D

Question: What advantage does the arc-discharge technique offer in terms of CNT structure?
A: Longer length
B: Faster production
C: Fewer structural defects
D: Increased diameter
E: Higher electrical conductivity
Answer: C

Question: Approximately how long can the nanotubes produced by the arc-discharge method be?
A: 10 micrometers
B: 25 micrometers
C: 30 micrometers
D: 40 micrometers
E: 50 micrometers
Answer: E
@
During CVD, a substrate is prepared with a layer of metal catalyst particles, most commonly nickel, cobalt,[15] iron, or a combination.[16] The metal nanoparticles can also be produced by other ways, including reduction of oxides or oxides solid solutions. The diameters of the nanotubes that are to be grown are related to the size of the metal particles. This can be controlled by patterned (or masked) deposition of the metal, annealing, or by plasma etching of a metal layer. The substrate is heated to approximately 700 °C. To initiate the growth of nanotubes, two gases are bled into the reactor: a process gas (such as ammonia, nitrogen or hydrogen) and a carbon-containing gas (such as acetylene, ethylene, ethanol or methane). Nanotubes grow at the sites of the metal catalyst; the carbon-containing gas is broken apart at the surface of the catalyst particle, and the carbon is transported to the edges of the particle, where it forms the nanotubes. This mechanism is still being studied.[17] The catalyst particles can stay at the tips of the growing nanotube during growth, or remain at the nanotube base, depending on the adhesion between the catalyst particle and the substrate.[18] Thermal catalytic decomposition of hydrocarbon has become an active area of research and can be a promising route for the bulk production of CNTs. Fluidised bed reactor is the most widely used reactor for CNT preparation. Scale-up of the reactor is the major challenge.[19][20]

CVD is the most widely used method for the production of carbon nanotubes.[21] For this purpose, the metal nanoparticles are mixed with a catalyst support such as MgO or Al2O3 to increase the surface area for higher yield of the catalytic reaction of the carbon feedstock with the metal particles. One issue in this synthesis route is the removal of the catalyst support via an acid treatment, which sometimes could destroy the original structure of the carbon nanotubes. However, alternative catalyst supports that are soluble in water have proven effective for nanotube growth.[22]

If a plasma is generated by the application of a strong electric field during growth (plasma-enhanced chemical vapor deposition), then the nanotube growth will follow the direction of the electric field.[23] By adjusting the geometry of the reactor it is possible to synthesize vertically aligned carbon nanotubes[24] (i.e., perpendicular to the substrate), a morphology that has been of interest to researchers interested in electron emission from nanotubes. Without the plasma, the resulting nanotubes are often randomly oriented. Under certain reaction conditions, even in the absence of a plasma, closely spaced nanotubes will maintain a vertical growth direction resulting in a dense array of tubes resembling a carpet or forest.

Of the various means for nanotube synthesis, CVD shows the most promise for industrial-scale deposition, because of its price/unit ratio, and because CVD is capable of growing nanotubes directly on a desired substrate, whereas the nanotubes must be collected in the other growth techniques. The growth sites are controllable by careful deposition of the catalyst.[
$
5
Question: Which metal is NOT commonly used as a catalyst particle during CVD?
A: Cobalt
B: Nickel
C: Iron
D: Silver
E: Cobalt
Answer: D

Question: In the CVD method, at what approximate temperature is the substrate heated?
A: 300 °C
B: 500 °C
C: 700 °C
D: 1,000 °C
E: 1,200 °C
Answer: C

Question: Which gas is NOT commonly used as a carbon-containing gas in CVD?
A: Methane
B: Ethylene
C: Ammonia
D: Ethanol
E: Acetylene
Answer: C

Question: What structure do nanotubes often resemble when they grow closely spaced and vertically without plasma in CVD?
A: Web
B: Lattice
C: Carpet
D: Forest
E: Hive
Answer: D

Question: What is a challenge with using metal nanoparticles mixed with a catalyst support in CVD synthesis?
A: Formation of short nanotubes
B: Overproduction of metallic nanotubes
C: Removal of the catalyst support
D: Limited availability of metal nanoparticles
E: Requirement of extremely high temperatures
Answer: C
@
Nanoscale metal catalysts are important ingredients for fixed- and fluidized-bed CVD synthesis of CNTs. They allow increasing the growth efficiency of CNTs and may give control over their structure and chirality.[49] During synthesis, catalysts can convert carbon precursors into tubular carbon structures but can also form encapsulating carbon overcoats. Together with metal oxide supports they may therefore attach to or become incorporated into the CNT product.[50] The presence of metal impurities can be problematic for many applications. Especially catalyst metals like nickel, cobalt or yttrium may be of toxicological concern.[51] While unencapsulated catalyst metals may be readily removable by acid washing, encapsulated ones require oxidative treatment for opening their carbon shell.[52] The effective removal of catalysts, especially of encapsulated ones, while preserving the CNT structure is a challenge and has been addressed in many studies.[53][54] A new approach to break carbonaceous catalyst encapsulations is based on rapid thermal annealing.[55]
$
5
Question: Catalyst metals such as nickel, cobalt, or yttrium may raise concerns due to their:
A: Magnetism
B: Weight
C: Toxicological concerns
D: Electrical conductivity
E: Reflectivity
Answer: C

Question: How can encapsulated catalyst metals be addressed for removal?
A: Magnetic separation
B: Rapid thermal annealing
C: High-pressure water jets
D: Ultraviolet exposure
E: Cold storage
Answer: B

Question: Encapsulated catalyst metals require what treatment to open their carbon shell?
A: Rapid cooling
B: Electrical charge
C: Oxidative treatment
D: Centrifugation
E: Acidity adjustment
Answer: C

Question: Which of the following is NOT a role of nanoscale metal catalysts in CVD synthesis?
A: Producing light during the reaction
B: Increasing the growth efficiency of CNTs
C: Converting carbon precursors into tubular carbon structures
D: Forming encapsulating carbon overcoats
E: Giving control over CNT structure and chirality
Answer: A

Question: What might happen to metal oxide supports during the CNT synthesis?
A: They evaporate due to high temperatures
B: They convert into gaseous form
C: They attach to or become incorporated into the CNT product
D: They transform into pure metal form
E: They serve as an energy source for the reaction
Answer: C
@
Many electronic applications of carbon nanotubes crucially rely on techniques of selectively producing either semiconducting or metallic CNTs, preferably of a certain chirality.[56] Several methods of separating semiconducting and metallic CNTs are known, but most of them are not yet suitable for large-scale technological processes. The most efficient method relies on density-gradient ultracentrifugation, which separates surfactant-wrapped nanotubes by the minute difference in their density. This density difference often translates into difference in the nanotube diameter and (semi)conducting properties.[48] Another method of separation uses a sequence of freezing, thawing, and compression of SWNTs embedded in agarose gel. This process results in a solution containing 70% metallic SWNTs and leaves a gel containing 95% semiconducting SWNTs. The diluted solutions separated by this method show various colors.[57] The separated carbon nanotubes using this method have been applied to electrodes, e.g. electric double-layer capacitor.[58] Moreover, SWNTs can be separated by the column chromatography method. Yield is 95% in semiconductor type SWNT and 90% in metallic type SWNT.[59]

In addition to separation of semiconducting and metallic SWNTs, it is possible to sort SWNTs by length, diameter, and chirality. The highest resolution length sorting, with length variation of <10%, has thus far been achieved by size exclusion chromatography (SEC) of DNA-dispersed carbon nanotubes (DNA-SWNT).[60] SWNT diameter separation has been achieved by density-gradient ultracentrifugation (DGU)[61] using surfactant-dispersed SWNTs and by ion-exchange chromatography (IEC) for DNA-SWNT.[62] Purification of individual chiralities has also been demonstrated with IEC of DNA-SWNT: specific short DNA oligomers can be used to isolate individual SWNT chiralities. Thus far, 12 chiralities have been isolated at purities ranging from 70% for (8,3) and (9,5) SWNTs to 90% for (6,5), (7,5) and (10,5) SWNTs.[63] Alternatively, carbon nanotubes have been successfully sorted by chirality using the aqueous two phase extraction method.[64][65][66] There have been successful efforts to integrate these purified nanotubes into electronic devices, such as field-effect transistors.[67]

An alternative to separation is development of a selective growth of semiconducting or metallic CNTs. This can be achieved by CVD that involves a combination of ethanol and methanol gases on a quartz substrate, resulting in horizontally aligned arrays of 95–98% semiconducting nanotubes.[68]

Nanotubes are usually grown on nanoparticles of magnetic metal (Fe, Co), which facilitates production of electronic (spintronic) devices. In particular, control of current through a field-effect transistor by magnetic field has been demonstrated in such a single-tube nanostructure.[69]
$
5
Question: Which method separates surfactant-wrapped nanotubes by the minute difference in their density?
A: Ion-exchange chromatography
B: Aqueous two-phase extraction
C: Freezing and thawing
D: Size exclusion chromatography
E: Density-gradient ultracentrifugation
Answer: E

Question: When SWNTs are embedded in agarose gel, then subjected to a sequence of freezing, thawing, and compression, the gel mostly contains:
A: Metallic SWNTs
B: Semiconducting SWNTs
C: A mix of metallic and semiconducting SWNTs
D: Carbon particles
E: Metal catalyst residues
Answer: B

Question: What property allows CVD to produce horizontally aligned arrays of 95–98% semiconducting nanotubes?
A: Use of ammonia gas
B: Combination of ethanol and methanol gases on a quartz substrate
C: High temperatures of 1,700 °C
D: Introduction of a magnetic field
E: Use of DNA oligomers
Answer: B

Question: How many chiralities of SWNTs have been isolated at purities ranging from 70% to 90% as of the provided information?
A: 5
B: 7
C: 10
D: 12
E: 15
Answer: D

Question: For many electronic applications of carbon nanotubes, what is a crucial aspect?
A: Their color and visual appearance
B: Their weight and density
C: The ability to selectively produce either semiconducting or metallic CNTs
D: Their length and flexibility
E: Their solubility in common solvents
Answer: C
@
Column chromatography in chemistry is a chromatography method used to isolate a single chemical compound from a mixture. Chromatography is able to separate substances based on differential adsorption of compounds to the adsorbent; compounds move through the column at different rates, allowing them to be separated into fractions. The technique is widely applicable, as many different adsorbents (normal phase, reversed phase, or otherwise) can be used with a wide range of solvents. The technique can be used on scales from micrograms up to kilograms. The main advantage of column chromatography is the relatively low cost and disposability of the stationary phase used in the process. The latter prevents cross-contamination and stationary phase degradation due to recycling. Column chromatography can be done using gravity to move the solvent, or using compressed gas to push the solvent through the column.

A thin-layer chromatograph can show how a mixture of compounds will behave when purified by column chromatography. The separation is first optimised using thin-layer chromatography before performing column chromatography.
$
5
Question: Which method can predict how a mixture of compounds will behave when purified by column chromatography?
A: High-performance liquid chromatography
B: Gas chromatography
C: Thin-layer chromatograph
D: Affinity chromatography
E: Ion-exchange chromatography
Answer: C

Question: What is the primary advantage of column chromatography concerning the stationary phase?
A: Reusability
B: High cost
C: Non-disposability
D: Prevention of cross-contamination
E: Use of compressed gas
Answer: D

Question: Chromatography separates substances based on:
A: Boiling points
B: Mass
C: Differential adsorption to the adsorbent
D: Solubility
E: Color
Answer: C

Question: Which of the following is NOT a type of adsorbent mentioned?
A: Condensed phase
B: Normal phase
C: Reversed phase
D: Aqueous phase
E: Otherwise
Answer: D

Question: In column chromatography, the solvent can be moved through the column using:
A: Only gravity
B: Only magnetic field
C: Gravity or compressed gas
D: Only centrifugation
E: Only electricity
Answer: C
@
A column is prepared by packing a solid adsorbent into a cylindrical glass or plastic tube. The size will depend on the amount of compound being isolated. The base of the tube contains a filter, either a cotton or glass wool plug, or glass frit to hold the solid phase in place. A solvent reservoir may be attached at the top of the column.

Two methods are generally used to prepare a column: the dry method and the wet method. For the dry method, the column is first filled with dry stationary phase powder, followed by the addition of mobile phase, which is flushed through the column until it is completely wet, and from this point is never allowed to run dry.[1] For the wet method, a slurry is prepared of the eluent with the stationary phase powder and then carefully poured into the column. The top of the silica should be flat, and the top of the silica can be protected by a layer of sand. Eluent is slowly passed through the column to advance the organic material.

The individual components are retained by the stationary phase differently and separate from each other while they are running at different speeds through the column with the eluent. At the end of the column they elute one at a time. During the entire chromatography process the eluent is collected in a series of fractions. Fractions can be collected automatically by means of fraction collectors. The productivity of chromatography can be increased by running several columns at a time. In this case multi stream collectors are used. The composition of the eluent flow can be monitored and each fraction is analyzed for dissolved compounds, e.g. by analytical chromatography, UV absorption spectra, or fluorescence. Colored compounds (or fluorescent compounds with the aid of a UV lamp) can be seen through the glass wall as moving bands.
$
5
Question: What prevents the solid phase from exiting the base of the column?
A: A rubber stopper
B: A cotton or glass wool plug
C: A metal sieve
D: A ceramic barrier
E: A plastic mesh
Answer: B

Question: How can the productivity of chromatography be increased?
A: By reducing the size of the column
B: By using a stronger eluent
C: By decreasing the elution time
D: By running several columns simultaneously
E: By using only one type of adsorbent
Answer: D

Question: In the dry method of preparing a column, what is done after filling the column with dry stationary phase powder?
A: The column is dried in an oven
B: A layer of sand is added on top
C: Mobile phase is added and flushed through until wet
D: The column is sealed and stored
E: An organic solvent is added to dissolve the stationary phase
Answer: C

Question: During the chromatography process, the separated components are collected in:
A: A single large container
B: In the mobile phase solvent
C: As a gas above the column
D: Attached to the stationary phase
E: A series of fractions
Answer: E

Question: Which device can be used to automatically collect fractions?
A: A pipette
B: A fraction collector
C: A chromatogram
D: An eluent mixer
E: A stationary phase dispenser
Answer: B
@
The mobile phase or eluent is a solvent or a mixture of solvents used to move the compounds through the column. It is chosen so that the retention factor value of the compound of interest is roughly around 0.2 - 0.3 in order to minimize the time and the amount of eluent to run the chromatography. The eluent has also been chosen so that the different compounds can be separated effectively. The eluent is optimized in small scale pretests, often using thin layer chromatography (TLC) with the same stationary phase, using solvents of different polarity until a suitable solvent system is found. Common mobile phase solvents, in order of increasing polarity, include hexane, dichloromethane, ethyl acetate, acetone, and methanol.[3] A common solvent system is a mixture of hexane and ethyl acetate, with proportions adjusted until the target compound has a retention factor of 0.2 - 0.3. Contrary to common misconception, methanol alone can be used as an eluent for highly polar compounds, and does not dissolve silica gel.

There is an optimum flow rate for each particular separation. A faster flow rate of the eluent minimizes the time required to run a column and thereby minimizes diffusion, resulting in a better separation. However, the maximum flow rate is limited because a finite time is required for the analyte to equilibrate between the stationary phase and mobile phase, see Van Deemter's equation. A simple laboratory column runs by gravity flow. The flow rate of such a column can be increased by extending the fresh eluent filled column above the top of the stationary phase or decreased by the tap controls. Faster flow rates can be achieved by using a pump or by using compressed gas (e.g. air, nitrogen, or argon) to push the solvent through the column (flash column chromatography).[4][5]

The particle size of the stationary phase is generally finer in flash column chromatography than in gravity column chromatography. For example, one of the most widely used silica gel grades in the former technique is mesh 230 – 400 (40 – 63 µm), while the latter technique typically requires mesh 70 – 230 (63 – 200 µm) silica gel.[6]
$
5
Question: Which of the following is NOT listed as a common mobile phase solvent?
A: Water
B: Hexane
C: Ethyl acetate
D: Acetone
E: Dichloromethane
Answer: A

Question: A retention factor value of the compound of interest should be roughly around:
A: 0 - 0.1
B: 0.2 - 0.3
C: 0.4 - 0.5
D: 0.6 - 0.7
E: 0.8 - 0.9
Answer: B

Question: Which column chromatography has a generally finer stationary phase particle size?
A: Thin-layer chromatography
B: Flash column chromatography
C: Gas chromatography
D: Gravity column chromatography
E: Affinity chromatography
Answer: B

Question: The flow rate of a simple laboratory column run by gravity can be increased by:
A: Heating the column
B: Extending the eluent-filled column above the stationary phase
C: Adding more stationary phase
D: Using a thinner column
E: Reducing the eluent polarity
Answer: B

Question: What does Van Deemter's equation relate to in chromatography?
A: Adsorbent selection
B: Color of compounds
C: Equilibration time between phases
D: Organic material advancement
E: Column diameter
Answer: C
@
For an adsorption column, the column resin (the stationary phase) is composed of microbeads. Even smaller particles such as proteins, carbohydrates, metal ions, or other chemical compounds are conjugated onto the microbeads. Each binding particle that is attached to the microbead can be assumed to bind in a 1:1 ratio with the solute sample sent through the column that needs to be purified or separated.

Binding between the target molecule to be separated and the binding molecule on the column beads can be modeled using a simple equilibrium reaction Keq = [CS]/([C][S]) where Keq is the equilibrium constant, [C] and [S] are the concentrations of the target molecule and the binding molecule on the column resin, respectively. [CS] is the concentration of the complex of the target molecule bound to the column resin.[8]

Using this as a basis, three different isotherms can be used to describe the binding dynamics of a column chromatography: linear, Langmuir, and Freundlich.

The linear isotherm occurs when the solute concentration needed to be purified is very small relative to the binding molecule. Thus, the equilibrium can be defined as:

[CS] = Keq[C].
For industrial scale uses, the total binding molecules on the column resin beads must be factored in because unoccupied sites must be taken into account. The Langmuir isotherm and Freundlich isotherm are useful in describing this equilibrium. The Langmuir isotherm is given by:

[CS] = (KeqStot[C])/(1 + Keq[C]), where Stot is the total binding molecules on the beads.
The Freundlich isotherm is given by:

[CS] = Keq[C]1/n
The Freundlich isotherm is used when the column can bind to many different samples in the solution that needs to be purified. Because the many different samples have different binding constants to the beads, there are many different Keqs. Therefore, the Langmuir isotherm is not a good model for binding in this case.[8]
$
5
Question: For an adsorption column, what is the stationary phase composed of?
A: Macrobeads
B: Glass wool
C: Microbeads
D: Organic solvents
E: Plastic particles
Answer: C

Question: According to the given information, which isotherm is suitable when the column can bind to many different samples with different binding constants?
A: Linear
B: Freundlich
C: Langmuir
D: Isoelectric
E: Hydrophobic
Answer: B

Question: In the given equilibrium reaction Keq = [CS]/([C][S]), what does [S] represent?
A: Concentration of the eluent
B: Concentration of the compound in the mobile phase
C: Concentration of the target molecule bound to the column resin
D: Concentration of the binding molecule on the column resin
E: Total concentration of all molecules in the column
Answer: D

Question: What does [CS] in the given equations represent?
A: The concentration of the compound in the mobile phase
B: The concentration of the solvent
C: The concentration of the complex of the target molecule bound to the column resin
D: The total concentration of all molecules in the column
E: The concentration of the eluent
Answer: C

Question: For industrial scale uses, which isotherm takes into account the total binding molecules on the column resin beads and unoccupied sites?
A: Linear
B: Freundlich
C: Langmuir
D: Hydrophilic
E: Van der Waals
Answer: C
@
Methane (US: /ˈmɛθeɪn/ METH-ayn, UK: /ˈmiːθeɪn/ MEE-thayn) is a chemical compound with the chemical formula CH4 (one carbon atom bonded to four hydrogen atoms). It is a group-14 hydride, the simplest alkane, and the main constituent of natural gas. The relative abundance of methane on Earth makes it an economically attractive fuel, although capturing and storing it poses technical challenges due to its gaseous state under normal conditions for temperature and pressure.

Naturally occurring methane is found both below ground and under the seafloor and is formed by both geological and biological processes. The largest reservoir of methane is under the seafloor in the form of methane clathrates. When methane reaches the surface and the atmosphere, it is known as atmospheric methane.[9]

The Earth's atmospheric methane concentration has increased by about 160% since 1750, with the overwhelming percentage caused by human activity.[10] It accounted for 20% of the total radiative forcing from all of the long-lived and globally mixed greenhouse gases, according to the 2021 Intergovernmental Panel on Climate Change report.[11] Strong, rapid and sustained reductions in methane emissions could limit near-term warming and improve air quality by reducing global surface ozone.[12]

Methane has also been detected on other planets, including Mars, which has implications for astrobiology research.[13]
$
5
Question: Which of the following best describes methane's chemical formula?
A: CH2O
B: H2O
C: CO2
D: CH4
E: C2H2
Answer: D

Question: Methane is the primary component of what commonly used resource?
A: Water
B: Petroleum
C: Coal
D: Natural gas
E: Alcohol
Answer: D

Question: Where is the largest reservoir of methane located?
A: In the Earth's atmosphere
B: Below ground
C: Within glaciers
D: Inside active volcanoes
E: Under the seafloor
Answer: E

Question: By how much has the Earth's atmospheric methane concentration increased since 1750?
A: About 10%
B: About 50%
C: About 100%
D: About 160%
E: About 200%
Answer: D

Question: Which planet other than Earth has detected methane which holds importance for astrobiology research?
A: Venus
B: Mars
C: Jupiter
D: Saturn
E: Mercury
Answer: B
@
Methane is a tetrahedral molecule with four equivalent C–H bonds. Its electronic structure is described by four bonding molecular orbitals (MOs) resulting from the overlap of the valence orbitals on C and H. The lowest-energy MO is the result of the overlap of the 2s orbital on carbon with the in-phase combination of the 1s orbitals on the four hydrogen atoms. Above this energy level is a triply degenerate set of MOs that involve overlap of the 2p orbitals on carbon with various linear combinations of the 1s orbitals on hydrogen. The resulting "three-over-one" bonding scheme is consistent with photoelectron spectroscopic measurements.

Methane is an odorless gas and appears to be colorless.[14] It does absorb visible light, especially at the red end of the spectrum, due to overtone bands, but the effect is only noticeable if the light path is very long. This is what gives Uranus and Neptune their blue or bluish-green colors, as light passes through their atmospheres containing methane and is then scattered back out.[15]

The familiar smell of natural gas as used in homes is achieved by the addition of an odorant, usually blends containing tert-butylthiol, as a safety measure. Methane has a boiling point of −161.5 °C at a pressure of one atmosphere.[3] As a gas, it is flammable over a range of concentrations (5.4%–17%) in air at standard pressure.

Solid methane exists in several modifications. Presently nine are known.[16] Cooling methane at normal pressure results in the formation of methane I. This substance crystallizes in the cubic system (space group Fm3m). The positions of the hydrogen atoms are not fixed in methane I, i.e. methane molecules may rotate freely. Therefore, it is a plastic crystal.[17]
$
5
Question: How is the electronic structure of methane primarily described?
A: By five bonding molecular orbitals
B: By two bonding molecular orbitals
C: By three bonding molecular orbitals
D: By four bonding molecular orbitals
E: By a singular molecular orbital
Answer: D

Question: What causes Uranus and Neptune to have their blue or bluish-green colors?
A: Presence of water vapor
B: Methane absorption of visible light
C: Refraction of light from their rings
D: High concentrations of nitrogen
E: Surface albedo effects
Answer: B

Question: Methane has a boiling point of:
A: 0°C
B: −89°C
C: 100°C
D: −273°C
E: −161.5°C
Answer: E

Question: Methane is flammable over which concentration range in air at standard pressure?
A: 1%-4%
B: 5.4%–17%
C: 18%–25%
D: 30%–50%
E: 60%–80%
Answer: B

Question: In its solid form, methane I is a:
A: Fluid crystal
B: Static crystal
C: Quartz crystal
D: Plastic crystal
E: Silicon crystal
Answer: D
@
Wetlands are the largest natural sources of methane to the atmosphere,[50] accounting for approximately 20 - 30% of atmospheric methane.[51] Climate change is increasing the amount of methane released from wetlands due to increased temperatures and altered rainfall patterns. This is phenomeon is called wetland methane feedback.[50]

Ruminants
Ruminants, such as cattle, belch methane, accounting for about 22% of the U.S. annual methane emissions to the atmosphere.[52] One study reported that the livestock sector in general (primarily cattle, chickens, and pigs) produces 37% of all human-induced methane.[53] A 2013 study estimated that livestock accounted for 44% of human-induced methane and about 15% of human-induced greenhouse gas emissions.[54] Many efforts are underway to reduce livestock methane production, such as medical treatments and dietary adjustments,[55][56] and to trap the gas to use its combustion energy.[57]

Seafloor sediments
Most of the subseafloor is anoxic because oxygen is removed by aerobic microorganisms within the first few centimeters of the sediment. Below the oxygen-replete seafloor, methanogens produce methane that is either used by other organisms or becomes trapped in gas hydrates.[45] These other organisms that utilize methane for energy are known as methanotrophs ('methane-eating'), and are the main reason why little methane generated at depth reaches the sea surface.[45] Consortia of Archaea and Bacteria have been found to oxidize methane via anaerobic oxidation of methane (AOM); the organisms responsible for this are anaerobic methanotrophic Archaea (ANME) and sulfate-reducing bacteria (SRB).[58]
$
5
Question: Approximately what percentage of atmospheric methane is contributed by wetlands?
A: 10-20%
B: 20-30%
C: 30-40%
D: 40-50%
E: 50-60%
Answer: B

Question: What percentage of U.S. annual methane emissions is due to ruminants like cattle?
A: 10%
B: 22%
C: 35%
D: 44%
E: 50%
Answer: B

Question: Methanogens in subseafloor sediments produce methane, but most of it doesn't reach the surface due to:
A: Physical barriers in the sediment
B: Absorption by sea plants
C: Consumption by methanotrophs
D: Dissipation into the atmosphere
E: Conversion into other gases
Answer: C

Question: What is the main reason why the majority of methane generated at depth in seafloor sediments does not reach the sea surface?
A: It is trapped in gas hydrates
B: It is consumed by deep-sea fishes
C: It is dissolved in the water column
D: It is used by other microorganisms
E: It evaporates immediately
Answer: D

Question: What is a significant outcome of the wetland methane feedback?
A: Reduction in methane emissions
B: Stabilization of global temperatures
C: Increased methane release due to climate change
D: Decreased methane production by wetland plants
E: Methane absorption by the wetland soils
Answer: C
@
As a liquid rocket propellant, a methane/liquid oxygen combination offers the advantage over kerosene/liquid oxygen combination, or kerolox, of producing small exhaust molecules. This deposits less soot on the internal parts of rocket motors, which is beneficial for reusable rocket designs. Methane is easier to store due to its higher boiling point and density, as well as its lack of hydrogen embrittlement.[30][31] In addition, it can be produced on other astronomical bodies such as the Moon and Mars, although the only methane fueled rocket that is designed to utilize this is SpaceX’s Starship spacecraft as of September 2023. The lower molecular weight of the exhaust also increases the fraction of the heat energy which is in the form of kinetic energy available for propulsion, increasing the specific impulse of the rocket. Because methalox powered engines can run at higher pressures than a kerolox powered engine, this means that a methalox engine may be roughly 20 percent more fuel efficient than a kerolox engine.[32] Compared to liquid hydrogen/liquid oxygen combination (hydrolox), the specific energy of methane is lower but this disadvantage is offset by methane's greater density and temperature range, allowing for smaller and lighter tankage for a given fuel mass. Liquid methane has a temperature range (91–112 K) nearly compatible with liquid oxygen (54–90 K).

Due to the advantages methane fuel provides, especially for reusable rockets, various companies and organizations, especially private space launch providers, aimed to develop methane-based launch systems during the 2010s. The competition between countries were dubbed the Methalox Race to Orbit. In July 2023, the Chinese private company LandSpace launched a Zhuque-2 methalox rocket, which became the first to reach orbit.[33][34][35] It delivered a test payload into sun-synchronous orbit (SSO).[36]
$
5
Question: Which rocket propellant combination produces smaller exhaust molecules compared to kerolox?
A: Hydrolox
B: Nitrox
C: Methalox
D: Hydralox
E: Ethalox
Answer: C

Question: What is a notable benefit of methane as a rocket propellant?
A: It leaves more soot in rocket motors
B: It can't be produced on other astronomical bodies
C: It deposits less soot on internal rocket motor parts
D: It has a very low boiling point
E: It requires larger storage tanks
Answer: C

Question: The efficiency of which type of engine can be increased by using methalox?
A: Diesel engine
B: Gasoline engine
C: Electric motor
D: Kerolox powered engine
E: Steam engine
Answer: D

Question: What significant event occurred with the Zhuque-2 methalox rocket in July 2023?
A: It exploded during launch
B: It was the first to reach orbit
C: It landed on Mars
D: It was launched by NASA
E: It transported the largest payload ever
Answer: B

Question: Methane in liquid form can be beneficial for rockets because of its:
A: Reduced specific impulse
B: Compatibility with nitrogen
C: High temperature range and density
D: Low energy content
E: Rapid evaporation rate
Answer: C
@
In thermochemistry, an endothermic process (from Greek ἔνδον (endon) 'within', and θερμ- (therm) 'hot, warm') is any thermodynamic process with an increase in the enthalpy H (or internal energy U) of the system.[1] In such a process, a closed system usually absorbs thermal energy from its surroundings, which is heat transfer into the system. Thus, an endothermic reaction generally leads to an increase in the temperature of the system and a decrease in that of the surroundings. It may be a chemical process, such as dissolving ammonium nitrate (NH4NO3) in water (H2O), or a physical process, such as the melting of ice cubes.

The term was coined by 19th-century French chemist Marcellin Berthelot. The opposite of an endothermic process is an exothermic process, one that releases or "gives out" energy, usually in the form of heat and sometimes as electrical energy. Thus, in each term (endothermic and exothermic) the prefix refers to where heat (or electrical energy) goes as the process occurs.
$
5
Question: What occurs during an endothermic process?
A: Energy is released from the system to the surroundings.
B: The system absorbs thermal energy from its surroundings.
C: Energy is converted from electrical to thermal within the system.
D: There's a decrease in the enthalpy H of the system.
E: Heat transfer occurs out of the system.
Answer: B

Question: An endothermic reaction results in a:
A: Decrease in the temperature of the system.
B: Increase in the temperature of the surroundings.
C: Increase in the temperature of the system.
D: Release of energy as heat.
E: Release of energy as electricity.
Answer: C

Question: Which is an example of an endothermic process?
A: Combustion of wood.
B: Dissolving sodium chloride in water.
C: Dissolving ammonium nitrate in water.
D: Freezing of water.
E: Lighting a matchstick.
Answer: C

Question: Who coined the term "endothermic"?
A: Isaac Newton.
B: Albert Einstein.
C: Marcellin Berthelot.
D: Dmitri Mendeleev.
E: Antoine Lavoisier.
Answer: C

Question: The prefix in "endothermic" refers to:
A: The state of the substance.
B: The release of heat.
C: The absorption of heat.
D: The location where the heat is transferred.
E: The chemical reaction taking place.
Answer: D
@
In thermodynamics, enthalpy /ˈɛnθəlpi/ i, is the sum of a thermodynamic system's internal energy and the product of its pressure and volume.[1] It is a state function used in many measurements in chemical, biological, and physical systems at a constant pressure, which is conveniently provided by the large ambient atmosphere. The pressure–volume term expresses the work required to establish the system's physical dimensions, i.e. to make room for it by displacing its surroundings.[2][3] The pressure-volume term is very small for solids and liquids at common conditions, and fairly small for gases. Therefore, enthalpy is a stand-in for energy in chemical systems; bond, lattice, solvation, and other chemial "energies" in are actually enthalpy differences. As a state function, enthalpy depends only on the final configuration of internal energy, pressure, and volume, not on the path taken to achieve it.

In the International System of Units (SI), the unit of measurement for enthalpy is the joule. Other historical conventional units still in use include the calorie and the British thermal unit (BTU).

The total enthalpy of a system cannot be measured directly because the internal energy contains components that are unknown, not easily accessible, or are not of interest for the thermodynamic problem at hand. In practice, a change in enthalpy is the preferred expression for measurements at constant pressure, because it simplifies the description of energy transfer. When transfer of matter into or out of the system is also prevented and no electrical or mechanical (sirring shaft or lift pumping) work is done, at constant pressure the enthalpy change equals the energy exchanged with the environment by heat.
$
5
Question: The unit of measurement for enthalpy in the International System of Units (SI) is:
A: Calorie.
B: British thermal unit.
C: Kilogram.
D: Celsius.
E: Joule.
Answer: E

Question: Enthalpy is the sum of a system's:
A: Pressure and temperature.
B: Internal energy and product of pressure and volume.
C: Volume and temperature.
D: Mass and acceleration.
E: Potential and kinetic energy.
Answer: B

Question: In practice, a change in enthalpy is often used for measurements at:
A: Constant volume.
B: Variable pressure.
C: Zero temperature.
D: Constant pressure.
E: Variable volume.
Answer: D

Question: Which term expresses the work required to establish the system's physical dimensions?
A: Internal energy.
B: Pressure–volume term.
C: Entropy.
D: Temperature.
E: Mass.
Answer: B

Question: The total enthalpy of a system cannot be directly measured because:
A: It is always zero.
B: The internal energy contains unknown components.
C: It is a constant value.
D: It does not change over time.
E: It is dependent on external factors.
Answer: B
@
In chemistry, the standard enthalpy of reaction is the enthalpy change when reactants in their standard states ( p = 1 bar ; usually T = 298 K ) change to products in their standard states.[4] This quantity is the standard heat of reaction at constant pressure and temperature, but it can be measured by calorimetric methods even if the temperature does vary during the measurement, provided that the initial and final pressure and temperature correspond to the standard state. The value does not depend on the path from initial to final state because enthalpy is a state function.

Enthalpies of chemical substances are usually listed for 1 bar (100 kPa) pressure as a standard state. Enthalpies and enthalpy changes for reactions vary as a function of temperature,[5] but tables generally list the standard heats of formation of substances at 25 °C (298 K). For endothermic (heat-absorbing) processes, the change ΔH is a positive value; for exothermic (heat-releasing) processes it is negative.

The enthalpy of an ideal gas is independent of its pressure or volume, and depends only on its temperature, which correlates to its thermal energy. Real gases at common temperatures and pressures often closely approximate this behavior, which simplifies practical thermodynamic design and analysis.
$
5
Question: The standard enthalpy of reaction measures the enthalpy change when reactants in their standard states change to:
A: Non-standard states.
B: Products in non-standard states.
C: Intermediate states.
D: Products in their standard states.
E: Gaseous state.
Answer: D

Question: For endothermic processes, ΔH is:
A: A negative value.
B: Equal to zero.
C: A positive value.
D: Indeterminable.
E: Constant.
Answer: C

Question: Enthalpy of an ideal gas is dependent on its:
A: Volume.
B: Pressure.
C: Color.
D: Temperature.
E: Mass.
Answer: D

Question: The standard heats of formation of substances are usually listed at:
A: 100°C.
B: 0°C.
C: 50°C.
D: 25°C.
E: 75°C.
Answer: D

Question: If a chemical reaction releases heat, it is:
A: Endothermic.
B: Adiabatic.
C: Isothermal.
D: Exothermic.
E: Isobaric.
Answer: D
@
In thermodynamics, an exothermic process (from Greek έξω (exō) 'outwards', and θερμικός (thermikόs) 'thermal'[1]) is a thermodynamic process or reaction that releases energy from the system to its surroundings, usually in the form of heat, but also in a form of light (e.g. a spark, flame, or flash), electricity (e.g. a battery), or sound (e.g. explosion heard when burning hydrogen). The term exothermic was first coined by 19th-century French chemist Marcellin Berthelot.

Exothermic refers to a transformation in which a closed system releases energy (heat) to the surroundings, expressed by

�
>
0.
{\displaystyle Q>0.}
When the transformation occurs at constant pressure and without exchange of electrical energy, heat Q is equal to the enthalpy change, i.e.

Δ
�
<
0
,
{\displaystyle \Delta H<0,}[2]
while at constant volume, according to the first law of thermodynamics it equals internal energy (U) change, i.e.

Δ
�
=
�
+
0
>
0.
{\displaystyle \Delta U=Q+0>0.}
In an adiabatic system (i.e. a system that does not exchange heat with the surroundings), an otherwise exothermic process results in an increase in temperature of the system.[3][page needed]

In exothermic chemical reactions, the heat that is released by the reaction takes the form of electromagnetic energy or kinetic energy of molecules. The transition of electrons from one quantum energy level to another causes light to be released. This light is equivalent in energy to some of the stabilization energy of the energy for the chemical reaction, i.e. the bond energy. This light that is released can be absorbed by other molecules in solution to give rise to molecular translations and rotations, which gives rise to the classical understanding of heat. In an exothermic reaction, the activation energy (energy needed to start the reaction) is less than the energy that is subsequently released, so there is a net release of energy.
$
5
Question: An exothermic process:
A: Absorbs energy from the surroundings.
B: Releases energy from the system to its surroundings.
C: Requires an external heat source.
D: Decreases the temperature of the surroundings.
E: Is always in the form of light.
Answer: B

Question: Who first introduced the term "exothermic"?
A: Albert Einstein.
B: James Clerk Maxwell.
C: Antoine Lavoisier.
D: Marcellin Berthelot.
E: Niels Bohr.
Answer: D

Question: In an adiabatic system, an exothermic process results in:
A: A decrease in temperature.
B: No change in temperature.
C: An external energy source.
D: An increase in temperature.
E: Absorption of heat from the surroundings.
Answer: D

Question: When electrons transition from one quantum energy level to another, it causes:
A: Absorption of light.
B: Release of heat.
C: Release of light.
D: Formation of bonds.
E: Breaking of bonds.
Answer: C

Question: In an exothermic reaction, if the activation energy is less than the energy subsequently released, there is:
A: A net absorption of energy.
B: No change in energy.
C: A net release of energy.
D: A balance in energy.
E: An indeterminate amount of energy change.
Answer: C
@
Wood is a structural tissue found in the stems and roots of trees and other woody plants. It is an organic material – a natural composite of cellulose fibers that are strong in tension and embedded in a matrix of lignin that resists compression. Wood is sometimes defined as only the secondary xylem in the stems of trees,[1] or it is defined more broadly to include the same type of tissue elsewhere such as in the roots of trees or shrubs.[citation needed] In a living tree it performs a support function, enabling woody plants to grow large or to stand up by themselves. It also conveys water and nutrients between the leaves, other growing tissues, and the roots. Wood may also refer to other plant materials with comparable properties, and to material engineered from wood, or woodchips or fiber.

Wood has been used for thousands of years for fuel, as a construction material, for making tools and weapons, furniture and paper. More recently it emerged as a feedstock for the production of purified cellulose and its derivatives, such as cellophane and cellulose acetate.

As of 2020, the growing stock of forests worldwide was about 557 billion cubic meters.[2] As an abundant, carbon-neutral[3] renewable resource, woody materials have been of intense interest as a source of renewable energy. In 2008, approximately 3.97 billion cubic meters of wood were harvested.[2] Dominant uses were for furniture and building construction.[4]
$
5
Question: What primarily provides strength against tension in wood?
A: Lignin
B: Cellulose fibers
C: Secondary xylem
D: Roots
E: Leaves
Answer: B

Question: Wood is a composite mainly of which two components?
A: Cellulose and lignin
B: Cellulose and water
C: Lignin and water
D: Lignin and leaves
E: Cellulose and leaves
Answer: A

Question: Which of these is NOT a traditional use for wood?
A: Building construction
B: As a feedstock for cellulose acetate
C: Making electronics
D: Fuel
E: Furniture
Answer: C

Question: Which material can be derived from purified cellulose?
A: Polyester
B: Nylon
C: Cellophane
D: PVC
E: Rubber
Answer: C

Question: As of 2020, approximately how much growing stock of forests was there worldwide?
A: 200 billion cubic meters
B: 557 billion cubic meters
C: 1 trillion cubic meters
D: 4 billion cubic meters
E: 980 billion cubic meters
Answer: B
@
Wood, in the strict sense, is yielded by trees, which increase in diameter by the formation, between the existing wood and the inner bark, of new woody layers which envelop the entire stem, living branches, and roots. This process is known as secondary growth; it is the result of cell division in the vascular cambium, a lateral meristem, and subsequent expansion of the new cells. These cells then go on to form thickened secondary cell walls, composed mainly of cellulose, hemicellulose and lignin.

Where the differences between the seasons are distinct, e.g. New Zealand, growth can occur in a discrete annual or seasonal pattern, leading to growth rings; these can usually be most clearly seen on the end of a log, but are also visible on the other surfaces. If the distinctiveness between seasons is annual (as is the case in equatorial regions, e.g. Singapore), these growth rings are referred to as annual rings. Where there is little seasonal difference growth rings are likely to be indistinct or absent. If the bark of the tree has been removed in a particular area, the rings will likely be deformed as the plant overgrows the scar.
$
5
Question: What process is responsible for increasing the diameter of trees?
A: Secondary digestion
B: Primary growth
C: Secondary growth
D: Cellular respiration
E: Cambium breakdown
Answer: C

Question: In which region are growth rings most likely to be indistinct or absent?
A: Equatorial regions
B: New Zealand
C: The Arctic
D: North America
E: Europe
Answer: A

Question: Growth rings that are distinct and occur annually are referred to as:
A: Seasonal rings
B: Monthly rings
C: Annual rings
D: Equatorial rings
E: Biennial rings
Answer: C

Question: What might cause growth rings to be deformed in a specific area of a tree?
A: Heavy rainfall
B: Tree age
C: Lack of sunlight
D: Removal of bark
E: High altitude
Answer: D

Question: Secondary growth in trees is the result of cell division in which meristem?
A: Apical meristem
B: Vascular cambium
C: Ground meristem
D: Lateral meristem
E: Root meristem
Answer: B
@
Wood is a heterogeneous, hygroscopic, cellular and anisotropic (or more specifically, orthotropic) material. It consists of cells, and the cell walls are composed of micro-fibrils of cellulose (40–50%) and hemicellulose (15–25%) impregnated with lignin (15–30%).[17]

In coniferous or softwood species the wood cells are mostly of one kind, tracheids, and as a result the material is much more uniform in structure than that of most hardwoods. There are no vessels ("pores") in coniferous wood such as one sees so prominently in oak and ash, for example.

The structure of hardwoods is more complex.[18] The water conducting capability is mostly taken care of by vessels: in some cases (oak, chestnut, ash) these are quite large and distinct, in others (buckeye, poplar, willow) too small to be seen without a hand lens. In discussing such woods it is customary to divide them into two large classes, ring-porous and diffuse-porous.[19]

In ring-porous species, such as ash, black locust, catalpa, chestnut, elm, hickory, mulberry, and oak,[19] the larger vessels or pores (as cross sections of vessels are called) are localized in the part of the growth ring formed in spring, thus forming a region of more or less open and porous tissue. The rest of the ring, produced in summer, is made up of smaller vessels and a much greater proportion of wood fibers. These fibers are the elements which give strength and toughness to wood, while the vessels are a source of weakness.[20]

In diffuse-porous woods the pores are evenly sized so that the water conducting capability is scattered throughout the growth ring instead of being collected in a band or row. Examples of this kind of wood are alder,[19] basswood,[21] birch,[19] buckeye, maple, willow, and the Populus species such as aspen, cottonwood and poplar.[19] Some species, such as walnut and cherry, are on the border between the two classes, forming an intermediate group.[21]
$
5
Question: What is the primary cell type found in coniferous or softwood species?
A: Vessels
B: Tracheids
C: Fibers
D: Pores
E: Parenchyma
Answer: B

Question: In which type of wood are the pores or vessels typically scattered throughout the growth ring?
A: Ring-porous
B: Equatorial-porous
C: Diffuse-porous
D: Non-porous
E: Semi-porous
Answer: C

Question: Which of these species is considered to be ring-porous?
A: Aspen
B: Birch
C: Willow
D: Ash
E: Basswood
Answer: D

Question: What component in wood primarily gives it strength and toughness?
A: Vessels
B: Fibers
C: Lignin
D: Cellulose
E: Hemicellulose
Answer: B

Question: In ring-porous wood, where are the larger vessels or pores mainly located?
A: In the summer growth
B: Throughout the ring
C: In the winter growth
D: In the spring growth
E: In the autumn growth
Answer: D
@
The chemical composition of wood varies from species to species, but is approximately 50% carbon, 42% oxygen, 6% hydrogen, 1% nitrogen, and 1% other elements (mainly calcium, potassium, sodium, magnesium, iron, and manganese) by weight.[31] Wood also contains sulfur, chlorine, silicon, phosphorus, and other elements in small quantity.

Aside from water, wood has three main components. Cellulose, a crystalline polymer derived from glucose, constitutes about 41–43%. Next in abundance is hemicellulose, which is around 20% in deciduous trees but near 30% in conifers. It is mainly five-carbon sugars that are linked in an irregular manner, in contrast to the cellulose. Lignin is the third component at around 27% in coniferous wood vs. 23% in deciduous trees. Lignin confers the hydrophobic properties reflecting the fact that it is based on aromatic rings. These three components are interwoven, and direct covalent linkages exist between the lignin and the hemicellulose. A major focus of the paper industry is the separation of the lignin from the cellulose, from which paper is made.

In chemical terms, the difference between hardwood and softwood is reflected in the composition of the constituent lignin. Hardwood lignin is primarily derived from sinapyl alcohol and coniferyl alcohol. Softwood lignin is mainly derived from coniferyl alcohol.[32]
$
5
Question: Which component of wood is derived from glucose and constitutes about 41-43%?
A: Lignin
B: Cellulose
C: Hemicellulose
D: Manganese
E: Calcium
Answer: B

Question: What property does lignin primarily confer to wood?
A: Elasticity
B: Hydrophobic properties
C: Ability to conduct electricity
D: Resistance to insects
E: Translucence
Answer: B

Question: In chemical terms, the difference between hardwood and softwood is mainly reflected in the composition of:
A: Cellulose
B: Hemicellulose
C: Lignin
D: Cellulose fibers
E: Water
Answer: C

Question: Softwood lignin is mainly derived from:
A: Sinapyl alcohol
B: Glucose
C: Coniferyl alcohol
D: Cellulose
E: Hemicellulose
Answer: C

Question: What is the major focus of the paper industry regarding wood components?
A: Increasing the amount of cellulose
B: Coloring the lignin
C: Separation of the lignin from the cellulose
D: Mixing lignin with hemicellulose
E: Increasing the amount of lignin
Answer: C
@
A solvent (from the Latin solvō, "loosen, untie, solve") is a substance that dissolves a solute, resulting in a solution. A solvent is usually a liquid but can also be a solid, a gas, or a supercritical fluid. Water is a solvent for polar molecules, and the most common solvent used by living things; all the ions and proteins in a cell are dissolved in water within the cell.

Major uses of solvents are in paints, paint removers, inks, and dry cleaning.[2] Specific uses for organic solvents are in dry cleaning (e.g. tetrachloroethylene); as paint thinners (toluene, turpentine); as nail polish removers and solvents of glue (acetone, methyl acetate, ethyl acetate); in spot removers (hexane, petrol ether); in detergents (citrus terpenes); and in perfumes (ethanol). Solvents find various applications in chemical, pharmaceutical, oil, and gas industries, including in chemical syntheses and purification processes.
$
5
Question: Which of the following is the most common solvent used by living organisms?
A: Methane
B: Ethanol
C: Water
D: Toluene
E: Turpentine
Answer: C

Question: In which of the following applications might tetrachloroethylene be utilized?
A: Paint thinner
B: Perfume making
C: Dry cleaning
D: Glue solvent
E: Detergent
Answer: C

Question: What does a solvent do to a solute?
A: Freezes it
B: Dissolves it
C: Solidifies it
D: Burns it
E: Insulates it
Answer: B

Question: Which solvent is used in nail polish removers and as a solvent for glue?
A: Ethyl acetate
B: Turpentine
C: Toluene
D: Hexane
E: Citrus terpenes
Answer: A

Question: Solvents play a significant role in which industries?
A: Chemical and pharmaceutical
B: Automotive and aerospace
C: Food and beverage
D: Telecommunication
E: Fashion and apparel
Answer: A
@
When one substance is dissolved into another, a solution is formed.[3] This is opposed to the situation when the compounds are insoluble like sand in water. In a solution, all of the ingredients are uniformly distributed at a molecular level and no residue remains. A solvent-solute mixture consists of a single phase with all solute molecules occurring as solvates (solvent-solute complexes), as opposed to separate continuous phases as in suspensions, emulsions and other types of non-solution mixtures. The ability of one compound to be dissolved in another is known as solubility; if this occurs in all proportions, it is called miscible.

In addition to mixing, the substances in a solution interact with each other at the molecular level. When something is dissolved, molecules of the solvent arrange around molecules of the solute. Heat transfer is involved and entropy is increased making the solution more thermodynamically stable than the solute and solvent separately. This arrangement is mediated by the respective chemical properties of the solvent and solute, such as hydrogen bonding, dipole moment and polarizability.[4] Solvation does not cause a chemical reaction or chemical configuration changes in the solute. However, solvation resembles a coordination complex formation reaction, often with considerable energetics (heat of solvation and entropy of solvation) and is thus far from a neutral process.
$
5
Question: When all ingredients in a mixture are uniformly distributed at a molecular level, what is the result?
A: Emulsion
B: Solution
C: Suspension
D: Compound
E: Mixture
Answer: B

Question: Which term best describes the ability of one compound to dissolve in another?
A: Miscibility
B: Solvation
C: Solubility
D: Solution
E: Mixture
Answer: C

Question: In a solution, the solvent molecules surround the _______ molecules.
A: Solute
B: Solvent
C: Suspended
D: Complex
E: Separated
Answer: A

Question: What does not happen during the solvation process?
A: Chemical reaction of the solute
B: Heat transfer
C: Entropy increase
D: Molecule interaction
E: Molecular arrangement
Answer: A

Question: Solvation that specifically involves water is termed as?
A: Solubilization
B: Hydration
C: Solution
D: Evaporation
E: Activation
Answer: B
@
Solvents can be broadly classified into two categories: polar and non-polar. A special case is elemental mercury, whose solutions are known as amalgams; also, other metal solutions exist which are liquid at room temperature.

Generally, the dielectric constant of the solvent provides a rough measure of a solvent's polarity. The strong polarity of water is indicated by its high dielectric constant of 88 (at 0 °C).[5] Solvents with a dielectric constant of less than 15 are generally considered to be nonpolar.[6]

The dielectric constant measures the solvent's tendency to partly cancel the field strength of the electric field of a charged particle immersed in it. This reduction is then compared to the field strength of the charged particle in a vacuum.[6] Heuristically, the dielectric constant of a solvent can be thought of as its ability to reduce the solute's effective internal charge. Generally, the dielectric constant of a solvent is an acceptable predictor of the solvent's ability to dissolve common ionic compounds, such as salts.
$
5
Question: What provides a rough measure of a solvent's polarity?
A: Melting point
B: Boiling point
C: Dielectric constant
D: Conductivity
E: Density
Answer: C

Question: Solvents with a dielectric constant less than 15 are typically:
A: Polar
B: Ionic
C: Nonpolar
D: Charged
E: Miscible
Answer: C

Question: Which solvent has a high dielectric constant of 88 (at 0 °C)?
A: Ethanol
B: Mercury
C: Turpentine
D: Toluene
E: Water
Answer: E

Question: The dielectric constant is associated with a solvent's ability to do what?
A: Increase solute temperature
B: Cancel the field strength of a charged particle
C: Enhance solute flavor
D: Change solute color
E: Decrease solute viscosity
Answer: B

Question: What can be described as a solvent's ability to dissolve common ionic compounds?
A: Boiling point
B: Dielectric constant
C: pH value
D: Melting point
E: Conductivity
Answer: B
@
Solvation describes the interaction of a solvent with dissolved molecules. Both ionized and uncharged molecules interact strongly with a solvent, and the strength and nature of this interaction influence many properties of the solute, including solubility, reactivity, and color, as well as influencing the properties of the solvent such as its viscosity and density.[1] If the attractive forces between the solvent and solute particles are greater than the attractive forces holding the solute particles together, the solvent particles pull the solute particles apart and surround them. The surrounded solute particles then move away from the solid solute and out into the solution. Ions are surrounded by a concentric shell of solvent. Solvation is the process of reorganizing solvent and solute molecules into solvation complexes and involves bond formation, hydrogen bonding, and van der Waals forces. Solvation of a solute by water is called hydration.[2]

Solubility of solid compounds depends on a competition between lattice energy and solvation, including entropy effects related to changes in the solvent structure.[3]

By a IUPAC definition,[4] solvation is an interaction of a solute with the solvent, which leads to stabilization of the solute species in the solution. In the solvated state, an ion or molecule in a solution is surrounded or complexed by solvent molecules. Solvated species can often be described by coordination number, and the complex stability constants. The concept of the solvation interaction can also be applied to an insoluble material, for example, solvation of functional groups on a surface of ion-exchange resin.

Solvation is, in concept, distinct from solubility. Solvation or dissolution is a kinetic process and is quantified by its rate. Solubility quantifies the dynamic equilibrium state achieved when the rate of dissolution equals the rate of precipitation. The consideration of the units makes the distinction clearer. The typical unit for dissolution rate is mol/s. The units for solubility express a concentration: mass per volume (mg/mL), molarity (mol/L), etc.[citation needed]
$
5
Question: What term describes the interaction between a solvent and dissolved molecules?
A: Solution
B: Dispersion
C: Emulsion
D: Solvation
E: Mixture
Answer: D

Question: When solute particles are surrounded and moved out into the solution by solvent particles, it indicates:
A: Solute particles are strongly bonded together.
B: Solute and solvent are miscible in any ratio.
C: Attractive forces between solvent and solute are stronger than those holding the solute particles together.
D: The solution is not thermodynamically stable.
E: Solvent is non-polar.
Answer: C

Question: What forces play a role in the solvation process?
A: Bond formation, hydrogen bonding, and van der Waals forces.
B: Electrostatic forces, dipole-dipole interactions, and London dispersion forces.
C: Ionic bonding, covalent bonding, and metallic bonding.
D: Hydrophobic interactions, dipole-induced dipole interactions, and ion-dipole interactions.
E: All of the above.
Answer: A

Question: What does the IUPAC define solvation as?
A: A process where a solvent is dissolved in a solute.
B: The interaction of a solute with the solvent leading to stabilization of the solute species in the solution.
C: The rate at which a solute dissolves in a solvent.
D: The maximum amount of solute that can be dissolved in a solvent.
E: The interaction between two insoluble substances.
Answer: B

Question: Which of the following is a key difference between solvation and solubility?
A: Solvation is a kinetic process, while solubility quantifies a dynamic equilibrium state.
B: Solvation quantifies a dynamic equilibrium state, while solubility is a kinetic process.
C: Solvation and solubility both describe the same kinetic process.
D: Solvation is the maximum amount a solute can dissolve, while solubility is the process of dissolution.
E: Solvation and solubility are terms that can be used interchangeably in all situations.
Answer: A
@
In chemistry, a hydrogen bond (or H-bond) is a primarily electrostatic force of attraction between a hydrogen (H) atom which is covalently bound to a more electronegative "donor" atom or group (Dn), and another electronegative atom bearing a lone pair of electrons—the hydrogen bond acceptor (Ac). Such an interacting system is generally denoted Dn−H···Ac, where the solid line denotes a polar covalent bond, and the dotted or dashed line indicates the hydrogen bond.[5] The most frequent donor and acceptor atoms are the period 2 elements nitrogen (N), oxygen (O), and fluorine (F).

Hydrogen bonds can be intermolecular (occurring between separate molecules) or intramolecular (occurring among parts of the same molecule).[6][7][8][9] The energy of a hydrogen bond depends on the geometry, the environment, and the nature of the specific donor and acceptor atoms and can vary between 1 and 40 kcal/mol.[10] This makes them somewhat stronger than a van der Waals interaction, and weaker than fully covalent or ionic bonds. This type of bond can occur in inorganic molecules such as water and in organic molecules like DNA and proteins. Hydrogen bonds are responsible for holding materials such as paper and felted wool together, and for causing separate sheets of paper to stick together after becoming wet and subsequently drying.

The hydrogen bond is responsible for many of the physical and chemical properties of compounds of N, O, and F that seem unusual compared with other similar structures. In particular, intermolecular hydrogen bonding is responsible for the high boiling point of water (100 °C) compared to the other group-16 hydrides that have much weaker hydrogen bonds.[11] Intramolecular hydrogen bonding is partly responsible for the secondary and tertiary structures of proteins and nucleic acids.
$
5
Question: Which of the following is the primary force of attraction in a hydrogen bond?
A: Ionic bond
B: Van der Waals interaction
C: Electrostatic force
D: Metallic bond
E: Covalent bond
Answer: C

Question: In which molecule does hydrogen bonding cause an unusually high boiling point when compared to its similar structures?
A: Methane
B: Helium
C: Carbon Dioxide
D: Water
E: Hydrogen Fluoride
Answer: D

Question: Which of the following elements are most commonly involved in hydrogen bonds?
A: Carbon, Oxygen, Nitrogen
B: Hydrogen, Oxygen, Fluorine
C: Nitrogen, Oxygen, Fluorine
D: Carbon, Hydrogen, Fluorine
E: Hydrogen, Carbon, Oxygen
Answer: C

Question: Hydrogen bonds can occur:
A: Only between molecules
B: Only within a molecule
C: In a solid-state only
D: Both between molecules and within a molecule
E: Only in liquids
Answer: D

Question: What is one function of hydrogen bonds in real-world applications?
A: Ensuring metals conduct electricity
B: Holding sheets of paper together when wet and dried
C: Making oil and water mix uniformly
D: Breaking DNA strands
E: Increasing the solubility of non-polar substances
Answer: B
@
The X−H distance is typically ≈110 pm, whereas the H···Y distance is ≈160 to 200 pm. The typical length of a hydrogen bond in water is 197 pm. The ideal bond angle depends on the nature of the hydrogen bond donor. The following hydrogen bond angles between a hydrofluoric acid donor and various acceptors have been determined experimentally:[23]

Acceptor···donor	VSEPR geometry	Angle (°)
HCN···HF	linear	180
H2CO···HF	trigonal planar	120
H2O···HF	pyramidal	46
H2S···HF	pyramidal	89
SO2···HF[verification needed]	trigonal	142
$
5
Question: What is the typical length of a hydrogen bond in water?
A: 110 pm
B: 180 pm
C: 197 pm
D: 210 pm
E: 250 pm
Answer: C

Question: For which of the following donor-acceptor pairs is the VSEPR geometry linear?
A: H2O···HF
B: HCN···HF
C: SO2···HF
D: H2S···HF
E: H2CO···HF
Answer: B

Question: The bond angle between which acceptor and donor is closest to 90 degrees?
A: H2O···HF
B: HCN···HF
C: SO2···HF
D: H2S···HF
E: H2CO···HF
Answer: D

Question: In which of the following pairs is the hydrogen bond angle pyramidal?
A: H2CO···HF
B: SO2···HF
C: HCN···HF
D: H2O···HF
E: None of the above
Answer: D

Question: Which acceptor-donor pair has a bond angle of 120 degrees?
A: H2S···HF
B: H2O···HF
C: HCN···HF
D: SO2···HF
E: H2CO···HF
Answer: E
@
A symmetric hydrogen bond is a special type of hydrogen bond in which the proton is spaced exactly halfway between two identical atoms. The strength of the bond to each of those atoms is equal. It is an example of a three-center four-electron bond. This type of bond is much stronger than a "normal" hydrogen bond. The effective bond order is 0.5, so its strength is comparable to a covalent bond. It is seen in ice at high pressure, and also in the solid phase of many anhydrous acids such as hydrofluoric acid and formic acid at high pressure. It is also seen in the bifluoride ion [F···H···F]−. Due to severe steric constraint, the protonated form of Proton Sponge (1,8-bis(dimethylamino)naphthalene) and its derivatives also have symmetric hydrogen bonds ([N···H···N]+),[51] although in the case of protonated Proton Sponge, the assembly is bent.[52]
$
5
Question: A symmetric hydrogen bond is stronger than a typical hydrogen bond because:
A: It has double the number of hydrogen atoms
B: The proton is spaced exactly between two identical atoms
C: It is always found in liquid state
D: It involves four hydrogen atoms instead of two
E: It always involves non-polar molecules
Answer: B

Question: The bond order of a symmetric hydrogen bond is closest to:
A: 0
B: 0.25
C: 0.5
D: 1
E: 1.5
Answer: C

Question: In which ion is a symmetric hydrogen bond observed?
A: OH-
B: H2O2-
C: [F···H···F]−
D: NH4+
E: CH3-
Answer: C

Question: The protonated form of which substance possesses a symmetric hydrogen bond?
A: Hydrofluoric acid
B: Formic acid
C: Proton Sponge
D: Water
E: Methane
Answer: C

Question: Symmetric hydrogen bonds are more comparable in strength to:
A: Van der Waals interactions
B: Ionic bonds
C: Typical hydrogen bonds
D: Covalent bonds
E: Metallic bonds
Answer: D
@
In the secondary structure of proteins, hydrogen bonds form between the backbone oxygens and amide hydrogens. When the spacing of the amino acid residues participating in a hydrogen bond occurs regularly between positions i and i + 4, an alpha helix is formed. When the spacing is less, between positions i and i + 3, then a 310 helix is formed. When two strands are joined by hydrogen bonds involving alternating residues on each participating strand, a beta sheet is formed. Hydrogen bonds also play a part in forming the tertiary structure of protein through interaction of R-groups. (See also protein folding).

Bifurcated H-bond systems are common in alpha-helical transmembrane proteins between the backbone amide C=O of residue i as the H-bond acceptor and two H-bond donors from residue i + 4: the backbone amide N−H and a side-chain hydroxyl or thiol H+. The energy preference of the bifurcated H-bond hydroxyl or thiol system is -3.4 kcal/mol or -2.6 kcal/mol, respectively. This type of bifurcated H-bond provides an intrahelical H-bonding partner for polar side-chains, such as serine, threonine, and cysteine within the hydrophobic membrane environments.[26]

The role of hydrogen bonds in protein folding has also been linked to osmolyte-induced protein stabilization. Protective osmolytes, such as trehalose and sorbitol, shift the protein folding equilibrium toward the folded state, in a concentration dependent manner. While the prevalent explanation for osmolyte action relies on excluded volume effects that are entropic in nature, circular dichroism (CD) experiments have shown osmolyte to act through an enthalpic effect.[47] The molecular mechanism for their role in protein stabilization is still not well established, though several mechanisms have been proposed. Computer molecular dynamics simulations suggest that osmolytes stabilize proteins by modifying the hydrogen bonds in the protein hydration layer.[48]
$
5
Question: In the secondary structure of proteins, hydrogen bonds that occur regularly between positions i and i + 4 form:
A: Beta sheet
B: 310 helix
C: Alpha helix
D: Tertiary structure
E: Quaternary structure
Answer: C

Question: Which of the following amino acid residues might be involved in a bifurcated H-bond within a hydrophobic membrane environment?
A: Glycine
B: Phenylalanine
C: Serine
D: Lysine
E: Histidine
Answer: C

Question: Hydrogen bonds play a key role in determining:
A: The primary structure of proteins
B: The nuclear structure of proteins
C: The polarity of proteins
D: The tertiary structure of proteins
E: The rate of protein synthesis
Answer: D

Question: Protective osmolytes, like trehalose, have been proposed to stabilize proteins by:
A: Forming additional hydrogen bonds with proteins
B: Disrupting the hydrogen bonds in the protein
C: Modifying the hydrogen bonds in the protein hydration layer
D: Breaking the peptide bonds in proteins
E: Increasing the ionic bonds in the protein
Answer: C

Question: In the alpha-helical transmembrane proteins, bifurcated H-bond systems commonly occur between the backbone amide C=O of residue i and two H-bond donors from residue:
A: i
B: i + 1
C: i + 2
D: i + 3
E: i + 4
Answer: E
@
Amino acids are organic compounds that contain both amino and carboxylic acid functional groups.[1] Although over 500 amino acids exist in nature, by far the most important are the α-amino acids, from which proteins are composed.[2] Only 22 α-amino acids appear in the genetic code of all life.[3][4]

Amino acids can be classified according to the locations of the core structural functional groups, as alpha- (α-), beta- (β-), gamma- (γ-) or delta- (δ-) amino acids; other categories relate to polarity, ionization, and side chain group type (aliphatic, acyclic, aromatic, containing hydroxyl or sulfur, etc.). In the form of proteins, amino acid residues form the second-largest component (water being the largest) of human muscles and other tissues.[5] Beyond their role as residues in proteins, amino acids participate in a number of processes such as neurotransmitter transport and biosynthesis. It is thought that they played a key role in enabling life on Earth and its emergence.

Amino acids are formally named by the IUPAC-IUBMB Joint Commission on Biochemical Nomenclature in terms of the fictitious "neutral" structure shown in the illustration. For example, the systematic name of alanine is 2-aminopropanoic acid, based on the formula CH3−CH(NH2)−COOH. The Commission justified this approach as follows:[6]

The systematic names and formulas given refer to hypothetical forms in which amino groups are unprotonated and carboxyl groups are undissociated. This convention is useful to avoid various nomenclatural problems but should not be taken to imply that these structures represent an appreciable fraction of the amino-acid molecules.
$
5
Question: Which functional groups do amino acids possess?
A: Keto and hydroxyl groups
B: Amino and carboxylic acid groups
C: Aldehyde and amino groups
D: Carboxylic acid and hydroxyl groups
E: Keto and aldehyde groups
Answer: B

Question: What is the most prevalent type of amino acid important for protein composition?
A: β-amino acids
B: γ-amino acids
C: δ-amino acids
D: α-amino acids
E: ε-amino acids
Answer: D

Question: Which component is the largest in human muscles and other tissues?
A: Carbohydrates
B: Fats
C: Proteins
D: Amino acid residues
E: Water
Answer: E

Question: In what form does alanine appear, based on the IUPAC-IUBMB nomenclature?
A: CH3−COOH−NH2
B: CH3−CH(NH2)−CH3
C: CH3−CH(NH2)−COOH
D: CH2−COOH−NH2
E: CH3−CH2−NH2
Answer: C

Question: Amino acids are primarily named after what kind of structure?
A: Zwitterionic structure
B: Neutral structure
C: Protonated structure
D: Dissociated structure
E: Ionized structure
Answer: B
@
The first few amino acids were discovered in the early 1800s.[7][8] In 1806, French chemists Louis-Nicolas Vauquelin and Pierre Jean Robiquet isolated a compound from asparagus that was subsequently named asparagine, the first amino acid to be discovered.[9][10] Cystine was discovered in 1810,[11] although its monomer, cysteine, remained undiscovered until 1884.[12][10][a] Glycine and leucine were discovered in 1820.[13] The last of the 20 common amino acids to be discovered was threonine in 1935 by William Cumming Rose, who also determined the essential amino acids and established the minimum daily requirements of all amino acids for optimal growth.[14][15]

The unity of the chemical category was recognized by Wurtz in 1865, but he gave no particular name to it.[16] The first use of the term "amino acid" in the English language dates from 1898,[17] while the German term, Aminosäure, was used earlier.[18] Proteins were found to yield amino acids after enzymatic digestion or acid hydrolysis. In 1902, Emil Fischer and Franz Hofmeister independently proposed that proteins are formed from many amino acids, whereby bonds are formed between the amino group of one amino acid with the carboxyl group of another, resulting in a linear structure that Fischer termed "peptide".[19]
$
5
Question: Which amino acid was the first to be discovered?
A: Cystine
B: Glycine
C: Leucine
D: Asparagine
E: Threonine
Answer: D

Question: Who determined the essential amino acids and established minimum daily requirements for optimal growth?
A: Emil Fischer
B: Franz Hofmeister
C: Louis-Nicolas Vauquelin
D: William Cumming Rose
E: Pierre Jean Robiquet
Answer: D

Question: In what year were glycine and leucine discovered?
A: 1806
B: 1810
C: 1820
D: 1884
E: 1935
Answer: C

Question: What term was used by Fischer to describe the linear structure formed by the bonding of amino acids?
A: Polypeptide
B: Dipeptide
C: Monopeptide
D: Tripeptide
E: Peptide
Answer: E

Question: When did the term "amino acid" first appear in the English language?
A: 1806
B: 1865
C: 1898
D: 1902
E: 1935
Answer: C
@
The common natural forms of amino acids have a zwitterionic structure, with −NH
+
3
 (−NH
+
2
− in the case of proline) and −CO
−
2
 functional groups attached to the same C atom, and are thus α-amino acids. With the exception of achiral glycine, natural amino acids have the L configuration,[22] and are the only ones found in proteins during translation in the ribosome.

The L and D convention for amino acid configuration refers not to the optical activity of the amino acid itself but rather to the optical activity of the isomer of glyceraldehyde from which that amino acid can, in theory, be synthesized (D-glyceraldehyde is dextrorotatory; L-glyceraldehyde is levorotatory).

An alternative convention is to use the (S) and (R) designators to specify the absolute configuration.[23] Almost all of the amino acids in proteins are (S) at the α carbon, with cysteine being (R) and glycine non-chiral.[24] Cysteine has its side chain in the same geometric location as the other amino acids, but the R/S terminology is reversed because sulfur has higher atomic number compared to the carboxyl oxygen which gives the side chain a higher priority by the Cahn-Ingold-Prelog sequence rules, whereas the atoms in most other side chains give them lower priority compared to the carboxyl group.[23]

Rarely, D-amino acid residues are found in proteins, and are converted from the l-amino acid as a post-translational modification.[25]
$
5
Question: What configuration do most natural amino acids have, excluding glycine?
A: D
B: R
C: S
D: L
E: Z
Answer: D

Question: Which amino acid residue is (R) at the α carbon in proteins?
A: Alanine
B: Serine
C: Glycine
D: Proline
E: Cysteine
Answer: E

Question: Which of the following is an example of post-translational modification in proteins?
A: Conversion of l-amino acid residues to D-amino acid residues
B: Conversion of D-amino acid residues to l-amino acid residues
C: Addition of hydroxyl groups to amino acid residues
D: Removal of amino groups from amino acid residues
E: Conversion of non-polar amino acids to polar amino acids
Answer: A

Question: The (S) and (R) designators used for amino acids specify what?
A: Electrical charge
B: Optical activity
C: Polarity
D: Absolute configuration
E: Relative configuration
Answer: D

Question: Which amino acid is not chiral?
A: Alanine
B: Valine
C: Threonine
D: Leucine
E: Glycine
Answer: E
@
The 20 canonical amino acids can be classified according to their properties. Important factors are charge, hydrophilicity or hydrophobicity, size, and functional groups.[22] These properties influence protein structure and protein–protein interactions. The water-soluble proteins tend to have their hydrophobic residues (Leu, Ile, Val, Phe, and Trp) buried in the middle of the protein, whereas hydrophilic side chains are exposed to the aqueous solvent. (In biochemistry, a residue refers to a specific monomer within the polymeric chain of a polysaccharide, protein or nucleic acid.) The integral membrane proteins tend to have outer rings of exposed hydrophobic amino acids that anchor them in the lipid bilayer. Some peripheral membrane proteins have a patch of hydrophobic amino acids on their surface that sticks to the membrane. In a similar fashion, proteins that have to bind to positively charged molecules have surfaces rich in negatively charged amino acids such as glutamate and aspartate, while proteins binding to negatively charged molecules have surfaces rich in positively charged amino acids like lysine and arginine. For example, lysine and arginine are present in large amounts in the low-complexity regions of nucleic-acid binding proteins.[33] There are various hydrophobicity scales of amino acid residues.[34]

Some amino acids have special properties. Cysteine can form covalent disulfide bonds to other cysteine residues. Proline forms a cycle to the polypeptide backbone, and glycine is more flexible than other amino acids.

Glycine and proline are strongly present within low complexity regions of both eukaryotic and prokaryotic proteins, whereas the opposite is the case with cysteine, phenylalanine, tryptophan, methionine, valine, leucine, isoleucine, which are highly reactive, or complex, or hydrophobic.[33][35][36]

Many proteins undergo a range of posttranslational modifications, whereby additional chemical groups are attached to the amino acid residue side chains sometimes producing lipoproteins (that are hydrophobic),[37] or glycoproteins (that are hydrophilic)[38] allowing the protein to attach temporarily to a membrane. For example, a signaling protein can attach and then detach from a cell membrane, because it contains cysteine residues that can have the fatty acid palmitic acid added to them and subsequently removed.[39]
$
5
Question: In a water-soluble protein, where are the hydrophobic residues typically located?
A: On the surface
B: At the ends
C: In the middle
D: Outside the protein
E: On the exterior
Answer: C

Question: What do you refer to a specific monomer within the chain of a protein?
A: Chain
B: Residue
C: Subunit
D: Segment
E: Domain
Answer: B

Question: Which two amino acids are abundantly present in low complexity regions of proteins?
A: Arginine and histidine
B: Glycine and proline
C: Tryptophan and methionine
D: Alanine and asparagine
E: Cysteine and valine
Answer: B

Question: Which amino acid can form covalent disulfide bonds with other residues of its type?
A: Leucine
B: Methionine
C: Glycine
D: Cysteine
E: Proline
Answer: D

Question: Which of the following modifications allows a signaling protein to temporarily attach to a cell membrane?
A: Addition of hydroxyl groups
B: Removal of amino groups
C: Addition of fatty acid palmitic acid
D: Conversion to a glycoprotein
E: Conversion to a lipoprotein
Answer: C
@
In chemistry, hydrophobicity is the physical property of a molecule that is seemingly repelled from a mass of water (known as a hydrophobe).[1] In contrast, hydrophiles are attracted to water.

Hydrophobic molecules tend to be nonpolar and, thus, prefer other neutral molecules and nonpolar solvents. Because water molecules are polar, hydrophobes do not dissolve well among them. Hydrophobic molecules in water often cluster together, forming micelles. Water on hydrophobic surfaces will exhibit a high contact angle.

Examples of hydrophobic molecules include the alkanes, oils, fats, and greasy substances in general. Hydrophobic materials are used for oil removal from water, the management of oil spills, and chemical separation processes to remove non-polar substances from polar compounds.[2]

Hydrophobic is often used interchangeably with lipophilic, "fat-loving". However, the two terms are not synonymous. While hydrophobic substances are usually lipophilic, there are exceptions, such as the silicones and fluorocarbons.[citation needed]

The term hydrophobe comes from the Ancient Greek ὑδρόφοβος (hydróphobos), "having a fear of water", constructed from Ancient Greek ὕδωρ (húdōr) 'water', and Ancient Greek φόβος (phóbos) 'fear'.[3]
$
5
Question: Which of the following is a physical property of a molecule that is seemingly repelled from a mass of water?
A: Hydrophilia
B: Amphiphilic
C: Lipophilic
D: Hydrophobicity
E: Solubility
Answer: D

Question: What typically forms when hydrophobic molecules cluster together in water?
A: Salts
B: Micelles
C: Ions
D: Crystals
E: Emulsions
Answer: B

Question: Which of the following is NOT synonymous with hydrophobic?
A: Hydrophilic
B: Fat-loving
C: Lipophilic
D: Water-repelling
E: Nonpolar-loving
Answer: A

Question: Which of the following molecules would most likely be hydrophobic?
A: Sugars
B: Alkanes
C: Salts
D: Water
E: Alcohols
Answer: B

Question: The term "hydrophobe" originates from which language?
A: Latin
B: German
C: French
D: Chinese
E: Ancient Greek
Answer: E
@
A hydrophilic molecule or portion of a molecule is one whose interactions with water and other polar substances are more thermodynamically favorable than their interactions with oil or other hydrophobic solvents.[2][3] They are typically charge-polarized and capable of hydrogen bonding. This makes these molecules soluble not only in water but also in other polar solvents.

Hydrophilic molecules (and portions of molecules) can be contrasted with hydrophobic molecules (and portions of molecules). In some cases, both hydrophilic and hydrophobic properties occur in a single molecule. An example of these amphiphilic molecules is the lipids that comprise the cell membrane. Another example is soap, which has a hydrophilic head and a hydrophobic tail, allowing it to dissolve in both water and oil.

Hydrophilic and hydrophobic molecules are also known as polar molecules and nonpolar molecules, respectively. Some hydrophilic substances do not dissolve. This type of mixture is called a colloid.

An approximate rule of thumb for hydrophilicity of organic compounds is that solubility of a molecule in water is more than 1 mass % if there is at least one neutral hydrophile group per 5 carbons, or at least one electrically charged hydrophile group per 7 carbons.[4]

Hydrophilic substances (ex: salts) can seem to attract water out of the air. Sugar is also hydrophilic, and like salt is sometimes used to draw water out of foods. Sugar sprinkled on cut fruit will "draw out the water" through hydrophilia, making the fruit mushy and wet, as in a common strawberry compote recipe.
$
5
Question: What kind of interaction do hydrophilic molecules typically have with water?
A: Repulsive
B: Neutral
C: Thermodynamically favorable
D: Hydrophobic
E: Apathetic
Answer: C

Question: Which of the following best describes the head of a soap molecule?
A: Hydrophobic
B: Neutral
C: Amphiphilic
D: Hydrophilic
E: Lipophobic
Answer: D

Question: How are hydrophilic and hydrophobic molecules also known respectively?
A: Acidic and Basic
B: Polar and Nonpolar
C: Soluble and Insoluble
D: Positive and Negative
E: Ionic and Covalent
Answer: B

Question: Sugar's interaction with cut fruit, making it mushy and wet, is because sugar is:
A: Hydrophobic
B: Acidic
C: Neutral
D: Amphiphilic
E: Hydrophilic
Answer: E

Question: Hydrophilic substances, like salt, can seemingly:
A: Repel water
B: Create micelles in water
C: Attract water out of the air
D: Increase oil content in water
E: Decrease water's boiling point
Answer: C
@
The origin of the hydrophobic effect is not fully understood. Some argue that the hydrophobic interaction is mostly an entropic effect originating from the disruption of highly dynamic hydrogen bonds between molecules of liquid water by the nonpolar solute.[16] A hydrocarbon chain or a similar nonpolar region of a large molecule is incapable of forming hydrogen bonds with water. Introduction of such a non-hydrogen bonding surface into water causes disruption of the hydrogen bonding network between water molecules. The hydrogen bonds are reoriented tangentially to such surface to minimize disruption of the hydrogen bonded 3D network of water molecules, and this leads to a structured water "cage" around the nonpolar surface. The water molecules that form the "cage" (or clathrate) have restricted mobility. In the solvation shell of small nonpolar particles, the restriction amounts to some 10%. For example, in the case of dissolved xenon at room temperature a mobility restriction of 30% has been found.[17] In the case of larger nonpolar molecules, the reorientational and translational motion of the water molecules in the solvation shell may be restricted by a factor of two to four; thus, at 25 °C the reorientational correlation time of water increases from 2 to 4-8 picoseconds. Generally, this leads to significant losses in translational and rotational entropy of water molecules and makes the process unfavorable in terms of the free energy in the system.[18] By aggregating together, nonpolar molecules reduce the surface area exposed to water and minimize their disruptive effect.

The hydrophobic effect can be quantified by measuring the partition coefficients of non-polar molecules between water and non-polar solvents. The partition coefficients can be transformed to free energy of transfer which includes enthalpic and entropic components, ΔG = ΔH - TΔS. These components are experimentally determined by calorimetry. The hydrophobic effect was found to be entropy-driven at room temperature because of the reduced mobility of water molecules in the solvation shell of the non-polar solute; however, the enthalpic component of transfer energy was found to be favorable, meaning it strengthened water-water hydrogen bonds in the solvation shell due to the reduced mobility of water molecules. At the higher temperature, when water molecules become more mobile, this energy gain decreases along with the entropic component. The hydrophobic effect depends on the temperature, which leads to "cold denaturation" of proteins.[19]

The hydrophobic effect can be calculated by comparing the free energy of solvation with bulk water. In this way, the hydrophobic effect not only can be localized but also decomposed into enthalpic and entropic contributions.[3]
$
5
Question: The hydrophobic effect is primarily due to:
A: The formation of hydrogen bonds with water
B: Disruption of hydrogen bonds in water
C: Ionic interactions with water
D: Covalent bonding with nonpolar molecules
E: Van der Waals forces
Answer: B

Question: Which type of bonding is a hydrocarbon chain incapable of forming with water?
A: Ionic
B: Covalent
C: Hydrogen
D: Metallic
E: Vanderwaal
Answer: C

Question: The formation of a structured water "cage" around a nonpolar surface restricts the mobility of:
A: Nonpolar solutes
B: Hydrocarbon chains
C: Hydrogen molecules
D: Water molecules
E: Hydrophobic particles
Answer: D

Question: The hydrophobic effect can be quantified by measuring:
A: Boiling points
B: Molar mass
C: Partition coefficients
D: pH values
E: Ionic radii
Answer: C

Question: The hydrophobic effect primarily depends on:
A: Pressure
B: pH level
C: Molar mass
D: Temperature
E: Electrical conductivity
Answer: D
@
Dettre and Johnson discovered in 1964 that the superhydrophobic lotus effect phenomenon was related to rough hydrophobic surfaces, and they developed a theoretical model based on experiments with glass beads coated with paraffin or TFE telomer. The self-cleaning property of superhydrophobic micro-nanostructured surfaces was reported in 1977.[16] Perfluoroalkyl, perfluoropolyether, and RF plasma -formed superhydrophobic materials were developed, used for electrowetting and commercialized for bio-medical applications between 1986 and 1995.[17][18][19][20] Other technology and applications have emerged since the mid-1990s.[21] A durable superhydrophobic hierarchical composition, applied in one or two steps, was disclosed in 2002 comprising nano-sized particles ≤ 100 nanometers overlaying a surface having micrometer-sized features or particles ≤ 100 micrometers. The larger particles were observed to protect the smaller particles from mechanical abrasion.[22]

In recent research, superhydrophobicity has been reported by allowing alkylketene dimer (AKD) to solidify into a nanostructured fractal surface.[23] Many papers have since presented fabrication methods for producing superhydrophobic surfaces including particle deposition,[24] sol-gel techniques,[25] plasma treatments,[26] vapor deposition,[24] and casting techniques.[27] Current opportunity for research impact lies mainly in fundamental research and practical manufacturing.[28] Debates have recently emerged concerning the applicability of the Wenzel and Cassie–Baxter models. In an experiment designed to challenge the surface energy perspective of the Wenzel and Cassie–Baxter model and promote a contact line perspective, water drops were placed on a smooth hydrophobic spot in a rough hydrophobic field, a rough hydrophobic spot in a smooth hydrophobic field, and a hydrophilic spot in a hydrophobic field.[29] Experiments showed that the surface chemistry and geometry at the contact line affected the contact angle and contact angle hysteresis, but the surface area inside the contact line had no effect. An argument that increased jaggedness in the contact line enhances droplet mobility has also been proposed.[30]
$
5
Question: What did Dettre and Johnson discover in relation to the superhydrophobic lotus effect?
A: It is due to smooth hydrophobic surfaces
B: It is a myth
C: It is due to rough hydrophobic surfaces
D: It is related to ionic interactions
E: It is due to chemical bonding with water
Answer: C

Question: Which of the following materials was not mentioned to have been developed for superhydrophobic uses?
A: RF plasma
B: Perfluoropolyether
C: Alkylketene dimer
D: Zinc oxide
E: Perfluoroalkyl
Answer: D

Question: In recent research, superhydrophobicity was achieved by letting which substance solidify into a nanostructured fractal surface?
A: Zinc oxide
B: RF plasma
C: Alkylketene dimer
D: Silicon
E: Polyethylene
Answer: C

Question: Which technique was NOT mentioned for producing superhydrophobic surfaces?
A: Particle deposition
B: Sol-gel techniques
C: Enzymatic reactions
D: Plasma treatments
E: Casting techniques
Answer: C

Question: In a recent experiment, which factor was shown to affect the contact angle and contact angle hysteresis?
A: Surface chemistry and geometry at the contact line
B: Surface area inside the contact line
C: Solubility of the material
D: Polarity of the substrate
E: Electrostatic interactions
Answer: A
@
The lotus effect refers to self-cleaning properties that are a result of ultrahydrophobicity as exhibited by the leaves of Nelumbo, the lotus flower.[1] Dirt particles are picked up by water droplets due to the micro- and nanoscopic architecture on the surface, which minimizes the droplet's adhesion to that surface. Ultrahydrophobicity and self-cleaning properties are also found in other plants, such as Tropaeolum (nasturtium), Opuntia (prickly pear), Alchemilla, cane, and also on the wings of certain insects.[2]
$
5
Question: What phenomenon does the "lotus effect" describe?
A: The growth pattern of the lotus flower
B: The scent of the lotus flower
C: The hydrophilic nature of lotus leaves
D: The self-cleaning properties due to ultrahydrophobicity of the lotus leaves
E: The pollination mechanism of the lotus flower
Answer: D

Question: Dirt particles on the leaves of Nelumbo are primarily removed by:
A: Wind
B: Manual wiping
C: UV radiation
D: Water droplets
E: Natural predators
Answer: D

Question: Which of the following is NOT mentioned as having ultrahydrophobicity and self-cleaning properties similar to the lotus effect?
A: Alchemilla
B: Prickly pear
C: Rose
D: Nasturtium
E: Certain insect wings
Answer: C

Question: The minimized adhesion of water droplets on the Nelumbo surface is due to:
A: Macroscopic architecture
B: Nanoscopic architecture
C: Thermal properties
D: Electrostatic properties
E: Photonic properties
Answer: B

Question: The self-cleaning properties seen in Nelumbo are also found in:
A: All plants
B: All insects
C: Some insects and plants
D: Only aquatic plants
E: Only desert plants
Answer: C
@
The high surface tension of water causes droplets to assume a nearly spherical shape, since a sphere has minimal surface area, and this shape therefore minimizes the solid-liquid surface energy. On contact of liquid with a surface, adhesion forces result in wetting of the surface. Either complete or incomplete wetting may occur depending on the structure of the surface and the fluid tension of the droplet.[12] The cause of self-cleaning properties is the hydrophobic water-repellent double structure of the surface.[13] This enables the contact area and the adhesion force between surface and droplet to be significantly reduced, resulting in a self-cleaning process.[14][15][16] This hierarchical double structure is formed out of a characteristic epidermis (its outermost layer called the cuticle) and the covering waxes. The epidermis of the lotus plant possesses papillae 10 μm to 20 μm in height and 10 μm to 15 μm in width on which the so-called epicuticular waxes are imposed. These superimposed waxes are hydrophobic and form the second layer of the double structure. This system regenerates. This biochemical property is responsible for the functioning of the water repellency of the surface.

The hydrophobicity of a surface can be measured by its contact angle. The higher the contact angle the higher the hydrophobicity of a surface. Surfaces with a contact angle < 90° are referred to as hydrophilic and those with an angle >90° as hydrophobic. Some plants show contact angles up to 160° and are called ultrahydrophobic, meaning that only 2–3% of the surface of a droplet (of typical size) is in contact. Plants with a double structured surface like the lotus can reach a contact angle of 170°, whereby the droplet's contact area is only 0.6%. All this leads to a self-cleaning effect.
$
5
Question: Why do water droplets assume a nearly spherical shape?
A: High vapor pressure of water
B: High surface tension of water
C: Gravitational forces
D: High density of water
E: High fluidity of water
Answer: B

Question: Surfaces with a contact angle greater than 90° are referred to as:
A: Hydrophilic
B: Neutral
C: Ultrahydrophobic
D: Hydrophobic
E: Water-neutral
Answer: D

Question: Which property determines the hydrophobicity of a surface?
A: Mass density
B: Surface temperature
C: Contact angle
D: Elasticity
E: Refractive index
Answer: C

Question: What is the contact area of a droplet on a surface like the lotus?
A: 10%
B: 50%
C: 1%
D: 2-3%
E: 0.6%
Answer: E

Question: The self-cleaning properties of certain plants is due to:
A: A hydrophilic water-attracting single structure
B: A neutral water structure
C: A hydrophilic water-repellent double structure
D: A hydrophobic water-repellent double structure
E: A hydrophobic water-attracting single structure
Answer: D
@
Dirt particles with an extremely reduced contact area are picked up by water droplets and are thus easily cleaned off the surface. If a water droplet rolls across such a contaminated surface the adhesion between the dirt particle, irrespective of its chemistry, and the droplet is higher than between the particle and the surface. This cleaning effect has been demonstrated on common materials such as stainless steel when a superhydrophobic surface is produced.[17] As this self-cleaning effect is based on the high surface tension of water it does not work with organic solvents. Therefore, the hydrophobicity of a surface is no protection against graffiti.

This effect is of a great importance for plants as a protection against pathogens like fungi or algae growth, and also for animals like butterflies, dragonflies and other insects not able to cleanse all their body parts. Another positive effect of self-cleaning is the prevention of contamination of the area of a plant surface exposed to light resulting in reduced photosynthesis.
$
5
Question: On what materials has the self-cleaning effect, similar to plants, been demonstrated?
A: Plastic
B: Glass
C: Wood
D: Stainless steel
E: Rubber
Answer: D

Question: Why doesn't the self-cleaning effect work with organic solvents?
A: Low fluid tension
B: Low surface tension
C: High density
D: High fluid tension
E: High surface tension
Answer: E

Question: Which of the following is NOT a benefit of the self-cleaning property in plants?
A: Increased photosynthesis
B: Resistance against fungi
C: Protection from predators
D: Prevention of algae growth
E: Reduction in water consumption
Answer: C

Question: The adhesion between a dirt particle and a water droplet is _______ than between the particle and a superhydrophobic surface.
A: Stronger
B: Weaker
C: Equal
D: Non-existent
E: Variable
Answer: A

Question: Which of the following is not protected by the hydrophobicity of a surface?
A: Water droplets
B: Inorganic solvents
C: Dirt particles
D: Fungi
E: Graffiti
Answer: E
@
Biomimetics or biomimicry is the emulation of the models, systems, and elements of nature for the purpose of solving complex human problems.[2][3][4] The terms "biomimetics" and "biomimicry" are derived from Ancient Greek: βίος (bios), life, and μίμησις (mīmēsis), imitation, from μιμεῖσθαι (mīmeisthai), to imitate, from μῖμος (mimos), actor. A closely related field is bionics.[5]

Nature has gone through evolution over the 3.8 billion years since life is estimated to have appeared on the Earth.[6] It has evolved species with high performance using commonly found materials. Surfaces of solids interact with other surfaces and the environment and derive the properties of materials. Biological materials are highly organized from the molecular to the nano-, micro-, and macroscales, often in a hierarchical manner with intricate nanoarchitecture that ultimately makes up a myriad of different functional elements.[7] Properties of materials and surfaces result from a complex interplay between surface structure and morphology and physical and chemical properties. Many materials, surfaces, and objects in general provide multifunctionality.

Various materials, structures, and devices have been fabricated for commercial interest by engineers, material scientists, chemists, and biologists, and for beauty, structure, and design by artists and architects. Nature has solved engineering problems such as self-healing abilities, environmental exposure tolerance and resistance, hydrophobicity, self-assembly, and harnessing solar energy. Economic impact of bioinspired materials and surfaces is significant, on the order of several hundred billion dollars per year worldwide.
$
5
Question: What is the primary objective of biomimetics?
A: To study nature
B: To clone natural organisms
C: To imitate nature to solve human problems
D: To replace natural systems with artificial ones
E: To enhance natural models for superior performance
Answer: C

Question: From which language are the terms "biomimetics" and "biomimicry" derived?
A: Latin
B: French
C: Sanskrit
D: German
E: Ancient Greek
Answer: E

Question: For approximately how long has life existed on Earth?
A: 1 billion years
B: 3.8 billion years
C: 5 billion years
D: 2.5 billion years
E: 10 billion years
Answer: B

Question: What does the study of biological materials reveal about their organization?
A: Random structures
B: Single-scale structures
C: Hierarchical structures from molecular to macro scales
D: Only macro structures
E: Only nano structures
Answer: C

Question: The economic impact of bioinspired materials and surfaces is in the order of:
A: Several hundred thousand dollars per year
B: Several million dollars per year
C: Several billion dollars per year
D: Several trillion dollars per year
E: It's negligible
Answer: C
@
Humans, or modern humans (Homo sapiens), are the most common and widespread species of primate. A great ape characterized by their hairlessness, bipedalism, and high intelligence, humans have a large brain and resulting cognitive skills that enable them to thrive in varied environments and develop complex societies and civilizations. Humans are highly social and tend to live in complex social structures composed of many cooperating and competing groups, from families and kinship networks to political states. As such, social interactions between humans have established a wide variety of values, social norms, languages, and rituals, each of which bolsters human society. The desire to understand and influence phenomena has motivated humanity's development of science, technology, philosophy, mythology, religion, and other conceptual frameworks.

Although some scientists equate the term "humans" with all members of the genus Homo, in common usage it generally refers to Homo sapiens, the only extant member. Other members of the genus Homo are known as archaic humans. Anatomically modern humans emerged around 300,000 years ago in Africa, evolving from Homo heidelbergensis or a similar species and migrating out of Africa, gradually replacing or interbreeding with local populations of archaic humans. For most of their history, humans were nomadic hunter-gatherers. Humans began exhibiting behavioral modernity about 160,000–60,000 years ago. The Neolithic Revolution, which began in Southwest Asia around 13,000 years ago (and separately in a few other places), saw the emergence of agriculture and permanent human settlement. As populations became larger and denser, forms of governance developed within and between communities, and a large number of civilizations have risen and fallen. Humans have continued to expand, with a global population of over 8 billion as of 2023.
$
5
Question: Which of the following characteristics is NOT associated with modern humans (Homo sapiens)?
A: Aquatic Respiration
B: Bipedalism
C: High intelligence
D: Development of complex societies
E: Living in varied environments
Answer: A

Question: What term is commonly used for humans belonging outside of the Homo sapiens species?
A: Modern humans
B: Australopithecus
C: Archaic humans
D: Neolithic humans
E: Homo erectus
Answer: C

Question: Approximately how long ago did anatomically modern humans emerge?
A: 60,000 years ago
B: 13,000 years ago
C: 300,000 years ago
D: 1 million years ago
E: 8 billion years ago
Answer: C

Question: For most of their history, humans primarily had which lifestyle?
A: Agriculturalists
B: Urbanites
C: Nomadic hunter-gatherers
D: Miners
E: Sailors
Answer: C

Question: The Neolithic Revolution led to what significant change in human history?
A: The migration out of Africa
B: The invention of the wheel
C: The development of written language
D: The emergence of agriculture
E: The rise of the Roman Empire
Answer: D
@
The genus Homo evolved from Australopithecus.[18][19] Though fossils from the transition are scarce, the earliest members of Homo share several key traits with Australopithecus.[20][21] The earliest record of Homo is the 2.8 million-year-old specimen LD 350-1 from Ethiopia, and the earliest named species are Homo habilis and Homo rudolfensis which evolved by 2.3 million years ago.[21] H. erectus (the African variant is sometimes called H. ergaster) evolved 2 million years ago and was the first archaic human species to leave Africa and disperse across Eurasia.[22] H. erectus also was the first to evolve a characteristically human body plan. Homo sapiens emerged in Africa around 300,000 years ago from a species commonly designated as either H. heidelbergensis or H. rhodesiensis, the descendants of H. erectus that remained in Africa.[23] H. sapiens migrated out of the continent, gradually replacing or interbreeding with local populations of archaic humans.[24][25][26] Humans began exhibiting behavioral modernity about 160,000–70,000 years ago,[27] and possibly earlier.[28]

The "out of Africa" migration took place in at least two waves, the first around 130,000 to 100,000 years ago, the second (Southern Dispersal) around 70,000 to 50,000 years ago.[29][30] H. sapiens proceeded to colonize all the continents and larger islands, arriving in Eurasia 125,000 years ago,[31][32] Australia around 65,000 years ago,[33] the Americas around 15,000 years ago, and remote islands such as Hawaii, Easter Island, Madagascar, and New Zealand between the years 300 and 1280 CE.[34][35]

Human evolution was not a simple linear or branched progression but involved interbreeding between related species.[36][37][38] Genomic research has shown that hybridization between substantially diverged lineages was common in human evolution.[39] DNA evidence suggests that several genes of Neanderthal origin are present among all non sub-Saharan African populations, and Neanderthals and other hominins, such as Denisovans, may have contributed up to 6% of their genome to present-day non sub-Saharan African humans.[36][40][41]
$
5
Question: Which species is considered to be the earliest record of the Homo genus?
A: Homo erectus
B: Homo sapiens
C: Homo habilis
D: Homo neanderthalensis
E: LD 350-1
Answer: E

Question: Which species was the first to leave Africa and spread across Eurasia?
A: Homo habilis
B: Homo sapiens
C: Homo rudolfensis
D: Homo erectus
E: Homo heidelbergensis
Answer: D

Question: What event is referred to as the "out of Africa" migration?
A: The movement of Homo sapiens from Europe to Asia
B: The migration of Homo erectus within Africa
C: The movement of Homo sapiens out of Africa to colonize other continents
D: The migration of Australopithecus into Europe
E: The spread of Neanderthals into Asia
Answer: C

Question: What percentage of the genome of present-day non sub-Saharan African humans might be contributed by Neanderthals and other hominins?
A: 10%
B: 50%
C: 0.5%
D: 6%
E: 25%
Answer: D

Question: The "Southern Dispersal" migration out of Africa took place approximately how many years ago?
A: 13,000 to 10,000 years ago
B: 2.3 million years ago
C: 130,000 to 100,000 years ago
D: 70,000 to 50,000 years ago
E: 300,000 to 250,000 years ago
Answer: D
@
An aspect unique to humans is their ability to transmit knowledge from one generation to the next and to continually build on this information to develop tools, scientific laws and other advances to pass on further.[401] This accumulated knowledge can be tested to answer questions or make predictions about how the universe functions and has been very successful in advancing human ascendancy.[402]

Aristotle has been described as the first scientist,[403] and preceded the rise of scientific thought through the Hellenistic period.[404] Other early advances in science came from the Han Dynasty in China and during the Islamic Golden Age.[405][86] The scientific revolution, near the end of the Renaissance, led to the emergence of modern science.[406]

A chain of events and influences led to the development of the scientific method, a process of observation and experimentation that is used to differentiate science from pseudoscience.[407] An understanding of mathematics is unique to humans, although other species of animals have some numerical cognition.[408] All of science can be divided into three major branches, the formal sciences (e.g., logic and mathematics), which are concerned with formal systems, the applied sciences (e.g., engineering, medicine), which are focused on practical applications, and the empirical sciences, which are based on empirical observation and are in turn divided into natural sciences (e.g., physics, chemistry, biology) and social sciences (e.g., psychology, economics, sociology).[409]
$
5
Question: What unique human ability has led to the development of various advances in tools, science, and other fields?
A: The ability to regenerate tissue
B: The ability to breathe underwater
C: The ability to transmit knowledge across generations
D: The ability to communicate through telepathy
E: The ability to harness solar energy directly
Answer: C

Question: Who has been described as the first scientist?
A: Isaac Newton
B: Albert Einstein
C: Archimedes
D: Aristotle
E: Galileo Galilei
Answer: D

Question: What process is used to differentiate science from pseudoscience?
A: The Neolithic process
B: The Renaissance method
C: The Roman process
D: The scientific method
E: The enlightenment process
Answer: D

Question: Mathematics is unique to which species?
A: Dolphins
B: Birds
C: Primates
D: Dogs
E: Humans
Answer: E

Question: Which branch of science is NOT one of the three major branches mentioned?
A: Marine sciences
B: Formal sciences
C: Applied sciences
D: Empirical sciences
E: Aesthetic sciences
Answer: A
@
Estimates of the population at the time agriculture emerged in around 10,000 BC have ranged between 1 million and 15 million.[133][134] Around 50–60 million people lived in the combined eastern and western Roman Empire in the 4th century AD.[135] Bubonic plagues, first recorded in the 6th century AD, reduced the population by 50%, with the Black Death killing 75–200 million people in Eurasia and North Africa alone.[136] Human population is believed to have reached one billion in 1800. It has since then increased exponentially, reaching two billion in 1930 and three billion in 1960, four in 1975, five in 1987 and six billion in 1999.[137] It passed seven billion in 2011[138] and passed eight billion in November 2022.[139] It took over two million years of human prehistory and history for the human population to reach one billion and only 207 years more to grow to 7 billion.[140] The combined biomass of the carbon of all the humans on Earth in 2018 was estimated at 60 million tons, about 10 times larger than that of all non-domesticated mammals.[132]

In 2018, 4.2 billion humans (55%) lived in urban areas, up from 751 million in 1950.[141] The most urbanized regions are Northern America (82%), Latin America (81%), Europe (74%) and Oceania (68%), with Africa and Asia having nearly 90% of the world's 3.4 billion rural population.[141] 
$
5
Question: Approximately how long did it take for the human population to reach one billion?
A: 13,000 years
B: 207 years
C: 2 million years
D: 1 billion years
E: 500,000 years
Answer: C

Question: The global human population reached eight billion in which year?
A: 2011
B: 1950
C: 1999
D: 2022
E: 2075
Answer: D

Question: As of 2018, what percentage of humans lived in urban areas?
A: 25%
B: 90%
C: 45%
D: 55%
E: 75%
Answer: D

Question: By what factor did the Black Death reduce the population?
A: 10%
B: 25%
C: 50%
D: 75%
E: 90%
Answer: C

Question: Which event or period marks the beginning of agriculture?
A: Renaissance
B: Neolithic Revolution
C: Scientific Revolution
D: Industrial Revolution
E: Roman Era
Answer: B
@
The Industrial Revolution, also known as the First Industrial Revolution, was a period of global transition of human economy towards more efficient and stable manufacturing processes that succeeded the Agricultural Revolution, starting from Great Britain, continental Europe, and the United States, that occurred during the period from around 1760 to about 1820–1840.[1] This transition included going from hand production methods to machines; new chemical manufacturing and iron production processes; the increasing use of water power and steam power; the development of machine tools; and the rise of the mechanized factory system. Output greatly increased, and a result was an unprecedented rise in population and in the rate of population growth. The textile industry was the first to use modern production methods,[2]: 40  and textiles became the dominant industry in terms of employment, value of output, and capital invested.

On a structural level the Industrial Revolution asked society the so-called social question, demanding new ideas for managing large groups of individuals. Visible poverty on one hand and growing population and materialistic wealth on the other caused tensions between the very rich and the poorest people within society.[3] These tensions were sometimes violently released[4] and led to philosophical ideas such as socialism, communism and anarchism.

The Industrial Revolution began in Great Britain, and many of the technological and architectural innovations were of British origin.[5][6] By the mid-18th century, Britain was the world's leading commercial nation,[7] controlling a global trading empire with colonies in North America and the Caribbean. Britain had major military and political hegemony on the Indian subcontinent; particularly with the proto-industrialised Mughal Bengal, through the activities of the East India Company.[8][9][10][11] The development of trade and the rise of business were among the major causes of the Industrial Revolution.[2]: 15 
$
5
Question: Which industry was the first to utilize modern production methods during the Industrial Revolution?
A: Iron and coal industry
B: Agriculture
C: Textile industry
D: Chemical manufacturing
E: Machine tool development
Answer: C

Question: Which philosophical ideas emerged due to the societal tensions during the Industrial Revolution?
A: Capitalism and Mercantilism
B: Socialism, Communism, and Anarchism
C: Renaissance and Enlightenment
D: Existentialism and Absurdism
E: Conservatism and Monarchism
Answer: B

Question: During which period did the Industrial Revolution occur?
A: 1600-1650
B: 1700-1750
C: 1650-1700
D: 1850-1900
E: 1760-1840
Answer: E

Question: Where did the Industrial Revolution begin?
A: United States
B: France
C: India
D: Belgium
E: Great Britain
Answer: E

Question: Which company played a significant role in Britain's influence on the Indian subcontinent during the Industrial Revolution?
A: West India Company
B: Hudson Bay Company
C: East India Company
D: British Trading Company
E: North India Company
Answer: C
@
The Industrial Revolution marked a major turning point in history. Comparable only to humanity's adoption of agriculture with respect to material advancement,[12] the Industrial Revolution influenced in some way almost every aspect of daily life. In particular, average income and population began to exhibit unprecedented sustained growth. Some economists have said the most important effect of the Industrial Revolution was that the standard of living for the general population in the Western world began to increase consistently for the first time in history, although others have said that it did not begin to improve meaningfully until the late 19th and 20th centuries.[13][14][15] GDP per capita was broadly stable before the Industrial Revolution and the emergence of the modern capitalist economy,[16] while the Industrial Revolution began an era of per-capita economic growth in capitalist economies.[17] Economic historians agree that the onset of the Industrial Revolution is the most important event in human history since the domestication of animals and plants.[18]

The precise start and end of the Industrial Revolution is still debated among historians, as is the pace of economic and social changes.[19][20][21][22] Eric Hobsbawm held that the Industrial Revolution began in Britain in the 1780s and was not fully felt until the 1830s or 1840s,[19] while T. S. Ashton held that it occurred roughly between 1760 and 1830.[20] Rapid industrialisation first began in Britain, starting with mechanized textiles spinning in the 1780s,[23] with high rates of growth in steam power and iron production occurring after 1800. Mechanized textile production spread from Great Britain to continental Europe and the United States in the early 19th century, with important centres of textiles, iron and coal emerging in Belgium and the United States and later textiles in France.[2]
$
5
Question: The onset of the Industrial Revolution is considered the most significant event since what other historical development?
A: The Renaissance
B: The Scientific Revolution
C: The Age of Exploration
D: The domestication of animals and plants
E: The Enlightenment
Answer: D

Question: What significant change did the Industrial Revolution introduce in capitalist economies?
A: Decline of per-capita economic growth
B: Stability of GDP per capita
C: Reduction in average income
D: Era of per-capita economic growth
E: Transition to agrarian societies
Answer: D

Question: In which country did rapid industrialization first start?
A: United States
B: Belgium
C: Germany
D: France
E: Great Britain
Answer: E

Question: Mechanized textile production first spread to all of the following EXCEPT:
A: France
B: United States
C: Belgium
D: India
E: Continental Europe
Answer: D

Question: Which historian believed that the Industrial Revolution began in Britain in the 1780s?
A: T. S. Ashton
B: Robert Lucas Jr.
C: John Locke
D: Eric Hobsbawm
E: Adam Smith
Answer: D
@
Some economists, such as Robert Lucas Jr., say that the real effect of the Industrial Revolution was that "for the first time in history, the living standards of the masses of ordinary people have begun to undergo sustained growth ... Nothing remotely like this economic behaviour is mentioned by the classical economists, even as a theoretical possibility."[13] Others argue that while the growth of the economy's overall productive powers was unprecedented during the Industrial Revolution, living standards for the majority of the population did not grow meaningfully until the late 19th and 20th centuries and that in many ways workers' living standards declined under early capitalism: some studies have estimated that real wages in Britain only increased 15% between the 1780s and 1850s and that life expectancy in Britain did not begin to dramatically increase until the 1870s.[14][15]

The average height of the population declined during the Industrial Revolution, implying that their nutritional status was also decreasing.[104][105]

During the Industrial Revolution, the life expectancy of children increased dramatically. The percentage of the children born in London who died before the age of five decreased from 74.5% in 1730–1749 to 31.8% in 1810–1829.[106] The effects on living conditions have been controversial and were hotly debated by economic and social historians from the 1950s to the 1980s.[107] Over the course of the period from 1813 to 1913, there was a significant increase in worker wages.[108][109]
$
5
Question: What did Robert Lucas Jr. emphasize about the Industrial Revolution's effect on living standards?
A: They declined rapidly
B: They remained stable
C: For the first time, they began sustained growth for ordinary people
D: Only the elite benefitted
E: No significant changes were observed
Answer: C

Question: Which of the following was an indication of declining nutritional status during the Industrial Revolution?
A: Increase in average height
B: Decrease in life expectancy
C: Increase in real wages
D: Decline in average height
E: Increase in worker productivity
Answer: D

Question: Life expectancy of children during the Industrial Revolution:
A: Remained the same
B: Declined dramatically
C: Increased dramatically
D: Was unaffected by industrial changes
E: Became highly variable
Answer: C

Question: What significant change happened to real wages over the course from 1813 to 1913?
A: They decreased significantly
B: They remained stable
C: They fluctuated without a clear trend
D: They increased significantly
E: They became unpredictable
Answer: D

Question: When did life expectancy in Britain begin to notably increase?
A: 1730s
B: 1780s
C: 1830s
D: 1870s
E: 1910s
Answer: D
@
In terms of social structure, the Industrial Revolution witnessed the triumph of a middle class of industrialists and businessmen over a landed class of nobility and gentry. Ordinary working people found increased opportunities for employment in mills and factories, but these were often under strict working conditions with long hours of labour dominated by a pace set by machines. As late as 1900, most industrial workers in the United States worked a 10-hour day (12 hours in the steel industry), yet earned 20–40% less than the minimum deemed necessary for a decent life;[154] however, most workers in textiles, which was by far the leading industry in terms of employment, were women and children.[43] For workers of the labouring classes, industrial life "was a stony desert, which they had to make habitable by their own efforts."[155]

Harsh working conditions were prevalent long before the Industrial Revolution took place. Pre-industrial society was very static and often cruel—child labour, dirty living conditions, and long working hours were just as prevalent before the Industrial Revolution.[156]
$
5
Question: By the 1900s, most industrial workers in the US worked how many hours a day?
A: 6 hours
B: 8 hours
C: 10 hours
D: 12 hours
E: 14 hours
Answer: C

Question: Which industry was the leading one in terms of employment during the Industrial Revolution?
A: Steel industry
B: Machine tool industry
C: Textiles
D: Agriculture
E: Chemical manufacturing
Answer: C

Question: What characterized the working conditions during the Industrial Revolution?
A: Short and flexible hours
B: Clean and safe environments
C: Strict conditions with long hours dictated by machines
D: Workers could choose their preferred working conditions
E: Predominantly outdoor jobs
Answer: C

Question: Pre-industrial society was marked by:
A: Clean living conditions and short working hours
B: Static society with predominantly middle-class people
C: Child labour, long working hours, and dirty living conditions
D: Highly dynamic and changing societal norms
E: Industrialized methods of production
Answer: C

Question: Which social class triumphed over the landed class of nobility and gentry during the Industrial Revolution?
A: Peasants
B: Soldiers
C: Priests
D: Middle class of industrialists and businessmen
E: Artisans and craftsmen
Answer: D
@
The Luddites were members of a 19th-century movement of English textile workers which opposed the use of certain types of cost-saving machinery, often by destroying the machines in clandestine raids. They protested against manufacturers who used machines in "a fraudulent and deceitful manner" to replace the skilled labour of workers and drive down wages by producing inferior goods.[1][2] Members of the group referred to themselves as Luddites, self-described followers of "Ned Ludd", a legendary weaver whose name was used as a pseudonym in threatening letters to mill owners and government officials.[3]

The Luddite movement began in Nottingham, England and spread to the North West and Yorkshire between 1811 and 1816.[4] Mill and factory owners took to shooting protesters and eventually the movement was suppressed with legal and military force, which included execution and penal transportation of accused and convicted Luddites.[5]

Over time, the term has been used to refer to those opposed to industrialisation, automation, computerisation, or new technologies in general.[6]
$
5
Question: What was the main grievance of the Luddites against manufacturers?
A: The production of inferior goods.
B: The use of machinery to replace skilled workers.
C: The introduction of new textile colors.
D: The removal of traditional weaving techniques.
E: The establishment of a new working hierarchy.
Answer: B

Question: In which city did the Luddite movement begin?
A: London
B: Birmingham
C: Manchester
D: Nottingham
E: Leeds
Answer: D

Question: Who was "Ned Ludd" in the context of the Luddite movement?
A: The founder of the movement.
B: A famous textile manufacturer.
C: A pseudonym used in threatening letters.
D: A government official against Luddites.
E: The leader of a rival group.
Answer: C

Question: How did the term "Luddite" evolve over time?
A: It referred exclusively to textile workers.
B: It became synonymous with skilled labor.
C: It was used to describe supporters of automation.
D: It denoted those opposed to new technologies.
E: It represented elite manufacturers.
Answer: D

Question: How was the Luddite movement primarily suppressed?
A: Through peaceful negotiations.
B: By reducing the cost of textiles.
C: By providing alternative employment.
D: Using legal and military force.
E: By acquiescing to their demands.
Answer: D
@
The machine-breaking of the Luddites followed from previous outbreaks of sabotage in the English textile industry, especially in the hosiery and woolen trades. Organized action by stockingers had occurred at various times since 1675.[10][11][12] In Lancashire, new cotton spinning technologies were met with violent resistance in 1768 and 1779. These new inventions produced textiles faster and cheaper because they could be operated by less-skilled, low-wage labourers.[13] These struggles sometimes resulted in government suppression, via Parliamentary acts such as the Protection of Stocking Frames, etc. Act 1788.

Periodic uprisings relating to asset prices also occurred in other contexts in the century before Luddism. Irregular rises in food prices provoked the Keelmen to riot in the port of Tyne in 1710[14] and tin miners to steal from granaries at Falmouth in 1727. [a] There was a rebellion in Northumberland and Durham in 1740, and an assault on Quaker corn dealers in 1756.

Malcolm L. Thomas argued in his 1970 history The Luddites that machine-breaking was one of the very few tactics that workers could use to increase pressure on employers, undermine lower-paid competing workers, and create solidarity among workers. "These attacks on machines did not imply any necessary hostility to machinery as such; machinery was just a conveniently exposed target against which an attack could be made."[12] Historian Eric Hobsbawm has called their machine wrecking "collective bargaining by riot", which had been a tactic used in Britain since the Restoration because manufactories were scattered throughout the country, and that made it impractical to hold large-scale strikes.[15][16] An agricultural variant of Luddism occurred during the widespread Swing Riots of 1830 in southern and eastern England, centring on breaking threshing machines.[17]
$
5
Question: What did the Luddites typically target in their protests?
A: Agricultural tools
B: Transportation methods
C: Machines that increased textile production
D: Government buildings
E: Residential properties
Answer: C

Question: What was a major tactic workers used against employers during this period?
A: Setting up alternative factories
B: Offering lower quality work
C: Organizing peaceful marches
D: Machine-breaking
E: Diplomatic negotiations
Answer: D

Question: Why were the Swing Riots of 1830 notable in the context of Luddism?
A: They opposed cotton production.
B: They targeted threshing machines.
C: They endorsed steam power.
D: They promoted skilled labor.
E: They opposed the use of child labor.
Answer: B

Question: Which of the following was a frequent consequence for the Luddites' machine-breaking activities?
A: Promotion to managerial roles
B: Government subsidies
C: Execution or deportation
D: A reward from mill owners
E: Public parades in their honor
Answer: C

Question: Why were machines like the "wide" knitting frames targeted by the Luddites?
A: They made high-quality lace articles.
B: They produced cheaper and inferior goods.
C: They enhanced the skills of the workers.
D: They were only used by the elite.
E: They were difficult to operate.
Answer: B
@
The Luddite movement emerged during the harsh economic climate of the Napoleonic Wars, which saw a rise in difficult working conditions in the new textile factories. Luddites objected primarily to the rising popularity of automated textile equipment, threatening the jobs and livelihoods of skilled workers as this technology allowed them to be replaced by cheaper and less skilled workers.[1][failed verification] The movement began in Arnold, Nottingham, on 11 March 1811 and spread rapidly throughout England over the following two years.[18][1] The British economy suffered greatly in 1810 to 1812, especially in terms of high unemployment and inflation. The causes included the high cost of the wars with Napoleon, Napoleon's Continental System of economic warfare, and escalating conflict with the United States. The crisis led to widespread protest and violence, but the middle classes and upper classes strongly supported the government, which used the army to suppress all working-class unrest, especially the Luddite movement.[19][20]

The Luddites met at night on the moors surrounding industrial towns to practice military-like drills and manoeuvres. Their main areas of operation began in Nottinghamshire in November 1811, followed by the West Riding of Yorkshire in early 1812, and then Lancashire by March 1813. They wrecked specific types of machinery that posed a threat to the particular industrial interests in each region. In the Midlands, these were the "wide" knitting frames used to make cheap and inferior lace articles. In the North West, weavers sought to eliminate the steam-powered looms threatening wages in the cotton trade. In Yorkshire, workers opposed the use of shearing frames and gig mills to finish woolen cloth.
$
5
Question: The Luddite movement was primarily a reaction against what trend?
A: Decreasing employment in mills.
B: Rising popularity of automated textile equipment.
C: The introduction of new agricultural techniques.
D: The abolition of manual labor.
E: Increasing trade with France.
Answer: B

Question: What economic challenges were faced by Britain between 1810 to 1812?
A: Widespread affluence and prosperity.
B: Declining unemployment and inflation rates.
C: High employment and deflation.
D: High unemployment and inflation.
E: Stable prices and increased exports.
Answer: D

Question: Which machines did the Luddites in the North West specifically target?
A: Machines producing lace.
B: Steam-powered looms.
C: Agricultural plows.
D: Wood carving tools.
E: Water-powered spinning wheels.
Answer: B

Question: In which year did the Luddite movement begin in Arnold, Nottingham?
A: 1799
B: 1805
C: 1811
D: 1822
E: 1830
Answer: C

Question: Why did the Luddites meet at night on the moors?
A: To celebrate their achievements.
B: To trade with other groups.
C: To practice drills and maneuvers.
D: To repair broken machinery.
E: To interact with the local communities.
Answer: C
@
The British government ultimately dispatched 12,000 troops to suppress Luddite activity, which as historian Eric Hobsbawm noted was a larger number than the army which the Duke of Wellington led during the Peninsular War.[28][b] Four Luddites, led by a man named George Mellor, ambushed and assassinated mill owner William Horsfall of Ottiwells Mill in Marsden, West Yorkshire, at Crosland Moor in Huddersfield. Horsfall had remarked that he would "Ride up to his saddle in Luddite blood".[29] Mellor fired the fatal shot to Horsfall's groin, and all four men were arrested. One of the men, Benjamin Walker, turned informant, and the other three were hanged.[30][31][32] Lord Byron denounced what he considered to be the plight of the working class, the government's inane policies and ruthless repression in the House of Lords on 27 February 1812: "I have been in some of the most oppressed provinces of Turkey; but never, under the most despotic of infidel governments, did I behold such squalid wretchedness as I have seen since my return, in the very heart of a Christian country".[33]

Government officials sought to suppress the Luddite movement with a mass trial at York in January 1813, following the attack on Cartwrights Mill at Rawfolds near Cleckheaton. The government charged over 60 men, including Mellor and his companions, with various crimes in connection with Luddite activities. While some of those charged were actual Luddites, many had no connection to the movement. Although the proceedings were legitimate jury trials, many were abandoned due to lack of evidence and 30 men were acquitted. These trials were certainly intended to act as show trials to deter other Luddites from continuing their activities. The harsh sentences of those found guilty, which included execution and penal transportation, quickly ended the movement.[5][34] Parliament made "machine breaking" (i.e. industrial sabotage) a capital crime with the Frame Breaking Act of 1812.[35] Lord Byron opposed this legislation, becoming one of the few prominent defenders of the Luddites after the treatment of the defendants at the York trials.[36]
$
5
Question: How many troops were dispatched by the British government to suppress the Luddites?
A: 1,200
B: 5,000
C: 8,000
D: 12,000
E: 20,000
Answer: D

Question: Who was assassinated by a group of Luddites, including George Mellor?
A: Lord Byron
B: Benjamin Walker
C: Eric Hobsbawm
D: William Horsfall
E: Duke of Wellington
Answer: D

Question: Which legislation made "machine breaking" a capital crime in 1812?
A: The Industrial Act
B: The Protection of Machinery Act
C: The Luddite Defense Act
D: The Frame Breaking Act
E: The Automation Safeguard Act
Answer: D

Question: What was the outcome of the mass trial at York in January 1813?
A: All Luddites were acquitted.
B: All Luddites were executed.
C: Some were executed or deported, others acquitted.
D: Luddites were given compensation.
E: The trial was postponed indefinitely.
Answer: C

Question: Who was a prominent defender of the Luddites, especially after the York trials?
A: Eric Hobsbawm
B: Benjamin Walker
C: William Horsfall
D: George Mellor
E: Lord Byron
Answer: E
@
The commencement of the Industrial Revolution is closely linked to a small number of innovations,[36] beginning in the second half of the 18th century. By the 1830s, the following gains had been made in important technologies:

Textiles – mechanised cotton spinning powered by water, and later steam, increased the output of a worker by a factor of around 500. The power loom increased the output of a worker by a factor of over 40.[37] The cotton gin increased productivity of removing seed from cotton by a factor of 50.[25] Large gains in productivity also occurred in spinning and weaving of wool and linen, but they were not as great as in cotton.[2]
Steam power – the efficiency of steam engines increased so that they used between one-fifth and one-tenth as much fuel. The adaptation of stationary steam engines to rotary motion made them suitable for industrial uses.[2]: 82  The high-pressure engine had a high power-to-weight ratio, making it suitable for transportation.[26] Steam power underwent a rapid expansion after 1800.
Iron making – the substitution of coke for charcoal greatly lowered the fuel cost of pig iron and wrought iron production.[2]: 89–93  Using coke also allowed larger blast furnaces,[38][39] resulting in economies of scale. The steam engine began being used to power blast air (indirectly by pumping water to a water wheel) in the 1750s, enabling a large increase in iron production by overcoming the limitation of water power.[40] The cast iron blowing cylinder was first used in 1760. It was later improved by making it double acting, which allowed higher blast furnace temperatures. The puddling process produced a structural grade iron at a lower cost than the finery forge.[41] The rolling mill was fifteen times faster than hammering wrought iron. Developed in 1828, hot blast greatly increased fuel efficiency in iron production in the following decades.
Invention of machine tools – the first machine tools were invented included the screw-cutting lathe, the cylinder boring machine, and the milling machine. Machine tools made the economical manufacture of precision metal parts possible, although it took several decades to develop effective techniques.[42]
$
5
Question: Which of the following powered the mechanized cotton spinning before steam?
A: Wind
B: Manpower
C: Electricity
D: Water
E: Animals
Answer: D

Question: By how much did the cotton gin increase the productivity of removing seed from cotton?
A: 50 times
B: 500 times
C: 100 times
D: 40 times
E: 25 times
Answer: A

Question: What change made stationary steam engines suitable for industrial uses?
A: Converting them to use wind power
B: Making them portable
C: Adapting them to produce electricity
D: Adapting them to rotary motion
E: Making them run on coal
Answer: D

Question: Which substance replaced charcoal, lowering the fuel cost of iron production?
A: Diesel
B: Gasoline
C: Coke
D: Wood
E: Oil
Answer: C

Question: Which of the following was NOT a machine tool invented during the Industrial Revolution?
A: Power loom
B: Cylinder boring machine
C: Screw-cutting lathe
D: Milling machine
E: Cast iron blowing cylinder
Answer: A
@
Pre-industrial machinery was built by various craftsmen—millwrights built watermills and windmills; carpenters made wooden framing; and smiths and turners made metal parts. Wooden components had the disadvantage of changing dimensions with temperature and humidity, and the various joints tended to rack (work loose) over time. As the Industrial Revolution progressed, machines with metal parts and frames became more common. Other important uses of metal parts were in firearms and threaded fasteners, such as machine screws, bolts, and nuts. There was also the need for precision in making parts. Precision would allow better working machinery, interchangeability of parts, and standardization of threaded fasteners.

The demand for metal parts led to the development of several machine tools. They have their origins in the tools developed in the 18th century by makers of clocks and watches and scientific instrument makers to enable them to batch-produce small mechanisms. Before the advent of machine tools, metal was worked manually using the basic hand tools of hammers, files, scrapers, saws, and chisels. Consequently, the use of metal machine parts was kept to a minimum. Hand methods of production were laborious and costly, and precision was difficult to achieve.[42][25]
$
5
Question: What disadvantage did wooden components have in pre-industrial machinery?
A: They caught fire easily.
B: They changed dimensions with temperature and humidity.
C: They were too heavy.
D: They corroded easily.
E: They were too expensive.
Answer: B

Question: Which industry originally developed the tools that became the foundation for machine tools?
A: Car manufacturing
B: Ship building
C: Clock and watch making
D: Textile manufacturing
E: Construction
Answer: C

Question: Before machine tools, what method was used to work metal?
A: Electrical machinery
B: Hand tools
C: Automated robots
D: Hydraulic systems
E: Steam-powered devices
Answer: B

Question: Machine tools enabled the economical manufacture of which of the following?
A: Wooden planks
B: Ceramic tiles
C: Precision metal parts
D: Rubber components
E: Glass items
Answer: C

Question: The need for precision in making parts would allow for all of the following EXCEPT:
A: Better working machinery
B: Increased use of wooden components
C: Interchangeability of parts
D: Standardization of threaded fasteners
E: More effective machines
Answer: B
@
The Industrial Revolution was the first period in history during which there was a simultaneous increase in both population and per capita income.[140] According to Robert Hughes in The Fatal Shore, the population of England and Wales, which had remained steady at six million from 1700 to 1740, rose dramatically after 1740. The population of England had more than doubled from 8.3 million in 1801 to 16.8 million in 1850 and, by 1901, had nearly doubled again to 30.5 million.[141] Improved conditions led to the population of Britain increasing from 10 million to 30 million in the 19th century.[142][143] Europe's population increased from about 100 million in 1700 to 400 million by 1900.[144]

Urbanization

The Black Country west of Birmingham, England
The growth of the modern industry since the late 18th century led to massive urbanisation and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. In 1800, only 3% of the world's population lived in cities,[145] compared to nearly 50% by the beginning of the 21st century.[146] Manchester had a population of 10,000 in 1717, but by 1911 it had burgeoned to 2.3 million.[147]
$
5
Question: During the Industrial Revolution, what simultaneous increase occurred for the first time in history?
A: Decline in population and decline in per capita income
B: Increase in population and decrease in per capita income
C: Increase in population and increase in per capita income
D: Stability in population and increase in per capita income
E: Decline in population and increase in per capita income
Answer: C

Question: By 1901, what was the population of England?
A: 10 million
B: 16.8 million
C: 30.5 million
D: 2.3 million
E: 8.3 million
Answer: C

Question: In 1800, approximately what percentage of the world's population lived in cities?
A: 3%
B: 10%
C: 25%
D: 50%
E: 75%
Answer: A

Question: Which city saw its population grow from 10,000 in 1717 to 2.3 million in 1911?
A: London
B: Birmingham
C: Manchester
D: Liverpool
E: Newcastle
Answer: C

Question: By the start of the 21st century, approximately what percentage of the world's population lived in cities?
A: 10%
B: 25%
C: 40%
D: 50%
E: 75%
Answer: D
@
The origins of the environmental movement lay in the response to increasing levels of smoke pollution in the atmosphere during the Industrial Revolution. The emergence of great factories and the concomitant immense growth in coal consumption gave rise to an unprecedented level of air pollution in industrial centers; after 1900 the large volume of industrial chemical discharges added to the growing load of untreated human waste.[176] The first large-scale, modern environmental laws came in the form of Britain's Alkali Acts, passed in 1863, to regulate the deleterious air pollution (gaseous hydrochloric acid) given off by the Leblanc process used to produce soda ash. An alkali inspector and four sub-inspectors were appointed to curb this pollution. The responsibilities of the inspectorate were gradually expanded, culminating in the Alkali Order 1958 which placed all major heavy industries that emitted smoke, grit, dust, and fumes under supervision.

The manufactured gas industry began in British cities in 1812–1820. The technique used produced highly toxic effluent that was dumped into sewers and rivers. The gas companies were repeatedly sued in nuisance lawsuits. They usually lost and modified the worst practices. The City of London repeatedly indicted gas companies in the 1820s for polluting the Thames and poisoning its fish. Finally, Parliament wrote company charters to regulate toxicity.[177] The industry reached the U.S. around 1850 causing pollution and lawsuits.[178]
$
5
Question: What gave rise to unprecedented levels of air pollution during the Industrial Revolution?
A: Deforestation
B: Consumption of coal and growth of factories
C: Overpopulation
D: Agricultural pesticides
E: Increased transportation by horses
Answer: B

Question: The Alkali Acts of 1863 were introduced to regulate air pollution caused by which process?
A: Coal mining
B: Burning of wood
C: Production of soda ash using the Leblanc process
D: Manufacture of machine tools
E: Electricity production
Answer: C

Question: What began in British cities around 1812-1820 and produced toxic effluent?
A: Sewage treatment plants
B: Coal power plants
C: Manufactured gas industry
D: Textile mills
E: Steel foundries
Answer: C

Question: In the 1820s, who repeatedly indicted gas companies for polluting the Thames?
A: The City of London
B: The Industrial Oversight Committee
C: Local businesses
D: The European Union
E: The British Royal Family
Answer: A

Question: The origins of the environmental movement can be traced back to the response to increased levels of what during the Industrial Revolution?
A: Water pollution
B: Soil erosion
C: Noise pollution
D: Animal extinction
E: Smoke pollution
Answer: E
@
A red dwarf is the smallest and coolest kind of star on the main sequence. Red dwarfs are by far the most common type of star in the Milky Way, at least in the neighborhood of the Sun. However, as a result of their low luminosity, individual red dwarfs cannot be easily observed. From Earth, not one star that fits the stricter definitions of a red dwarf is visible to the naked eye.[1] Proxima Centauri, the nearest star to the Sun, is a red dwarf, as are fifty of the sixty nearest stars. According to some estimates, red dwarfs make up three-quarters of the stars in the Milky Way.[2]

The coolest red dwarfs near the Sun have a surface temperature of about 2,000 K and the smallest have radii about 9% that of the Sun, with masses about 7.5% that of the Sun. These red dwarfs have spectral types of L0 to L2. There is some overlap with the properties of brown dwarfs, since the most massive brown dwarfs at lower metallicity can be as hot as 3,600 K and have late M spectral types.
$
5
Question: Which star is the nearest to the Sun?
A: Betelgeuse
B: Sirius
C: Vega
D: Proxima Centauri
E: Altair
Answer: D

Question: What is the approximate surface temperature of the coolest red dwarfs near the Sun?
A: 3,600 K
B: 5,000 K
C: 2,000 K
D: 4,000 K
E: 3,000 K
Answer: C

Question: Which spectral types correspond to the coolest red dwarfs?
A: M0 to M2
B: K0 to K2
C: G0 to G2
D: F0 to F2
E: L0 to L2
Answer: E

Question: Red dwarfs are the most common type of star in which galaxy?
A: Andromeda
B: Triangulum
C: Milky Way
D: Messier 83
E: Whirlpool
Answer: C

Question: How do red dwarfs compare to the Sun in terms of size?
A: Similar size
B: About 9% the size
C: Double the size
D: Half the size
E: About 25% the size
Answer: B
@
Definitions and usage of the term "red dwarf" vary on how inclusive they are on the hotter and more massive end. One definition is synonymous with stellar M dwarfs (M-type main sequence stars), yielding a maximum temperature of 3,900 K and 0.6 M☉. One includes all stellar M-type main-sequence and all K-type main-sequence stars (K dwarf), yielding a maximum temperature of 5,200 K and 0.8 M☉. Some definitions include any stellar M dwarf and part of the K dwarf classification. Other definitions are also in use. Many of the coolest, lowest mass M dwarfs are expected to be brown dwarfs, not true stars, and so those would be excluded from any definition of red dwarf.

Stellar models indicate that red dwarfs less than 0.35 M☉ are fully convective.[3] Hence, the helium produced by the thermonuclear fusion of hydrogen is constantly remixed throughout the star, avoiding helium buildup at the core, thereby prolonging the period of fusion. Low-mass red dwarfs therefore develop very slowly, maintaining a constant luminosity and spectral type for trillions of years, until their fuel is depleted. Because of the comparatively short age of the universe, no red dwarfs yet exist at advanced stages of evolution.
$
5
Question: Which stellar models indicate that red dwarfs less than 0.35 M☉ are fully convective?
A: Stellar supernovae models
B: Stellar accretion models
C: Stellar red giant models
D: Stellar main sequence models
E: Stellar pulsar models
Answer: D

Question: What happens to helium produced by the thermonuclear fusion of hydrogen in red dwarfs less than 0.35 M☉?
A: It escapes into space
B: It builds up at the core
C: It is constantly remixed throughout the star
D: It converts to heavier elements immediately
E: It condenses into solid form
Answer: C

Question: Which of the following spectral classifications can sometimes overlap with the properties of red dwarfs?
A: O-type
B: B-type
C: G-type
D: K-type
E: T-type
Answer: D

Question: What is the maximum temperature for stellar M dwarfs?
A: 2,500 K
B: 5,200 K
C: 4,500 K
D: 3,900 K
E: 3,000 K
Answer: D

Question: Red dwarfs below a certain mass maintain a constant luminosity and spectral type for how long?
A: Billions of years
B: Millions of years
C: Trillions of years
D: Hundreds of years
E: Thousands of years
Answer: C
@
Because low-mass red dwarfs are fully convective, helium does not accumulate at the core, and compared to larger stars such as the Sun, they can burn a larger proportion of their hydrogen before leaving the main sequence. As a result, red dwarfs have estimated lifespans far longer than the present age of the universe, and stars less than 0.8 M☉ have not had time to leave the main sequence. The lower the mass of a red dwarf, the longer the lifespan. It is believed that the lifespan of these stars exceeds the expected 10-billion-year lifespan of the Sun by the third or fourth power of the ratio of the solar mass to their masses; thus, a 0.1 M☉ red dwarf may continue burning for 10 trillion years.[15][19] As the proportion of hydrogen in a red dwarf is consumed, the rate of fusion declines and the core starts to contract. The gravitational energy released by this size reduction is converted into heat, which is carried throughout the star by convection.[20]

Properties of typical M-type main-sequence stars[21][22]
Spectral
type[23]	Mass (M☉)	Radius (R☉)	Luminosity (L☉)	Effective
temperature
(K)	Color
index
(B − V)
M0V	0.57	0.588	0.069	3,850	1.42
M1V	0.50	0.501	0.041	3,660	1.49
M2V	0.44	0.446	0.029	3,560	1.51
M3V	0.37	0.361	0.016	3,430	1.53
M4V	0.23	0.274	7.2x10−3	3,210	1.65
M5V	0.162	0.196	3.0x10−3	3,060	1.83
M6V	0.102	0.137	1.0x10−3	2,810	2.01
M7V	0.090	0.120	6.5x10−4	2,680	2.12
M8V	0.085	0.114	5.2x10−4	2,570	2.15
M9V	0.079	0.102	3.0x10−4	2,380	2.17
According to computer simulations, the minimum mass a red dwarf must have to eventually evolve into a red giant is 0.25 M☉; less massive objects, as they age, would increase their surface temperatures and luminosities becoming blue dwarfs and finally white dwarfs.[18]
$
5
Question: Which type of stars can burn a larger proportion of their hydrogen before leaving the main sequence?
A: Blue giants
B: Yellow dwarfs
C: White dwarfs
D: Red dwarfs
E: Neutron stars
Answer: D

Question: What is the expected lifespan of the Sun in billion years?
A: 5 billion
B: 15 billion
C: 10 billion
D: 20 billion
E: 30 billion
Answer: C

Question: What happens to the core of a red dwarf as the proportion of hydrogen is consumed?
A: It expands rapidly
B: It cools down quickly
C: It starts to contract
D: It becomes solid
E: It ejects mass into space
Answer: C

Question: What is the minimum mass a red dwarf must have to evolve into a red giant?
A: 1.0 M☉
B: 0.5 M☉
C: 0.25 M☉
D: 0.75 M☉
E: 0.1 M☉
Answer: C

Question: The color index of an M4V star is closest to which value?
A: 1.83
B: 1.51
C: 1.65
D: 2.12
E: 1.42
Answer: C
@
Many red dwarfs are orbited by exoplanets, but large Jupiter-sized planets are comparatively rare. Doppler surveys of a wide variety of stars indicate about 1 in 6 stars with twice the mass of the Sun are orbited by one or more of Jupiter-sized planets, versus 1 in 16 for Sun-like stars and the frequency of close-in giant planets (Jupiter size or larger) orbiting red dwarfs is only 1 in 40.[34] On the other hand, microlensing surveys indicate that long-orbital-period Neptune-mass planets are found around one in three red dwarfs.[35] Observations with HARPS further indicate 40% of red dwarfs have a "super-Earth" class planet orbiting in the habitable zone where liquid water can exist on the surface.[36] Computer simulations of the formation of planets around low-mass stars predict that Earth-sized planets are most abundant, but more than 90% of the simulated planets are at least 10% water by mass, suggesting that many Earth-sized planets orbiting red dwarf stars are covered in deep oceans.[37]

At least four and possibly up to six exoplanets were discovered orbiting within the Gliese 581 planetary system between 2005 and 2010. One planet has about the mass of Neptune, or 16 Earth masses (M🜨). It orbits just 6 million kilometers (0.04 AU) from its star, and is estimated to have a surface temperature of 150°C, despite the dimness of its star. In 2006, an even smaller exoplanet (only 5.5 M🜨) was found orbiting the red dwarf OGLE-2005-BLG-390L; it lies 390 million km (2.6 AU) from the star and its surface temperature is −220 °C (53 K).
$
5
Question: Which sized planets are comparatively rare around red dwarfs?
A: Earth-sized
B: Neptune-sized
C: Jupiter-sized
D: Mars-sized
E: Saturn-sized
Answer: C

Question: According to observations with HARPS, what percentage of red dwarfs have a "super-Earth" class planet in the habitable zone?
A: 60%
B: 20%
C: 40%
D: 10%
E: 80%
Answer: C

Question: Computer simulations predict that many Earth-sized planets orbiting red dwarf stars are covered in what?
A: Lava
B: Sand
C: Forests
D: Deep oceans
E: Ice
Answer: D

Question: How many exoplanets were discovered orbiting within the Gliese 581 planetary system between 2005 and 2010?
A: Two
B: Ten
C: Four
D: Six
E: Eight
Answer: C

Question: What is the estimated surface temperature of the Neptune-mass planet orbiting just 6 million kilometers from its star in the Gliese 581 system?
A: 0°C
B: 150°C
C: -220°C
D: 50°C
E: 100°C
Answer: B
@
The conventional colour description takes into account only the peak of the stellar spectrum. In actuality, however, stars radiate in all parts of the spectrum. Because all spectral colours combined appear white, the actual apparent colours the human eye would observe are far lighter than the conventional colour descriptions would suggest. This characteristic of 'lightness' indicates that the simplified assignment of colours within the spectrum can be misleading. Excluding colour-contrast effects in dim light, in typical viewing conditions there are no green, cyan, indigo, or violet stars. "Yellow" dwarfs such as the Sun are white, "red" dwarfs are a deep shade of yellow/orange, and "brown" dwarfs do not literally appear brown, but hypothetically would appear dim red or grey/black to a nearby observer.

The modern classification system is known as the Morgan–Keenan (MK) classification. Each star is assigned a spectral class (from the older Harvard spectral classification, which did not include luminosity[1]) and a luminosity class using Roman numerals as explained below, forming the star's spectral type.

Other modern stellar classification systems, such as the UBV system, are based on color indices—the measured differences in three or more color magnitudes.[2] Those numbers are given labels such as "U−V" or "B−V", which represent the colors passed by two standard filters (e.g. Ultraviolet, Blue and Visual).
$
5
Question: What is the implication of stars radiating in all parts of the spectrum?
A: Stars can be of any color.
B: Stars cannot be seen by the human eye.
C: The perceived colors of stars are lighter than often depicted.
D: Stars do not emit any visible light.
E: Stars emit light in only one color.
Answer: C

Question: Which of the following stars does not appear as its conventional color description suggests?
A: Cyan
B: White dwarf
C: Brown dwarf
D: Red dwarf
E: Blue giant
Answer: C

Question: What is the Morgan-Keenan classification system based on?
A: Only the color of stars
B: The peak of the stellar spectrum
C: Spectral class and luminosity class
D: Only the luminosity of stars
E: The distance of stars from Earth
Answer: C

Question: In the UBV system, what do labels like "U−V" or "B−V" represent?
A: Spectral classes of stars
B: Luminosity classes
C: Colors passed by two standard filters
D: Distance between two stars
E: The age of the stars
Answer: C

Question: In typical viewing conditions, which of the following colors of stars are not observed?
A: Red and Orange
B: Brown and White
C: Green and Indigo
D: Yellow and Blue
E: Black and Grey
Answer: C
@
Harvard spectral classification
The Harvard system is a one-dimensional classification scheme by astronomer Annie Jump Cannon, who re-ordered and simplified the prior alphabetical system by Draper (see #History). Stars are grouped according to their spectral characteristics by single letters of the alphabet, optionally with numeric subdivisions. Main-sequence stars vary in surface temperature from approximately 2,000 to 50,000 K, whereas more-evolved stars can have temperatures above 100,000 K[citation needed]. Physically, the classes indicate the temperature of the star's atmosphere and are normally listed from hottest to coldest.

Class	Effective temperature[3][4]	Vega-relative chromaticity[5][6][a]	Chromaticity (D65)[7][8][5][b]	Main-sequence mass[3][9]
(solar masses)	Main-sequence radius[3][9]
(solar radii)	Main-sequence luminosity[3][9]
(bolometric)	Hydrogen
lines	Fraction of all
main-sequence stars[c][10]
O	≥ 30,000 K	blue	blue	≥ 16 M☉	≥ 6.6 R☉	≥ 30,000 L☉	Weak	0.000030%
B	10,000–30,000 K	bluish white	deep bluish white	2.1–16 M☉	1.8–6.6 R☉	25–30,000 L☉	Medium	0.12%
A	7,500–10,000 K	white	bluish white	1.4–2.1 M☉	1.4–1.8 R☉	5–25 L☉	Strong	0.61%
F	6,000–7,500 K	yellowish white	white	1.04–1.4 M☉	1.15–1.4 R☉	1.5–5 L☉	Medium	3.0%
G	5,200–6,000 K	yellow	yellowish white	0.8–1.04 M☉	0.96–1.15 R☉	0.6–1.5 L☉	Weak	7.6%
K	3,700–5,200 K	light orange	pale yellowish orange	0.45–0.8 M☉	0.7–0.96 R☉	0.08–0.6 L☉	Very weak	12%
M	2,400–3,700 K	orangish red	light orangish red	0.08–0.45 M☉	≤ 0.7 R☉	≤ 0.08 L☉	Very weak	76%
$
5
Question: Who created the Harvard system of spectral classification?
A: William Wilson Morgan
B: Edith Kellman
C: Annie Jump Cannon
D: Philip C. Keenan
E: Draper
Answer: C

Question: What does the Harvard system primarily indicate about a star?
A: Age
B: Distance from Earth
C: Luminosity
D: Surface temperature
E: Size
Answer: D

Question: Which spectral class from the Harvard system represents blue stars with a temperature of ≥ 30,000 K?
A: M
B: K
C: O
D: G
E: B
Answer: C

Question: Which class has the largest fraction of all main-sequence stars?
A: A
B: F
C: K
D: M
E: B
Answer: D

Question: Which spectral class from the Harvard system has yellow stars?
A: F
B: A
C: G
D: M
E: O
Answer: C
@
The Yerkes spectral classification, also called the MK, or Morgan-Keenan (alternatively referred to as the MKK, or Morgan-Keenan-Kellman)[18][19] system from the authors' initials, is a system of stellar spectral classification introduced in 1943 by William Wilson Morgan, Philip C. Keenan, and Edith Kellman from Yerkes Observatory.[20] This two-dimensional (temperature and luminosity) classification scheme is based on spectral lines sensitive to stellar temperature and surface gravity, which is related to luminosity (whilst the Harvard classification is based on just surface temperature). Later, in 1953, after some revisions to the list of standard stars and classification criteria, the scheme was named the Morgan–Keenan classification, or MK,[21] which remains in use today.

Denser stars with higher surface gravity exhibit greater pressure broadening of spectral lines. The gravity, and hence the pressure, on the surface of a giant star is much lower than for a dwarf star because the radius of the giant is much greater than a dwarf of similar mass. Therefore, differences in the spectrum can be interpreted as luminosity effects and a luminosity class can be assigned purely from examination of the spectrum.

A number of different luminosity classes are distinguished, as listed in the table below.[22]

Yerkes luminosity classes
Luminosity class	Description	Examples
0 or Ia+	hypergiants or extremely luminous supergiants	Cygnus OB2#12 – B3-4Ia+[23]
Ia	luminous supergiants	Eta Canis Majoris – B5Ia[24]
Iab	intermediate-size luminous supergiants	Gamma Cygni – F8Iab[25]
Ib	less luminous supergiants	Zeta Persei – B1Ib[26]
II	bright giants	Beta Leporis – G0II[27]
III	normal giants	Arcturus – K0III[28]
IV	subgiants	Gamma Cassiopeiae – B0.5IVpe[29]
V	main-sequence stars (dwarfs)	Achernar – B6Vep[26]
sd (prefix) or VI	subdwarfs	HD 149382 – sdB5 or B5VI[30]
D (prefix) or VII	white dwarfs[d]	van Maanen 2 – DZ8[31]
$
5
Question: What is a distinguishing feature of denser stars with higher surface gravity?
A: Stronger hydrogen lines
B: Greater brightness
C: Greater pressure broadening of spectral lines
D: A higher rate of rotation
E: A cooler temperature
Answer: C

Question: In the Yerkes classification, which luminosity class describes hypergiants or extremely luminous supergiants?
A: Ia
B: Ib
C: III
D: 0 or Ia+
E: IV
Answer: D

Question: Which luminosity class in the Yerkes system represents main-sequence stars?
A: Ia
B: II
C: V
D: IV
E: III
Answer: C

Question: Which spectral classification system is two-dimensional, considering both temperature and luminosity?
A: Harvard spectral classification
B: UBV system
C: Morgan-Keenan classification
D: Yerkes spectral classification
E: Draper system
Answer: D

Question: What is the luminosity class for normal giants in the Yerkes system?
A: Ia
B: II
C: IV
D: V
E: III
Answer: E
@
Young brown dwarfs have low surface gravities because they have larger radii and lower masses compared to the field stars of similar spectral type. These sources are marked by a letter beta (β) for intermediate surface gravity and gamma (γ) for low surface gravity. Indication for low surface gravity are weak CaH, KI and NaI lines, as well as strong VO line.[109] Alpha (α) stands for normal surface gravity and is usually dropped. Sometimes an extremely low surface gravity is denoted by a delta (δ).[111] The suffix "pec" stands for peculiar. The peculiar suffix is still used for other features that are unusual and summarizes different properties, indicative of low surface gravity, subdwarfs and unresolved binaries.[112] The prefix sd stands for subdwarf and only includes cool subdwarfs. This prefix indicates a low metallicity and kinematic properties that are more similar to halo stars than to disk stars.[108] Subdwarfs appear bluer than disk objects.[113] The red suffix describes objects with red color, but an older age. This is not interpreted as low surface gravity, but as a high dust content.[110][111] The blue suffix describes objects with blue near-infrared colors that cannot be explained with low metallicity. Some are explained as L+T binaries, others are not binaries, such as 2MASS J11263991−5003550 and are explained with thin and/or large-grained clouds.[111]
$
5
Question: Why do young brown dwarfs have low surface gravities?
A: They have a high density.
B: They are extremely old.
C: They have larger radii and lower masses than similar field stars.
D: They are located closer to Earth.
E: They have strong hydrogen lines.
Answer: C

Question: What does the suffix "pec" denote?
A: Extremely low surface gravity
B: Intermediate surface gravity
C: Normal surface gravity
D: Features that are unusual, including low surface gravity or subdwarf characteristics
E: High dust content
Answer: D

Question: What does the prefix "sd" stand for?
A: Subdwarf
B: Supergiant
C: Subgiant
D: Dwarf
E: Main-sequence star
Answer: A

Question: Which letter represents an intermediate surface gravity in brown dwarfs?
A: α
B: β
C: γ
D: δ
E: ε
Answer: B

Question: What is the primary characteristic of objects labeled with the "blue" suffix?
A: They have high surface gravity.
B: They are binaries.
C: They have low metallicity.
D: They have blue near-infrared colors possibly due to thin/large-grained clouds.
E: They are older but have a high dust content.
Answer: D
@
Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass more than two and a half times that of all the other planets in the Solar System combined, and slightly less than one one-thousandth the mass of the Sun. Jupiter orbits the Sun at a distance of 5.20 AU (778.5 Gm) with an orbital period of 11.86 years. Jupiter is the third brightest natural object in the Earth's night sky after the Moon and Venus, and it has been observed since prehistoric times. It was named after Jupiter, the chief deity of ancient Roman religion.

Jupiter was the first planet to form, and its inward migration during the primordial Solar System impacted much of the formation history of the other planets. Jupiter is primarily composed of hydrogen (90% by volume), followed by helium, which makes up a quarter of its mass and a tenth of its volume. The ongoing contraction of Jupiter's interior generates more heat than the planet receives from the Sun. Its internal structure is believed to comprise an outer mantle of liquid metallic hydrogen, and a diffuse inner core of denser material. Because of its rapid rotation rate of 1 rotation per 10 hours, Jupiter's shape is an oblate spheroid: it has a slight but noticeable bulge around the equator. The outer atmosphere is divided into a series of latitudinal bands, with turbulence and storms along their interacting boundaries. The most obvious result of this is the Great Red Spot, a giant storm which has been observed since 1831 and possibly earlier.

Jupiter is surrounded by a faint planetary ring system and has a powerful magnetosphere, the second largest contiguous structure in the Solar System (after the heliosphere). Jupiter forms a system of 95 known moons and probably many more, including the four large moons discovered by Galileo Galilei in 1610: Io, Europa, Ganymede, and Callisto. Ganymede, the largest of the four, is larger than the planet Mercury. Callisto is the second largest; Io and Europa are approximately the size of Earth's Moon.

Since 1973, Jupiter has been visited by nine robotic probes: seven flybys and two dedicated orbiters, with two more either en route or awaiting launch.
$
5
Question: What is the rank of Jupiter in terms of distance from the Sun?
A: Third
B: Fourth
C: Fifth
D: Second
E: First
Answer: C

Question: How does Jupiter's brightness rank in the Earth's night sky?
A: First
B: Second
C: Third
D: Fourth
E: Fifth
Answer: C

Question: Which of the following is NOT a moon of Jupiter?
A: Io
B: Ganymede
C: Europa
D: Callisto
E: Triton
Answer: E

Question: What is the most prominent storm observed on Jupiter?
A: Great White Spot
B: Eye of Jupiter
C: Great Solar Flare
D: Great Red Spot
E: Jovian Typhoon
Answer: D

Question: How many robotic probes have visited Jupiter since 1973?
A: Four
B: Nine
C: Eleven
D: Seven
E: Five
Answer: B
@
Jupiter is believed to be the oldest planet in the Solar System.[23] Current models of Solar System formation suggest that Jupiter formed at or beyond the snow line: a distance from the early Sun where the temperature was sufficiently cold for volatiles such as water to condense into solids.[24] The planet began as a solid core, which then accumulated its gaseous atmosphere. As a consequence, the planet must have formed before the solar nebula was fully dispersed.[25] During its formation, Jupiter's mass gradually increased until it had 20 times the mass of the Earth, approximately half of which was made up of silicates, ices and other heavy-element constituents.[23] When the proto-Jupiter grew larger than 50 Earth masses it created a gap in the solar nebula.[23] Thereafter, the growing planet reached its final mass in 3–4 million years.[23]

According to the "grand tack hypothesis", Jupiter began to form at a distance of roughly 3.5 AU (520 million km; 330 million mi) from the Sun. As the young planet accreted mass, interaction with the gas disk orbiting the Sun and orbital resonances with Saturn caused it to migrate inward.[24][26] This upset the orbits of several super-Earths orbiting closer to the Sun, causing them to collide destructively.[27] Saturn would later have begun to migrate inwards too, much faster than Jupiter, until the two planets became captured in a 3:2 mean motion resonance at approximately 1.5 AU (220 million km; 140 million mi) from the Sun.[28] This changed the direction of migration, causing them to migrate away from the Sun and out of the inner system to their current locations.[27] All of this happened over a period of 3–6 million years, with the final migration of Jupiter occurring over several hundred thousand years.[26][29] Jupiter's migration from the inner solar system eventually allowed the inner planets—including Earth—to form from the rubble.[30]

There are several unresolved issues with the grand tack hypothesis. The resulting formation timescales of terrestrial planets appear to be inconsistent with the measured elemental composition.[31] It is likely that Jupiter would have settled into an orbit much closer to the Sun if it had migrated through the solar nebula.[32] Some competing models of Solar System formation predict the formation of Jupiter with orbital properties that are close to those of the present day planet.[25] Other models predict Jupiter forming at distances much farther out, such as 18 AU (2.7 billion km; 1.7 billion mi).[33][34]

Based on Jupiter's composition, researchers have made the case for an initial formation outside the molecular nitrogen (N2) snowline, which is estimated at 20–30 AU (3.0–4.5 billion km; 1.9–2.8 billion mi) from the Sun, and possibly even outside the argon snowline, which may be as far as 40 AU (6.0 billion km; 3.7 billion mi).[35][36] Having formed at one of these extreme distances, Jupiter would then have, over a roughly 700,000-year period, migrated inwards to its current location.[33][34] during an epoch approximately 2–3 million years after the planet began to form. In this model, Saturn, Uranus and Neptune would have formed even further out than Jupiter, and Saturn would also have migrated inwards.[33]
$
5
Question: In the grand tack hypothesis, which planet's orbital resonance caused Jupiter to migrate away from the Sun?
A: Mars
B: Venus
C: Saturn
D: Neptune
E: Uranus
Answer: C

Question: How long did the final migration of Jupiter take place, according to the grand tack hypothesis?
A: 700,000 years
B: 2-3 million years
C: Several hundred thousand years
D: 1 million years
E: 500,000 years
Answer: C

Question: In which region might Jupiter have originally formed according to some models, based on its composition?
A: Inside the argon snowline
B: Outside the argon snowline
C: Inside the molecular nitrogen snowline
D: Inside the water snowline
E: Inside the methane snowline
Answer: B

Question: How much time did Jupiter take to migrate to its current position after its formation?
A: Approximately 700,000 years
B: 1-2 million years
C: 2-3 million years
D: 4-5 million years
E: 5-6 million years
Answer: A

Question: What caused the change in the direction of Jupiter's migration in the grand tack hypothesis?
A: A collision with another planet
B: A supernova explosion nearby
C: A 3:2 mean motion resonance with Saturn
D: Interaction with the solar nebula
E: A powerful solar flare
Answer: C
@
By mass, Jupiter's atmosphere is approximately 76% hydrogen and 24% helium, though, because helium atoms are more massive than hydrogen molecules, Jupiter's upper atmosphere is about 90% hydrogen and 10% helium by volume.[41] The atmosphere also contains trace amounts of methane, water vapour, ammonia, and silicon-based compounds as well as fractional amounts of carbon, ethane, hydrogen sulfide, neon, oxygen, phosphine, and sulfur.[42] The outermost layer of the atmosphere contains crystals of frozen ammonia.[43] Through infrared and ultraviolet measurements, trace amounts of benzene and other hydrocarbons have also been found.[44] The interior of Jupiter contains denser materials—by mass it is roughly 71% hydrogen, 24% helium, and 5% other elements.[45][46]

The atmospheric proportions of hydrogen and helium are close to the theoretical composition of the primordial solar nebula.[47] Neon in the upper atmosphere only consists of 20 parts per million by mass, which is about a tenth as abundant as in the Sun.[48] Jupiter's helium abundance is about 80% that of the Sun due to precipitation of these elements as helium-rich droplets, a process that happens deep in the planet's interior.[49][50]

Based on spectroscopy, Saturn is thought to be similar in composition to Jupiter, but the other giant planets Uranus and Neptune have relatively less hydrogen and helium and relatively more of the next most common elements, including oxygen, carbon, nitrogen, and sulfur.[51] Those planets are known as ice giants, because the majority of their volatile compounds are in solid form.[52]
$
5
Question: What is the primary component of Jupiter's atmosphere by mass?
A: Helium
B: Methane
C: Ammonia
D: Hydrogen
E: Neon
Answer: D

Question: In Jupiter's upper atmosphere, helium comprises about what percentage by volume?
A: 5%
B: 15%
C: 10%
D: 20%
E: 50%
Answer: C

Question: Which of the following is NOT a trace element in Jupiter's atmosphere?
A: Methane
B: Water vapour
C: Carbon dioxide
D: Ammonia
E: Neon
Answer: C

Question: The outermost layer of Jupiter's atmosphere contains crystals of what compound?
A: Water ice
B: Methane
C: Carbon dioxide
D: Ammonia
E: Helium
Answer: D

Question: Which planets are known as ice giants?
A: Mercury and Mars
B: Venus and Earth
C: Saturn and Jupiter
D: Uranus and Neptune
E: Io and Europa
Answer: D
@
Jupiter's magnetic field is the strongest of any planet in the Solar System,[99] with a dipole moment of 4.170 gauss (0.4170 mT) that is tilted at an angle of 10.31° to the pole of rotation. The surface magnetic field strength varies from 2 gauss (0.20 mT) up to 20 gauss (2.0 mT).[120] This field is thought to be generated by eddy currents—swirling movements of conducting materials—within the liquid, metallic hydrogen core. At about 75 Jupiter radii from the planet, the interaction of the magnetosphere with the solar wind generates a bow shock. Surrounding Jupiter's magnetosphere is a magnetopause, located at the inner edge of a magnetosheath—a region between it and the bow shock. The solar wind interacts with these regions, elongating the magnetosphere on Jupiter's lee side and extending it outward until it nearly reaches the orbit of Saturn. The four largest moons of Jupiter all orbit within the magnetosphere, which protects them from solar wind.[64]: 69 

The volcanoes on the moon Io emit large amounts of sulfur dioxide, forming a gas torus along its orbit. The gas is ionized in Jupiter's magnetosphere, producing sulfur and oxygen ions. They, together with hydrogen ions originating from the atmosphere of Jupiter, form a plasma sheet in Jupiter's equatorial plane. The plasma in the sheet co-rotates with the planet, causing deformation of the dipole magnetic field into that of a magnetodisk. Electrons within the plasma sheet generate a strong radio signature, with short, superimposed bursts in the range of 0.6–30 MHz that are detectable from Earth with consumer-grade shortwave radio receivers.[121][122] As Io moves through this torus, the interaction generates Alfvén waves that carry ionized matter into the polar regions of Jupiter. As a result, radio waves are generated through a cyclotron maser mechanism, and the energy is transmitted out along a cone-shaped surface. When Earth intersects this cone, the radio emissions from Jupiter can exceed the radio output of the Sun.[123]
$
5
Question: How does Jupiter's magnetic field strength compare to other planets in the Solar System?
A: Weakest
B: Moderate
C: Average
D: Strongest
E: Comparable to Earth
Answer: D

Question: What generates Jupiter's magnetic field?
A: Its solid iron core
B: Eddy currents in the liquid metallic hydrogen core
C: Its powerful solar winds
D: The Great Red Spot storm
E: Its rapid rotation
Answer: B

Question: Which moon of Jupiter emits large amounts of sulfur dioxide, influencing the planet's magnetosphere?
A: Europa
B: Ganymede
C: Callisto
D: Titan
E: Io
Answer: E

Question: At what frequency range can the radio emissions from Jupiter be detected from Earth?
A: 0.6-30 MHz
B: 40-70 MHz
C: 100-120 MHz
D: 1-5 GHz
E: 500-1000 KHz
Answer: A

Question: What happens to the shape of Jupiter's magnetic field when the plasma in the sheet co-rotates with the planet?
A: It remains unchanged
B: It forms a magnetodisk
C: It forms a cylindrical shape
D: It dissipates
E: It contracts into a tighter sphere
Answer: B
@
In astronomy, the barycenter (or barycentre; from Ancient Greek βαρύς (barús) 'heavy', and κέντρον (kéntron) 'center')[1] is the center of mass of two or more bodies that orbit one another and is the point about which the bodies orbit. A barycenter is a dynamical point, not a physical object. It is an important concept in fields such as astronomy and astrophysics. The distance from a body's center of mass to the barycenter can be calculated as a two-body problem.

If one of the two orbiting bodies is much more massive than the other and the bodies are relatively close to one another, the barycenter will typically be located within the more massive object. In this case, rather than the two bodies appearing to orbit a point between them, the less massive body will appear to orbit about the more massive body, while the more massive body might be observed to wobble slightly. This is the case for the Earth–Moon system, whose barycenter is located on average 4,671 km (2,902 mi) from Earth's center, which is 75% of Earth's radius of 6,378 km (3,963 mi). When the two bodies are of similar masses, the barycenter will generally be located between them and both bodies will orbit around it. This is the case for Pluto and Charon, one of Pluto's natural satellites, as well as for many binary asteroids and binary stars. When the less massive object is far away, the barycenter can be located outside the more massive object. This is the case for Jupiter and the Sun; despite the Sun being a thousandfold more massive than Jupiter, their barycenter is slightly outside the Sun due to the relatively large distance between them.[2]

In astronomy, barycentric coordinates are non-rotating coordinates with the origin at the barycenter of two or more bodies. The International Celestial Reference System (ICRS) is a barycentric coordinate system centered on the Solar System's barycenter.
$
5
Question 1: What is the barycenter?
A: A specific star in a galaxy
B: The point of highest gravity between two orbiting bodies
C: The physical center of a planet
D: The center of mass of two or more bodies that orbit one another
E: The furthest point in an elliptical orbit
Answer: D

Question 2: Where is the Earth-Moon system's barycenter located in relation to Earth's center?
A: Exactly at Earth's center
B: 3,000 km from Earth's center
C: 4,671 km from Earth's center
D: Outside Earth's atmosphere
E: At the Moon's center
Answer: C

Question 3: In the case of Pluto and Charon, where is the barycenter located?
A: Inside Pluto
B: Inside Charon
C: At an equal distance between Pluto and Charon
D: Outside both Pluto and Charon
E: At the center of Pluto
Answer: D

Question 4: What is the International Celestial Reference System (ICRS)?
A: A star map used by ancient civilizations
B: A telescope designed to locate barycenters
C: A barycentric coordinate system centered on the Solar System's barycenter
D: The international standard for measuring distances in space
E: A global system for tracking satellites
Answer: C

Question 5: What happens to the more massive body when the barycenter is located within it?
A: It orbits the less massive body
B: It appears stationary
C: It appears to wobble slightly
D: It collapses into the less massive body
E: Its rotation is reversed
Answer: C
@
In classical mechanics, the two-body problem is to predict the motion of two massive objects which are abstractly viewed as point particles. The problem assumes that the two objects interact only with one another; the only force affecting each object arises from the other one, and all other objects are ignored.

The most prominent case of the classical two-body problem is the gravitational case (see also Kepler problem), arising in astronomy for predicting the orbits (or escapes from orbit) of objects such as satellites, planets, and stars. A two-point-particle model of such a system nearly always describes its behavior well enough to provide useful insights and predictions.

A simpler "one body" model, the "central-force problem", treats one object as the immobile source of a force acting on the other. One then seeks to predict the motion of the single remaining mobile object. Such an approximation can give useful results when one object is much more massive than the other (as with a light planet orbiting a heavy star, where the star can be treated as essentially stationary).

However, the one-body approximation is usually unnecessary except as a stepping stone. For many forces, including gravitational ones, the general version of the two-body problem can be reduced to a pair of one-body problems, allowing it to be solved completely, and giving a solution simple enough to be used effectively.

By contrast, the three-body problem (and, more generally, the n-body problem for n ≥ 3) cannot be solved in terms of first integrals, except in special cases.
$
5
Question 6: In the classical two-body problem, what assumption is made regarding external forces?
A: Both objects experience multiple external forces
B: Only one object is affected by an external force
C: No external forces affect the two objects
D: The objects create external forces for each other
E: External forces are the main factor in the problem
Answer: C

Question 7: What does the "central-force problem" primarily focus on?
A: Predicting the motion of two bodies
B: Predicting the motion of a single mobile object
C: Analyzing the effects of external forces on two objects
D: Analyzing how two bodies collide
E: Analyzing how forces change over time
Answer: B

Question 8: How is the three-body problem different from the two-body problem in terms of solvability?
A: It can be easily solved with simple equations
B: It can only be solved in terms of first integrals
C: It cannot be solved in terms of first integrals, except in special cases
D: It is an identical problem with one extra object
E: There is no difference between the two problems
Answer: C

Question 9: What does a two-point-particle model describe effectively in astronomy?
A: The motion of galaxies
B: The orbits of satellites, planets, and stars
C: The interaction between light and matter
D: The effects of dark matter on celestial bodies
E: The trajectory of comets across the sky
Answer: B

Question 10: Which of the following is NOT an approximation model for the two-body problem?
A: One-body problem
B: Three-body problem
C: Central-force problem
D: Kepler problem
E: Elliptical orbit model
Answer: B
@
The barycenter is one of the foci of the elliptical orbit of each body. This is an important concept in the fields of astronomy and astrophysics. If a is the semi-major axis of the system, r1 is the semi-major axis of the primary's orbit around the barycenter, and r2 = a − r1 is the semi-major axis of the secondary's orbit. When the barycenter is located within the more massive body, that body will appear to "wobble" rather than to follow a discernible orbit. In a simple two-body case, the distance from the center of the primary to the barycenter, r1, is given by:

�
1
=
�
⋅
�
2
�
1
+
�
2
=
�
1
+
�
1
�
2
{\displaystyle r_{1}=a\cdot {\frac {m_{2}}{m_{1}+m_{2}}}={\frac {a}{1+{\frac {m_{1}}{m_{2}}}}}}
where :

r1 is the distance from body 1's center to the barycenter
a is the distance between the centers of the two bodies
m1 and m2 are the masses of the two bodies.
Primary–secondary examples
The following table sets out some examples from the Solar System. Figures are given rounded to three significant figures. The terms "primary" and "secondary" are used to distinguish between involved participants, with the larger being the primary and the smaller being the secondary.

m1 is the mass of the primary in Earth masses (M🜨)
m2 is the mass of the secondary in Earth masses (M🜨)
a (km) is the average orbital distance between the centers of the two bodies
r1 (km) is the distance from the center of the primary to the barycenter
R1 (km) is the radius of the primary
r1
/
R1
 a value less than one means the barycenter lies inside the primary
Primary–secondary examples
Primary	m1
(M🜨)	Secondary	m2
(M🜨)	a
(km)	r1
(km)	R1
(km)	
r1
/
R1
Earth	1	Moon	0.0123	384,000	4,670[3]	6,380	0.732[a]
Pluto	0.0021	Charon	0.000254
(0.121 M♇)	  19,600	2,110	1,150	1.83[b]
Sun	333,000	Earth	1	150,000,000
(1 AU)	449	696,000	0.000646[c]
Sun	333,000	Jupiter	318
(0.000955 M☉)	778,000,000
(5.20 AU)	742,000	696,000	1.07[5][d]
Sun	333,000	Saturn	95.2	1,430,000,000
(9.58 AU)	409,000	696,000	0.588
$
5
Question 11: The barycenter acts as one of the foci for what kind of orbit?
A: Circular
B: Hyperbolic
C: Parabolic
D: Elliptical
E: Spiral
Answer: D

Question 12: In a two-body system, if 
�
1
r 
1
​
  represents the distance from the center of the primary body to the barycenter, what does 
�
2
r 
2
​
  represent?
A: The distance between the two bodies
B: The semi-major axis of the primary's orbit around the barycenter
C: The distance between the barycenter and an external point
D: The semi-major axis of the secondary's orbit around the barycenter
E: The radius of the primary body
Answer: D

Question 13: In the provided table, which celestial body pair has a barycenter located outside the primary body?
A: Earth and Moon
B: Pluto and Charon
C: Sun and Earth
D: Sun and Jupiter
E: Sun and Saturn
Answer: D

Question 14: Which body has a radius of 1,150 km as per the given table?
A: Earth
B: Moon
C: Charon
D: Pluto
E: Jupiter
Answer: D

Question 15: For which celestial pair does the barycenter lie inside the primary with a value less than one?
A: Sun and Earth
B: Sun and Jupiter
C: Sun and Saturn
D: Earth and Moon
E: Pluto and Charon
Answer: D
@
The two-body problem is interesting in astronomy because pairs of astronomical objects are often moving rapidly in arbitrary directions (so their motions become interesting), widely separated from one another (so they will not collide) and even more widely separated from other objects (so outside influences will be small enough to be ignored safely).

Under the force of gravity, each member of a pair of such objects will orbit their mutual center of mass in an elliptical pattern, unless they are moving fast enough to escape one another entirely, in which case their paths will diverge along other planar conic sections. If one object is very much heavier than the other, it will move far less than the other with reference to the shared center of mass. The mutual center of mass may even be inside the larger object.

For the derivation of the solutions to the problem, see Classical central-force problem or Kepler problem.

In principle, the same solutions apply to macroscopic problems involving objects interacting not only through gravity, but through any other attractive scalar force field obeying an inverse-square law, with electrostatic attraction being the obvious physical example. In practice, such problems rarely arise. Except perhaps in experimental apparatus or other specialized equipment, we rarely encounter electrostatically interacting objects which are moving fast enough, and in such a direction, as to avoid colliding, and/or which are isolated enough from their surroundings.

The dynamical system of a two-body system under the influence of torque turns out to be a Sturm-Liouville equation.[1]
$
5
Question 16: Why is the two-body problem significant in astronomy?
A: It predicts the collision of celestial bodies
B: It explains the nature of dark matter
C: It analyzes stationary objects in space
D: It predicts the motion of pairs of astronomical objects
E: It describes the behavior of solitary objects
Answer: D

Question 17: If an object in a two-body system is significantly heavier than the other, what can be said about its motion relative to the shared center of mass?
A: It moves in a straight line
B: It moves more than the lighter object
C: It doesn't move at all
D: It moves far less than the other object
E: Its motion is unpredictable
Answer: D

Question 18: In a two-body system, if the objects are moving fast enough to avoid each other, their paths will:
A: Intersect at a single point
B: Follow a circular trajectory
C: Move parallel to each other
D: Diverge along planar conic sections
E: Converge at the center of mass
Answer: D

Question 19: For which type of force do the solutions of the two-body problem apply, apart from gravity?
A: Magnetic
B: Frictional
C: Electrostatic attraction obeying an inverse-square law
D: Centripetal
E: Nuclear
Answer: C

Question 20: The dynamical system of a two-body system under the influence of torque is described by:
A: Newton's equations
B: Kepler's laws
C: Einstein's theory of relativity
D: The Pythagoras theorem
E: The Sturm-Liouville equation
Answer: E
@
A star is an astronomical object comprising a luminous spheroid of plasma held together by self-gravity. The nearest star to Earth is the Sun. Many other stars are visible to the naked eye at night; their immense distances from Earth make them appear as fixed points of light. The most prominent stars have been categorised into constellations and asterisms, and many of the brightest stars have proper names. Astronomers have assembled star catalogues that identify the known stars and provide standardized stellar designations. The observable universe contains an estimated 1022 to 1024 stars. Only about 4,000 of these stars are visible to the naked eye—all within the Milky Way galaxy.[1]

A star's life begins with the gravitational collapse of a gaseous nebula of material largely comprising hydrogen, helium, and trace heavier elements. Its total mass mainly determines its evolution and eventual fate. A star shines for most of its active life due to the thermonuclear fusion of hydrogen into helium in its core. This process releases energy that traverses the star's interior and radiates into outer space. At the end of a star's lifetime, its core becomes a stellar remnant: a white dwarf, a neutron star, or—if it is sufficiently massive—a black hole.
$
5
Question: What is the nearest star to Earth?
A: Sirius
B: Polaris
C: Vega
D: Betelgeuse
E: Sun
Answer: E

Question: Why does a star shine during its active life?
A: Nuclear fission of uranium
B: Reflection of light from planets
C: Thermonuclear fusion of hydrogen into helium
D: Emission of light from its moons
E: Refraction of interstellar light
Answer: C

Question: What might the core of a star become at the end of its lifetime?
A: Nebula
B: Supernova
C: Black hole
D: Galaxy
E: Planet
Answer: C

Question: Approximately how many stars are visible to the naked eye from Earth?
A: 400
B: 4,000
C: 40,000
D: 400,000
E: 4 million
Answer: B

Question: How does a star's life typically begin?
A: Explosion of a planet
B: Gravitational collapse of a gaseous nebula
C: Splitting from another star
D: Fusion of two planets
E: Accumulation of cosmic dust
Answer: B
@
Historically, stars have been important to civilizations throughout the world. They have been part of religious practices, used for celestial navigation and orientation, to mark the passage of seasons, and to define calendars.

Early astronomers recognized a difference between "fixed stars", whose position on the celestial sphere does not change, and "wandering stars" (planets), which move noticeably relative to the fixed stars over days or weeks.[5] Many ancient astronomers believed that the stars were permanently affixed to a heavenly sphere and that they were immutable. By convention, astronomers grouped prominent stars into asterisms and constellations and used them to track the motions of the planets and the inferred position of the Sun.[3] The motion of the Sun against the background stars (and the horizon) was used to create calendars, which could be used to regulate agricultural practices.[6] The Gregorian calendar, currently used nearly everywhere in the world, is a solar calendar based on the angle of the Earth's rotational axis relative to its local star, the Sun.

The oldest accurately dated star chart was the result of ancient Egyptian astronomy in 1534 BC.[7] The earliest known star catalogues were compiled by the ancient Babylonian astronomers of Mesopotamia in the late 2nd millennium BC, during the Kassite Period (c. 1531 BC – c. 1155 BC).[8]

The first star catalogue in Greek astronomy was created by Aristillus in approximately 300 BC, with the help of Timocharis.[9] The star catalog of Hipparchus (2nd century BC) included 1,020 stars, and was used to assemble Ptolemy's star catalogue.[10] Hipparchus is known for the discovery of the first recorded nova (new star).[11] Many of the constellations and star names in use today derive from Greek astronomy.

In spite of the apparent immutability of the heavens, Chinese astronomers were aware that new stars could appear.[12] In 185 AD, they were the first to observe and write about a supernova, now known as SN 185.[13] The brightest stellar event in recorded history was the SN 1006 supernova, which was observed in 1006 and written about by the Egyptian astronomer Ali ibn Ridwan and several Chinese astronomers.[14] The SN 1054 supernova, which gave birth to the Crab Nebula, was also observed by Chinese and Islamic astronomers.[15][16][17]

Medieval Islamic astronomers gave Arabic names to many stars that are still used today and they invented numerous astronomical instruments that could compute the positions of the stars. They built the first large observatory research institutes, mainly for the purpose of producing Zij star catalogues.[18] Among these, the Book of Fixed Stars (964) was written by the Persian astronomer Abd al-Rahman al-Sufi, who observed a number of stars, star clusters (including the Omicron Velorum and Brocchi's Clusters) and galaxies (including the Andromeda Galaxy).[19] According to A. Zahoor, in the 11th century, the Persian polymath scholar Abu Rayhan Biruni described the Milky Way galaxy as a multitude of fragments having the properties of nebulous stars, and gave the latitudes of various stars during a lunar eclipse in 1019.[20]
$
5
Question: What did ancient astronomers distinguish between in the night sky?
A: Galaxies and Nebulae
B: Moons and Asteroids
C: Fixed stars and wandering stars
D: Supernovae and Comets
E: Black holes and Neutron stars
Answer: C

Question: Which calendar is predominantly based on the motion of the Earth relative to the Sun?
A: Lunar calendar
B: Gregorian calendar
C: Mayan calendar
D: Julian calendar
E: Babylonian calendar
Answer: B

Question: Which civilization produced the oldest accurately dated star chart?
A: Greek
B: Mesopotamian
C: Chinese
D: Egyptian
E: Roman
Answer: D

Question: Who is credited for the creation of the first star catalogue in Greek astronomy?
A: Plato
B: Hipparchus
C: Socrates
D: Aristillus
E: Ptolemy
Answer: D

Question: In which year was the brightest stellar event in recorded history, the SN 1006 supernova, observed?
A: 1006
B: 1054
C: 185
D: 1534
E: 300
Answer: A
@
Although stellar parameters can be expressed in SI units or Gaussian units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of nominal solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters:

nominal solar luminosity	L☉ = 3.828×1026 W [54]
nominal solar radius	R☉ = 6.957×108 m [54]
The solar mass M☉ was not explicitly defined by the IAU due to the large relative uncertainty (10−4) of the Newtonian constant of gravitation G. Since the product of the Newtonian constant of gravitation and solar mass together (GM☉) has been determined to much greater precision, the IAU defined the nominal solar mass parameter to be:

nominal solar mass parameter:	GM☉ = 1.3271244×1020 m3/s2 [54]
The nominal solar mass parameter can be combined with the most recent (2014) CODATA estimate of the Newtonian constant of gravitation G to derive the solar mass to be approximately 1.9885×1030 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters.

Large lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m.[54]
$
5
Question: How is the solar luminosity, as of the 2015 IAU definition, expressed?
A: 1.9885×1030 W
B: 6.957×108 W
C: 149,597,870,700 W
D: 1.3271244×1020 W
E: 3.828×1026 W
Answer: E

Question: Which unit is commonly used to represent large lengths such as the radius of a giant star?
A: Kilometer
B: Light-year
C: Parsec
D: Astronomical unit
E: Mile
Answer: D

Question: How is the 2012 IAU defined astronomical constant expressed in meters?
A: 3.828×1026 m
B: 6.957×108 m
C: 149,597,870,700 m
D: 1.3271244×1020 m
E: 1.9885×1030 m
Answer: C

Question: How can the solar mass be derived?
A: By combining the IAU nominal solar radius with the CODATA estimate of G
B: By directly using the IAU defined value
C: By combining the nominal solar mass parameter with the CODATA estimate of G
D: By dividing the IAU solar luminosity by the solar radius
E: By combining the solar luminosity with the astronomical constant
Answer: C

Question: What is the IAU defined value for the nominal solar radius?
A: 1.9885×1030 m
B: 149,597,870,700 m
C: 1.3271244×1020 m
D: 3.828×1026 m
E: 6.957×108 m
Answer: E
@
Stars are not spread uniformly across the universe but are normally grouped into galaxies along with interstellar gas and dust. A typical large galaxy like the Milky Way contains hundreds of billions of stars. There are more than 2 trillion (1012) galaxies, though most are less than 10% the mass of the Milky Way.[104] Overall, there are likely to be between 1022 and 1024 stars[105][106] (more stars than all the grains of sand on planet Earth).[107][108][109] Most stars are within galaxies, but between 10 and 50% of the starlight in large galaxy clusters may come from stars outside of any galaxy.[110][111][112]

A multi-star system consists of two or more gravitationally bound stars that orbit each other. The simplest and most common multi-star system is a binary star, but systems of three or more stars exist. For reasons of orbital stability, such multi-star systems are often organized into hierarchical sets of binary stars.[113] Larger groups are called star clusters. These range from loose stellar associations with only a few stars to open clusters with dozens to thousands of stars, up to enormous globular clusters with hundreds of thousands of stars. Such systems orbit their host galaxy. The stars in an open or globular cluster all formed from the same giant molecular cloud, so all members normally have similar ages and compositions.[88]

Many stars are observed, and most or all may have originally formed in gravitationally bound, multiple-star systems. This is particularly true for very massive O and B class stars, 80% of which are believed to be part of multiple-star systems. The proportion of single star systems increases with decreasing star mass, so that only 25% of red dwarfs are known to have stellar companions. As 85% of all stars are red dwarfs, more than two thirds of stars in the Milky Way are likely single red dwarfs.[114] In a 2017 study of the Perseus molecular cloud, astronomers found that most of the newly formed stars are in binary systems. In the model that best explained the data, all stars initially formed as binaries, though some binaries later split up and leave single stars behind.[115][116]
$
5
Question: In which structures are stars commonly found?
A: Atmospheres
B: Black holes
C: Galaxies
D: Planets
E: Nebulae
Answer: C

Question: What does a binary star system consist of?
A: One star and a planet
B: Two gravitationally bound planets
C: A star and a black hole
D: Two gravitationally bound stars
E: A star and a moon
Answer: D

Question: How many stars are likely to exist, in total?
A: 1022 to 1024
B: 1000 to 1500
C: 1 million to 10 million
D: 2 trillion to 4 trillion
E: 1022 to 1030
Answer: A

Question: The majority of which type of stars are likely to be single?
A: O and B class stars
B: White dwarfs
C: Red dwarfs
D: Neutron stars
E: Supernovas
Answer: C

Question: What do most stars in a star cluster, such as an open or globular cluster, share?
A: A black hole at the center
B: The same rotation speed
C: Similar ages and compositions
D: Same distance from Earth
E: Same temperature and color
Answer: C
@
A stellar magnetic field is a magnetic field generated by the motion of conductive plasma inside a star. This motion is created through convection, which is a form of energy transport involving the physical movement of material. A localized magnetic field exerts a force on the plasma, effectively increasing the pressure without a comparable gain in density. As a result, the magnetized region rises relative to the remainder of the plasma, until it reaches the star's photosphere. This creates starspots on the surface, and the related phenomenon of coronal loops.[1]

The magnetic field of a star can be measured by means of the Zeeman effect. Normally the atoms in a star's atmosphere will absorb certain frequencies of energy in the electromagnetic spectrum, producing characteristic dark absorption lines in the spectrum. When the atoms are within a magnetic field, however, these lines become split into multiple, closely spaced lines. The energy also becomes polarized with an orientation that depends on orientation of the magnetic field. Thus the strength and direction of the star's magnetic field can be determined by examination of the Zeeman effect lines.[2][3]

A stellar spectropolarimeter is used to measure the magnetic field of a star. This instrument consists of a spectrograph combined with a polarimeter. The first instrument to be dedicated to the study of stellar magnetic fields was NARVAL, which was mounted on the Bernard Lyot Telescope at the Pic du Midi de Bigorre in the French Pyrenees mountains.[4]

Various measurements—including magnetometer measurements over the last 150 years;[5] 14C in tree rings; and 10Be in ice cores[6]—have established substantial magnetic variability of the Sun on decadal, centennial and millennial time scales.[7]
$
5
Question: How is a stellar magnetic field generated?
A: By the gravitational pull of planets.
B: By the star's rotation.
C: By the motion of conductive plasma inside a star.
D: By its interaction with other stars.
E: By absorbing cosmic radiation.
Answer: C

Question: Which effect is used to measure the magnetic field of a star by observing the splitting of absorption lines?
A: Doppler effect.
B: Zeeman effect.
C: Stark effect.
D: Photoelectric effect.
E: Hall effect.
Answer: B

Question: Which instrument is used to measure the magnetic field of a star?
A: Spectrometer.
B: Barometer.
C: Thermometer.
D: Stellar spectropolarimeter.
E: Gravimeter.
Answer: D

Question: Where was the NARVAL mounted?
A: Hale Telescope.
B: Keck Observatory.
C: Mauna Kea Observatories.
D: Bernard Lyot Telescope at the Pic du Midi de Bigorre.
E: VLT at Paranal.
Answer: D

Question: What establishes the magnetic variability of the Sun on different time scales?
A: Observations of the Moon's magnetic field.
B: Magnetometer measurements and 14C in tree rings.
C: Tidal patterns and solar flares.
D: Cosmic ray measurements.
E: Neutrino flux from the solar core.
Answer: B
@
Starspots are regions of intense magnetic activity on the surface of a star. (On the Sun they are termed sunspots.) These form a visible component of magnetic flux tubes that are formed within a star's convection zone. Due to the differential rotation of the star, the tube becomes curled up and stretched, inhibiting convection and producing zones of lower than normal temperature.[9] Coronal loops often form above starspots, forming from magnetic field lines that stretch out into the stellar corona. These in turn serve to heat the corona to temperatures over a million kelvins.[10]

The magnetic fields linked to starspots and coronal loops are linked to flare activity, and the associated coronal mass ejection. The plasma is heated to tens of millions of kelvins, and the particles are accelerated away from the star's surface at extreme velocities.[11]

Surface activity appears to be related to the age and rotation rate of main-sequence stars. Young stars with a rapid rate of rotation exhibit strong activity. By contrast middle-aged, Sun-like stars with a slow rate of rotation show low levels of activity that varies in cycles. Some older stars display almost no activity, which may mean they have entered a lull that is comparable to the Sun's Maunder minimum. Measurements of the time variation in stellar activity can be useful for determining the differential rotation rates of a star.[12]
$
5
Question: What is the term for regions of intense magnetic activity on the Sun's surface?
A: Moonspots.
B: Nebulaspots.
C: Earthspots.
D: Starspots.
E: Sunspots.
Answer: E

Question: What forms above starspots?
A: Meteor showers.
B: Black holes.
C: Coronal loops.
D: Nebulae.
E: Star clusters.
Answer: C

Question: Which factor appears to be related to the surface activity of main-sequence stars?
A: Distance from the nearest planet.
B: Luminosity.
C: Amount of iron content.
D: Age and rotation rate.
E: Mass and density.
Answer: D

Question: Coronal loops often lead to which phenomena?
A: Planetary alignment.
B: Formation of nebulas.
C: Stellar mergers.
D: Flare activity and coronal mass ejections.
E: Supernovas.
Answer: D

Question: Which stars display almost no activity and might be in a lull similar to the Sun's Maunder minimum?
A: Neutron stars.
B: Older stars.
C: White dwarfs.
D: Supernovas.
E: Red giants.
Answer: B
@
A T Tauri star is a type of pre-main-sequence star that is being heated through gravitational contraction and has not yet begun to burn hydrogen at its core. They are variable stars that are magnetically active. The magnetic field of these stars is thought to interact with its strong stellar wind, transferring angular momentum to the surrounding protoplanetary disk. This allows the star to brake its rotation rate as it collapses.[15]

Small, M-class stars (with 0.1–0.6 solar masses) that exhibit rapid, irregular variability are known as flare stars. These fluctuations are hypothesized to be caused by flares, although the activity is much stronger relative to the size of the star. The flares on this class of stars can extend up to 20% of the circumference, and radiate much of their energy in the blue and ultraviolet portion of the spectrum.[16]

Straddling the boundary between stars that undergo nuclear fusion in their cores and non-hydrogen fusing brown dwarfs are the ultracool dwarfs. These objects can emit radio waves due to their strong magnetic fields. Approximately 5–10% of these objects have had their magnetic fields measured.[17] The coolest of these, 2MASS J10475385+2124234 with a temperature of 800-900 K, retains a magnetic field stronger than 1.7 kG, making it some 3000 times stronger than the Earth's magnetic field.[18] Radio observations also suggest that their magnetic fields periodically change their orientation, similar to the Sun during the solar cycle.[19]
$
5
Question: What is a T Tauri star?
A: A supernova remnant.
B: A star in the process of forming its first planetary system.
C: A pre-main-sequence star being heated through gravitational contraction.
D: A mature star that has burned out its core.
E: A star that has undergone a supernova explosion.
Answer: C

Question: Which class of stars exhibit rapid, irregular variability?
A: G-class stars.
B: Neutron stars.
C: Flare stars.
D: T Tauri stars.
E: Pulsars.
Answer: C

Question: Which type of astronomical objects are known to emit radio waves due to their strong magnetic fields?
A: Main-sequence stars.
B: Ultracool dwarfs.
C: White giants.
D: Novas.
E: Red dwarfs.
Answer: B

Question: How strong is the magnetic field of the coolest ultracool dwarf, 2MASS J10475385+2124234?
A: 50 times stronger than the Earth's magnetic field.
B: Equal to the Earth's magnetic field.
C: 100 times weaker than the Earth's magnetic field.
D: 500 times stronger than the Earth's magnetic field.
E: 3000 times stronger than the Earth's magnetic field.
Answer: E

Question: What do radio observations suggest about ultracool dwarfs?
A: They have weak gravitational pulls.
B: Their magnetic fields periodically change orientation.
C: They are mostly found in binary systems.
D: They have very high surface temperatures.
E: They emit gamma-ray bursts.
Answer: B
@
After some massive stars have ceased thermonuclear fusion, a portion of their mass collapses into a compact body of neutrons called a neutron star. These bodies retain a significant magnetic field from the original star, but the collapse in size causes the strength of this field to increase dramatically. The rapid rotation of these collapsed neutron stars results in a pulsar, which emits a narrow beam of energy that can periodically point toward an observer.

Compact and fast-rotating astronomical objects (white dwarfs, neutron stars and black holes) have extremely strong magnetic fields. The magnetic field of a newly born fast-spinning neutron star is so strong (up to 108 teslas) that it electromagnetically radiates enough energy to quickly (in a matter of few million years) damp down the star rotation by 100 to 1000 times. Matter falling on a neutron star also has to follow the magnetic field lines, resulting in two hot spots on the surface where it can reach and collide with the star's surface. These spots are literally a few feet (about a metre) across but tremendously bright. Their periodic eclipsing during star rotation is hypothesized to be the source of pulsating radiation (see pulsars).

An extreme form of a magnetized neutron star is the magnetar. These are formed as the result of a core-collapse supernova.[21] The existence of such stars was confirmed in 1998 with the measurement of the star SGR 1806-20. The magnetic field of this star has increased the surface temperature to 18 million K and it releases enormous amounts of energy in gamma ray bursts.[22]

Jets of relativistic plasma are often observed along the direction of the magnetic poles of active black holes in the centers of very young galaxies.
$
5
Question: What is formed from the collapsed mass of some massive stars after they've stopped thermonuclear fusion?
A: White dwarf.
B: Black hole.
C: Neutron star.
D: Supernova.
E: Main-sequence star.
Answer: C

Question: How strong can the magnetic field of a newly born fast-spinning neutron star be?
A: Up to 10 teslas.
B: Up to 104 teslas.
C: Up to 108 teslas.
D: Up to 102 teslas.
E: Up to 106 teslas.
Answer: C

Question: What is the source of pulsating radiation from a neutron star?
A: The star's strong gravitational pull.
B: Interaction with other nearby stars.
C: Two hot spots on the surface of the star.
D: The star's rapid expansion.
E: Gamma-ray bursts from the core.
Answer: C

Question: What is an extreme form of a magnetized neutron star called?
A: Neutromag.
B: Pulsar.
C: Magnetar.
D: Stellarmag.
E: Neutronflare.
Answer: C

Question: What was confirmed in 1998 about magnetars?
A: They can be smaller than previously believed.
B: They have no impact on nearby planets.
C: They are formed through neutron star collisions.
D: The existence of magnetars with the measurement of the star SGR 1806-20.
E: Their magnetic fields can change directions.
Answer: D
@
A planetary nebula is a type of emission nebula consisting of an expanding, glowing shell of ionized gas ejected from red giant stars late in their lives.[4]

The term "planetary nebula" is a misnomer because they are unrelated to planets. The term originates from the planet-like round shape of these nebulae observed by astronomers through early telescopes. The first usage may have occurred during the 1780s with the English astronomer William Herschel who described these nebulae as resembling planets; however, as early as January 1779, the French astronomer Antoine Darquier de Pellepoix described in his observations of the Ring Nebula, "very dim but perfectly outlined; it is as large as Jupiter and resembles a fading planet".[5][6][7] Though the modern interpretation is different, the old term is still used.

All planetary nebulae form at the end of the life of a star of intermediate mass, about 1-8 solar masses. It is expected that the Sun will form a planetary nebula at the end of its life cycle.[8] They are relatively short-lived phenomena, lasting perhaps a few tens of millennia, compared to considerably longer phases of stellar evolution.[9] Once all of the red giant's atmosphere has been dissipated, energetic ultraviolet radiation from the exposed hot luminous core, called a planetary nebula nucleus (P.N.N.), ionizes the ejected material.[4] Absorbed ultraviolet light then energizes the shell of nebulous gas around the central star, causing it to appear as a brightly coloured planetary nebula.

Planetary nebulae probably play a crucial role in the chemical evolution of the Milky Way by expelling elements into the interstellar medium from stars where those elements were created. Planetary nebulae are observed in more distant galaxies, yielding useful information about their chemical abundances.
$
5
Question 1: Why is the term "planetary nebula" considered a misnomer?
A: They are made of planets.
B: They orbit planets.
C: They are related to planets.
D: They were believed to be newly forming planets.
E: Their round appearance resembled planets through early telescopes.
Answer: E

Question 2: At what stage in a star's life does a planetary nebula form?
A: During its birth
B: In its early life
C: At its peak luminosity
D: Late in its life
E: It can form at any stage of a star's life
Answer: D

Question 3: For what duration do planetary nebulae typically last?
A: Billions of years
B: Millions of years
C: Tens of millennia
D: Thousands of years
E: Centuries
Answer: C

Question 4: What role do planetary nebulae likely play in the Milky Way's chemical evolution?
A: They absorb elements from the interstellar medium.
B: They convert hydrogen into helium.
C: They expel elements into the interstellar medium.
D: They help in the formation of new stars.
E: They prevent the formation of other nebulae.
Answer: C

Question 5: Which star is expected to form a planetary nebula at the end of its life cycle?
A: Proxima Centauri
B: Polaris
C: Sirius
D: Vega
E: The Sun
Answer: E
@
Stars greater than 8 solar masses (M⊙) will probably end their lives in dramatic supernovae explosions, while planetary nebulae seemingly only occur at the end of the lives of intermediate and low mass stars between 0.8 M⊙ to 8.0 M⊙.[27] Progenitor stars that form planetary nebulae will spend most of their lifetimes converting their hydrogen into helium in the star's core by nuclear fusion at about 15 million K. This generated energy creates outward pressure from fusion reactions in the core, balancing the crushing inward pressures of the star's gravity.[28] This state of equilibrium is known as the main sequence, which can last for tens of millions to billions of years, depending on the mass.

When the hydrogen source in the core starts to diminish, gravity starts compressing the core, causing a rise in temperature to about 100 million K.[29] Such higher core temperatures then make the star's cooler outer layers expand to create much larger red giant stars. This end phase causes a dramatic rise in stellar luminosity, where the released energy is distributed over a much larger surface area, which in fact causes the average surface temperature to be lower. In stellar evolution terms, stars undergoing such increases in luminosity are known as asymptotic giant branch stars (AGB).[29] During this phase, the star can lose 50–70% of its total mass from its stellar wind.[30]

For the more massive asymptotic giant branch stars that form planetary nebulae, whose progenitors exceed about 3M⊙, their cores will continue to contract. When temperatures reach about 100 million K, the available helium nuclei fuse into carbon and oxygen, so that the star again resumes radiating energy, temporarily stopping the core's contraction. This new helium burning phase (fusion of helium nuclei) forms a growing inner core of inert carbon and oxygen. Above it is a thin helium-burning shell, surrounded in turn by a hydrogen-burning shell. However, this new phase lasts only 20,000 years or so, a very short period compared to the entire lifetime of the star.

The venting of atmosphere continues unabated into interstellar space, but when the outer surface of the exposed core reaches temperatures exceeding about 30,000 K, there are enough emitted ultraviolet photons to ionize the ejected atmosphere, causing the gas to shine as a planetary nebula.[29]
$
5
Question 6: Stars with a mass greater than how many solar masses will likely end in supernovae explosions?
A: 3 M⊙
B: 5 M⊙
C: 8 M⊙
D: 10 M⊙
E: 15 M⊙
Answer: C

Question 7: What is the phase in stellar evolution where stars undergo a dramatic increase in luminosity?
A: Red Dwarf Stage
B: Neutron Star Stage
C: White Dwarf Stage
D: Main Sequence
E: Asymptotic Giant Branch
Answer: E

Question 8: What happens when the hydrogen source in a star's core diminishes?
A: The core temperature decreases.
B: The core stops compressing.
C: The star's outer layers expand.
D: The star immediately explodes.
E: The star turns into a white dwarf.
Answer: C

Question 9: What is formed when helium nuclei fuse in the more massive AGB stars?
A: Hydrogen and Nitrogen
B: Neon and Sodium
C: Carbon and Oxygen
D: Iron and Nickel
E: Silicon and Magnesium
Answer: C

Question 10: What causes the gas in a planetary nebula to shine?
A: The emitted gamma rays from the star
B: The nebula's own heat production
C: The visible light emitted by other stars
D: Ultraviolet photons emitted by the exposed core of the star
E: Reflection of light from nearby galaxies
Answer: D
@
A typical planetary nebula is roughly one light year across, and consists of extremely rarefied gas, with a density generally from 100 to 10,000 particles per cm3.[40] (The Earth's atmosphere, by comparison, contains 2.5×1019 particles per cm3.) Young planetary nebulae have the highest densities, sometimes as high as 106 particles per cm3. As nebulae age, their expansion causes their density to decrease. The masses of planetary nebulae range from 0.1 to 1 solar masses.[40]

Radiation from the central star heats the gases to temperatures of about 10,000 K.[41] The gas temperature in central regions is usually much higher than at the periphery reaching 16,000–25,000 K.[42] The volume in the vicinity of the central star is often filled with a very hot (coronal) gas having the temperature of about 1,000,000 K. This gas originates from the surface of the central star in the form of the fast stellar wind.[43]

Nebulae may be described as matter bounded or radiation bounded. In the former case, there is not enough matter in the nebula to absorb all the UV photons emitted by the star, and the visible nebula is fully ionized. In the latter case, there are not enough UV photons being emitted by the central star to ionize all the surrounding gas, and an ionization front propagates outward into the circumstellar envelope of neutral atoms.[44]
$
5
Question 11: How does the density of a young planetary nebula compare to Earth's atmosphere?
A: It's denser
B: It's about the same
C: It's less dense but comparable
D: It's far less dense
E: They have equal densities
Answer: D

Question 12: As nebulae age, what happens to their density?
A: It remains constant.
B: It increases slightly.
C: It decreases.
D: It increases significantly.
E: It fluctuates without a pattern.
Answer: C

Question 13: How does the temperature of the gas in the central regions of a nebula compare to its periphery?
A: They are about the same.
B: The central region is cooler.
C: The periphery is significantly cooler.
D: The central region is slightly warmer.
E: The periphery has a temperature close to absolute zero.
Answer: B

Question 14: Where does the very hot gas in the vicinity of the central star of a planetary nebula originate from?
A: From nearby stars
B: From the interstellar medium
C: From the surface of the central star in the form of the fast stellar wind
D: From cosmic radiation
E: From merging with other nebulae
Answer: C

Question 15: What determines if a nebula is matter bounded or radiation bounded?
A: Its age and life cycle
B: The presence or absence of other stars nearby
C: Whether there are enough UV photons emitted by the star to ionize all the surrounding gas
D: The distance from its central star
E: Its interaction with other galaxies
Answer: C
@
Planetary nebulae may play a very important role in galactic evolution. Newly born stars consist almost entirely of hydrogen and helium,[33] but as stars evolve through the asymptotic giant branch phase,[34] they create heavier elements via nuclear fusion which are eventually expelled by strong stellar winds.[35] Planetary nebulae usually contain larger proportions of elements such as carbon, nitrogen and oxygen, and these are recycled into the interstellar medium via these powerful winds. In this way, planetary nebulae greatly enrich the Milky Way and their nebulae with these heavier elements – collectively known by astronomers as metals and specifically referred to by the metallicity parameter Z.[36]

Subsequent generations of stars formed from such nebulae also tend to have higher metallicities. Although these metals are present in stars in relatively tiny amounts, they have marked effects on stellar evolution and fusion reactions. When stars formed earlier in the universe they theoretically contained smaller quantities of heavier elements.[37] Known examples are the metal poor Population II stars. (See Stellar population.)[38][39] Identification of stellar metallicity content is found by spectroscopy.
$
5
Question 16: As compared to newly born stars, what is unique about the composition of planetary nebulae?
A: They have lesser hydrogen.
B: They contain more heavier elements like carbon, nitrogen, and oxygen.
C: They are primarily made of metals.
D: They have no helium.
E: They are mainly composed of inert gases.
Answer: B

Question 17: How do planetary nebulae influence the metallicity of the Milky Way?
A: They decrease it.
B: They stabilize it.
C: They have no impact on it.
D: They increase it.
E: They intermittently influence it.
Answer: D

Question 18: What is the metallicity parameter Z referred to in astronomy?
A: The amount of zinc in a star
B: The amount of metals in the core of a star
C: The collective measure of heavier elements in a star
D: The ratio of hydrogen to helium in a star
E: The age of a star
Answer: C

Question 19: How do the metallicities of stars formed earlier in the universe compare to those formed from planetary nebulae?
A: They have higher metallicities.
B: They have equal metallicities.
C: They have varied metallicities with no consistent pattern.
D: They have lower metallicities.
E: They have metallicities only in the core.
Answer: D

Question 20: How is a star's metallicity content identified?
A: By measuring its mass
B: Through spectroscopy
C: By its luminosity
D: By observing its size
E: By its position in the galaxy
Answer: B
@
The Milky Way[c] is the galaxy that includes the Solar System, with the name describing the galaxy's appearance from Earth: a hazy band of light seen in the night sky formed from stars that cannot be individually distinguished by the naked eye. The term Milky Way is a translation of the Latin via lactea, from the Greek γαλακτικὸς κύκλος (galaktikòs kýklos), meaning "milky circle".[26][27] From Earth, the Milky Way appears as a band because its disk-shaped structure is viewed from within. Galileo Galilei first resolved the band of light into individual stars with his telescope in 1610. Until the early 1920s, most astronomers thought that the Milky Way contained all the stars in the Universe.[28] Following the 1920 Great Debate between the astronomers Harlow Shapley and Heber Doust Curtis,[29] observations by Edwin Hubble showed that the Milky Way is just one of many galaxies.

The Milky Way is a barred spiral galaxy with a D25 isophotal diameter estimated at 26.8 ± 1.1 kiloparsecs (87,400 ± 3,600 light-years),[10] but only about 1,000 light-years thick at the spiral arms (more at the bulge). Recent simulations suggest that a dark matter area, also containing some visible stars, may extend up to a diameter of almost 2 million light-years (613 kpc).[30][31] The Milky Way has several satellite galaxies and is part of the Local Group of galaxies, which form part of the Virgo Supercluster, which is itself a component of the Laniakea Supercluster.[32][33]
$
5
Question: What term describes the appearance of the Milky Way from Earth?
A: Cosmic Web
B: Galactic Halo
C: Luminous Arc
D: Milky Circle
E: Hazy Band
Answer: E

Question: Who first resolved the Milky Way's band of light into individual stars?
A: Edwin Hubble
B: Heber Doust Curtis
C: Harlow Shapley
D: Galileo Galilei
E: Sir Isaac Newton
Answer: D

Question: Until the early 1920s, most astronomers believed the Milky Way contained what?
A: The only black hole
B: Half of the stars in the Universe
C: All the stars in the Universe
D: Only the stars visible from Earth
E: Just our Solar System
Answer: C

Question: What type of galaxy is the Milky Way classified as?
A: Elliptical
B: Irregular
C: Peculiar
D: Ring
E: Barred spiral
Answer: E

Question: The Milky Way is a part of which supercluster?
A: Perseus-Pisces
B: Coma
C: Hydra-Centaurus
D: Virgo
E: Laniakea
Answer: E
@
It is estimated to contain 100–400 billion stars[34][35] and at least that number of planets.[36][37] The Solar System is located at a radius of about 27,000 light-years (8.3 kpc) from the Galactic Center,[38] on the inner edge of the Orion Arm, one of the spiral-shaped concentrations of gas and dust. The stars in the innermost 10,000 light-years form a bulge and one or more bars that radiate from the bulge. The Galactic Center is an intense radio source known as Sagittarius A*, a supermassive black hole of 4.100 (± 0.034) million solar masses.[39][40] Stars and gases at a wide range of distances from the Galactic Center orbit at approximately 220 kilometers per second (136 miles per second). The constant rotational speed appears to contradict the laws of Keplerian dynamics and suggests that much (about 90%)[7][8] of the mass of the Milky Way is invisible to telescopes, neither emitting nor absorbing electromagnetic radiation. This conjectural mass has been termed "dark matter".[41] The rotational period is about 212 million years at the radius of the Sun.[16]

The Milky Way as a whole is moving at a velocity of approximately 600 km per second (372 miles per second) with respect to extragalactic frames of reference. The oldest stars in the Milky Way are nearly as old as the Universe itself and thus probably formed shortly after the Dark Ages of the Big Bang.[42]
$
5
Question: Approximately how many stars does the Milky Way contain?
A: 10-20 billion
B: 50-100 billion
C: 100-400 billion
D: 500-900 billion
E: 1-2 trillion
Answer: C

Question: What intense radio source is found at the Galactic Center?
A: Orion Nebula
B: Taurus A*
C: Sagittarius A*
D: Cygnus X-1
E: Pleiades Cluster
Answer: C

Question: The rotational speed of stars and gases near the Galactic Center seems to contradict the laws of which dynamics?
A: Newtonian
B: Einsteinian
C: Keplerian
D: Galilean
E: Lorentzian
Answer: C

Question: What has been termed to describe the conjectural mass that is invisible to telescopes in the Milky Way?
A: Invisible Matter
B: Ghost Matter
C: Anti-Matter
D: Dark Matter
E: Void Matter
Answer: D

Question: How fast is the Milky Way moving with respect to extragalactic frames of reference?
A: 200 km/s
B: 400 km/s
C: 600 km/s
D: 800 km/s
E: 1000 km/s
Answer: C
@
The Milky Way is visible as a hazy band of white light, some 30° wide, arching the night sky.[57] Although all the individual naked-eye stars in the entire sky are part of the Milky Way Galaxy, the term "Milky Way" is limited to this band of light.[58][59] The light originates from the accumulation of unresolved stars and other material located in the direction of the galactic plane. Brighter regions around the band appear as soft visual patches known as star clouds. The most conspicuous of these is the Large Sagittarius Star Cloud, a portion of the central bulge of the galaxy.[60] Dark regions within the band, such as the Great Rift and the Coalsack, are areas where interstellar dust blocks light from distant stars. Peoples of the southern hemisphere, including the Inca and Australian aborigines, identified these regions as dark cloud constellations.[61] The area of sky that the Milky Way obscures is called the Zone of Avoidance.[62]

The Milky Way has a relatively low surface brightness. Its visibility can be greatly reduced by background light, such as light pollution or moonlight. The sky needs to be darker than about 20.2 magnitude per square arcsecond in order for the Milky Way to be visible.[63] It should be visible if the limiting magnitude is approximately +5.1 or better and shows a great deal of detail at +6.1.[64] This makes the Milky Way difficult to see from brightly lit urban or suburban areas, but very prominent when viewed from rural areas when the Moon is below the horizon.[d] Maps of artificial night sky brightness show that more than one-third of Earth's population cannot see the Milky Way from their homes due to light pollution.[65]

As viewed from Earth, the visible region of the Milky Way's galactic plane occupies an area of the sky that includes 30 constellations.[e] The Galactic Center lies in the direction of Sagittarius, where the Milky Way is brightest. From Sagittarius, the hazy band of white light appears to pass around to the galactic anticenter in Auriga. The band then continues the rest of the way around the sky, back to Sagittarius, dividing the sky into two roughly equal hemispheres.[citation needed]
$
5
Question: Which term is specifically used to describe the band of light in the sky formed by the Milky Way?
A: Galactic Path
B: Starry Way
C: Celestial Road
D: Milky Trail
E: Milky Way
Answer: E

Question: What reduces the visibility of the Milky Way's surface brightness the most?
A: Cosmic dust
B: Solar flares
C: Light pollution
D: Gravitational waves
E: Galactic wind
Answer: C

Question: The area of the sky that the Milky Way obscures is termed as?
A: Dark Zone
B: Void Patch
C: Nebulous Strip
D: Zone of Avoidance
E: Obscured Crescent
Answer: D

Question: Which is a prominent soft visual patch in the Milky Way?
A: Small Orion Star Cluster
B: Big Cassiopeia Nebula
C: Large Sagittarius Star Cloud
D: Great Hercules Constellation
E: Huge Taurus Dust Belt
Answer: C

Question: The Milky Way divides the sky into how many roughly equal hemispheres?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: B
@
The Sun is near the inner rim of the Orion Arm, within the Local Fluff of the Local Bubble, between the Radcliffe wave and Split linear structures (formerly Gould Belt).[95] Based upon studies of stellar orbits around Sgr A* by Gillessen et al. (2016), the Sun lies at an estimated distance of 27.14 ± 0.46 kly (8.32 ± 0.14 kpc)[38] from the Galactic Center. Boehle et al. (2016) found a smaller value of 25.64 ± 0.46 kly (7.86 ± 0.14 kpc), also using a star orbit analysis.[96] The Sun is currently 5–30 parsecs (16–98 ly) above, or north of, the central plane of the Galactic disk.[97] The distance between the local arm and the next arm out, the Perseus Arm, is about 2,000 parsecs (6,500 ly).[98] The Sun, and thus the Solar System, is located in the Milky Way's galactic habitable zone.[99][100]

There are about 208 stars brighter than absolute magnitude 8.5 within a sphere with a radius of 15 parsecs (49 ly) from the Sun, giving a density of one star per 69 cubic parsecs, or one star per 2,360 cubic light-years (from List of nearest bright stars). On the other hand, there are 64 known stars (of any magnitude, not counting 4 brown dwarfs) within 5 parsecs (16 ly) of the Sun, giving a density of about one star per 8.2 cubic parsecs, or one per 284 cubic light-years (from List of nearest stars). This illustrates the fact that there are far more faint stars than bright stars: in the entire sky, there are about 500 stars brighter than apparent magnitude 4 but 15.5 million stars brighter than apparent magnitude 14.[101]
$
5
Question: The Sun is located near the inner rim of which arm of the Milky Way?
A: Cygnus Arm
B: Centaurus Arm
C: Orion Arm
D: Perseus Arm
E: Sagittarius Arm
Answer: C

Question: Approximately how far is the Sun from the Galactic Center?
A: 8.3 kpc
B: 10 kpc
C: 12 kpc
D: 15 kpc
E: 20 kpc
Answer: A

Question: The distance between the Orion Arm and the Perseus Arm is about?
A: 500 parsecs
B: 1,000 parsecs
C: 1,500 parsecs
D: 2,000 parsecs
E: 2,500 parsecs
Answer: D

Question: The Solar System is located in which zone of the Milky Way?
A: Galactic dead zone
B: Galactic conflict zone
C: Galactic habitable zone
D: Galactic neutral zone
E: Galactic buffer zone
Answer: C

Question: How many stars brighter than apparent magnitude 4 are there in the entire sky?
A: 50
B: 250
C: 500
D: 1000
E: 2000
Answer: C
@
In mechanics and physics, simple harmonic motion (sometimes abbreviated SHM) is a special type of periodic motion an object experiences due to a restoring force whose magnitude is directly proportional to the distance of the object from an equilibrium position and acts towards the equilibrium position. It results in an oscillation that is described by a sinusoid which continues indefinitely (if uninhibited by friction or any other dissipation of energy).

Simple harmonic motion can serve as a mathematical model for a variety of motions, but is typified by the oscillation of a mass on a spring when it is subject to the linear elastic restoring force given by Hooke's law. The motion is sinusoidal in time and demonstrates a single resonant frequency. Other phenomena can be modeled by simple harmonic motion, including the motion of a simple pendulum, although for it to be an accurate model, the net force on the object at the end of the pendulum must be proportional to the displacement (and even so, it is only a good approximation when the angle of the swing is small; see small-angle approximation). Simple harmonic motion can also be used to model molecular vibration.

Simple harmonic motion provides a basis for the characterization of more complicated periodic motion through the techniques of Fourier analysis.
$
5
Question 1: Which type of motion is experienced by an object due to a restoring force proportional to its distance from equilibrium?
A: Rotational motion
B: Projectile motion
C: Free fall motion
D: Simple harmonic motion
E: Translational motion
Answer: D

Question 2: Which of the following best describes the graph of simple harmonic motion?
A: Parabola
B: Hyperbola
C: Straight line
D: Circle
E: Sinusoid
Answer: E

Question 3: A mass oscillating on a spring subjected to a linear elastic restoring force obeys which law?
A: Newton's law
B: Archimedes' principle
C: Ohm's law
D: Hooke's law
E: Pascal's principle
Answer: D

Question 4: What is one condition for a simple pendulum's motion to be accurately modeled by simple harmonic motion?
A: The net force on the object must be opposite to displacement.
B: The swing's angle must be large.
C: The swing's angle must be small.
D: The pendulum must be made of metal.
E: The pendulum must be long.
Answer: C

Question 5: Which analysis technique provides a foundation for characterizing more intricate periodic motions based on simple harmonic motion?
A: Differential calculus
B: Pythagoras analysis
C: Fourier analysis
D: Bayesian inference
E: Quadratic analysis
Answer: C
@
The motion of a particle moving along a straight line with an acceleration whose direction is always towards a fixed point on the line and whose magnitude is proportional to the distance from the fixed point is called simple harmonic motion.[1]

In the diagram, a simple harmonic oscillator, consisting of a weight attached to one end of a spring, is shown. The other end of the spring is connected to a rigid support such as a wall. If the system is left at rest at the equilibrium position then there is no net force acting on the mass. However, if the mass is displaced from the equilibrium position, the spring exerts a restoring elastic force that obeys Hooke's law.

Mathematically, the restoring force F is given by

�
=
−
�
�
,
{\displaystyle \mathbf {F} =-k\mathbf {x} ,}
where F is the restoring elastic force exerted by the spring (in SI units: N), k is the spring constant (N·m−1), and x is the displacement from the equilibrium position (m).
For any simple mechanical harmonic oscillator:

When the system is displaced from its equilibrium position, a restoring force that obeys Hooke's law tends to restore the system to equilibrium.
Once the mass is displaced from its equilibrium position, it experiences a net restoring force. As a result, it accelerates and starts going back to the equilibrium position. When the mass moves closer to the equilibrium position, the restoring force decreases. At the equilibrium position, the net restoring force vanishes. However, at x = 0, the mass has momentum because of the acceleration that the restoring force has imparted. Therefore, the mass continues past the equilibrium position, compressing the spring. A net restoring force then slows it down until its velocity reaches zero, whereupon it is accelerated back to the equilibrium position again.

As long as the system has no energy loss, the mass continues to oscillate. Thus simple harmonic motion is a type of periodic motion. If energy is lost in the system, then the mass exhibits damped oscillation.

Note if the real space and phase space plot are not co-linear, the phase space motion becomes elliptical. The area enclosed depends on the amplitude and the maximum momentum.
$
5
Question 6: What happens to the restoring force as the mass moves closer to its equilibrium position in simple harmonic motion?
A: It increases exponentially.
B: It remains constant.
C: It becomes zero.
D: It decreases.
E: It changes direction.
Answer: D

Question 7: What kind of motion will a system exhibit if there is energy loss in its simple harmonic motion?
A: Projectile motion
B: Damped oscillation
C: Unchanged oscillation
D: Accelerated oscillation
E: Rotational motion
Answer: B

Question 8: In the mathematical representation 
�
=
−
�
�
F=−kx, what does 
�
k represent?
A: Displacement
B: Restoring force
C: Mass of the object
D: Spring constant
E: Acceleration
Answer: D

Question 9: In a simple harmonic oscillator, where is the net restoring force zero?
A: At maximum compression of the spring
B: Halfway between equilibrium and maximum displacement
C: At maximum displacement
D: Just after passing the equilibrium position
E: At the equilibrium position
Answer: E

Question 10: What happens to the mass once it moves past the equilibrium position in a simple harmonic oscillator?
A: It stops moving.
B: It compresses the spring.
C: It accelerates infinitely.
D: It undergoes a damping force.
E: It reverses its direction instantly.
Answer: B
@
In physics, Hooke's law is an empirical law which states that the force (F) needed to extend or compress a spring by some distance (x) scales linearly with respect to that distance—that is, Fs = kx, where k is a constant factor characteristic of the spring (i.e., its stiffness), and x is small compared to the total possible deformation of the spring. The law is named after 17th-century British physicist Robert Hooke. He first stated the law in 1676 as a Latin anagram.[1][2] He published the solution of his anagram in 1678[3] as: ut tensio, sic vis ("as the extension, so the force" or "the extension is proportional to the force"). Hooke states in the 1678 work that he was aware of the law since 1660.

Hooke's equation holds (to some extent) in many other situations where an elastic body is deformed, such as wind blowing on a tall building, and a musician plucking a string of a guitar. An elastic body or material for which this equation can be assumed is said to be linear-elastic or Hookean.

Hooke's law is only a first-order linear approximation to the real response of springs and other elastic bodies to applied forces. It must eventually fail once the forces exceed some limit, since no material can be compressed beyond a certain minimum size, or stretched beyond a maximum size, without some permanent deformation or change of state. Many materials will noticeably deviate from Hooke's law well before those elastic limits are reached.

On the other hand, Hooke's law is an accurate approximation for most solid bodies, as long as the forces and deformations are small enough. For this reason, Hooke's law is extensively used in all branches of science and engineering, and is the foundation of many disciplines such as seismology, molecular mechanics and acoustics. It is also the fundamental principle behind the spring scale, the manometer, the galvanometer, and the balance wheel of the mechanical clock.
$
5
Question 11: According to Hooke's law, how is the force needed to compress or extend a spring related to the distance of deformation?
A: Inversely proportional
B: Not related
C: Exponentially related
D: Linearly proportional
E: Quadratically related
Answer: D

Question 12: Hooke's law is only an approximation and fails once forces exceed what?
A: A specific limit
B: Zero
C: The spring constant
D: The mass of the object
E: The elastic limit
Answer: A

Question 13: On what principle is the spring scale based?
A: Archimedes' principle
B: Ohm's law
C: Hooke's law
D: Pascal's principle
E: Bernoulli's principle
Answer: C

Question 14: In which of the following situations can Hooke's law be applied?
A: Motion of planets
B: Flow of electricity in a circuit
C: Wind blowing on a tall building
D: Chemical reactions
E: Heat conduction
Answer: C

Question 15: Which individual is credited with the formulation of Hooke's law in the 17th century?
A: Isaac Newton
B: Galileo Galilei
C: Robert Boyle
D: Robert Hooke
E: Albert Einstein
Answer: D
@
In the small-angle approximation, the motion of a simple pendulum is approximated by simple harmonic motion. The period of a mass attached to a pendulum of length l with gravitational acceleration 
�
g is given by

�
=
2
�
�
�
{\displaystyle T=2\pi {\sqrt {\frac {l}{g}}}}
This shows that the period of oscillation is independent of the amplitude and mass of the pendulum but not of the acceleration due to gravity, 
�
g, therefore a pendulum of the same length on the Moon would swing more slowly due to the Moon's lower gravitational field strength. Because the value of 
�
g varies slightly over the surface of the earth, the time period will vary slightly from place to place and will also vary with height above sea level.

This approximation is accurate only for small angles because of the expression for angular acceleration α being proportional to the sine of the displacement angle:

−
�
�
�
sin
⁡
�
=
�
�
,
{\displaystyle -mgl\sin \theta =I\alpha ,}
where I is the moment of inertia. When θ is small, sin θ ≈ θ and therefore the expression becomes
−
�
�
�
�
=
�
�{\displaystyle -mgl\theta =I\alpha }
which makes angular acceleration directly proportional and opposite to θ, satisfying the definition of simple harmonic motion (that net force is directly proportional to the displacement from the mean position and is directed towards the mean position).
$
5
Question 16: The period of a simple pendulum is independent of which of the following?
A: Length of the pendulum
B: Gravitational acceleration
C: Amplitude of oscillation
D: Mass of the pendulum
E: Both C and D
Answer: E

Question 17: If a pendulum's length remains constant, how would its oscillation period change on the Moon compared to Earth?
A: It would oscillate faster on the Moon.
B: It would oscillate at the same speed on both.
C: It would oscillate slower on the Moon.
D: The pendulum would not oscillate on the Moon.
E: The oscillation would be erratic on the Moon.
Answer: C

Question 18: The small-angle approximation for a simple pendulum is valid when the displacement angle is:
A: Large
B: 45 degrees
C: Small
D: 90 degrees
E: Exactly equal to gravitational acceleration
Answer: C

Question 19: In the small-angle approximation, the period of a simple pendulum is determined by:
A: Its amplitude and mass
B: Its maximum displacement and height above sea level
C: Its length and gravitational acceleration
D: Its material and the angle of release
E: Its height and mass
Answer: C

Question 20: The motion of a pendulum is approximated by simple harmonic motion under which condition?
A: When net force is inversely proportional to displacement.
B: When angular acceleration is directly proportional and opposite to displacement.
C: When the length of the pendulum is very long.
D: When the pendulum swings in multiple directions.
E: When the pendulum has a large mass.
Answer: B
@
Beginning in 1973, several spacecraft performed planetary flyby manoeuvres that brought them within observation range of Jupiter. The Pioneer missions obtained the first close-up images of Jupiter's atmosphere and several of its moons. They discovered that the radiation fields near the planet were much stronger than expected, but both spacecraft managed to survive in that environment. The trajectories of these spacecraft were used to refine the mass estimates of the Jovian system. Radio occultations by the planet resulted in better measurements of Jupiter's diameter and the amount of polar flattening.[54]: 47 [170]

Six years later, the Voyager missions vastly improved the understanding of the Galilean moons and discovered Jupiter's rings. They also confirmed that the Great Red Spot was anticyclonic. Comparison of images showed that the Spot had changed hues since the Pioneer missions, turning from orange to dark brown. A torus of ionized atoms was discovered along Io's orbital path, which were found to come from erupting volcanoes on the moon's surface. As the spacecraft passed behind the planet, it observed flashes of lightning in the night side atmosphere.[54]: 87 [171]

The next mission to encounter Jupiter was the Ulysses solar probe. In February 1992, it performed a flyby manoeuvre to attain a polar orbit around the Sun. During this pass, the spacecraft studied Jupiter's magnetosphere, although it had no cameras to photograph the planet. The spacecraft passed by Jupiter six years later, this time at a much greater distance.[169]

In 2000, the Cassini probe flew by Jupiter on its way to Saturn, and provided higher-resolution images.[172]

The New Horizons probe flew by Jupiter in 2007 for a gravity assist en route to Pluto.[173] The probe's cameras measured plasma output from volcanoes on Io and studied all four Galilean moons in detail.[174]
$
5
Question: Which spacecrafts provided the first close-up images of Jupiter's atmosphere?
A: Ulysses
B: New Horizons
C: Pioneer
D: Voyager
E: Cassini
Answer: C

Question: What did the Voyager missions discover about Jupiter's Great Red Spot?
A: It has changed hues over time.
B: It is a source of radiation.
C: It is cyclonic in nature.
D: It has shrunk in size.
E: It rotates around the planet.
Answer: A

Question: Which mission discovered a torus of ionized atoms along Io's orbital path?
A: Pioneer
B: Ulysses
C: Voyager
D: Cassini
E: New Horizons
Answer: C

Question: Which spacecraft had no cameras to photograph Jupiter but studied its magnetosphere?
A: Pioneer
B: Voyager
C: Ulysses
D: New Horizons
E: Galileo
Answer: C

Question: In what year did the New Horizons probe fly by Jupiter for a gravity assist en route to Pluto?
A: 1995
B: 2000
C: 2007
D: 2012
E: 2015
Answer: C
@
The first spacecraft to orbit Jupiter was the Galileo mission, which reached the planet on December 7, 1995.[60] It remained in orbit for over seven years, conducting multiple flybys of all the Galilean moons and Amalthea. The spacecraft also witnessed the impact of Comet Shoemaker–Levy 9 when it collided with Jupiter in 1994. Some of the goals for the mission were thwarted due to a malfunction in Galileo's high-gain antenna.[175]

A 340-kilogram titanium atmospheric probe was released from the spacecraft in July 1995, entering Jupiter's atmosphere on December 7.[60] It parachuted through 150 km (93 mi) of the atmosphere at a speed of about 2,575 km/h (1600 mph)[60] and collected data for 57.6 minutes until the spacecraft was destroyed.[176] The Galileo orbiter itself experienced a more rapid version of the same fate when it was deliberately steered into the planet on September 21, 2003. NASA destroyed the spacecraft to avoid any possibility of the spacecraft crashing into and possibly contaminating the moon Europa, which may harbour life.[175]

Data from this mission revealed that hydrogen composes up to 90% of Jupiter's atmosphere.[60] The recorded temperature was more than 300 °C (570 °F) and the windspeed measured more than 644 km/h (>400 mph) before the probes vaporized.[60]
$
5
Question: What was the primary purpose of the 340-kilogram titanium atmospheric probe released from Galileo?
A: To map Jupiter's surface
B: To study the Galilean moons
C: To enter Jupiter's atmosphere and collect data
D: To capture images of the Great Red Spot
E: To study Jupiter's magnetosphere
Answer: C

Question: Why was the Galileo orbiter deliberately steered into Jupiter in 2003?
A: To collect data from Jupiter's core
B: It had malfunctioned and lost communication
C: To avoid contaminating the moon Europa
D: It had run out of fuel
E: To study the impact effects on Jupiter
Answer: C

Question: What did data from the Galileo mission reveal about Jupiter's atmospheric composition?
A: It is mainly composed of helium.
B: It has up to 90% hydrogen.
C: It contains significant amounts of methane.
D: It has a high concentration of oxygen.
E: It is primarily made of nitrogen.
Answer: B

Question: In what year did the Galileo mission witness the impact of Comet Shoemaker–Levy 9 with Jupiter?
A: 1990
B: 1992
C: 1994
D: 1996
E: 1999
Answer: C

Question: How fast did the Galileo probe travel through Jupiter's atmosphere?
A: 1,575 km/h
B: 2,000 km/h
C: 2,575 km/h
D: 3,000 km/h
E: 3,500 km/h
Answer: C
@
NASA's Juno mission arrived at Jupiter on July 4, 2016 with the goal of studying the planet in detail from a polar orbit. The spacecraft was originally intended to orbit Jupiter thirty-seven times over a period of twenty months.[177][72][178] During the mission, the spacecraft will be exposed to high levels of radiation from Jupiter's magnetosphere, which may cause future failure of certain instruments.[179] On August 27, 2016, the spacecraft completed its first fly-by of Jupiter and sent back the first-ever images of Jupiter's north pole.[180]

Juno completed 12 orbits before the end of its budgeted mission plan, ending July 2018.[181] In June of that year, NASA extended the mission operations plan to July 2021, and in January of that year the mission was extended to September 2025 with four lunar flybys: one of Ganymede, one of Europa, and two of Io.[182][183] When Juno reaches the end of the mission, it will perform a controlled deorbit and disintegrate into Jupiter's atmosphere. This will avoid the risk of collision with Jupiter's moons.[184][185]
$
5
Question: On what date did NASA's Juno mission arrive at Jupiter?
A: July 4, 2015
B: July 4, 2016
C: December 7, 2016
D: August 27, 2016
E: December 7, 2015
Answer: B

Question: What was the primary goal of the Juno mission?
A: To study the Galilean moons
B: To study Jupiter in detail from a polar orbit
C: To map Jupiter's surface
D: To collect data on Jupiter's magnetic field
E: To determine the age of Jupiter
Answer: B

Question: How many orbits did Juno complete before the end of its budgeted mission plan in July 2018?
A: 7 orbits
B: 12 orbits
C: 20 orbits
D: 30 orbits
E: 37 orbits
Answer: B

Question: Which moon of Jupiter is believed to possibly harbor life, leading to the controlled deorbit of Juno to prevent contamination?
A: Ganymede
B: Callisto
C: Io
D: Europa
E: Amalthea
Answer: D

Question: When Juno completes its mission, what will happen to the spacecraft?
A: It will return to Earth.
B: It will stay in orbit around Jupiter.
C: It will be sent to study Saturn.
D: It will disintegrate into Jupiter's atmosphere.
E: It will continue to orbit the sun indefinitely.
Answer: D
@
In 1955, Bernard Burke and Kenneth Franklin discovered that Jupiter emits bursts of radio waves at a frequency of 22.2 MHz.[64]: 36  The period of these bursts matched the rotation of the planet, and they used this information to determine a more precise value for Jupiter's rotation rate. Radio bursts from Jupiter were found to come in two forms: long bursts (or L-bursts) lasting up to several seconds, and short bursts (or S-bursts) lasting less than a hundredth of a second.[161]

Scientists have discovered three forms of radio signals transmitted from Jupiter:

Decametric radio bursts (with a wavelength of tens of metres) vary with the rotation of Jupiter, and are influenced by the interaction of Io with Jupiter's magnetic field.[162]
Decimetric radio emission (with wavelengths measured in centimetres) was first observed by Frank Drake and Hein Hvatum in 1959.[64]: 36  The origin of this signal is a torus-shaped belt around Jupiter's equator, which generates cyclotron radiation from electrons that are accelerated in Jupiter's magnetic field.[163]
Thermal radiation is produced by heat in the atmosphere of Jupiter.[64]: 43 
$
5
Question: Who discovered in 1955 that Jupiter emits bursts of radio waves?
A: Robert Hooke and Frank Drake
B: Bernard Burke and Kenneth Franklin
C: Hein Hvatum and Robert Hooke
D: Frank Drake and Hein Hvatum
E: Bernard Burke and Hein Hvatum
Answer: B

Question: Which type of radio bursts from Jupiter varies with the planet's rotation and is influenced by the interaction of Io with Jupiter's magnetic field?
A: L-bursts
B: S-bursts
C: Decametric radio bursts
D: Decimetric radio emission
E: Thermal radiation
Answer: C

Question: Where does the decimetric radio emission from Jupiter originate?
A: From Jupiter's poles
B: From the Galilean moons
C: From a torus-shaped belt around Jupiter's equator
D: From Jupiter's core
E: From the planet's Great Red Spot
Answer: C

Question: What is the cause of thermal radiation produced by Jupiter?
A: Interaction with Io
B: Heat in the atmosphere of Jupiter
C: Cyclotron radiation from electrons
D: Radioactive decay in Jupiter's core
E: Interaction with the sun's rays
Answer: B

Question: The long bursts of radio waves from Jupiter are also referred to as?
A: S-bursts
B: Thermal bursts
C: Decimetric bursts
D: L-bursts
E: Decametric bursts
Answer: D
@
There are 95 moons of Jupiter with confirmed orbits as of 23 March 2023.[1][note 1] This number does not include a number of meter-sized moonlets thought to be shed from the inner moons, nor hundreds of possible kilometer-sized outer irregular moons that were only briefly captured by telescopes.[4] All together, Jupiter's moons form a satellite system called the Jovian system. The most massive of the moons are the four Galilean moons: Io, Europa, Ganymede, and Callisto, which were independently discovered in 1610 by Galileo Galilei and Simon Marius and were the first objects found to orbit a body that was neither Earth nor the Sun. Much more recently, beginning in 1892, dozens of far smaller Jovian moons have been detected and have received the names of lovers (or other sexual partners) or daughters of the Roman god Jupiter or his Greek equivalent Zeus. The Galilean moons are by far the largest and most massive objects to orbit Jupiter, with the remaining 91 known moons and the rings together composing just 0.003% of the total orbiting mass.

Of Jupiter's moons, eight are regular satellites with prograde and nearly circular orbits that are not greatly inclined with respect to Jupiter's equatorial plane. The Galilean satellites are nearly spherical in shape due to their planetary mass, and are just massive enough that they would be considered major planets if they were in direct orbit around the Sun. The other four regular satellites, known as the inner moons, are much smaller and closer to Jupiter; these serve as sources of the dust that makes up Jupiter's rings. The remainder of Jupiter's moons are outer irregular satellites whose prograde and retrograde orbits are much farther from Jupiter and have high inclinations and eccentricities. The largest of these moons were likely asteroids that were captured from solar orbits by Jupiter before impacts with other small bodies shattered them into many kilometer-sized fragments, forming collisional families of moons sharing similar orbits. Jupiter is expected to have about 100 irregular moons larger than 1 km (0.6 mi) in diameter, plus around 500 more smaller retrograde moons down to diameters of 0.8 km (0.5 mi).[5] Of the 87 known irregular moons of Jupiter, 38 of them have not yet been officially named.
$
5
Question: As of 23 March 2023, how many moons of Jupiter have confirmed orbits?
A: 65
B: 75
C: 85
D: 95
E: 105
Answer: D

Question: Which moons were the first to be discovered orbiting a body that was neither Earth nor the Sun?
A: The inner moons
B: The irregular moons
C: The Galilean moons
D: The outer moons
E: The Jovian moons
Answer: C

Question: What percentage of the total orbiting mass around Jupiter is composed by the non-Galilean moons and the rings?
A: 0.003%
B: 0.03%
C: 0.3%
D: 3%
E: 30%
Answer: A

Question: How many of the irregular moons of Jupiter have not been officially named?
A: 28
B: 38
C: 48
D: 58
E: 68
Answer: B

Question: Which of the following is not one of the Galilean moons?
A: Titan
B: Callisto
C: Ganymede
D: Europa
E: Io
Answer: A
@
Jupiter's regular satellites are believed to have formed from a circumplanetary disk, a ring of accreting gas and solid debris analogous to a protoplanetary disk.[6][7] They may be the remnants of a score of Galilean-mass satellites that formed early in Jupiter's history.[6][8]

Simulations suggest that, while the disk had a relatively high mass at any given moment, over time a substantial fraction (several tens of a percent) of the mass of Jupiter captured from the solar nebula was passed through it. However, only 2% of the proto-disk mass of Jupiter is required to explain the existing satellites.[6] Thus, several generations of Galilean-mass satellites may have been in Jupiter's early history. Each generation of moons might have spiraled into Jupiter, because of drag from the disk, with new moons then forming from the new debris captured from the solar nebula.[6] By the time the present (possibly fifth) generation formed, the disk had thinned so that it no longer greatly interfered with the moons' orbits.[8] The current Galilean moons were still affected, falling into and being partially protected by an orbital resonance with each other, which still exists for Io, Europa, and Ganymede: they are in a 1:2:4 resonance. Ganymede's larger mass means that it would have migrated inward at a faster rate than Europa or Io.[6] Tidal dissipation in the Jovian system is still ongoing and Callisto will likely be captured into the resonance in about 1.5 billion years, creating a 1:2:4:8 chain.[9]

The outer, irregular moons are thought to have originated from captured asteroids, whereas the protolunar disk was still massive enough to absorb much of their momentum and thus capture them into orbit. Many are believed to have been broken up by mechanical stresses during capture, or afterward by collisions with other small bodies, producing the moons we see today.[10]
$
5
Question: Jupiter's regular satellites are believed to have formed from what?
A: A circumplanetary disk
B: Captured asteroids
C: Tidal dissipation
D: Collisional fragments
E: Gravitational pull of the sun
Answer: A

Question: What ratio represents the orbital resonance of Io, Europa, and Ganymede?
A: 1:2:3
B: 2:4:6
C: 1:2:4
D: 2:3:4
E: 1:3:5
Answer: C

Question: The irregular moons of Jupiter are thought to have originated from?
A: Solar flares
B: Collisions between Galilean moons
C: Captured asteroids
D: Protoplanetary disks
E: Tidal forces of the sun
Answer: C

Question: How many generations of Galilean-mass satellites may have existed in Jupiter's early history?
A: One
B: Two
C: Three
D: Four
E: Five or more
Answer: E

Question: When will Callisto likely be captured into the resonance, creating a 1:2:4:8 chain?
A: In about 500 million years
B: In about 1 billion years
C: In about 1.5 billion years
D: In about 2 billion years
E: Never
Answer: C
@
Prograde satellites:
Themisto is the innermost irregular moon and is not part of a known family.[4][60]
The Himalia group is confined within semi-major axes between 11–12 million km (6.8–7.5 million mi), inclinations between 27–29°, and eccentricities between 0.12 and 0.21.[62] It has been suggested that the group could be a remnant of the break-up of an asteroid from the asteroid belt.[60]
The Carpo group includes two known moons on very high orbital inclinations of 50° and semi-major axes between 16–17 million km (9.9–10.6 million mi).[4] Due to their exceptionally high inclinations, the moons of the Carpo group are subject to gravitational perturbations that induce the Lidov–Kozai resonance in their orbits, which cause their eccentricities and inclinations to periodically oscillate in correspondence with each other.[35] The Lidov–Kozai resonance can significantly alter the orbits of these moons: for example, the eccentricity and inclination of the group's namesake Carpo can fluctuate between 0.19–0.69 and 44–59°, respectively.[35]
Valetudo is the outermost prograde moon and is not part of a known family. Its prograde orbit crosses paths with several moons that have retrograde orbits and may in the future collide with them.[37]
Retrograde satellites:
The Carme group is tightly confined within semi-major axes between 22–24 million km (14–15 million mi), inclinations between 164–166°, and eccentricities between 0.25 and 0.28.[62] It is very homogeneous in color (light red) and is believed to have originated as collisional fragments from a D-type asteroid progenitor, possibly a Jupiter trojan.[27]
The Ananke group has a relatively wider spread than the previous groups, with semi-major axes between 19–22 million km (12–14 million mi), inclinations between 144–156°, and eccentricities between 0.09 and 0.25.[62] Most of the members appear gray, and are believed to have formed from the breakup of a captured asteroid.[27]
The Pasiphae group is quite dispersed, with semi-major axes spread over 22–25 million km (14–16 million mi), inclinations between 141° and 157°, and higher eccentricities between 0.23 and 0.44.[62] The colors also vary significantly, from red to grey, which might be the result of multiple collisions. Sinope, sometimes included in the Pasiphae group,[27] is red and, given the difference in inclination, it could have been captured independently;[60] Pasiphae and Sinope are also trapped in secular resonances with Jupiter.[63]
$
5
Question: What is the characteristic of the Carpo group of moons?
A: Confined within semi-major axes of 11–12 million km
B: Have very high orbital inclinations of 50°
C: Tightly confined within semi-major axes between 22–24 million km
D: Dispersion with semi-major axes over 22–25 million km
E: Not part of any known family
Answer: B

Question: What color is the Carme group?
A: Light blue
B: Gray
C: Light red
D: Dark brown
E: Golden
Answer: C

Question: Which group might have originated as collisional fragments from a D-type asteroid progenitor?
A: The Carpo group
B: The Himalia group
C: The Ananke group
D: The Pasiphae group
E: The Carme group
Answer: E

Question: Themisto belongs to which type of satellite?
A: Galilean satellite
B: Prograde satellite
C: Retrograde satellite
D: Inner moon
E: Outer moon
Answer: B

Question: Which group of satellites has members that appear gray and are believed to have formed from the breakup of a captured asteroid?
A: The Himalia group
B: The Carpo group
C: The Ananke group
D: The Pasiphae group
E: The Carme group
Answer: C
@
Europa /jʊˈroʊpə/ i, or Jupiter II, is the smallest of the four Galilean moons orbiting Jupiter, and the sixth-closest to the planet of all the 95 known moons of Jupiter. It is also the sixth-largest moon in the Solar System. Europa was discovered independently by Simon Marius and Galileo Galilei[1] and was named (by Marius) after Europa, the Phoenician mother of King Minos of Crete and lover of Zeus (the Greek equivalent of the Roman god Jupiter).

Slightly smaller than Earth's Moon, Europa is made of silicate rock and has a water-ice crust[14] and probably an iron–nickel core. It has a very thin atmosphere, composed primarily of oxygen. Its white-beige surface is striated by light tan cracks and streaks, but craters are relatively few. In addition to Earth-bound telescope observations, Europa has been examined by a succession of space-probe flybys, the first occurring in the early 1970s. In September 2022, the Juno spacecraft flew within about 320 km (200 miles) of Europa for a more recent close-up view.[15]

Europa has the smoothest surface of any known solid object in the Solar System. The apparent youth and smoothness of the surface have led to the hypothesis that a water ocean exists beneath the surface, which could conceivably harbor extraterrestrial life.[16] The predominant model suggests that heat from tidal flexing causes the ocean to remain liquid and drives ice movement similar to plate tectonics, absorbing chemicals from the surface into the ocean below.[17][18] Sea salt from a subsurface ocean may be coating some geological features on Europa, suggesting that the ocean is interacting with the sea floor. This may be important in determining whether Europa could be habitable.[19] In addition, the Hubble Space Telescope detected water vapor plumes similar to those observed on Saturn's moon Enceladus, which are thought to be caused by erupting cryogeysers.[20] In May 2018, astronomers provided supporting evidence of water plume activity on Europa, based on an updated analysis of data obtained from the Galileo space probe, which orbited Jupiter from 1995 to 2003. Such plume activity could help researchers in a search for life from the subsurface Europan ocean without having to land on the moon.[21][22][23][24]
$
5
Question: Which moon is the smallest of the four Galilean moons orbiting Jupiter?
A: Callisto
B: Ganymede
C: Io
D: Europa
E: Titan
Answer: D

Question: Europa was named after a Phoenician mother of whom?
A: Perseus
B: Hercules
C: Achilles
D: Minos
E: Orion
Answer: D

Question: What is distinctive about Europa's surface in the Solar System?
A: It has the highest number of craters
B: It has the smoothest surface
C: It is the most reflective surface
D: It has the most active volcanoes
E: It has the longest canyons
Answer: B

Question: What is the hypothesis regarding the surface of Europa?
A: It has a dry desert beneath
B: It has a molten lava ocean beneath
C: It has a water ocean beneath
D: It has a thick methane atmosphere
E: It is made up entirely of ice
Answer: C

Question: In May 2018, what evidence was provided supporting the presence of water plume activity on Europa?
A: Observations from Earth's telescopes
B: Observations from the Hubble Space Telescope
C: Data from the Galileo space probe
D: Data from the Juno spacecraft
E: Observations from the Kepler Telescope
Answer: C
@
A white dwarf is a stellar core remnant composed mostly of electron-degenerate matter. A white dwarf is very dense: its mass is comparable to the Sun's, while its volume is comparable to the Earth's. A white dwarf's low luminosity comes from the emission of residual thermal energy; no fusion takes place in a white dwarf.[1] The nearest known white dwarf is Sirius B, at 8.6 light years, the smaller component of the Sirius binary star. There are currently thought to be eight white dwarfs among the hundred star systems nearest the Sun.[2] The unusual faintness of white dwarfs was first recognized in 1910.[3]: 1  The name white dwarf was coined by Willem Luyten in 1922.

White dwarfs are thought to be the final evolutionary state of stars whose mass is not high enough to become a neutron star or black hole. This includes over 97% of the stars in the Milky Way.[4]: §1  After the hydrogen-fusing period of a main-sequence star of low or medium mass ends, such a star will expand to a red giant during which it fuses helium to carbon and oxygen in its core by the triple-alpha process. If a red giant has insufficient mass to generate the core temperatures required to fuse carbon (around 1 billion K), an inert mass of carbon and oxygen will build up at its center. After such a star sheds its outer layers and forms a planetary nebula, it will leave behind a core, which is the remnant white dwarf.[5] Usually, white dwarfs are composed of carbon and oxygen (CO white dwarf). If the mass of the progenitor is between 8 and 10.5 solar masses (M☉), the core temperature will be sufficient to fuse carbon but not neon, in which case an oxygen–neon–magnesium (ONeMg or ONe) white dwarf may form.[6] Stars of very low mass will be unable to fuse helium; hence, a helium white dwarf[7][8] may form by mass loss in binary systems.
$
5
Question: What is the primary composition of a white dwarf?
A: Hydrogen and helium
B: Oxygen and neon
C: Electron-degenerate matter
D: Carbon and nitrogen
E: Proton-based matter
Answer: C

Question: How does the mass of a white dwarf compare to the Sun?
A: It is much less.
B: It is about the same.
C: It is much greater.
D: It is about half.
E: It is negligible.
Answer: B

Question: Which component of the Sirius binary star is a white dwarf?
A: Sirius A
B: Sirius B
C: Sirius C
D: Sirius D
E: None of the above
Answer: B

Question: Who coined the term "white dwarf"?
A: Albert Einstein
B: Isaac Newton
C: Chandrasekhar
D: Willem Luyten
E: Richard Feynman
Answer: D

Question: Approximately what percentage of stars in the Milky Way will not evolve into neutron stars or black holes and instead become white dwarfs?
A: 50%
B: 85%
C: 97%
D: 70%
E: 33%
Answer: C
@
The material in a white dwarf no longer undergoes fusion reactions, so the star has no source of energy. As a result, it cannot support itself by the heat generated by fusion against gravitational collapse, but is supported only by electron degeneracy pressure, causing it to be extremely dense. The physics of degeneracy yields a maximum mass for a non-rotating white dwarf, the Chandrasekhar limit — approximately 1.44 times M☉ — beyond which it cannot be supported by electron degeneracy pressure. A carbon–oxygen white dwarf that approaches this mass limit, typically by mass transfer from a companion star, may explode as a type Ia supernova via a process known as carbon detonation;[1][5] SN 1006 is thought to be a famous example.

A white dwarf is very hot when it forms, but because it has no source of energy, it will gradually cool as it radiates its energy away. This means that its radiation, which initially has a high color temperature, will lessen and redden with time. Over a very long time, a white dwarf will cool and its material will begin to crystallize, starting with the core. The star's low temperature means it will no longer emit significant heat or light, and it will become a cold black dwarf.[5] Because the length of time it takes for a white dwarf to reach this state is calculated to be longer than the current age of the known universe (approximately 13.8 billion years),[9] it is thought that no black dwarfs yet exist.[1][4] The oldest known white dwarfs still radiate at temperatures of a few thousand kelvins, which establishes an observational limit on the maximum possible age of the universe.[10]
$
5
Question: What supports a white dwarf against gravitational collapse?
A: Fusion reactions
B: Heat generated by radiation
C: Proton pressure
D: Electron degeneracy pressure
E: External energy source
Answer: D

Question: What is the maximum mass for a non-rotating white dwarf supported by electron degeneracy pressure?
A: 1 M☉
B: 2 M☉
C: 0.5 M☉
D: 1.44 M☉
E: 3 M☉
Answer: D

Question: What event can a carbon-oxygen white dwarf undergo if it approaches the Chandrasekhar limit?
A: Implosion
B: Type Ia supernova
C: Black hole formation
D: Red giant expansion
E: Neutron star conversion
Answer: B

Question: Over time, a white dwarf will:
A: Increase in temperature
B: Stay the same in temperature
C: Decrease in temperature
D: Fluctuate in temperature
E: Become a neutron star
Answer: C

Question: Black dwarfs are:
A: Extremely hot white dwarfs
B: White dwarfs that have cooled and no longer emit significant heat or light
C: Theoretical white dwarfs that are pure carbon
D: The most abundant type of star in the universe
E: Predecessors to white dwarfs
Answer: B
@
Although white dwarfs are known with estimated masses as low as 0.17 M☉[26] and as high as 1.33 M☉,[27] the mass distribution is strongly peaked at 0.6 M☉, and the majority lie between 0.5 and 0.7 M☉.[27] The estimated radii of observed white dwarfs are typically 0.8–2% the radius of the Sun;[28] this is comparable to the Earth's radius of approximately 0.9% solar radius. A white dwarf, then, packs mass comparable to the Sun's into a volume that is typically a million times smaller than the Sun's; the average density of matter in a white dwarf must therefore be, very roughly, 1,000,000 times greater than the average density of the Sun, or approximately 106 g/cm3, or 1 tonne per cubic centimetre.[1] A typical white dwarf has a density of between 104 and 107 g/cm3. White dwarfs are composed of one of the densest forms of matter known, surpassed only by other compact stars such as neutron stars, quark stars (hypothetical),[29] and black holes.

White dwarfs were found to be extremely dense soon after their discovery. If a star is in a binary system, as is the case for Sirius B or 40 Eridani B, it is possible to estimate its mass from observations of the binary orbit. This was done for Sirius B by 1910,[30] yielding a mass estimate of 0.94 M☉, which compares well with a more modern estimate of 1.00 M☉.[31] Since hotter bodies radiate more energy than colder ones, a star's surface brightness can be estimated from its effective surface temperature, and that from its spectrum.

$
5
Question: The average density of a white dwarf is approximately:
A: Equal to the Sun
B: 1000 times greater than the Sun
C: 1,000,000 times greater than the Sun
D: 10 times less than the Sun
E: Equal to Earth's
Answer: C

Question: The mass distribution of white dwarfs peaks at approximately:
A: 0.3 M☉
B: 0.6 M☉
C: 1 M☉
D: 1.5 M☉
E: 2 M☉
Answer: B

Question: How does the typical radius of a white dwarf compare to the Sun's radius?
A: About 50% the Sun's radius
B: Equal to the Sun's radius
C: About 0.8-2% the Sun's radius
D: Twice the Sun's radius
E: About 10% the Sun's radius
Answer: C

Question: Which type of star is denser than a white dwarf?
A: Red giant
B: Main-sequence star
C: Neutron star
D: Yellow dwarf
E: Blue giant
Answer: C

Question: The effective surface temperature of a star can be estimated from:
A: Its mass
B: Its rotation speed
C: Its volume
D: Its spectrum
E: Its age
Answer: D
@
Although most white dwarfs are thought to be composed of carbon and oxygen, spectroscopy typically shows that their emitted light comes from an atmosphere which is observed to be either hydrogen or helium dominated. The dominant element is usually at least 1,000 times more abundant than all other elements. As explained by Schatzman in the 1940s, the high surface gravity is thought to cause this purity by gravitationally separating the atmosphere so that heavy elements are below and the lighter above.[87][88]: §§5–6  This atmosphere, the only part of the white dwarf visible to us, is thought to be the top of an envelope which is a residue of the star's envelope in the AGB phase and may also contain material accreted from the interstellar medium. The envelope is believed to consist of a helium-rich layer with mass no more than 1⁄100 of the star's total mass, which, if the atmosphere is hydrogen-dominated, is overlain by a hydrogen-rich layer with mass approximately 1⁄10,000 of the star's total mass.[61][89]: §§4–5 

Although thin, these outer layers determine the thermal evolution of the white dwarf. The degenerate electrons in the bulk of a white dwarf conduct heat well. Most of a white dwarf's mass is therefore at almost the same temperature (isothermal), and it is also hot: a white dwarf with surface temperature between 8,000 K and 16,000 K will have a core temperature between approximately 5,000,000 K and 20,000,000 K. The white dwarf is kept from cooling very quickly only by its outer layers' opacity to radiation.[61]
$
5
Question: The emitted light from most white dwarfs comes from an atmosphere that is primarily composed of:
A: Carbon and oxygen
B: Hydrogen or helium
C: Electron-degenerate matter
D: Carbon and neon
E: Iron and nickel
Answer: B

Question: The high surface gravity of white dwarfs is believed to cause:
A: Fusion reactions
B: Carbon detonation
C: Gravitational separation of the atmosphere
D: Helium burning
E: Chandrasekhar limit breaches
Answer: C

Question: What is the estimated surface temperature range of a typical white dwarf?
A: 1,000 K to 5,000 K
B: 5,000 K to 10,000 K
C: 8,000 K to 16,000 K
D: 20,000 K to 30,000 K
E: 100,000 K to 200,000 K
Answer: C

Question: What prevents a white dwarf from cooling very quickly?
A: Its fusion reactions
B: Its large mass
C: Its high gravity
D: Its outer layers' opacity to radiation
E: Its proximity to a companion star
Answer: D

Question: Which part of the white dwarf is visible to us?
A: The core
B: The outer envelope
C: The atmosphere
D: The fusion zone
E: The neutron layer
Answer: C
@
Helium (from Greek: ἥλιος, romanized: helios, lit. 'sun') is a chemical element with the symbol He and atomic number 2. It is a colorless, odorless, tasteless, non-toxic, inert, monatomic gas and the first in the noble gas group in the periodic table.[a] Its boiling point is the lowest among all the elements, and it does not have a melting point at standard pressure. It is the second-lightest and second most abundant element in the observable universe, after hydrogen. It is present at about 24% of the total elemental mass, which is more than 12 times the mass of all the heavier elements combined. Its abundance is similar to this in both the Sun and Jupiter, because of the very high nuclear binding energy (per nucleon) of helium-4, with respect to the next three elements after helium. This helium-4 binding energy also accounts for why it is a product of both nuclear fusion and radioactive decay. The most common isotope of helium in the universe is helium-4, the vast majority of which was formed during the Big Bang. Large amounts of new helium are created by nuclear fusion of hydrogen in stars.
$
5
Question 1: What is the atomic number of Helium?
A: 3
B: 4
C: 1
D: 2
E: 5
Answer: D

Question 2: Which of the following elements has the lowest boiling point?
A: Oxygen
B: Hydrogen
C: Neon
D: Helium
E: Nitrogen
Answer: D

Question 3: What is the most common isotope of helium in the universe?
A: Helium-3
B: Helium-2
C: Helium-1
D: Helium-5
E: Helium-4
Answer: E

Question 4: Helium accounts for approximately what percentage of the total elemental mass in the observable universe?
A: 24%
B: 10%
C: 50%
D: 75%
E: 5%
Answer: A

Question 5: Helium is the product of which of the following processes?
A: Osmosis
B: Nuclear fusion
C: Electrolysis
D: Carbon cycle
E: Hydrolysis
Answer: B
@
Liquid helium is used in cryogenics (its largest single use, consuming about a quarter of production), and in the cooling of superconducting magnets, with its main commercial application in MRI scanners. Helium's other industrial uses—as a pressurizing and purge gas, as a protective atmosphere for arc welding, and in processes such as growing crystal to make silicon wafers—account for half of the gas produced. A small but well-known use is as a lifting gas in balloons and airships.[19] As with any gas whose density differs from that of air, inhaling a small volume of helium temporarily changes the timbre and quality of the human voice. In scientific research, the behavior of the two fluid phases of helium-4 (helium I and helium II) is important to researchers studying quantum mechanics (in particular the property of superfluidity) and to those looking at the phenomena, such as superconductivity, produced in matter near absolute zero.

On Earth, it is relatively rare—5.2 ppm by volume in the atmosphere. Most terrestrial helium present today is created by the natural radioactive decay of heavy radioactive elements (thorium and uranium, although there are other examples), as the alpha particles emitted by such decays consist of helium-4 nuclei. This radiogenic helium is trapped with natural gas in concentrations as great as 7% by volume, from which it is extracted commercially by a low-temperature separation process called fractional distillation. Terrestrial helium is a non-renewable resource because once released into the atmosphere, it promptly escapes into space. Its supply is thought to be rapidly diminishing.[20][21] However, some studies suggest that helium produced deep in the Earth by radioactive decay can collect in natural gas reserves in larger-than-expected quantities,[22] in some cases having been released by volcanic activity.[23]
$
5
Question 6: What is the primary commercial application of helium in its liquid state?
A: Airships
B: Arc welding
C: MRI scanners
D: Silicon wafer production
E: Voice modulation
Answer: C

Question 7: Inhaling helium causes the human voice to:
A: Remain unchanged
B: Deepen
C: Become raspy
D: Become higher-pitched
E: Become softer
Answer: D

Question 8: Most of the terrestrial helium today is created by the decay of which elements?
A: Carbon and Nitrogen
B: Gold and Silver
C: Thorium and Uranium
D: Iron and Zinc
E: Sodium and Chlorine
Answer: C

Question 9: Once released into the atmosphere, what does terrestrial helium primarily do?
A: Dissolves in the ocean
B: Reacts with oxygen
C: Is absorbed by plants
D: Escapes into space
E: Forms compounds with nitrogen
Answer: D

Question 10: What is the main method used to commercially extract helium from natural gas?
A: Photosynthesis
B: Electrolysis
C: Fractional distillation
D: Carbon fixation
E: Fermentation
Answer: C
@
There are nine known isotopes of helium of which two, helium-3 and helium-4, are stable. In the Earth's atmosphere, one atom is 3
He for every million that are 4
He.[25] Unlike most elements, helium's isotopic abundance varies greatly by origin, due to the different formation processes. The most common isotope, helium-4, is produced on Earth by alpha decay of heavier radioactive elements; the alpha particles that emerge are fully ionized helium-4 nuclei. Helium-4 is an unusually stable nucleus because its nucleons are arranged into complete shells. It was also formed in enormous quantities during Big Bang nucleosynthesis.[105]

Helium-3 is present on Earth only in trace amounts. Most of it has been present since Earth's formation, though some falls to Earth trapped in cosmic dust.[106] Trace amounts are also produced by the beta decay of tritium.[107] Rocks from the Earth's crust have isotope ratios varying by as much as a factor of ten, and these ratios can be used to investigate the origin of rocks and the composition of the Earth's mantle.[106] 3
He is much more abundant in stars as a product of nuclear fusion. Thus in the interstellar medium, the proportion of 3
He to 4
He is about 100 times higher than on Earth.[108] Extraplanetary material, such as lunar and asteroid regolith, have trace amounts of helium-3 from being bombarded by solar winds. The Moon's surface contains helium-3 at concentrations on the order of 10 ppb, much higher than the approximately 5 ppt found in the Earth's atmosphere.[109][110] A number of people, starting with Gerald Kulcinski in 1986,[111] have proposed to explore the Moon, mine lunar regolith, and use the helium-3 for fusion.
$
5
Question 11: How many known isotopes of helium exist?
A: 3
B: 6
C: 5
D: 9
E: 12
Answer: D

Question 12: Which isotope of helium was mainly formed during the Big Bang?
A: Helium-1
B: Helium-3
C: Helium-4
D: Helium-5
E: Helium-6
Answer: C

Question 13: In the Earth's atmosphere, what proportion of helium atoms is Helium-3 compared to Helium-4?
A: 1 to 1,000
B: 1 to 10,000
C: 1 to 100,000
D: 1 to 1,000,000
E: 1 to 10
Answer: D

Question 14: What is the primary source of Helium-3 on Earth?
A: Nuclear reactors
B: Ocean water
C: Solar wind
D: Earth's formation
E: Atmospheric capture
Answer: D

Question 15: Which celestial body is known to have significant concentrations of helium-3 on its surface?
A: Mars
B: Venus
C: Jupiter's moon Europa
D: The Moon
E: Neptune
Answer: D
@
In the perspective of quantum mechanics, helium is the second simplest atom to model, following the hydrogen atom. Helium is composed of two electrons in atomic orbitals surrounding a nucleus containing two protons and (usually) two neutrons. As in Newtonian mechanics, no system that consists of more than two particles can be solved with an exact analytical mathematical approach (see 3-body problem) and helium is no exception. Thus, numerical mathematical methods are required, even to solve the system of one nucleus and two electrons. Such computational chemistry methods have been used to create a quantum mechanical picture of helium electron binding which is accurate to within < 2% of the correct value, in a few computational steps.[85] Such models show that each electron in helium partly screens the nucleus from the other, so that the effective nuclear charge Zeff which each electron sees is about 1.69 units, not the 2 charges of a classic "bare" helium nucleus.

The nucleus of the helium-4 atom is identical with an alpha particle. High-energy electron-scattering experiments show its charge to decrease exponentially from a maximum at a central point, exactly as does the charge density of helium's own electron cloud. This symmetry reflects similar underlying physics: the pair of neutrons and the pair of protons in helium's nucleus obey the same quantum mechanical rules as do helium's pair of electrons (although the nuclear particles are subject to a different nuclear binding potential), so that all these fermions fully occupy 1s orbitals in pairs, none of them possessing orbital angular momentum, and each cancelling the other's intrinsic spin. Adding another of any of these particles would require angular momentum and would release substantially less energy (in fact, no nucleus with five nucleons is stable). This arrangement is thus energetically extremely stable for all these particles, and this stability accounts for many crucial facts regarding helium in nature.
$
5
Question 16: After which atom is helium the second simplest to model in quantum mechanics?
A: Carbon
B: Neon
C: Hydrogen
D: Oxygen
E: Nitrogen
Answer: C

Question 17: How many protons are present in the nucleus of a helium atom?
A: 1
B: 3
C: 2
D: 4
E: 5
Answer: C

Question 18: Which particle's nucleus is identical to the nucleus of the helium-4 atom?
A: Beta particle
B: Gamma particle
C: Neutron
D: Alpha particle
E: Delta particle
Answer: D

Question 19: What effective nuclear charge do each electron in helium see due to screening by the other electron?
A: 1
B: 2
C: 1.5
D: 1.69
E: 2.5
Answer: D

Question 20: The nucleons in helium's nucleus occupy 1s orbitals in pairs and cancel each other's intrinsic spin. This means they possess what kind of angular momentum?
A: High angular momentum
B: No angular momentum
C: Negative angular momentum
D: Positive angular momentum
E: Unpredictable angular momentum
Answer: B
@
A gravitational singularity, spacetime singularity or simply singularity is a condition in which gravity is predicted to be so intense that spacetime itself would break down catastrophically. As such, a singularity is by definition no longer part of the regular spacetime and cannot be determined by "where" or "when". Gravitational singularities exist at a junction between general relativity and quantum mechanics; therefore, the properties of the singularity cannot be described without an established theory of quantum gravity. Trying to find a complete and precise definition of singularities in the theory of general relativity, the current best theory of gravity, remains a difficult problem.[1][2] A singularity in general relativity can be defined by the scalar invariant curvature becoming infinite[3] or, better, by a geodesic being incomplete.[4]

Gravitational singularities are mainly considered in the context of general relativity, where density would become infinite at the center of a black hole without corrections from quantum mechanics, and within astrophysics and cosmology as the earliest state of the universe during the Big Bang. Physicists are undecided whether the prediction of singularities means that they actually exist (or existed at the start of the Big Bang), or that current knowledge is insufficient to describe what happens at such extreme densities.[5]

General relativity predicts that any object collapsing beyond a certain point (for stars this is the Schwarzschild radius) would form a black hole, inside which a singularity (covered by an event horizon) would be formed.[2] The Penrose–Hawking singularity theorems define a singularity to have geodesics that cannot be extended in a smooth manner.[6] The termination of such a geodesic is considered to be the singularity.

The initial state of the universe, at the beginning of the Big Bang, is also predicted by modern theories to have been a singularity.[7] In this case, the universe did not collapse into a black hole, because currently-known calculations and density limits for gravitational collapse are usually based upon objects of relatively constant size, such as stars, and do not necessarily apply in the same way to rapidly expanding space such as the Big Bang. Neither general relativity nor quantum mechanics can currently describe the earliest moments of the Big Bang,[8] but in general, quantum mechanics does not permit particles to inhabit a space smaller than their wavelengths.[9]
$
5
Question 1: What defines the condition of a gravitational singularity?
A: The point where the force of electromagnetism is infinite.
B: The condition where gravity is so intense that spacetime breaks down.
C: The location where light can escape a black hole.
D: The junction between classical physics and quantum mechanics.
E: The boundary where time and space cease to exist.
Answer: B

Question 2: Why can't the properties of a singularity be described without a theory of quantum gravity?
A: It does not involve gravity.
B: It is located at the junction between general relativity and quantum mechanics.
C: It is only relevant in quantum mechanics.
D: It doesn't involve quantum mechanics.
E: It is a well-established concept in classical physics.
Answer: B

Question 3: Where would density become infinite without corrections from quantum mechanics according to general relativity?
A: At the event horizon of a neutron star.
B: At the center of a planet.
C: At the center of a black hole.
D: At the outer edges of a galaxy.
E: Within the core of a sun.
Answer: C

Question 4: What does the Penrose–Hawking singularity theorem define a singularity as?
A: An object with infinite mass.
B: A point in spacetime where gravity becomes repulsive.
C: A boundary beyond which events cannot affect an outside observer.
D: A location with constant scalar invariant curvature.
E: Having geodesics that cannot be extended in a smooth manner.
Answer: E

Question 5: Why can't general relativity or quantum mechanics describe the earliest moments of the Big Bang?
A: They lack a common theoretical foundation.
B: They contradict each other's predictions.
C: Quantum mechanics does not permit particles to inhabit a space smaller than their wavelengths.
D: The Big Bang is purely a hypothetical concept.
E: They both describe the same thing in different words.
Answer: C
@
Many theories in physics have mathematical singularities of one kind or another. Equations for these physical theories predict that the ball of mass of some quantity becomes infinite or increases without limit. This is generally a sign for a missing piece in the theory, as in the ultraviolet catastrophe, re-normalization, and instability of a hydrogen atom predicted by the Larmor formula.

In classical field theories, including special relativity but not general relativity, one can say that a solution has a singularity at a particular point in spacetime where certain physical properties become ill-defined, with spacetime serving as a background field to locate the singularity. A singularity in general relativity, on the other hand, is more complex because spacetime itself becomes ill-defined, and the singularity is no longer part of the regular spacetime manifold. In general relativity, a singularity cannot be defined by "where" or "when".[10]

Some theories, such as the theory of loop quantum gravity, suggest that singularities may not exist.[11] This is also true for such classical unified field theories as the Einstein–Maxwell–Dirac equations. The idea can be stated in the form that, due to quantum gravity effects, there is a minimum distance beyond which the force of gravity no longer continues to increase as the distance between the masses becomes shorter, or alternatively that interpenetrating particle waves mask gravitational effects that would be felt at a distance.
$
5
Question 6: What do mathematical singularities in physical theories usually indicate?
A: Completion of a theory.
B: A missing piece in the theory.
C: A constant value in the theory.
D: The theory is completely correct.
E: The presence of a black hole.
Answer: B

Question 7: How is a singularity defined in classical field theories?
A: A point where the quantum state collapses.
B: A solution where physical properties become ill-defined at a specific point in spacetime.
C: A boundary beyond which light cannot escape.
D: A point where gravity becomes repulsive.
E: The core of a star.
Answer: B

Question 8: Which theory suggests that singularities might not exist?
A: The Schwarzschild black hole theory.
B: The Einstein equation.
C: The theory of special relativity.
D: The theory of loop quantum gravity.
E: The Newtonian theory of gravitation.
Answer: D

Question 9: Due to quantum gravity effects, what is believed regarding gravitational effects at a minimum distance?
A: Gravity increases without limit.
B: The force of gravity no longer continues to increase as distance becomes shorter.
C: Gravity becomes repulsive.
D: Gravitational force becomes infinite.
E: Gravitational effects do not exist.
Answer: B

Question 10: In which theory does spacetime itself become ill-defined at a singularity?
A: Quantum mechanics.
B: Special relativity.
C: General relativity.
D: Electromagnetic theory.
E: Thermodynamics.
Answer: C
@
In astrophysics, an event horizon is a boundary beyond which events cannot affect an observer. Wolfgang Rindler coined the term in the 1950s.[1]

In 1784, John Michell proposed that gravity can be strong enough in the vicinity of massive compact objects that even light cannot escape. At that time, the Newtonian theory of gravitation and the so-called corpuscular theory of light were dominant. In these theories, if the escape velocity of the gravitational influence of a massive object exceeds the speed of light, then light originating inside or from it can escape temporarily but will return. In 1958, David Finkelstein used general relativity to introduce a stricter definition of a local black hole event horizon as a boundary beyond which events of any kind cannot affect an outside observer, leading to information and firewall paradoxes, encouraging the re-examination of the concept of local event horizons and the notion of black holes. Several theories were subsequently developed, some with and some without event horizons. One of the leading developers of theories to describe black holes, Stephen Hawking, suggested that an apparent horizon should be used instead of an event horizon, saying, "Gravitational collapse produces apparent horizons but no event horizons." He eventually concluded that "the absence of event horizons means that there are no black holes – in the sense of regimes from which light can't escape to infinity."[2][3]

Any object approaching the horizon from the observer's side appears to slow down, never quite crossing the horizon.[4] Due to gravitational redshift, its image reddens over time as the object moves away from the observer.[5]
$
5
Question 11: Who coined the term "event horizon"?
A: Stephen Hawking.
B: Albert Einstein.
C: Wolfgang Rindler.
D: Saul Teukolsky.
E: John Michell.
Answer: C

Question 12: What did John Michell propose in 1784?
A: The existence of a cosmic censorship hypothesis.
B: The principle of equivalence in general relativity.
C: Gravity can be so strong near massive objects that even light cannot escape.
D: The presence of naked singularities in the universe.
E: The concept of wormholes in spacetime.
Answer: C

Question 13: Which physicist suggested using an apparent horizon instead of an event horizon?
A: David Finkelstein.
B: John Michell.
C: Saul Teukolsky.
D: Stuart Shapiro.
E: Stephen Hawking.
Answer: E

Question 14: What happens to an object approaching the horizon from the observer's side?
A: It speeds up.
B: It disappears instantly.
C: It appears to slow down, never quite crossing the horizon.
D: Its mass becomes infinite.
E: It reverses direction.
Answer: C

Question 15: Due to gravitational redshift, how does the image of an object change as it moves away from an observer near a horizon?
A: It turns blue.
B: It remains unchanged.
C: It brightens.
D: It turns green.
E: It reddens.
Answer: E
@
There are different types of singularities, each with different physical features which have characteristics relevant to the theories from which they originally emerged, such as the different shape of the singularities, conical and curved. They have also been hypothesized to occur without event horizons, structures which delineate one spacetime section from another in which events cannot affect past the horizon; these are called naked.

A conical singularity occurs when there is a point where the limit of some diffeomorphism invariant quantity does not exist or is infinite, in which case spacetime is not smooth at the point of the limit itself. Thus, spacetime looks like a cone around this point, where the singularity is located at the tip of the cone. The metric can be finite everywhere the coordinate system is used.

An example of such a conical singularity is a cosmic string and a Schwarzschild black hole.[17]

Solutions to the equations of general relativity or another theory of gravity (such as supergravity) often result in encountering points where the metric blows up to infinity. However, many of these points are completely regular, and the infinities are merely a result of using an inappropriate coordinate system at this point. In order to test whether there is a singularity at a certain point, one must check whether at this point diffeomorphism invariant quantities (i.e. scalars) become infinite. Such quantities are the same in every coordinate system, so these infinities will not "go away" by a change of coordinates.

Until the early 1990s, it was widely believed that general relativity hides every singularity behind an event horizon, making naked singularities impossible. This is referred to as the cosmic censorship hypothesis. However, in 1991, physicists Stuart Shapiro and Saul Teukolsky performed computer simulations of a rotating plane of dust that indicated that general relativity might allow for "naked" singularities. What these objects would actually look like in such a model is unknown. Nor is it known whether singularities would still arise if the simplifying assumptions used to make the simulation were removed. However, it is hypothesized that light entering a singularity would similarly have its geodesics terminated, thus making the naked singularity look like a black hole.[19][20][21]
$
5
Question 16: Which type of singularity has spacetime looking like a cone around it?
A: Curved singularity.
B: Naked singularity.
C: Conical singularity.
D: Black hole singularity.
E: Event singularity.
Answer: C

Question 17: What does a cosmic string represent as an example of?
A: A black hole singularity.
B: A naked singularity.
C: A curved singularity.
D: An event horizon.
E: A conical singularity.
Answer: E

Question 18: How can one check for a singularity at a certain point in spacetime?
A: By observing black holes.
B: By checking if the speed of light becomes infinite.
C: By checking whether diffeomorphism invariant quantities become infinite.
D: By observing the event horizon.
E: By analyzing gravitational waves.
Answer: C

Question 19: What was widely believed about singularities in general relativity until the early 1990s?
A: Every singularity is visible.
B: Singularities can exist outside black holes.
C: No singularities exist.
D: Every singularity is hidden behind an event horizon.
E: Singularities can be observed directly.
Answer: D

Question 20: What is hypothesized about light entering a naked singularity?
A: It would be amplified.
B: It would be redirected.
C: Its geodesics would terminate, making it look like a black hole.
D: It would change color.
E: It would speed up indefinitely.
Answer: C
@
In general relativity, Kruskal–Szekeres coordinates, named after Martin Kruskal and George Szekeres, are a coordinate system for the Schwarzschild geometry for a black hole. These coordinates have the advantage that they cover the entire spacetime manifold of the maximally extended Schwarzschild solution and are well-behaved everywhere outside the physical singularity. There is no misleading coordinate singularity at the horizon.

The Kruskal–Szekeres coordinates also apply to space-time around a spherical object, but in that case do not give a description of space-time inside the radius of the object. Space-time in a region where a star is collapsing into a black hole is approximated by the Kruskal–Szekeres coordinates (or by the Schwarzschild coordinates). The surface of the star remains outside the event horizon in the Schwarzschild coordinates, but crosses it in the Kruskal–Szekeres coordinates. (In any "black hole" which we observe, we see it at a time when its matter has not yet finished collapsing, so it is not really a black hole yet.) Similarly, objects falling into a black hole remain outside the event horizon in Schwarzschild coordinates, but cross it in Kruskal–Szekeres coordinates.
$
5
Question: Which coordinates provide a description of the entire spacetime manifold of the maximally extended Schwarzschild solution?
A: Schwarzschild coordinates
B: Lambert W coordinates
C: Kruskal–Szekeres coordinates
D: Minkowski coordinates
E: Cartesian coordinates
Answer: C

Question: Where are Kruskal–Szekeres coordinates well-behaved?
A: Inside the physical singularity
B: Everywhere outside the physical singularity
C: Only at the event horizon
D: In the vicinity of a white hole
E: In non-relativistic scenarios
Answer: B

Question: In the Schwarzschild coordinates, where does an object remain when it falls into a black hole?
A: Inside the event horizon
B: On the event horizon
C: Outside the event horizon
D: At the physical singularity
E: Beyond the black hole
Answer: C

Question: In Kruskal–Szekeres coordinates, where does the surface of a star remain during its collapse into a black hole?
A: Inside the event horizon
B: On the event horizon
C: Outside the event horizon
D: It disappears
E: At the physical singularity
Answer: A
@

Kruskal–Szekeres coordinates on a black hole geometry are defined, from the Schwarzschild coordinates 
(
�
,
�
,
�
,
�
)
(t,r,\theta,\phi), by replacing t and r by a new timelike coordinate T and a new spacelike coordinate 
�
X:

�
=
(
�
2
�
�
−
1
)
1
/
2
�
�
/
4
�
�
sinh
⁡
(
�
4
�
�
)
T = \left(\frac{r}{2GM} - 1\right)^{1/2}e^{r/4GM}\sinh\left(\frac{t}{4GM}\right)
�
=
(
�
2
�
�
−
1
)
1
/
2
�
�
/
4
�
�
cosh
⁡
(
�
4
�
�
)
X = \left(\frac{r}{2GM} - 1\right)^{1/2}e^{r/4GM}\cosh\left(\frac{t}{4GM}\right)
for the exterior region 
�
>
2
�
�
{\displaystyle r>2GM} outside the event horizon and:

�
=
(
1
−
�
2
�
�
)
1
/
2
�
�
/
4
�
�
cosh
⁡
(
�
4
�
�
)
T = \left(1 - \frac{r}{2GM}\right)^{1/2}e^{r/4GM}\cosh\left(\frac{t}{4GM}\right)
�
=
(
1
−
�
2
�
�
)
1
/
2
�
�
/
4
�
�
sinh
⁡
(
�
4
�
�
)
X = \left(1 - \frac{r}{2GM}\right)^{1/2}e^{r/4GM}\sinh\left(\frac{t}{4GM}\right)
for the interior region 
0
<
�
<
2
�
�
0<r<2GM. Here 
�
�
GM is the gravitational constant multiplied by the Schwarzschild mass parameter, and this article is using units where 
�
c = 1.

It follows that on the union of the exterior region, the event horizon and the interior region the Schwarzschild radial coordinate 
�
r (not to be confused with the Schwarzschild radius 
�
s
=
2
�
�
{\displaystyle r_{\text{s}}=2GM}), is determined in terms of Kruskal–Szekeres coordinates as the (unique) solution of the equation:

�
2
−
�
2
=
(
1
−
�
2
�
�
)
�
�
/
2
�
�
 
,
�
2
−
�
2
<
1
{\displaystyle T^{2}-X^{2}=\left(1-{\frac {r}{2GM}}\right)e^{r/2GM}\ ,T^{2}-X^{2}<1}
Using the Lambert W function the solution is written as:

�
=
2
�
�
(
1
+
�
0
(
�
2
−
�
2
�
)
)
{\displaystyle r=2GM\left(1+W_{0}\left({\frac {X^{2}-T^{2}}{e}}\right)\right)}.
Moreover one sees immediately that in the region external to the black hole 
�
2
−
�
2
<
0
,
 
�
>
0
{\displaystyle T^{2}-X^{2}<0,\ X>0}

�
=
4
�
�
a
r
t
a
n
h
⁡
(
�
/
�
)
{\displaystyle t=4GM\mathop {\mathrm {artanh} } (T/X)}
whereas in the region internal to the black hole 
0
<
�
2
−
�
2
<
1
,
 
�
>
0
{\displaystyle 0<T^{2}-X^{2}<1,\ T>0}

�
=
4
�
�
a
r
t
a
n
h
⁡
(
�
/
�
)
{\displaystyle t=4GM\mathop {\mathrm {artanh} } (X/T)}
In these new coordinates the metric of the Schwarzschild black hole manifold is given by

�
=
32
�
3
�
3
�
�
−
�
/
2
�
�
(
−
�
�
2
+
�
�
2
)
+
�
2
�
Ω
,
{\displaystyle g={\frac {32G^{3}M^{3}}{r}}e^{-r/2GM}(-dT^{2}+dX^{2})+r^{2}g_{\Omega },}
written using the (− + + +) metric signature convention and where the angular component of the metric (the Riemannian metric of the 2-sphere) is:

�
Ω
 
=
d
e
f
 
�
�
2
+
sin
2
⁡
�
�
�
2
{\displaystyle g_{\Omega }\ {\stackrel {\mathrm {def} }{=}}\ d\theta ^{2}+\sin ^{2}\theta \,d\phi ^{2}}.
Expressing the metric in this form shows clearly that radial null geodesics i.e. with constant 
Ω
=
Ω
(
�
,
�
)
{\displaystyle \Omega =\Omega (\theta ,\phi )} are parallel to one of the lines 
�
=
±
�
{\displaystyle T=\pm X}. In the Schwarzschild coordinates, the Schwarzschild radius 
�
s
=
2
�
�
{\displaystyle r_{\text{s}}=2GM} is the radial coordinate of the event horizon 
�
=
�
s
=
2
�
�
{\displaystyle r=r_{\text{s}}=2GM}. In the Kruskal–Szekeres coordinates the event horizon is given by 
�
2
−
�
2
=
0
{\displaystyle T^{2}-X^{2}=0}. Note that the metric is perfectly well defined and non-singular at the event horizon. The curvature singularity is located at 
�
2
−
�
2
=
1
T^2 - X^2 = 1.
$
5
Question: In Kruskal–Szekeres coordinates, how is the event horizon determined?
A: 
�
2
−
�
2
=
0
r 
2
 −X 
2
 =0
B: 
�
2
−
�
2
=
0
T 
2
 −X 
2
 =0
C: 
�
2
+
�
2
=
1
T 
2
 +X 
2
 =1
D: 
�
2
+
�
2
=
0
r 
2
 +X 
2
 =0
E: 
�
2
−
�
2
=
1
T 
2
 −X 
2
 =1
Answer: B

Question: Where is the curvature singularity located in terms of Kruskal–Szekeres coordinates?
A: 
�
2
−
�
2
=
1
T 
2
 −X 
2
 =1
B: 
�
2
−
�
2
=
0
T 
2
 −X 
2
 =0
C: 
�
2
+
�
2
=
0
T 
2
 +X 
2
 =0
D: 
�
2
−
�
2
=
1
r 
2
 −X 
2
 =1
E: 
�
2
+
�
2
=
1
r 
2
 +X 
2
 =1
Answer: A

Question: In the Kruskal–Szekeres coordinates, what is the radial coordinate of the event horizon in Schwarzschild coordinates?
A: 
�
=
2
�
�
r=2GM
B: 
�
=
�
�
r=GM
C: 
�
=
�
�
/
2
r=GM/2
D: 
�
=
3
�
�
r=3GM
E: 
�
=
4
�
�
r=4GM
Answer: A

Question: What is the equation that relates Schwarzschild radial coordinate 
�
r with Kruskal–Szekeres coordinates T and X?
A: 
�
2
−
�
2
=
(
1
−
�
2
�
�
)
�
�
/
2
�
�
T 
2
 −X 
2
 =(1− 
2GM
r
​
 )e 
r/2GM
 
B: 
�
=
2
�
�
(
1
+
�
0
(
�
2
−
�
2
�
)
)
r=2GM(1+W 
0
​
 ( 
e
X 
2
 −T 
2
 
​
 ))
C: 
�
2
−
�
2
=
(
1
−
�
2
�
�
)
�
�
/
2
�
�
r 
2
 −X 
2
 =(1− 
2GM
r
​
 )e 
r/2GM
 
D: 
�
=
�
�
(
1
+
�
0
(
�
2
−
�
2
�
)
)
r=GM(1+W 
0
​
 ( 
e
X 
2
 −T 
2
 
​
 ))
E: 
�
=
�
2
−
�
2
r=T 
2
 −X 
2
 
Answer: A
@
Kruskal–Szekeres coordinates have a number of useful features which make them helpful for building intuitions about the Schwarzschild spacetime. Chief among these is the fact that all radial light-like geodesics (the world lines of light rays moving in a radial direction) look like straight lines at a 45-degree angle when drawn in a Kruskal–Szekeres diagram (this can be derived from the metric equation given above, which guarantees that if 
�
�
=
±
�
�
dX = \plusmn dT\, then the proper time 
�
�
=
0
ds = 0).[2] All timelike world lines of slower-than-light objects will at every point have a slope closer to the vertical time axis (the T coordinate) than 45 degrees. So, a light cone drawn in a Kruskal–Szekeres diagram will look just the same as a light cone in a Minkowski diagram in special relativity.

The event horizons bounding the black hole and white hole interior regions are also a pair of straight lines at 45 degrees, reflecting the fact that a light ray emitted at the horizon in a radial direction (aimed outward in the case of the black hole, inward in the case of the white hole) would remain on the horizon forever. Thus the two black hole horizons coincide with the boundaries of the future light cone of an event at the center of the diagram (at T=X=0), while the two white hole horizons coincide with the boundaries of the past light cone of this same event. Any event inside the black hole interior region will have a future light cone that remains in this region (such that any world line within the event's future light cone will eventually hit the black hole singularity, which appears as a hyperbola bounded by the two black hole horizons), and any event inside the white hole interior region will have a past light cone that remains in this region (such that any world line within this past light cone must have originated in the white hole singularity, a hyperbola bounded by the two white hole horizons). Note that although the horizon looks as though it is an outward expanding cone, the area of this surface, given by r is just 
16
�
�
2
16\pi M^2, a constant. I.e., these coordinates can be deceptive if care is not exercised.
$
5
Question: How do radial light-like geodesics appear when drawn on a Kruskal–Szekeres diagram?
A: Curved lines
B: Hyperbolic curves
C: Straight lines at 90-degree angles
D: Elliptical paths
E: Straight lines at a 45-degree angle
Answer: E

Question: What is the slope of the world lines for slower-than-light objects in a Kruskal–Szekeres diagram?
A: Greater than 45 degrees
B: Equal to 45 degrees
C: Less than 45 degrees
D: Horizontal
E: Vertical
Answer: C

Question: What happens to a light ray emitted at the horizon aimed outward in the case of the black hole in a Kruskal–Szekeres diagram?
A: It diverges away from the horizon
B: It converges towards the center
C: It remains on the horizon forever
D: It oscillates around the horizon
E: It disappears into the singularity
Answer: C

Question: How is the area of the horizon surface given in terms of 
�
M in a Kruskal–Szekeres diagram?
A: 
4
�
�
4πM
B: 
8
�
�
2
8πM 
2
 
C: 
16
�
�
16πM
D: 
32
�
�
2
32πM 
2
 
E: 
16
�
�
2
16πM 
2
 
Answer: E
@
The Schwarzschild coordinate system can only cover a single exterior region and a single interior region, such as regions I and II in the Kruskal–Szekeres diagram. The Kruskal–Szekeres coordinate system, on the other hand, can cover a "maximally extended" spacetime which includes the region covered by Schwarzschild coordinates. Here, "maximally extended" refers to the idea that the spacetime should not have any "edges": any geodesic path can be extended arbitrarily far in either direction unless it runs into a gravitational singularity. Technically, this means that a maximally extended spacetime is either "geodesically complete" (meaning any geodesic can be extended to arbitrarily large positive or negative values of its 'affine parameter',[3] which in the case of a timelike geodesic could just be the proper time), or if any geodesics are incomplete, it can only be because they end at a singularity.[4][5] In order to satisfy this requirement, it was found that in addition to the black hole interior region (region II) which particles enter when they fall through the event horizon from the exterior (region I), there has to be a separate white hole interior region (region IV) which allows us to extend the trajectories of particles which an outside observer sees rising up away from the event horizon, along with a separate exterior region (region III) which allows us to extend some possible particle trajectories in the two interior regions. There are actually multiple possible ways to extend the exterior Schwarzschild solution into a maximally extended spacetime, but the Kruskal–Szekeres extension is unique in that it is a maximal, analytic, simply connected vacuum solution in which all maximally extended geodesics are either complete or else the curvature scalar diverges along them in finite affine time.[6]
$
5
Question: What does a "maximally extended" spacetime refer to?
A: A spacetime that is infinitely expansive
B: A spacetime that has no "edges"
C: A spacetime that has multiple black holes
D: A spacetime that exists only in two dimensions
E: A spacetime that is continuously curved
Answer: B

Question: Which coordinate system can cover a "maximally extended" spacetime for the Schwarzschild solution?
A: Schwarzschild coordinates
B: Lambert W coordinates
C: Kruskal–Szekeres coordinates
D: Minkowski coordinates
E: Cartesian coordinates
Answer: C

Question: In a "maximally extended" spacetime, what does it mean for it to be "geodesically complete"?
A: It has multiple gravitational singularities
B: Any geodesic can be extended to arbitrarily large values of its 'affine parameter'
C: It can be perfectly mapped using any coordinate system
D: All geodesics converge at a common point
E: It has a well-defined boundary or edge
Answer: B

Question: In a Kruskal–Szekeres diagram, how are past and future singularities represented?
A: As vertical lines on the left and right, respectively
B: As vertical lines on the top and bottom, respectively
C: As horizontal lines on the left and right, respectively
D: As horizontal lines on the top and bottom, respectively
E: They are not represented in the diagram
Answer: B
@
Quantum gravity (QG) is a field of theoretical physics that seeks to describe gravity according to the principles of quantum mechanics. It deals with environments in which neither gravitational nor quantum effects can be ignored,[1] such as in the vicinity of black holes or similar compact astrophysical objects, such as neutron stars[2][3] as well as in the early stages of the universe moments after the Big Bang.[4]

Three of the four fundamental forces of nature are described within the framework of quantum mechanics and quantum field theory: the electromagnetic interaction, the strong force, and the weak force; this leaves gravity as the only interaction that has not been fully accommodated. The current understanding of gravity is based on Albert Einstein's general theory of relativity, which incorporates his theory of special relativity and deeply modifies the understanding of concepts like time and space. Although General Relativity is highly regarded for its elegance it is not without limitations: the gravitational singularities inside of black holes, the ad hoc postulation of Dark Matter, as well as Dark Energy and its relation to the Cosmological Constant are among the current unsolved mysteries regarding gravity;[5] all of which signal the collapse of the general theory of relativity at different scales and highlight the need for a gravitational theory that goes into the quantum realm. At distances close to the Planck length, like those near the center of the black hole, quantum fluctuations of spacetime are expected to play an important role.[6] The breakdown of general relativity at galactic and cosmological scales also points out the necessity for a more robust theory. Finally the discrepancies between the predicted value for the vacuum energy and the observed values (which, depending on the considerations, can be of 60 or 120 orders of magnitude[7]) highlight the necessity for a quantum theory of gravity.
$
5
Question: What is the primary goal of quantum gravity?
A: Unifying electromagnetic force with gravity
B: Describing gravity according to quantum mechanics
C: Explaining the nature of black holes
D: Defining the concept of time
E: Exploring quantum fluctuations
Answer: B

Question: Which of the following is NOT described within the quantum mechanics framework?
A: Electromagnetic interaction
B: Strong force
C: Weak force
D: Gravity
E: Nuclear forces
Answer: D

Question: What current theory deeply modifies the understanding of time and space?
A: Quantum mechanics
B: Thermodynamics
C: General relativity
D: Quantum field theory
E: String theory
Answer: C

Question: At what distance are quantum fluctuations of spacetime expected to play an important role?
A: Near the center of a star
B: Near the event horizon of a black hole
C: Near the center of the black hole
D: Outside the black hole
E: At the edge of the observable universe
Answer: C

Question: Which value shows a significant discrepancy between its predicted and observed values, emphasizing the need for a quantum theory of gravity?
A: Vacuum energy
B: Quantum fluctuation
C: Electromagnetic force
D: Strong force
E: Weak force
Answer: A
@
The field of quantum gravity is actively developing, and theorists are exploring a variety of approaches to the problem of quantum gravity, the most popular being M-theory and loop quantum gravity.[8] All of these approaches aim to describe the quantum behavior of the gravitational field, which does not necessarily include unifying all fundamental interactions into a single mathematical framework. However, many approaches to quantum gravity, such as string theory, try to develop a framework that describes all fundamental forces. Such a theory is often referred to as a theory of everything. Some of the approaches, such as loop quantum gravity, make no such attempt; instead, they make an effort to quantize the gravitational field while it is kept separate from the other forces. Other lesser-known but no less important theories include Causal dynamical triangulation, Noncommutative geometry, and Twistor theory.[9]

One of the difficulties of formulating a quantum gravity theory is that direct observation of quantum gravitational effects is thought to only appear at length scales near the Planck scale, around 10−35 meters, a scale far smaller, and hence only accessible with far higher energies, than those currently available in high energy particle accelerators. Therefore, physicists lack experimental data which could distinguish between the competing theories which have been proposed.[n.b. 1][n.b. 2]

Thought experiment approaches have been suggested as a testing tool for quantum gravity theories.[10][11] In the field of quantum gravity there are several open questions - e.g., it is not known how spin of elementary particles sources gravity, and thought experiments could provide a pathway to explore possible resolutions to these questions,[12] even in the absence of lab experiments or physical observations.

In the early 21st century, new experiment designs and technologies have arisen which suggest that indirect approaches to testing quantum gravity may be feasible over the next few decades.[13][14][15][16] This field of study is called phenomenological quantum gravity.
$
5
Question: Which theory seeks to unify all fundamental forces into a single framework?
A: Gravitational field theory
B: Loop quantum gravity
C: Causal dynamical triangulation
D: String theory
E: Twistor theory
Answer: D

Question: At what scale are direct observations of quantum gravitational effects believed to occur?
A: Atomic scale
B: Molecular scale
C: Planck scale
D: Stellar scale
E: Galactic scale
Answer: C

Question: Thought experiments in the context of quantum gravity aim to address which of the following issues?
A: Confirming the existence of gravitons
B: Determining the nature of black holes
C: Resolving questions about the spin of elementary particles and gravity
D: Proving string theory
E: Explaining the nature of dark matter
Answer: C

Question: What term refers to the potential experimental testing of quantum gravity?
A: Quantum gravity phenomenology
B: Gravitational entanglement
C: Planck scale exploration
D: Spacetime foam fluctuations
E: Lorentz invariance
Answer: A

Question: The direct observation of quantum gravitational effects is challenging due to:
A: The large scale required
B: The extremely weak nature of the effects
C: The interference of electromagnetic waves
D: The need for high temperature
E: The abundance of dark matter
Answer: B
@
In theories of quantum gravity, the graviton is the hypothetical quantum of gravity, an elementary particle that mediates the force of gravitational interaction. There is no complete quantum field theory of gravitons due to an outstanding mathematical problem with renormalization in general relativity. In string theory, believed by some to be a consistent theory of quantum gravity, the graviton is a massless state of a fundamental string.

If it exists, the graviton is expected to be massless because the gravitational force has a very long range, and appears to propagate at the speed of light. The graviton must be a spin-2 boson because the source of gravitation is the stress–energy tensor, a second-order tensor (compared with electromagnetism's spin-1 photon, the source of which is the four-current, a first-order tensor). Additionally, it can be shown that any massless spin-2 field would give rise to a force indistinguishable from gravitation, because a massless spin-2 field would couple to the stress–energy tensor in the same way gravitational interactions do. This result suggests that, if a massless spin-2 particle is discovered, it must be the graviton.[5]
$
5
Question: What is the proposed quantum of gravity?
A: Photon
B: Proton
C: Electron
D: Neutrino
E: Graviton
Answer: E

Question: Why is the graviton expected to be massless?
A: Because it originates from dark matter
B: Because the gravitational force has a long range
C: Because it is a result of quantum fluctuations
D: Because it is linked to the electromagnetic force
E: Because it is smaller than the Planck scale
Answer: B

Question: Which particle is responsible for mediating the electromagnetic interaction?
A: Gluon
B: Quark
C: Proton
D: Photon
E: Graviton
Answer: D

Question: What would a discovered massless spin-2 particle likely be identified as?
A: A quark
B: A photon
C: A gluon
D: A neutrino
E: A graviton
Answer: E

Question: If the graviton exists and has mass, it would imply:
A: The gravitational force has a short range
B: The gravitational force propagates faster than light
C: The gravitational force is mediated by another particle
D: The gravitational force is a quantum effect
E: The gravitational force is equivalent to electromagnetic force
Answer: A
@
As was emphasized above, quantum gravitational effects are extremely weak and therefore difficult to test. For this reason, the possibility of experimentally testing quantum gravity had not received much attention prior to the late 1990s. However, in the past decade, physicists have realized that evidence for quantum gravitational effects can guide the development of the theory. Since theoretical development has been slow, the field of phenomenological quantum gravity, which studies the possibility of experimental tests, has obtained increased attention.[65]

The most widely pursued possibilities for quantum gravity phenomenology include gravitationally mediated entanglement,[66][67] violations of Lorentz invariance, imprints of quantum gravitational effects in the cosmic microwave background (in particular its polarization), and decoherence induced by fluctuations[68][69][70] in the space-time foam.[71]

ESA's INTEGRAL satellite measured polarization of photons of different wavelengths and was able to place a limit in the granularity of space that is less than 10−48 m, or 13 orders of magnitude below the Planck scale.[72][73]

The BICEP2 experiment detected what was initially thought to be primordial B-mode polarization caused by gravitational waves in the early universe. Had the signal in fact been primordial in origin, it could have been an indication of quantum gravitational effects, but it soon transpired that the polarization was due to interstellar dust interference.[74]
$
5
Question: What is the study of the possibility of experimental tests in quantum gravity called?
A: Quantum gravity postulation
B: Gravitational wave analysis
C: Phenomenological quantum gravity
D: Quantum metric analysis
E: Spacetime foam exploration
Answer: C

Question: Which satellite was used to measure the polarization of photons to infer granularity of space?
A: Hubble Space Telescope
B: Kepler Telescope
C: INTEGRAL satellite
D: James Webb Space Telescope
E: BICEP2
Answer: C

Question: What interfered with the BICEP2 experiment's initial detection of primordial B-mode polarization?
A: Cosmic microwave background radiation
B: Interstellar gas
C: Gamma-ray bursts
D: Interstellar dust
E: Galactic radiation
Answer: D

Question: Violations of which principle might provide evidence for quantum gravity effects?
A: Quantum coherence
B: Heisenberg's uncertainty principle
C: Schrödinger's wave equation
D: Lorentz invariance
E: Hubble's law of cosmic expansion
Answer: D

Question: Which of the following is NOT mentioned as a possibility for quantum gravity phenomenology?
A: Gravitational singularities
B: Gravitationally mediated entanglement
C: Decoherence induced by fluctuations in space-time foam
D: Quantum gravitational effects in the cosmic microwave background
E: Lorentz invariance violations
Answer: A
@
In condensed matter physics, a string-net is an extended object whose collective behavior has been proposed as a physical mechanism for topological order by Michael A. Levin and Xiao-Gang Wen. A particular string-net model may involve only closed loops; or networks of oriented, labeled strings obeying branching rules given by some gauge group; or still more general networks.[1]

The string-net model is claimed to show the derivation of photons, electrons, and U(1) gauge charge, small (relative to the Planck mass) but nonzero masses, and suggestions that the leptons, quarks, and gluons can be modeled in the same way. In other words, string-net condensation provides a unified origin for photons and electrons (or gauge bosons and fermions). It can be viewed as an origin of light and electron (or gauge interactions and Fermi statistics). However, their model does not account for the chiral coupling between the fermions and the SU(2) gauge bosons in the standard model.

For strings labeled by the positive integers, string-nets are the spin networks studied in loop quantum gravity. This has led to the proposal by Levin and Wen,[2] and Smolin, Markopoulou and Konopka[3] that loop quantum gravity's spin networks can give rise to the standard model of particle physics through this mechanism, along with fermi statistics and gauge interactions. To date, a rigorous derivation from LQG's spin networks to Levin and Wen's spin lattice has yet to be done, but the project to do so is called quantum graphity, and in a more recent paper, Tomasz Konopka, Fotini Markopoulou, Simone Severini argued that there are some similarities to spin networks (but not necessarily an exact equivalence) that gives rise to U(1) gauge charge and electrons in the string net mechanism.[4]

Herbertsmithite may be an example of string-net matter.[5][6]
$
5
Question: Which of the following is an example of string-net matter?
A: Helium-3
B: Fermion
C: Herbertsmithite
D: Topological insulator
E: Spin-liquid
Answer: C

Question: String-net condensation provides a unified origin for which of the following?
A: Protons and neutrons
B: Photons and electrons
C: Muons and neutrinos
D: Gluons and bosons
E: Leptons and mesons
Answer: B

Question: The spin networks studied in loop quantum gravity when labeled by positive integers are equivalent to?
A: U(1) gauge charge
B: Topological insulators
C: String-nets
D: Fermi statistics
E: Chiral coupling
Answer: C

Question: The string-net model does NOT account for which of the following?
A: U(1) gauge charge
B: Chiral coupling between fermions and SU(2) gauge bosons
C: The behavior of photons
D: The characteristics of electrons
E: Gauge interactions
Answer: B

Question: What project aims to derive from LQG's spin networks to Levin and Wen's spin lattice?
A: Topological quantum computer
B: Quantum topology
C: Quantum entanglement
D: Quantum graphity
E: Quantum Hall effect
Answer: D
@
In physics, topological order[1] is a kind of order in the zero-temperature phase of matter (also known as quantum matter). Macroscopically, topological order is defined and described by robust ground state degeneracy[2] and quantized non-Abelian geometric phases of degenerate ground states.[1] Microscopically, topological orders correspond to patterns of long-range quantum entanglement.[3] States with different topological orders (or different patterns of long range entanglements) cannot change into each other without a phase transition.

Various topologically ordered states have interesting properties, such as (1) topological degeneracy and fractional statistics or non-Abelian statistics that can be used to realize a topological quantum computer; (2) perfect conducting edge states that may have important device applications; (3) emergent gauge field and Fermi statistics that suggest a quantum information origin of elementary particles;[4] (4) topological entanglement entropy that reveals the entanglement origin of topological order, etc. Topological order is important in the study of several physical systems such as spin liquids[5][6][7][8] and the quantum Hall effect,[9][10] along with potential applications to fault-tolerant quantum computation.[11]

Topological insulators[12] and topological superconductors (beyond 1D) do not have topological order as defined above, their entanglements being only short-ranged.
$
5
Question: What is NOT a property of various topologically ordered states?
A: Topological quantum computer feasibility
B: Emergent gauge field and Fermi statistics
C: Perfect conducting edge states
D: Chiral coupling
E: Topological entanglement entropy
Answer: D

Question: States with different topological orders cannot transform into each other without what?
A: Fusion
B: Quantum entanglement
C: Phase transition
D: Ground state degeneracy
E: Quantum fluctuation
Answer: C

Question: Topological insulators and topological superconductors in dimensions beyond 1D are characterized by?
A: Short-ranged entanglements
B: Long-range quantum entanglement
C: Quantized non-Abelian geometric phases
D: Robust ground state degeneracy
E: Quantum graphity
Answer: A

Question: What system is NOT associated with the study of topological order?
A: Spin liquids
B: Quantum Hall effect
C: Fermi-Dirac distribution
D: Topological quantum computation
E: Topological insulators
Answer: C

Question: What defines macroscopic topological order?
A: High temperature phase of matter
B: Non-quantized Abelian geometric phases
C: Short-range quantum entanglement
D: Robust ground state degeneracy
E: Maxwell-Boltzmann statistics
Answer: D
@
We know that group theory is the mathematical foundation of symmetry-breaking orders. What is the mathematical foundation of topological order? It was found that a subclass of 2+1D topological orders—Abelian topological orders—can be classified by a K-matrix approach.[40][41][42][43] The string-net condensation suggests that tensor category (such as fusion category or monoidal category) is part of the mathematical foundation of topological order in 2+1D. The more recent researches suggest that (up to invertible topological orders that have no fractionalized excitations):

2+1D bosonic topological orders are classified by unitary modular tensor categories.
2+1D bosonic topological orders with symmetry G are classified by G-crossed tensor categories.
2+1D bosonic/fermionic topological orders with symmetry G are classified by unitary braided fusion categories over symmetric fusion category, that has modular extensions. The symmetric fusion category Rep(G) for bosonic systems and sRep(G) for fermionic systems.
Topological order in higher dimensions may be related to n-Category theory. Quantum operator algebra is a very important mathematical tool in studying topological orders.

Some also suggest that topological order is mathematically described by extended quantum symmetry.[44]
$
5
Question: Which mathematical approach is believed to classify 2+1D bosonic topological orders?
A: Quantum operator algebra
B: n-Category theory
C: Extended quantum symmetry
D: Unitary modular tensor categories
E: Quantum field theory
Answer: D

Question: Quantum operator algebra is an essential tool in studying what?
A: Fermi-Dirac statistics
B: Bose–Einstein statistics
C: Extended quantum symmetry
D: Topological orders
E: Quantum Hall effect
Answer: D

Question: Which theory is NOT related to the classification of topological order in higher dimensions?
A: Tensor category
B: n-Category theory
C: K-matrix approach
D: G-crossed tensor categories
E: Unitary braided fusion categories
Answer: C

Question: The string-net condensation suggests that what is part of the mathematical foundation of topological order in 2+1D?
A: Quantum operator algebra
B: n-Category theory
C: Fusion category
D: Maxwell-Boltzmann statistics
E: Fermi statistics
Answer: C

Question: 2+1D bosonic topological orders with symmetry G are classified by?
A: n-Category theory
B: G-crossed tensor categories
C: Unitary modular tensor categories
D: Quantum operator algebra
E: Fusion category
Answer: B
@
Fermi–Dirac statistics is a type of quantum statistics that applies to the physics of a system consisting of many non-interacting, identical particles that obey the Pauli exclusion principle. A result is the Fermi–Dirac distribution of particles over energy states. It is named after Enrico Fermi and Paul Dirac, each of whom derived the distribution independently in 1926 (although Fermi derived it before Dirac).[1][2] Fermi–Dirac statistics is a part of the field of statistical mechanics and uses the principles of quantum mechanics.

Fermi–Dirac statistics applies to identical and indistinguishable particles with half-integer spin (1/2, 3/2, etc.), called fermions, in thermodynamic equilibrium. For the case of negligible interaction between particles, the system can be described in terms of single-particle energy states. A result is the Fermi–Dirac distribution of particles over these states where no two particles can occupy the same state, which has a considerable effect on the properties of the system. Fermi–Dirac statistics is most commonly applied to electrons, a type of fermion with spin 1/2.

A counterpart to Fermi–Dirac statistics is Bose–Einstein statistics, which applies to identical and indistinguishable particles with integer spin (0, 1, 2, etc.) called bosons. In classical physics, Maxwell–Boltzmann statistics is used to describe particles that are identical and treated as distinguishable. For both Bose–Einstein and Maxwell–Boltzmann statistics, more than one particle can occupy the same state, unlike Fermi–Dirac statistics.
$
5
Question: Fermi–Dirac statistics apply to particles with which spin characteristic?
A: Integer spin
B: Zero spin
C: Quarter spin
D: Half-integer spin
E: Double spin
Answer: D

Question: Fermi–Dirac statistics is a part of which field?
A: Classical mechanics
B: Statistical mechanics
C: Newtonian physics
D: Electromagnetism
E: Thermodynamics
Answer: B

Question: Fermi–Dirac statistics prevents what from occurring?
A: Two particles occupying different states
B: Particles from having half-integer spin
C: More than one particle from occupying the same state
D: Particles from interacting
E: Particles from achieving thermodynamic equilibrium
Answer: C

Question: Which particles are most commonly described by Fermi–Dirac statistics?
A: Gluons
B: Bosons
C: Protons
D: Electrons
E: Neutrinos
Answer: D

Question: What statistics describes identical particles with integer spin?
A: Fermi–Dirac statistics
B: Maxwell–Boltzmann statistics
C: Einstein statistics
D: Newtonian statistics
E: Bose–Einstein statistics
Answer: E
@
The Standard Model recognizes two types of elementary fermions: quarks and leptons. In all, the model distinguishes 24 different fermions. There are six quarks (up, down, strange, charm, bottom and top), and six leptons (electron, electron neutrino, muon, muon neutrino, tauon and tauon neutrino), along with the corresponding antiparticle of each of these.

Mathematically, there are many varieties of fermions, with the three most common types being:

Weyl fermions (massless),
Dirac fermions (massive), and
Majorana fermions (each its own antiparticle).
Most Standard Model fermions are believed to be Dirac fermions, although it is unknown at this time whether the neutrinos are Dirac or Majorana fermions (or both). Dirac fermions can be treated as a combination of two Weyl fermions.[3]: 106  In July 2015, Weyl fermions have been experimentally realized in Weyl semimetals.
$
5
Question: How many different fermions are recognized by the Standard Model?
A: 12
B: 6
C: 24
D: 3
E: 9
Answer: C

Question: Which fermion is each its own antiparticle?
A: Weyl fermion
B: Dirac fermion
C: Majorana fermion
D: Up quark
E: Electron
Answer: C

Question: Which type of fermion has been experimentally realized in Weyl semimetals?
A: Majorana fermion
B: Dirac fermion
C: Weyl fermion
D: Up quark
E: Tauon neutrino
Answer: C

Question: The Standard Model distinguishes how many types of quarks?
A: 3
B: 4
C: 6
D: 5
E: 2
Answer: C

Question: It is currently unknown whether which particle is a Dirac or Majorana fermion?
A: Proton
B: Electron
C: Neutrino
D: Photon
E: Muon
Answer: C
@
Composite particles (such as hadrons, nuclei, and atoms) can be bosons or fermions depending on their constituents. More precisely, because of the relation between spin and statistics, a particle containing an odd number of fermions is itself a fermion. It will have half-integer spin.

Examples include the following:

A baryon, such as the proton or neutron, contains three fermionic quarks.
The nucleus of a carbon-13 atom contains six protons and seven neutrons.
The atom helium-3 (3He) consists of two protons, one neutron, and two electrons. The deuterium atom consists of one proton, one neutron, and one electron.
The number of bosons within a composite particle made up of simple particles bound with a potential has no effect on whether it is a boson or a fermion.

Fermionic or bosonic behavior of a composite particle (or system) is only seen at large (compared to size of the system) distances. At proximity, where spatial structure begins to be important, a composite particle (or system) behaves according to its constituent makeup.

Fermions can exhibit bosonic behavior when they become loosely bound in pairs. This is the origin of superconductivity and the superfluidity of helium-3: in superconducting materials, electrons interact through the exchange of phonons, forming Cooper pairs, while in helium-3, Cooper pairs are formed via spin fluctuations.

The quasiparticles of the fractional quantum Hall effect are also known as composite fermions; they consist of electrons with an even number of quantized vortices attached to them.
$
5
Question: A particle containing how many fermions will itself be a fermion?
A: An odd number
B: An even number
C: A prime number
D: Zero
E: Any number
Answer: A

Question: What is the origin of superconductivity in materials?
A: Phonon exchange forming Dirac fermions
B: Phonon exchange forming Cooper pairs
C: Phonon exchange forming quarks
D: Vortex exchange forming Weyl fermions
E: Vortex exchange forming Majorana fermions
Answer: B

Question: The quasiparticles of the fractional quantum Hall effect are known as?
A: Weyl fermions
B: Cooper pairs
C: Phonons
D: Composite fermions
E: Majorana fermions
Answer: D

Question: A helium-3 atom is made up of how many protons?
A: 1
B: 2
C: 3
D: 4
E: 5
Answer: B

Question: Fermions can exhibit which behavior when they become loosely bound in pairs?
A: Magnetic
B: Elastic
C: Bosonic
D: Dielectric
E: Neutronic
Answer: C
@
In particle physics, a boson (/ˈboʊzɒn/[1] /ˈboʊsɒn/[2]) is a subatomic particle whose spin quantum number has an integer value (0, 1, 2, ...). Bosons form one of the two fundamental classes of subatomic particle, the other being fermions, which have odd half-integer spin (1⁄2, 3⁄2, 5⁄2, ...). Every observed subatomic particle is either a boson or a fermion.

Some bosons are elementary particles occupying a special role in particle physics, distinct from the role of fermions (which are sometimes described as the constituents of "ordinary matter"). Certain elementary bosons (e.g. gluons) act as force carriers, which give rise to forces between other particles, while one (the Higgs boson) contributes to the phenomenon of mass. Other bosons, such as mesons, are composite particles made up of smaller constituents.

Outside the realm of particle physics, multiple identical composite bosons (in this context sometimes known as 'bose particles') behave at high densities or low temperatures in a characteristic manner described by Bose–Einstein statistics: for example a gas of helium-4 atoms becomes a superfluid at temperatures close to absolute zero. Similarly, superconductivity arises because some quasiparticles, such as Cooper pairs, behave in the same way.
$
5
Question: Which subatomic particle class has an odd half-integer spin quantum number?
A: Mesons
B: Fermions
C: Bosons
D: Quarks
E: Leptons
Answer: B

Question: The Higgs boson is associated with which phenomenon?
A: Electromagnetism
B: Strong force
C: Mass
D: Spin
E: Weak force
Answer: C

Question: Which of these particles is not an elementary boson?
A: Gluon
B: Meson
C: Photon
D: Higgs boson
E: Neutral weak boson
Answer: B

Question: Helium-4 atoms exhibit a specific behavior at temperatures close to absolute zero, what is this behavior?
A: Superfluidity
B: Superconductivity
C: Supersymmetry
D: Superposition
E: Supertransparency
Answer: A

Question: Composite particles with what behavior arise due to some quasiparticles like Cooper pairs?
A: Radioactivity
B: Superconductivity
C: Fluidity
D: Elasticity
E: Transparency
Answer: B
@
All observed elementary particles are either bosons (with integer spin) or fermions (with odd half-integer spin).[8] Whereas the elementary particles that make up ordinary matter (leptons and quarks) are fermions, elementary bosons occupy a special role in particle physics. They act either as force carriers which give rise to forces between other particles, or in one case give rise to the phenomenon of mass.

According to the Standard Model of Particle Physics there are five elementary bosons:

One scalar boson (spin = 0)

H0
 Higgs boson – the particle that contributes to the phenomenon of mass via the Higgs mechanism
Four vector bosons (spin = 1) that act as force carriers. These are the gauge bosons:

γ
   Photon – the force carrier of the electromagnetic field

g
   Gluons (eight different types) – force carriers that mediate the strong force

Z
   Neutral weak boson – the force carrier that mediates the weak force

W±
   Charged weak bosons (two types) – also force carriers that mediate the weak force
A second order tensor boson (spin = 2) called the graviton (G) has been hypothesised as the force carrier for gravity, but so far all attempts to incorporate gravity into the Standard Model have failed.[a]

Composite particles (such as hadrons, nuclei, and atoms) can be bosons or fermions depending on their constituents. Since bosons have integer spin and fermions odd half-integer spin, any composite particle made up of an even number of fermions is a boson.

Composite bosons include:

All types of meson
Stable nuclei of even mass number such as deuterium, helium-4 (the alpha particle),[9] carbon-12 and lead-208.[b]
$
5
Question: Which elementary boson gives rise to the phenomenon of mass?
A: Photon
B: Gluon
C: Neutral weak boson
D: Higgs boson
E: Charged weak boson
Answer: D

Question: How many elementary bosons are there according to the Standard Model of Particle Physics?
A: 3
B: 4
C: 5
D: 6
E: 7
Answer: C

Question: The graviton, hypothesized as the force carrier for gravity, has what spin?
A: 0
B: 1
C: 1/2
D: 2
E: 3
Answer: D

Question: Any composite particle made up of an even number of fermions is a?
A: Meson
B: Fermion
C: Photon
D: Neutrino
E: Boson
Answer: E

Question: Which of the following is NOT an elementary boson?
A: Gluon
B: Photon
C: Deuterium
D: Neutral weak boson
E: Charged weak boson
Answer: C
@
The behavior of the electromagnetic field can be divided into four different parts of a loop:[7]

the electric and magnetic fields are generated by moving electric charges,
the electric and magnetic fields interact with each other,
the electric and magnetic fields produce forces on electric charges,
the electric charges move in space.
A common misunderstanding is that (a) the quanta of the fields act in the same manner as (b) the charged particles, such as electrons, that generate the fields. In our everyday world, electrons travel slowly through conductors with a drift velocity of a fraction of a centimeter per second and through a vacuum tube at speeds of around 1000 km/s,[8] but fields propagate at the speed of light, approximately 300 000 kilometers (or 186 000 miles) per second. The speed ratio between charged particles in a conductor and field quanta is on the order of one to a million. Maxwell's equations relate (a) the presence and movement of charged particles with (b) the generation of fields. Those fields can then affect the force on, and can then move other slowly moving charged particles. Charged particles can move at relativistic speeds nearing field propagation speeds, but, as Albert Einstein showed[citation needed], this requires enormous field energies, which are not present in our everyday experiences with electricity, magnetism, matter, and time and space.
$
5
Question: Which phenomenon is a common misunderstanding related to the quanta of the fields and charged particles?
A: That quanta of fields travel slower than charged particles.
B: That charged particles produce fields that move at the same speed as themselves.
C: That fields propagate at the same speed as electrons in a conductor.
D: That field energies always require enormous energies.
E: That charged particles and quanta of the fields act in the same manner.
Answer: E

Question: What is the speed of propagation of fields?
A: 1000 km/s
B: A fraction of a centimeter per second
C: 500,000 kilometers per second
D: 300,000 kilometers per second
E: One million kilometers per second
Answer: D

Question: Which scientist showed that reaching relativistic speeds requires enormous field energies?
A: James Clerk Maxwell
B: Isaac Newton
C: Niels Bohr
D: Albert Einstein
E: Michael Faraday
Answer: D

Question: How do electrons travel through conductors?
A: At the speed of light
B: At speeds of around 1000 km/s in a vacuum
C: With a drift velocity of a fraction of a centimeter per second
D: At relativistic speeds
E: At a consistent speed regardless of the medium
Answer: C

Question: Maxwell's equations relate the presence and movement of charged particles with the generation of what?
A: Gravitational fields
B: Thermal energy
C: Electromagnetic fields
D: Quantum fields
E: Radio waves
Answer: C
@
The feedback loop can be summarized in a list, including phenomena belonging to each part of the loop:[citation needed]

charged particles generate electric and magnetic fields
the fields interact with each other
changing electric field acts like a current, generating 'vortex' of magnetic field
Faraday induction: changing magnetic field induces (negative) vortex of electric field
Lenz's law: negative feedback loop between electric and magnetic fields
fields act upon particles
Lorentz force: force due to electromagnetic field
electric force: same direction as electric field
magnetic force: perpendicular both to magnetic field and to velocity of charge
charged particles move
current is movement of particles
charged particles generate more electric and magnetic fields; cycle repeats
$
5
Question: What generates electric and magnetic fields?
A: Neutrons
B: Protons
C: Charged particles
D: Free electrons
E: Photons
Answer: C

Question: What is the principle behind the electric generator?
A: A changing electric field creates a magnetic field.
B: A magnetic force acts perpendicular to the magnetic field.
C: A changing magnetic field creates an electric field.
D: Lorentz force due to electromagnetic field.
E: Negative feedback loop between electric and magnetic fields.
Answer: C

Question: Which force is perpendicular both to the magnetic field and to the velocity of the charge?
A: Electric force
B: Lorentz force
C: Gravitational force
D: Magnetic force
E: Nuclear force
Answer: D

Question: What is the movement of charged particles called?
A: Current
B: Vortex
C: Feedback
D: Induction
E: Resistance
Answer: A

Question: Which law refers to the negative feedback loop between electric and magnetic fields?
A: Ampère's Law
B: Faraday's Law
C: Ohm's Law
D: Lenz's law
E: Kirchhoff's law
Answer: D
@
Reciprocal behavior of electric and magnetic fields
The two Maxwell equations, Faraday's Law and the Ampère-Maxwell Law, illustrate a very practical feature of the electromagnetic field. Faraday's Law may be stated roughly as 'a changing magnetic field creates an electric field'. This is the principle behind the electric generator.

Ampere's Law roughly states that 'a changing electric field creates a magnetic field'. Thus, this law can be applied to generate a magnetic field and run an electric motor.

Behavior of the fields in the absence of charges or currents
Maxwell's equations take the form of an electromagnetic wave in a volume of space not containing charges or currents (free space) – that is, where 
�\rho  and J are zero. Under these conditions, the electric and magnetic fields satisfy the electromagnetic wave equation:[11]

(
∇
2
−
1
�
2
∂
2
∂
�
2
)
�
 
 
=
 
 
0
{\displaystyle \left(\nabla ^{2}-{1 \over {c}^{2}}{\partial ^{2} \over \partial t^{2}}\right)\mathbf {E} \ \ =\ \ 0}
(
∇
2
−
1
�
2
∂
2
∂
�
2
)
�
 
 
=
 
 
0
{\displaystyle \left(\nabla ^{2}-{1 \over {c}^{2}}{\partial ^{2} \over \partial t^{2}}\right)\mathbf {B} \ \ =\ \ 0}
James Clerk Maxwell was the first to obtain this relationship by his completion of Maxwell's equations with the addition of a displacement current term to Ampere's circuital law.
$
5
Question: What does Faraday's Law roughly state?
A: A changing electric field creates a magnetic field.
B: A changing magnetic field creates an electric field.
C: Electric and magnetic fields are mutually exclusive.
D: Electric fields move charged particles in space.
E: Magnetic fields act upon moving charges.
Answer: B

Question: Which law can be applied to run an electric motor?
A: Faraday's Law
B: Ampère's Law
C: Maxwell's equation
D: Einstein's equation
E: Newton's third law
Answer: B

Question: Who was the first to obtain the relationship described by the electromagnetic wave equation?
A: Albert Einstein
B: Nikola Tesla
C: Michael Faraday
D: James Clerk Maxwell
E: Richard Feynman
Answer: D

Question: What does Ampère's Law roughly state?
A: A changing magnetic field creates an electric field.
B: A magnetic field can move electric charges.
C: A changing electric field creates a magnetic field.
D: Electric fields are always perpendicular to magnetic fields.
E: Electric fields can induce magnetic fields without change.
Answer: C

Question: In which conditions do the electric and magnetic fields satisfy the electromagnetic wave equation?
A: In charged space
B: In a magnetic field
C: In free space not containing charges or currents
D: Near high-energy particles
E: Within conductors only
Answer: C
@
An EM field that varies in time has two "causes" in Maxwell's equations. One is charges and currents (so-called "sources"), and the other cause for an E or M field is a change in the other type of field (this last cause also appears in "free space" very far from currents and charges).

An electromagnetic field very far from currents and charges (sources) is called electromagnetic radiation (EMR) since it radiates from the charges and currents in the source, and has no "feedback" effect on them, and is also not affected directly by them in the present time (rather, it is indirectly produced by a sequences of changes in fields radiating out from them in the past). EMR consists of the radiations in the electromagnetic spectrum, including radio waves, microwave, infrared, visible light, ultraviolet light, X-rays, and gamma rays. The many commercial applications of these radiations are discussed in the named and linked articles.

A notable application of visible light is that this type of energy from the Sun powers all life on Earth that either makes or uses oxygen.

A changing electromagnetic field which is physically close to currents and charges (see near and far field for a definition of "close") will have a dipole characteristic that is dominated by either a changing electric dipole, or a changing magnetic dipole. This type of dipole field near sources is called an electromagnetic near-field.
$
5
Question: What is called an electromagnetic field far from currents and charges?
A: Electromagnetic spectrum
B: Electromagnetic radiation
C: Electromagnetic wave
D: Electromagnetic feedback
E: Electromagnetic dipole
Answer: B

Question: Which type of energy from the Sun powers all life on Earth that either makes or uses oxygen?
A: X-rays
B: Infrared radiation
C: Gamma rays
D: Radio waves
E: Visible light
Answer: E

Question: Electromagnetic radiation includes all of the following EXCEPT:
A: Radio waves
B: Microwaves
C: Ultraviolet light
D: Alpha particles
E: X-rays
Answer: D

Question: A changing electromagnetic field close to currents and charges will have what characteristic?
A: Electromagnetic radiation
B: Electromagnetic feedback
C: Electromagnetic spectrum
D: Electromagnetic near-field
E: Electromagnetic far-field
Answer: D

Question: What is NOT a cause for a changing E or M field in Maxwell's equations?
A: A change in the other type of field
B: Changes in temperature
C: Charges and currents
D: Electromagnetic radiation
E: Displacement current term
Answer: B
@
Early attempts to quantitatively describe the electromagnetic force were made in the mid-18th century. It was proposed that the force on magnetic poles, by Johann Tobias Mayer and others in 1760,[16] and electrically charged objects, by Henry Cavendish in 1762,[17] obeyed an inverse-square law. However, in both cases the experimental proof was neither complete nor conclusive. It was not until 1784 when Charles-Augustin de Coulomb, using a torsion balance, was able to definitively show through experiment that this was true.[18] Soon after the discovery in 1820 by Hans Christian Ørsted that a magnetic needle is acted on by a voltaic current, André-Marie Ampère that same year was able to devise through experimentation the formula for the angular dependence of the force between two current elements.[19][20] In all these descriptions, the force was always described in terms of the properties of the matter involved and the distances between two masses or charges rather than in terms of electric and magnetic fields.[21]

The modern concept of electric and magnetic fields first arose in the theories of Michael Faraday, particularly his idea of lines of force, later to be given full mathematical description by Lord Kelvin and James Clerk Maxwell.[22] From a modern perspective it is possible to identify in Maxwell's 1865 formulation of his field equations a form of the Lorentz force equation in relation to electric currents,[4] although in the time of Maxwell it was not evident how his equations related to the forces on moving charged objects. J. J. Thomson was the first to attempt to derive from Maxwell's field equations the electromagnetic forces on a moving charged object in terms of the object's properties and external fields. Interested in determining the electromagnetic behavior of the charged particles in cathode rays, Thomson published a paper in 1881 wherein he gave the force on the particles due to an external magnetic field as[6][23]

�
=
�
2
�
×
�
.
{\displaystyle \mathbf {F} ={\frac {q}{2}}\mathbf {v} \times \mathbf {B} .}
$
5
Question: Which scientist using a torsion balance demonstrated the inverse-square law for forces on charged objects?
A: Michael Faraday
B: J. J. Thomson
C: Henry Cavendish
D: Charles-Augustin de Coulomb
E: André-Marie Ampère
Answer: D

Question: In which year did Hans Christian Ørsted discover that a magnetic needle is acted on by a voltaic current?
A: 1820
B: 1762
C: 1784
D: 1881
E: 1760
Answer: A

Question: Whose theories introduced the modern concept of electric and magnetic fields?
A: Johann Tobias Mayer
B: Lord Kelvin
C: Charles-Augustin de Coulomb
D: Michael Faraday
E: Henry Cavendish
Answer: D

Question: Who first tried to derive electromagnetic forces on a moving charged object from Maxwell's field equations?
A: André-Marie Ampère
B: J. J. Thomson
C: Charles-Augustin de Coulomb
D: Henry Cavendish
E: Michael Faraday
Answer: B

Question: Which scientist's experimentations in the 1820s formulated the angular dependence of the force between two current elements?
A: Michael Faraday
B: Hans Christian Ørsted
C: André-Marie Ampère
D: Charles-Augustin de Coulomb
E: Johann Tobias Mayer
Answer: C
@
Thomson derived the correct basic form of the formula, but, because of some miscalculations and an incomplete description of the displacement current, included an incorrect scale-factor of a half in front of the formula. Oliver Heaviside invented the modern vector notation and applied it to Maxwell's field equations; he also (in 1885 and 1889) had fixed the mistakes of Thomson's derivation and arrived at the correct form of the magnetic force on a moving charged object.[6][24][25] Finally, in 1895,[5][26] Hendrik Lorentz derived the modern form of the formula for the electromagnetic force which includes the contributions to the total force from both the electric and the magnetic fields. Lorentz began by abandoning the Maxwellian descriptions of the ether and conduction. Instead, Lorentz made a distinction between matter and the luminiferous aether and sought to apply the Maxwell equations at a microscopic scale. Using Heaviside's version of the Maxwell equations for a stationary ether and applying Lagrangian mechanics (see below), Lorentz arrived at the correct and complete form of the force law that now bears his name.[27][28]
$
5
Question: Who corrected the mistakes in Thomson's derivation of the formula for the electromagnetic force?
A: Hendrik Lorentz
B: Michael Faraday
C: Oliver Heaviside
D: James Clerk Maxwell
E: Lord Kelvin
Answer: C

Question: In which year did Hendrik Lorentz derive the modern form of the formula for the electromagnetic force?
A: 1895
B: 1865
C: 1885
D: 1889
E: 1881
Answer: A

Question: Lorentz used which form of mechanics to arrive at his force law?
A: Newtonian Mechanics
B: Quantum Mechanics
C: Classical Mechanics
D: Relativistic Mechanics
E: Lagrangian Mechanics
Answer: E

Question: Who initiated modern vector notation?
A: J. J. Thomson
B: Oliver Heaviside
C: Hendrik Lorentz
D: James Clerk Maxwell
E: Michael Faraday
Answer: B

Question: Thomson's formula had an incorrect scale-factor of:
A: one-fourth
B: one
C: one-half
D: two
E: three-fourths
Answer: C
@
While the modern Maxwell's equations describe how electrically charged particles and currents or moving charged particles give rise to electric and magnetic fields, the Lorentz force law completes that picture by describing the force acting on a moving point charge q in the presence of electromagnetic fields.[12][29] The Lorentz force law describes the effect of E and B upon a point charge, but such electromagnetic forces are not the entire picture. Charged particles are possibly coupled to other forces, notably gravity and nuclear forces. Thus, Maxwell's equations do not stand separate from other physical laws, but are coupled to them via the charge and current densities. The response of a point charge to the Lorentz law is one aspect; the generation of E and B by currents and charges is another.

In real materials the Lorentz force is inadequate to describe the collective behavior of charged particles, both in principle and as a matter of computation. The charged particles in a material medium not only respond to the E and B fields but also generate these fields. Complex transport equations must be solved to determine the time and spatial response of charges, for example, the Boltzmann equation or the Fokker–Planck equation or the Navier–Stokes equations. For example, see magnetohydrodynamics, fluid dynamics, electrohydrodynamics, superconductivity, stellar evolution. An entire physical apparatus for dealing with these matters has developed. See for example, Green–Kubo relations and Green's function (many-body theory).
$
5
Question: The Lorentz force law complements Maxwell's equations by describing the force on a moving charge in the presence of:
A: Nuclear fields
B: Electromagnetic fields
C: Gravitational fields
D: Ether
E: Thermal fields
Answer: B

Question: Apart from electromagnetic forces, charged particles could be influenced by:
A: Kinetic forces
B: Dark forces
C: Entropic forces
D: Nuclear forces
E: Cohesive forces
Answer: D

Question: Which equation is commonly used for understanding complex transport of charged particles in a material medium?
A: Pythagorean theorem
B: Navier–Stokes equations
C: Kepler's laws
D: Newton's second law
E: Bernoulli's equation
Answer: B

Question: In real materials, what is one reason why the Lorentz force can be seen as inadequate to describe the behavior of charged particles?
A: Particles only respond to gravitational fields
B: Charged particles only generate electric fields
C: Charged particles in a material both respond to and generate E and B fields
D: Lorentz force is only applicable to stationary particles
E: Particles do not have any charge
Answer: C

Question: For analyzing the behavior of charged particles, what has developed as a physical apparatus?
A: Classical mechanics tools
B: Lagrangian methods
C: Green–Kubo relations
D: Thermodynamics equations
E: Relativity postulates
Answer: C
@
Hendrik Antoon Lorentz (/ˈlɒrənts/; 18 July 1853 – 4 February 1928) was a Dutch physicist who shared the 1902 Nobel Prize in Physics with Pieter Zeeman for the discovery and theoretical explanation of the Zeeman effect. He derived the Lorentz transformation of the special theory of relativity, as well as the Lorentz force, which describes the combined electric and magnetic forces acting on a charged particle in an electromagnetic field. Lorentz was also responsible for the Lorentz oscillator model, a classical model used to describe the anomalous dispersion observed in dielectric materials when the driving frequency of the electric field was near the resonant frequency, resulting in abnormal refractive indices.

According to the biography published by the Nobel Foundation, "It may well be said that Lorentz was regarded by all theoretical physicists as the world's leading spirit, who completed what was left unfinished by his predecessors and prepared the ground for the fruitful reception of the new ideas based on the quantum theory."[2] He received many other honours and distinctions, including a term as chairman of the International Committee on Intellectual Cooperation,[3] the forerunner of UNESCO, between 1925 and 1928.
$
5
Question: Who shared the 1902 Nobel Prize in Physics with Hendrik Antoon Lorentz?
A: Michael Faraday
B: James Clerk Maxwell
C: Pieter Zeeman
D: Johann Tobias Mayer
E: André-Marie Ampère
Answer: C

Question: Lorentz derived the transformation related to:
A: Quantum Mechanics
B: General Relativity
C: Special Theory of Relativity
D: Newtonian Mechanics
E: Thermodynamics
Answer: C

Question: The Lorentz oscillator model is a classical representation used to describe:
A: Forces between atoms
B: Resonance in musical instruments
C: Motion of planets
D: Anomalous dispersion in dielectric materials
E: Quantum tunneling of particles
Answer: D

Question: Hendrik Antoon Lorentz was from which country?
A: Germany
B: France
C: Switzerland
D: Netherlands
E: England
Answer: D

Question: The Zeeman effect pertains to:
A: The effect of gravity on light
B: Electromagnetic radiation due to nuclear decay
C: The splitting of spectral lines due to magnetic fields
D: The heating effect of current
E: Quantum entanglement of photons
Answer: C
@
Nikola Tesla (/ˈtɛslə/; Serbian Cyrillic: Никола Тесла,[2] pronounced [nǐkola têsla];[a] 10 July [O.S. 28 June] 1856 – 7 January 1943) was a Serbian-American[5][6] inventor, electrical engineer, mechanical engineer, and futurist best known for his contributions to the design of the modern alternating current (AC) electricity supply system.[7]

Born and raised in the Austrian Empire, Tesla studied engineering and physics in the 1870s without receiving a degree, gaining practical experience in the early 1880s working in telephony and at Continental Edison in the new electric power industry. In 1884 he emigrated to the United States, where he became a naturalized citizen. He worked for a short time at the Edison Machine Works in New York City before he struck out on his own. With the help of partners to finance and market his ideas, Tesla set up laboratories and companies in New York to develop a range of electrical and mechanical devices. His AC induction motor and related polyphase AC patents, licensed by Westinghouse Electric in 1888, earned him a considerable amount of money and became the cornerstone of the polyphase system which that company eventually marketed.
$
5
Question: Nikola Tesla is best known for his contributions to which design in the electricity supply system?
A: Direct Current (DC)
B: Radio Transmission
C: Alternating Current (AC)
D: Battery Technology
E: Wireless Communication
Answer: C

Question: Where did Tesla gain practical experience in the early 1880s?
A: Tesla Electric Company
B: Continental Edison
C: Westinghouse Electric
D: Liberty Laboratories
E: Brown & Peck Industries
Answer: B

Question: Tesla emigrated to which country in 1884?
A: Canada
B: Germany
C: United States
D: France
E: Russia
Answer: C

Question: Which company licensed Tesla's AC induction motor and related patents in 1888?
A: Brown & Peck Industries
B: Liberty Laboratories
C: Continental Edison
D: Tesla Electric Company
E: Westinghouse Electric
Answer: E

Question: Tesla was born and raised in which empire?
A: Ottoman Empire
B: British Empire
C: Roman Empire
D: Austrian Empire
E: Russian Empire
Answer: D
@
In late 1886, Tesla met Alfred S. Brown, a Western Union superintendent, and New York attorney Charles Fletcher Peck.[63] The two men were experienced in setting up companies and promoting inventions and patents for financial gain.[64] Based on Tesla's new ideas for electrical equipment, including a thermo-magnetic motor idea,[65] they agreed to back the inventor financially and handle his patents. Together they formed the Tesla Electric Company in April 1887, with an agreement that profits from generated patents would go 1⁄3 to Tesla, 1⁄3 to Peck and Brown, and 1⁄3 to fund development.[64] They set up a laboratory for Tesla at 89 Liberty Street in Manhattan, where he worked on improving and developing new types of electric motors, generators, and other devices.

In 1887, Tesla developed an induction motor that ran on alternating current (AC), a power system format that was rapidly expanding in Europe and the United States because of its advantages in long-distance, high-voltage transmission. The motor used polyphase current, which generated a rotating magnetic field to turn the motor (a principle that Tesla claimed to have conceived in 1882).[66][67][68] This innovative electric motor, patented in May 1888, was a simple self-starting design that did not need a commutator, thus avoiding sparking and the high maintenance of constantly servicing and replacing mechanical brushes.[69][70]
$
5
Question: Who did Tesla meet in late 1886 that later became his financial backers?
A: Michael Faraday and John Edison
B: Alfred S. Brown and Charles Fletcher Peck
C: Thomas Edison and George Westinghouse
D: Maxwell James and Henry Ford
E: Robert Hertz and Isaac Newton
Answer: B

Question: Which of the following describes the motor Tesla developed in 1887?
A: Direct Current (DC) motor
B: Alternating Current (AC) induction motor
C: Wireless motor
D: Telephonic motor
E: Battery-operated motor
Answer: B

Question: Tesla's innovative electric motor patented in 1888 did NOT require which component?
A: Rotor
B: Stator
C: Brushes
D: Commutator
E: Coil
Answer: D

Question: In which city was Tesla's laboratory located?
A: Washington D.C.
B: Boston
C: Los Angeles
D: San Francisco
E: Manhattan
Answer: E

Question: What did Tesla's induction motor use to generate a rotating magnetic field?
A: Bipolar current
B: Direct current
C: Triphase current
D: Polyphase current
E: Unipolar current
Answer: D
@
From the 1890s through 1906, Tesla spent a great deal of his time and fortune on a series of projects trying to develop the transmission of electrical power without wires. It was an expansion of his idea of using coils to transmit power that he had been demonstrating in wireless lighting. He saw this as not only a way to transmit large amounts of power around the world but also, as he had pointed out in his earlier lectures, a way to transmit worldwide communications.

At the time Tesla was formulating his ideas, there was no feasible way to wirelessly transmit communication signals over long distances, let alone large amounts of power. Tesla had studied radio waves early on, and came to the conclusion that part of the existing study on them, by Hertz, was incorrect.[138][139][140] Also, this new form of radiation was widely considered at the time to be a short-distance phenomenon that seemed to die out in less than a mile.[141] Tesla noted that, even if theories on radio waves were true, they were totally worthless for his intended purposes since this form of "invisible light" would diminish over a distance just like any other radiation and would travel in straight lines right out into space, becoming "hopelessly lost".[142]

By the mid-1890s, Tesla was working on the idea that he might be able to conduct electricity long distance through the Earth or the atmosphere, and began working on experiments to test this idea including setting up a large resonance transformer magnifying transmitter in his East Houston Street lab.[143][144][145] Seeming to borrow from a common idea at the time that the Earth's atmosphere was conductive,[146][147] he proposed a system composed of balloons suspending, transmitting, and receiving, electrodes in the air above 30,000 feet (9,100 m) in altitude, where he thought the lower pressure would allow him to send high voltages (millions of volts) long distances.
$
5
Question: Tesla's projects from the 1890s through 1906 primarily focused on what?
A: Wireless telecommunication
B: Battery development
C: Wireless transmission of electrical power
D: Alternating Current (AC) systems
E: Direct Current (DC) systems
Answer: C

Question: Tesla believed that he could conduct electricity long distance through what mediums?
A: The Earth and the atmosphere
B: Water and metal
C: Space and the Moon
D: Magnetic fields and ether
E: Plants and animals
Answer: A

Question: What was a unique aspect of Tesla's system for transmitting power at high altitudes?
A: Solar-powered stations
B: Conductive balloons suspending electrodes
C: Weather-controlled devices
D: Space satellites
E: Bird-powered mechanisms
Answer: B

Question: Tesla proposed that electrical power could be transmitted without which component?
A: Wires
B: Electrons
C: Electromagnetic waves
D: Coils
E: Magnetic fields
Answer: A

Question: At what altitude did Tesla believe that the lower pressure would allow high voltage transmission over long distances?
A: Above 50,000 feet
B: Above 30,000 feet
C: Above 10,000 feet
D: Above 5,000 feet
E: Above 60,000 feet
Answer: B
@
A Tesla coil is an electrical resonant transformer circuit designed by inventor Nikola Tesla in 1891.[1] It is used to produce high-voltage, low-current, high-frequency alternating-current electricity.[2] Tesla experimented with a number of different configurations consisting of two, or sometimes three, coupled resonant electric circuits.

Tesla used these circuits to conduct innovative experiments in electrical lighting, phosphorescence, X-ray generation, high-frequency alternating current phenomena, electrotherapy, and the transmission of electrical energy without wires. Tesla coil circuits were used commercially in spark-gap radio transmitters for wireless telegraphy until the 1920s,[1][3] and in medical equipment such as electrotherapy and violet ray devices. Today, their main usage is for entertainment and educational displays, although small coils are still used as leak detectors for high-vacuum systems.[4][5]

Originally, Tesla coils used fixed spark gaps or rotary spark gaps to provide intermittent excitation of the resonant circuit; more recently, electronic devices are used to provide the switching action required.
$
5
Question: In which year was the Tesla coil designed by Nikola Tesla?
A: 1895
B: 1901
C: 1880
D: 1891
E: 1878
Answer: D

Question: Tesla coils were used commercially in which application until the 1920s?
A: Air conditioning systems
B: Wireless telegraphy spark-gap radio transmitters
C: Refrigeration systems
D: Television sets
E: Electric cars
Answer: B

Question: What was the primary function of the Tesla coil?
A: To produce low-voltage, high-current electricity
B: To produce high-voltage, low-current, high-frequency alternating-current electricity
C: To detect wireless signals
D: To convert AC to DC
E: To generate static electricity
Answer: B

Question: The Tesla coil circuits were later used in medical equipment such as?
A: X-ray machines
B: MRI scanners
C: Electrotherapy and violet ray devices
D: Heart pacemakers
E: Ultrasound devices
Answer: C

Question: What provided the intermittent excitation of the resonant circuit in the original Tesla coils?
A: Electronic switches
B: Battery cells
C: Fixed spark gaps or rotary spark gaps
D: Solar cells
E: Magnetron tubes
Answer: C
@














































































































