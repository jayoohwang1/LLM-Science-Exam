@
In physics, gravity (from Latin gravitas 'weight'[1]) is a fundamental interaction which causes mutual attraction between all things that have mass. Gravity is, by far, the weakest of the four fundamental interactions, approximately 1038 times weaker than the strong interaction, 1036 times weaker than the electromagnetic force and 1029 times weaker than the weak interaction. As a result, it has no significant influence at the level of subatomic particles.[2] However, gravity is the most significant interaction between objects at the macroscopic scale, and it determines the motion of planets, stars, galaxies, and even light.

On Earth, gravity gives weight to physical objects, and the Moon's gravity is responsible for sublunar tides in the oceans (the corresponding antipodal tide is caused by the inertia of the Earth and Moon orbiting one another). Gravity also has many important biological functions, helping to guide the growth of plants through the process of gravitropism and influencing the circulation of fluids in multicellular organisms.

The gravitational attraction between the original gaseous matter in the universe caused it to coalesce and form stars which eventually condensed into galaxies, so gravity is responsible for many of the large-scale structures in the universe. Gravity has an infinite range, although its effects become weaker as objects get farther away.

Gravity is most accurately described by the general theory of relativity (proposed by Albert Einstein in 1915), which describes gravity not as a force, but as the curvature of spacetime, caused by the uneven distribution of mass, and causing masses to move along geodesic lines. The most extreme example of this curvature of spacetime is a black hole, from which nothing—not even light—can escape once past the black hole's event horizon.[3] However, for most applications, gravity is well approximated by Newton's law of universal gravitation, which describes gravity as a force causing any two bodies to be attracted toward each other, with magnitude proportional to the product of their masses and inversely proportional to the square of the distance between them.

Current models of particle physics imply that the earliest instance of gravity in the universe, possibly in the form of quantum gravity, supergravity or a gravitational singularity, along with ordinary space and time, developed during the Planck epoch (up to 10−43 seconds after the birth of the universe), possibly from a primeval state, such as a false vacuum, quantum vacuum or virtual particle, in a currently unknown manner.[4] Scientists are currently working to develop a theory of gravity consistent with quantum mechanics, a quantum gravity theory,[5] which would allow gravity to be united in a common mathematical framework (a theory of everything) with the other three fundamental interactions of physics.

Definitions
Gravitation is the mutual attraction between all masses in the universe, also known as gravitational attraction. Gravity is the gravitational attraction at the surface of a planet or other celestial body.[6]
$
10
Question 1: Which fundamental interaction causes mutual attraction between objects with mass?
A: Electromagnetic force
B: Strong interaction
C: Weak interaction
D: Gravity
E: Magnetic interaction

Answer: D

Question 2: How does gravity compare in strength to the electromagnetic force?
A: It is stronger by a factor of 10^36.
B: It is weaker by a factor of 10^36.
C: They are of the same strength.
D: It is weaker by a factor of 10^29.
E: It is stronger by a factor of 10^38.

Answer: B

Question 3: What determines the motion of planets, stars, galaxies, and even light on a macroscopic scale?
A: Electromagnetic force
B: Strong interaction
C: Gravity
D: Weak interaction
E: Magnetic interaction

Answer: C

Question 4: On Earth, what is responsible for sublunar tides in the oceans?
A: The Earth's rotation
B: Electromagnetic force
C: The Moon's gravity
D: Sun's gravity
E: The Earth's magnetic field

Answer: C

Question 5: How does the general theory of relativity, proposed by Einstein, describe gravity?
A: As a magnetic force between objects
B: As the result of charged particles
C: As a force causing two bodies to attract each other
D: As the curvature of spacetime caused by the uneven distribution of mass
E: As a wave propagating through space

Answer: D

Question 6: From which point in a black hole can nothing, including light, escape?
A: Gravitational well
B: Singularity
C: Event horizon
D: Black center
E: Photon sphere

Answer: C

Question 7: For most common applications, which law provides a good approximation of gravity's behavior?
A: Einstein's law of relativity
B: Planck's law of quantum gravity
C: Newton's law of universal gravitation
D: Feynman's law of interaction
E: Hubble's law of cosmic expansion

Answer: C

Question 8: What is believed to be the earliest instance of gravity in the universe?
A: Quantum gravity
B: Strong interaction
C: Weak interaction
D: Electromagnetic force
E: Newtonian gravity

Answer: A

Question 9: Scientists are working to develop a theory of gravity that is consistent with which other branch of physics?
A: Classical mechanics
B: Thermodynamics
C: Relativity
D: Quantum mechanics
E: Electrodynamics

Answer: D

Question 10: How is "gravitation" defined in contrast to "gravity"?
A: Gravitation is the force felt on the surface of a planet, while gravity is the mutual attraction between all masses.
B: Gravitation and gravity are the same.
C: Gravitation is the mutual attraction between all masses in the universe, while gravity is the gravitational attraction at the surface of a planet or other celestial body.
D: Gravitation is the force between two specific masses, while gravity is a general force affecting all objects.
E: Gravitation only applies in outer space, while gravity is an Earth-specific phenomenon.

Answer: C
@
In physics, theories of gravitation postulate mechanisms of interaction governing the movements of bodies with mass. There have been numerous theories of gravitation since ancient times. The first extant sources discussing such theories are found in ancient Greek philosophy. This work was furthered through the Middle Ages by Indian, Islamic, and European scientists, before gaining great strides during the Renaissance and Scientific Revolution—culminating in the formulation of Newton's law of gravity. This was superseded by Albert Einstein's theory of relativity in the early 20th century.

Greek philosopher Aristotle (fl. 4th century BCE) believed that objects tend toward a point due to their inner gravitas (heaviness). Vitruvius (fl. 1st century BCE) understood that objects fall based on their specific gravity. In the 6th century CE, Byzantine Alexandrian scholar John Philoponus modified the Aristotelian concept of gravity with the theory of impetus. In the 7th century, Indian astronomer Brahmagupta spoke of gravity as an attractive force. In the 14th century, European philosophers Jean Buridan and Albert of Saxony—who were influenced by certain Islamic scholars[a]—developed the theory of impetus and linked it to the acceleration and mass of objects. Albert also developed a law of proportion regarding the relationship between the speed of an object in free fall and the time elapsed.

In the early 17th century, Galileo Galilei found that all objects tend to accelerate equally in free fall. In 1632, he put forth the basic principle of relativity. The existence of the gravitational constant was explored by various researchers from the mid-17th century, helping Isaac Newton formulate his law of universal gravitation. Newton's classical mechanics were superseded in the early 20th century, when Einstein developed the special and general theories of relativity. The hypothetical force carrier of gravity remains an outlier in the search for a theory of everything, for which various models of quantum gravity are candidates.
$
10
Question 1: Who is credited with the formulation of the law of universal gravitation?
A: Galileo Galilei
B: Jean Buridan
C: Albert of Saxony
D: Aristotle
E: Isaac Newton

Answer: E

Question 2: According to Aristotle, why did objects move towards a point?
A: Because of their mass
B: Because of their impetus
C: Because of their inner gravitas
D: Due to the theory of relativity
E: Because of the gravitational constant

Answer: C

Question 3: Which scholar modified the Aristotelian concept of gravity with the theory of impetus in the 6th century CE?
A: Brahmagupta
B: John Philoponus
C: Vitruvius
D: Albert Einstein
E: Galileo Galilei

Answer: B

Question 4: What did Galileo Galilei discover about objects in free fall?
A: They move at varying speeds based on their mass.
B: They all tend to accelerate equally.
C: They move towards a point due to their gravitas.
D: Their acceleration is proportional to their mass.
E: They are influenced by the impetus.

Answer: B

Question 5: Which two European philosophers developed the theory of impetus and linked it to the acceleration and mass of objects in the 14th century?
A: Brahmagupta and John Philoponus
B: Galileo Galilei and Isaac Newton
C: Vitruvius and Aristotle
D: Jean Buridan and Albert of Saxony
E: Albert Einstein and Isaac Newton

Answer: D

Question 6: In which century did Einstein develop the special and general theories of relativity?
A: 14th century
B: 6th century
C: 1st century
D: 20th century
E: 17th century

Answer: D

Question 7: Who introduced the basic principle of relativity in 1632?
A: Brahmagupta
B: Albert of Saxony
C: Galileo Galilei
D: John Philoponus
E: Jean Buridan

Answer: C

Question 8: Which Indian astronomer spoke of gravity as an attractive force in the 7th century?
A: Albert of Saxony
B: Vitruvius
C: John Philoponus
D: Brahmagupta
E: Galileo Galilei

Answer: D

Question 9: Which classical theory was superseded by Einstein's theories of relativity in the 20th century?
A: Theory of impetus
B: Aristotle's theory of gravitas
C: Galileo's basic principle of relativity
D: Newton's classical mechanics
E: Jean Buridan's acceleration theory

Answer: D

Question 10: The force carrier of gravity remains an outlier in the search for what overarching theory?
A: Law of universal gravitation
B: Theory of impetus
C: Principle of relativity
D: Theory of everything
E: Theory of gravitas

Answer: D
@
Albert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[38] German: [ˈalbɛɐt ˈʔaɪnʃtaɪn] i; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist,[39] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century.[1][40] His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called "the world's most famous equation".[41] He received the 1921 Nobel Prize in Physics "for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect",[42] a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science.[43][44] In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time.[45] His intellectual achievements and originality have made Einstein synonymous with genius.[46]

In 1905, a year sometimes described as his annus mirabilis (miracle year), Einstein published four groundbreaking papers.[47] These outlined a theory of the photoelectric effect, explained Brownian motion, introduced his special theory of relativity—a theory which addressed the inability of classical mechanics to account satisfactorily for the behavior of the electromagnetic field—and demonstrated that if the special theory is correct, mass and energy are equivalent to each other. In 1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. A cosmological paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole.[48][49] The middle part of his career also saw him making important contributions to statistical mechanics and quantum theory. Especially notable was his work on the quantum physics of radiation, in which light consists of particles, subsequently called photons.

For much of the last phase of his academic life, Einstein worked on two endeavors that proved ultimately unsuccessful. Firstly, he fought a long rearguard action against quantum theory's introduction of fundamental randomness into science's picture of the world, objecting that "God does not play dice".[50] Secondly, he attempted to devise a unified field theory by generalizing his geometric theory of gravitation to include electromagnetism too. As a result, he became increasingly isolated from the mainstream of modern physics.

Born in the German Empire, Einstein moved to Switzerland in 1895, forsaking his German citizenship (as a subject of the Kingdom of Württemberg)[note 1] the following year. In 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the Swiss Federal polytechnic school in Zürich, graduating in 1900. In 1901, he acquired Swiss citizenship, which he kept for the rest of his life. In 1903, he secured a permanent position at the Swiss Patent Office in Bern. In 1905, he submitted a successful PhD dissertation to the University of Zurich. In 1914, he moved to Berlin in order to join the Prussian Academy of Sciences and the Humboldt University of Berlin. In 1917, he became director of the Kaiser Wilhelm Institute for Physics; he also became a German citizen again, this time as a subject of the Kingdom of Prussia.[note 1] In 1933, while he was visiting the United States, Adolf Hitler came to power in Germany. Horrified by the Nazi "war of extermination" against his fellow Jews,[51] Einstein decided to remain in the US, and was granted American citizenship in 1940.[52] On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential German nuclear weapons program and recommending that the US begin similar research. Einstein supported the Allies but generally viewed the idea of nuclear weapons with great dismay.[53]
$
10
Question 1: Which equation arising from the theory of relativity is considered "the world's most famous equation"?
A: E = mc^3
B: F = ma
C: E = mc^2
D: E = mc
E: m = E/c^2

Answer: C

Question 2: For what discovery did Einstein receive the 1921 Nobel Prize in Physics?
A: Theory of relativity
B: Mass-energy equivalence formula
C: Quantum mechanics contributions
D: Law of the photoelectric effect
E: Brownian motion explanation

Answer: D

Question 3: What year is sometimes referred to as Einstein's "miracle year", during which he published four groundbreaking papers?
A: 1915
B: 1921
C: 1895
D: 1905
E: 1933

Answer: D

Question 4: Which theory proposed by Einstein in 1915 incorporated gravitation and extended his system of mechanics?
A: Special theory of relativity
B: Quantum physics of radiation
C: Theory of the photoelectric effect
D: General theory of relativity
E: Unified field theory

Answer: D

Question 5: What did Einstein famously object to, stating that "God does not play dice"?
A: Theory of relativity
B: Introduction of fundamental randomness in quantum theory
C: Photoelectric effect
D: Theory of everything
E: Brownian motion

Answer: B

Question 6: Where did Einstein move to in 1895, leaving his German citizenship behind?
A: France
B: United States
C: Switzerland
D: Austria
E: Belgium

Answer: C

Question 7: At which institution did Einstein secure a permanent position in 1903?
A: University of Zurich
B: Swiss Federal polytechnic school in Zürich
C: Humboldt University of Berlin
D: Swiss Patent Office in Bern
E: Kaiser Wilhelm Institute for Physics

Answer: D

Question 8: Which position did Einstein assume in 1917 in Berlin?
A: Head of the Prussian Academy of Sciences
B: Director of the Humboldt University of Berlin
C: Director of the Swiss Patent Office
D: Director of the Kaiser Wilhelm Institute for Physics
E: Chancellor of the German Empire

Answer: D

Question 9: What led Einstein to decide to stay in the US and eventually acquire American citizenship in 1940?
A: The US research program on nuclear weapons
B: Hitler's rise to power in Germany and the Nazi persecution of Jews
C: The onset of World War I
D: Disagreements with the Swiss government
E: His admiration for President Franklin D. Roosevelt

Answer: B

Question 10: Why did Einstein endorse a letter to President Franklin D. Roosevelt on the eve of World War II?
A: To recommend an alliance with Germany
B: To advocate for peace negotiations with the Axis powers
C: To alert him to the potential German nuclear weapons program and recommend US research on the same
D: To seek asylum in the US for European scientists
E: To protest against the use of nuclear weapons

Answer: C
@
Superconducting quantum computing is a branch of solid state quantum computing that implements superconducting electronic circuits using superconducting qubits as artificial atoms, or quantum dots. For superconducting qubits, the two logic states are the ground state and the excited state, denoted 
|
�
⟩
 and 
|
�
⟩{\displaystyle |g\rangle {\text{ and }}|e\rangle }respectively.[1] Research in superconducting quantum computing is conducted by companies such as Google,[2] IBM,[3] IMEC,[4] BBN Technologies,[5] Rigetti,[6] and Intel.[7] Many recently developed QPUs (quantum processing units, or quantum chips) utilize superconducting architecture.

As of May 2016, up to 9 fully controllable qubits are demonstrated in the 1D array,[8] and up to 16 in 2D architecture.[3] In October 2019, the Martinis group, partnered with Google, published an article demonstrating novel quantum supremacy, using a chip composed of 53 superconducting qubits.[9]

Background
Classical computation models rely on physical implementations consistent with the laws of classical mechanics.[10] Classical descriptions are accurate only for specific systems consisting of a relatively large number of atoms. A more general description of nature is given by quantum mechanics. Quantum computation studies quantum phenomena applications beyond the scope of classical approximation for the purpose of informing artificial intelligence processing and communication. Various models of quantum computation exist, but the most popular models incorporate concepts of qubits and quantum gates (or gate-based superconducting quantum computing).

Superconductors are implemented due to the fact that at low temperatures they have almost infinite conductivity and almost zero resistance. Each qubit is built using semiconductor circuits with an LC circuit: a capacitor and an inductor.[citation needed]

Superconducting capacitors and inductors are used to produce a resonant circuit that dissipates almost no energy, as heat can disrupt quantum information. The superconducting resonant circuits are a class of artificial atoms that can be used as qubits. Theoretical and physical implementations of quantum circuits are widely different. Implementing a quantum circuit had its own set of challenges and must abide by DiVincenzo's criteria, conditions proposed by theoretical physicist David P DiVincenzo,[11] which is set of criteria for the physical implementation of superconducting quantum computing, where the initial five criteria ensure that the quantum computer is in line with the postulates of quantum mechanics and the remaining two pertaining to the relaying of this information over a network.[citation needed]

We map the ground and excited states of these atoms to the 0 and 1 state as these are discrete and distinct energy values and therefore it is in line with the postulates of quantum mechanics. In such a construction however an electron can jump to multiple other energy states and not be confined to our excited state; therefore, it is imperative that the system be limited to be affected only by photons with energy difference required to jump from the ground state to the excited state.[12] However, this leaves one major issue, we require uneven spacing between our energy levels to prevent photons with the same energy from causing transitions between neighboring pairs of states. Josephson junctions are superconducting elements with a nonlinear inductance, which is critically important for qubit implementation.[12] The use of this nonlinear element in the resonant superconducting circuit produces uneven spacings between the energy levels.[citation needed]
$
10
Question 1: What are the two logic states for superconducting qubits?
A: 0 and 1
B: Ground state and top state
C: Positive and negative
D: Ground state and excited state
E: On and off

Answer: D

Question 2: Which company partnered with the Martinis group in October 2019 to demonstrate novel quantum supremacy?
A: IBM
B: Intel
C: Rigetti
D: Google
E: IMEC

Answer: D

Question 3: As of May 2016, how many fully controllable qubits were demonstrated in the 2D architecture?
A: 9
B: 53
C: 16
D: 10
E: 5

Answer: C

Question 4: Classical computation models rely on physical implementations consistent with which laws?
A: Laws of quantum mechanics
B: DiVincenzo's criteria
C: Laws of classical mechanics
D: Superconducting principles
E: Law of relativity

Answer: C

Question 5: Why are superconductors implemented in quantum computing?
A: They have strong resistance.
B: They function at high temperatures.
C: They have almost infinite conductivity at low temperatures.
D: They rely on quantum dots.
E: They utilize high-energy photons.

Answer: C

Question 6: What is the primary concern when using superconducting resonant circuits in quantum computing?
A: They have high resistance.
B: Heat can disrupt quantum information.
C: They can cause short circuits.
D: They are difficult to produce.
E: They require high maintenance.

Answer: B

Question 7: Which criteria are proposed by David P DiVincenzo for the physical implementation of superconducting quantum computing?
A: Quantum gate criteria
B: Quantum dot criteria
C: Superconductor criteria
D: DiVincenzo's criteria
E: Quantum resonance criteria

Answer: D

Question 8: The ground and excited states of the artificial atoms used in quantum circuits map to which states?
A: Positive and negative
B: On and off
C: 0 and 1
D: Upper and lower
E: Forward and backward

Answer: C

Question 9: What type of superconducting elements are used due to their nonlinear inductance, which is crucial for qubit implementation?
A: Quantum capacitors
B: DiVincenzo circuits
C: Josephson junctions
D: Quantum resonators
E: Quantum inductors

Answer: C

Question 10: What is the major challenge when constructing quantum circuits using the ground and excited states?
A: Heat dissipation
B: Need for uneven spacing between energy levels
C: Production cost
D: Size of the qubits
E: Energy consumption

Answer: B
@
Aristotle (/ˈærɪstɒtəl/;[1] Greek: Ἀριστοτέλης Aristotélēs, pronounced [aristotélɛːs]; 384–322 BC) was an Ancient Greek philosopher and polymath. His writings cover a broad range of subjects spanning the natural sciences, philosophy, linguistics, economics, politics, psychology and the arts. As the founder of the Peripatetic school of philosophy in the Lyceum in Athens, he began the wider Aristotelian tradition that followed, which set the groundwork for the development of modern science.

Little is known about Aristotle's life. He was born in the city of Stagira in northern Greece during the Classical period. His father, Nicomachus, died when Aristotle was a child, and he was brought up by a guardian. At 17 or 18 he joined Plato's Academy in Athens and remained there till the age of 37 (c. 347 BC). Shortly after Plato died, Aristotle left Athens and, at the request of Philip II of Macedon, tutored his son Alexander the Great beginning in 343 BC. He established a library in the Lyceum which helped him to produce many of his hundreds of books on papyrus scrolls.

Though Aristotle wrote many elegant treatises and dialogues for publication, only around a third of his original output has survived, none of it intended for publication. Aristotle provided a complex synthesis of the various philosophies existing prior to him. It was above all from his teachings that the West inherited its intellectual lexicon, as well as problems and methods of inquiry. As a result, his philosophy has exerted a unique influence on almost every form of knowledge in the West and it continues to be a subject of contemporary philosophical discussion.

Aristotle's views profoundly shaped medieval scholarship. The influence of physical science extended from late antiquity and the Early Middle Ages into the Renaissance, and were not replaced systematically until the Enlightenment and theories such as classical mechanics were developed. Some of Aristotle's zoological observations found in his biology, such as on the hectocotyl (reproductive) arm of the octopus, were disbelieved until the 19th century. He influenced Judeo-Islamic philosophies during the Middle Ages, as well as Christian theology, especially the Neoplatonism of the Early Church and the scholastic tradition of the Catholic Church. Aristotle was revered among medieval Muslim scholars as "The First Teacher", and among medieval Christians like Thomas Aquinas as simply "The Philosopher", while the poet Dante called him "the master of those who know". His works contain the earliest known formal study of logic, and were studied by medieval scholars such as Peter Abelard and Jean Buridan. Aristotle's influence on logic continued well into the 19th century. In addition, his ethics, though always influential, gained renewed interest with the modern advent of virtue ethics.
$
10
Question 1: In which city was Aristotle born?
A: Athens
B: Lyceum
C: Macedon
D: Stagira
E: Rome
Answer: D

Question 2: What was the name of the school of philosophy founded by Aristotle?
A: Platonic Academy
B: Socratic Seminar
C: Peripatetic School
D: Lyceum Institution
E: Athenian Thought Circle
Answer: C

Question 3: Who did Aristotle tutor at the request of Philip II of Macedon?
A: Nicomachus
B: Plato
C: Dante
D: Philip II
E: Alexander the Great
Answer: E

Question 4: For how long did Aristotle remain at Plato's Academy in Athens?
A: Until the age of 37
B: 18 years
C: Until Plato's death
D: 20 years
E: From 384-322 BC
Answer: A

Question 5: What did Aristotle establish in the Lyceum?
A: A school of philosophy
B: Plato's Academy
C: A library
D: The Peripatetic school
E: An observatory
Answer: C

Question 6: How much of Aristotle's original written output has survived?
A: Half
B: Two-thirds
C: One-quarter
D: All of it
E: Around a third
Answer: E

Question 7: Which of the following areas was NOT mentioned as a subject Aristotle wrote on?
A: Mathematics
B: Economics
C: Psychology
D: The arts
E: Philosophy
Answer: A

Question 8: What title was Aristotle given by medieval Muslim scholars?
A: The Great Thinker
B: The First Teacher
C: The Philosopher
D: Master of Knowledge
E: Guardian of Logic
Answer: B

Question 9: Aristotle's observations on which animal's reproductive arm were disbelieved until the 19th century?
A: Squid
B: Seahorse
C: Whale
D: Octopus
E: Starfish
Answer: D

Question 10: Who among the following medieval scholars studied Aristotle's formal study of logic?
A: Philip II of Macedon
B: Jean Buridan
C: Dante
D: Nicomachus
E: Alexander the Great
Answer: B
@
Natural science is one of the branches of science concerned with the description, understanding and prediction of natural phenomena, based on empirical evidence from observation and experimentation.[1] Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances.

Natural science can be divided into two main branches: life science and physical science. Life science is alternatively known as biology, and physical science is subdivided into branches: physics, chemistry, earth science, and astronomy. These branches of natural science may be further divided into more specialized branches (also known as fields). As empirical sciences, natural sciences use tools from the formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the "laws of nature".[2]

Modern natural science succeeded more classical approaches to natural philosophy, usually traced to Taoist traditions in Asia and to ancient Greece in Europe. Galileo, Kepler, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science.[3] Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on.[4] Today, "natural history" suggests observational descriptions aimed at popular audiences.[5]
$
10
Question 1: What is the primary focus of natural science?
A: Analyzing societal constructs
B: Predicting future historical events
C: Understanding and predicting natural phenomena
D: Philosophical perspectives on nature
E: Describing ancient traditions
Answer: C

Question 2: Which mechanism ensures the validity of scientific advances?
A: Logical deductions
B: Peer review
C: Repeated experimentation without observation
D: Subjective judgments
E: Personal testimonies
Answer: B

Question 3: Which two main branches is natural science divided into?
A: Chemistry and physics
B: Earth science and astronomy
C: Life science and physical science
D: Mathematics and logic
E: Natural history and discovery science
Answer: C

Question 4: What is another name for life science?
A: Physics
B: Chemistry
C: Astronomy
D: Biology
E: Earth Science
Answer: D

Question 5: Which of the following is NOT a branch of physical science as mentioned in the text?
A: Biology
B: Chemistry
C: Physics
D: Astronomy
E: Earth Science
Answer: A

Question 6: What tools do natural sciences use from the formal sciences?
A: Speculation and intuition
B: Physics and biology
C: Mathematics and logic
D: Peer review and repeatability
E: Philosophical perspectives
Answer: C

Question 7: Who among the following debated the benefits of more mathematical and experimental approaches to natural science?
A: Taoists
B: Kepler
C: Ancient Greeks
D: Popular audiences
E: Discovery scientists
Answer: B

Question 8: What did natural history primarily focus on in the 16th century?
A: Predicting natural phenomena
B: Describing and classifying plants, animals, minerals
C: Peer reviewing scientific advances
D: Developing experimental methods
E: Debating the laws of nature
Answer: B

Question 9: What does "natural history" mainly suggest today?
A: Advanced scientific experiments
B: Peer-reviewed articles
C: Observational descriptions for popular audiences
D: Mathematical and logical analyses
E: Experimental approaches to philosophy
Answer: C

Question 10: Modern natural science succeeded which approach to natural philosophy?
A: Modern mathematics
B: Experimental psychology
C: Classical approaches traced to Taoist traditions and ancient Greece
D: Discovery science methodologies
E: Observational methods of the 19th century
Answer: C
@
Chemistry is the scientific study of the properties and behavior of matter.[1] It is a physical science under natural sciences that covers the elements that make up matter to the compounds made of atoms, molecules and ions: their composition, structure, properties, behavior and the changes they undergo during a reaction with other substances.[2][3][4][5] Chemistry also addresses the nature of chemical bonds in chemical compounds.

In the scope of its subject, chemistry occupies an intermediate position between physics and biology.[6] It is sometimes called the central science because it provides a foundation for understanding both basic and applied scientific disciplines at a fundamental level.[7] For example, chemistry explains aspects of plant growth (botany), the formation of igneous rocks (geology), how atmospheric ozone is formed and how environmental pollutants are degraded (ecology), the properties of the soil on the moon (cosmochemistry), how medications work (pharmacology), and how to collect DNA evidence at a crime scene (forensics).

Chemistry is a study that has existed since ancient times.[8] Over this time frame, it has evolved, and now chemistry encompasses various areas of specialisation, or subdisciplines, that continue to increase in number and interrelate to create further interdisciplinary fields of study. The applications of various fields of chemistry are used frequently for economic purposes in the chemical industry.

Modern principles

Laboratory, Institute of Biochemistry, University of Cologne in Germany
The current model of atomic structure is the quantum mechanical model.[12] Traditional chemistry starts with the study of elementary particles, atoms, molecules,[13] substances, metals, crystals and other aggregates of matter. Matter can be studied in solid, liquid, gas and plasma states, in isolation or in combination. The interactions,[disambiguation needed] reactions and transformations that are studied in chemistry are usually the result of interactions between atoms, leading to rearrangements of the chemical bonds which hold atoms together. Such behaviors are studied in a chemistry laboratory.

The chemistry laboratory stereotypically uses various forms of laboratory glassware. However glassware is not central to chemistry, and a great deal of experimental (as well as applied/industrial) chemistry is done without it.


Solutions of substances in reagent bottles, including ammonium hydroxide and nitric acid, illuminated in different colors
A chemical reaction is a transformation of some substances into one or more different substances.[14] The basis of such a chemical transformation is the rearrangement of electrons in the chemical bonds between atoms. It can be symbolically depicted through a chemical equation, which usually involves atoms as subjects. The number of atoms on the left and the right in the equation for a chemical transformation is equal. (When the number of atoms on either side is unequal, the transformation is referred to as a nuclear reaction or radioactive decay.) The type of chemical reactions a substance may undergo and the energy changes that may accompany it are constrained by certain basic rules, known as chemical laws.

Energy and entropy considerations are invariably important in almost all chemical studies. Chemical substances are classified in terms of their structure, phase, as well as their chemical compositions. They can be analyzed using the tools of chemical analysis, e.g. spectroscopy and chromatography. Scientists engaged in chemical research are known as chemists.[15] Most chemists specialize in one or more sub-disciplines. Several concepts are essential for the study of chemistry; some of them are:[16]
$
10
Question 1: What does chemistry primarily study?
A: The structure of DNA
B: The behavior of animals
C: The properties and behavior of matter
D: The process of nuclear reactions
E: The ecological impact of pollutants
Answer: C

Question 2: Chemistry provides a foundation for understanding both basic and applied scientific disciplines at a fundamental level, hence it is sometimes called the __________.
A: Applied science
B: Interdisciplinary science
C: Central science
D: Atomic science
E: Reactive science
Answer: C

Question 3: Which of these is NOT mentioned as an aspect explained by chemistry?
A: The properties of soil on Mars
B: Formation of igneous rocks
C: How atmospheric ozone is formed
D: How medications work
E: The growth aspects of plants
Answer: A

Question 4: The current model of atomic structure is the _________.
A: Chemical model
B: Traditional model
C: Atomic bomb model
D: Quantum mechanical model
E: Electrochemical model
Answer: D

Question 5: What are the states in which matter can be studied?
A: Elementary, secondary, tertiary, and quaternary
B: Solid, liquid, air, and mixture
C: Solid, liquid, gas, and plasma
D: Solid, aqueous, gas, and solution
E: Atomic, molecular, ionic, and crystalline
Answer: C

Question 6: In a chemical reaction, the basis of transformation is the rearrangement of __________.
A: Atoms
B: Chemical compounds
C: Molecules
D: Electrons in the chemical bonds
E: Protons in the nucleus
Answer: D

Question 7: What happens to the number of atoms in a typical chemical equation representing a transformation?
A: Atoms multiply
B: Atoms disappear
C: Number of atoms is equal on both sides
D: Atoms are reduced to half
E: Number of atoms is tripled on one side
Answer: C

Question 8: If the number of atoms on either side of a chemical equation is unequal, the transformation is known as __________.
A: Ionic bond formation
B: A covalent reaction
C: Radioactive decay
D: Electrolysis
E: Chemical dissolution
Answer: C

Question 9: Which tool is NOT primarily associated with the study of chemistry, according to the text?
A: Spectroscopy
B: Chromatography
C: Microscopy
D: Chemical analysis
E: Laboratory glassware
Answer: C

Question 10: Scientists who engage in chemical research are called __________.
A: Physicists
B: Biologists
C: Astronomers
D: Chemists
E: Geologists
Answer: D
@
Quantum mechanics is a fundamental theory in physics that provides a description of the physical properties of nature at the scale of atoms and subatomic particles.[2]: 1.1  It is the foundation of all quantum physics including quantum chemistry, quantum field theory, quantum technology, and quantum information science.

Classical physics, the collection of theories that existed before the advent of quantum mechanics, describes many aspects of nature at an ordinary (macroscopic) scale, but is not sufficient for describing them at small (atomic and subatomic) scales. Most theories in classical physics can be derived from quantum mechanics as an approximation valid at large (macroscopic) scale.[3]

Quantum mechanics differs from classical physics in that energy, momentum, angular momentum, and other quantities of a bound system are restricted to discrete values (quantization); objects have characteristics of both particles and waves (wave–particle duality); and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (the uncertainty principle).

Quantum mechanics arose gradually from theories to explain observations that could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and the correspondence between energy and frequency in Albert Einstein's 1905 paper, which explained the photoelectric effect. These early attempts to understand microscopic phenomena, now known as the "old quantum theory", led to the full development of quantum mechanics in the mid-1920s by Niels Bohr, Erwin Schrödinger, Werner Heisenberg, Max Born, Paul Dirac and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical entity called the wave function provides information, in the form of probability amplitudes, about what measurements of a particle's energy, momentum, and other physical properties may yield.
$
10
Question 1: Quantum mechanics provides a description of physical properties at the scale of ________.
A: Stars and galaxies
B: Mountains and oceans
C: Continents
D: Atoms and subatomic particles
E: Macroscopic organisms
Answer: D

Question 2: Which of the following is NOT a foundation of quantum physics?
A: Quantum meteorology
B: Quantum technology
C: Quantum chemistry
D: Quantum field theory
E: Quantum information science
Answer: A

Question 3: Classical physics describes nature at which scale?
A: Microscopic
B: Atomic
C: Ordinary (macroscopic)
D: Quantum
E: Molecular
Answer: C

Question 4: In quantum mechanics, objects have characteristics of both ________.
A: Solids and liquids
B: Particles and waves
C: Protons and electrons
D: Energy and force
E: Atoms and molecules
Answer: B

Question 5: The uncertainty principle states there are limits to how accurately ________.
A: A wave can be observed
B: The value of a physical quantity can be predicted prior to its measurement
C: Atoms can be split
D: Quantum mechanics can be taught
E: Classical physics can be understood
Answer: B

Question 6: Who provided a solution to the black-body radiation problem in 1900?
A: Niels Bohr
B: Werner Heisenberg
C: Albert Einstein
D: Erwin Schrödinger
E: Max Planck
Answer: E

Question 7: Who explained the correspondence between energy and frequency in 1905, which also explained the photoelectric effect?
A: Max Born
B: Albert Einstein
C: Niels Bohr
D: Paul Dirac
E: Werner Heisenberg
Answer: B

Question 8: The "old quantum theory" led to the development of quantum mechanics in the mid-1920s by various scientists. Which of the following scientists was NOT mentioned in this context?
A: Richard Feynman
B: Erwin Schrödinger
C: Niels Bohr
D: Werner Heisenberg
E: Max Born
Answer: A

Question 9: In quantum mechanics, what provides information about measurements of a particle's properties like energy and momentum?
A: Quantum equation
B: Energy function
C: Momentum ratio
D: Particle chart
E: Wave function
Answer: E

Question 10: Quantum mechanics differs from classical physics in several ways. Which of the following is NOT a feature of quantum mechanics mentioned in the text?
A: Energy is always continuous
B: Energy, momentum, and other quantities are quantized
C: Wave-particle duality exists
D: There's the uncertainty principle
E: It arose from the need to explain observations not reconciled with classical physics
Answer: A
@
The theory of relativity usually encompasses two interrelated physics theories by Albert Einstein: special relativity and general relativity, proposed and published in 1905 and 1915, respectively.[1] Special relativity applies to all physical phenomena in the absence of gravity. General relativity explains the law of gravitation and its relation to the forces of nature.[2] It applies to the cosmological and astrophysical realm, including astronomy.[3]

The theory transformed theoretical physics and astronomy during the 20th century, superseding a 200-year-old theory of mechanics created primarily by Isaac Newton.[3][4][5] It introduced concepts including 4-dimensional spacetime as a unified entity of space and time, relativity of simultaneity, kinematic and gravitational time dilation, and length contraction. In the field of physics, relativity improved the science of elementary particles and their fundamental interactions, along with ushering in the nuclear age. With relativity, cosmology and astrophysics predicted extraordinary astronomical phenomena such as neutron stars, black holes, and gravitational waves.[3][4][5]

Albert Einstein published the theory of special relativity in 1905, building on many theoretical results and empirical findings obtained by Albert A. Michelson, Hendrik Lorentz, Henri Poincaré and others. Max Planck, Hermann Minkowski and others did subsequent work.

Einstein developed general relativity between 1907 and 1915, with contributions by many others after 1915. The final form of general relativity was published in 1916.[3]

Special relativity
Special relativity is a theory of the structure of spacetime. It was introduced in Einstein's 1905 paper "On the Electrodynamics of Moving Bodies" (for the contributions of many other physicists and mathematicians, see History of special relativity). Special relativity is based on two postulates which are contradictory in classical mechanics:

The laws of physics are the same for all observers in any inertial frame of reference relative to one another (principle of relativity).
The speed of light in a vacuum is the same for all observers, regardless of their relative motion or of the motion of the light source.
The resultant theory copes with experiment better than classical mechanics. For instance, postulate 2 explains the results of the Michelson–Morley experiment. Moreover, the theory has many surprising and counterintuitive consequences. Some of these are:

Relativity of simultaneity: Two events, simultaneous for one observer, may not be simultaneous for another observer if the observers are in relative motion.
Time dilation: Moving clocks are measured to tick more slowly than an observer's "stationary" clock.
Length contraction: Objects are measured to be shortened in the direction that they are moving with respect to the observer.
Maximum speed is finite: No physical object, message or field line can travel faster than the speed of light in a vacuum.
The effect of gravity can only travel through space at the speed of light, not faster or instantaneously.
Mass–energy equivalence: E = mc2, energy and mass are equivalent and transmutable.
Relativistic mass, idea used by some researchers.[9]
The defining feature of special relativity is the replacement of the Galilean transformations of classical mechanics by the Lorentz transformations. (See Maxwell's equations of electromagnetism.)

General relativity
Main articles: General relativity and Introduction to general relativity
General relativity is a theory of gravitation developed by Einstein in the years 1907–1915. The development of general relativity began with the equivalence principle, under which the states of accelerated motion and being at rest in a gravitational field (for example, when standing on the surface of the Earth) are physically identical. The upshot of this is that free fall is inertial motion: an object in free fall is falling because that is how objects move when there is no force being exerted on them, instead of this being due to the force of gravity as is the case in classical mechanics. This is incompatible with classical mechanics and special relativity because in those theories inertially moving objects cannot accelerate with respect to each other, but objects in free fall do so. To resolve this difficulty Einstein first proposed that spacetime is curved. Einstein discussed his idea with mathematician Marcel Grossmann and they concluded that general relativity could be formulated in the context of Riemannian geometry which had been developed in the 1800s.[10] In 1915, he devised the Einstein field equations which relate the curvature of spacetime with the mass, energy, and any momentum within it.

Some of the consequences of general relativity are:

Gravitational time dilation: Clocks run slower in deeper gravitational wells.[11]
Precession: Orbits precess in a way unexpected in Newton's theory of gravity. (This has been observed in the orbit of Mercury and in binary pulsars).
Light deflection: Rays of light bend in the presence of a gravitational field.
Frame-dragging: Rotating masses "drag along" the spacetime around them.
Expansion of the universe: The universe is expanding, and certain components within the universe can accelerate the expansion.
Technically, general relativity is a theory of gravitation whose defining feature is its use of the Einstein field equations. The solutions of the field equations are metric tensors which define the topology of the spacetime and how objects move inertially.
$
10
Question 1: The theory of relativity was proposed by:
A: Isaac Newton
B: Max Planck
C: Hermann Minkowski
D: Albert Einstein
E: Henri Poincaré
Answer: D

Question 2: Special relativity applies to all physical phenomena in the absence of:
A: Light
B: Momentum
C: Gravity
D: Time
E: Energy
Answer: C

Question 3: Which theory explains the law of gravitation and its relation to the forces of nature?
A: Quantum Mechanics
B: Classical Mechanics
C: Special Relativity
D: General Relativity
E: Electromagnetism
Answer: D

Question 4: Before the theory of relativity, the dominant theory of mechanics was created by:
A: Albert A. Michelson
B: Hendrik Lorentz
C: Isaac Newton
D: Albert Einstein
E: Hermann Minkowski
Answer: C

Question 5: In special relativity, what is always the same for all observers, regardless of their motion?
A: Time
B: Gravity
C: Speed of light in a vacuum
D: Momentum
E: Energy
Answer: C

Question 6: Which experiment is explained by the second postulate of special relativity?
A: Einstein's gravity experiment
B: Planck's black body experiment
C: Michelson–Morley experiment
D: Lorentz's motion experiment
E: Poincaré's simultaneity experiment
Answer: C

Question 7: What did Einstein first propose to resolve the difficulty between inertial motion and objects in free fall?
A: Spacetime is a flat entity
B: Spacetime is curved
C: Spacetime is infinite
D: Spacetime doesn't interact with gravity
E: Spacetime is constant
Answer: B

Question 8: In general relativity, what happens to clocks in deeper gravitational wells?
A: They run faster
B: They stop
C: They reverse
D: They run slower
E: They remain unaffected
Answer: D

Question 9: According to general relativity, rays of light do what in the presence of a gravitational field?
A: Accelerate
B: Stop
C: Bend
D: Disintegrate
E: Change color
Answer: C

Question 10: The Einstein field equations of general relativity relate the curvature of spacetime with which of the following?
A: Speed of light
B: Gravitational waves
C: Quantum fields
D: Mass, energy, and any momentum within it
E: Electromagnetic fields
Answer: D
@
Gravitational time dilation is a form of time dilation, an actual difference of elapsed time between two events as measured by observers situated at varying distances from a gravitating mass. The lower the gravitational potential (the closer the clock is to the source of gravitation), the slower time passes, speeding up as the gravitational potential increases (the clock getting away from the source of gravitation). Albert Einstein originally predicted this effect in his theory of relativity and it has since been confirmed by tests of general relativity.[1]

This has been demonstrated by noting that atomic clocks at differing altitudes (and thus different gravitational potential) will eventually show different times. The effects detected in such Earth-bound experiments are extremely small, with differences being measured in nanoseconds. Relative to Earth's age in billions of years, Earth's core is effectively 2.5 years younger than its surface.[2] Demonstrating larger effects would require greater distances from the Earth or a larger gravitational source.

Gravitational time dilation was first described by Albert Einstein in 1907[3] as a consequence of special relativity in accelerated frames of reference. In general relativity, it is considered to be a difference in the passage of proper time at different positions as described by a metric tensor of spacetime. The existence of gravitational time dilation was first confirmed directly by the Pound–Rebka experiment in 1959, and later refined by Gravity Probe A and other experiments.

Gravitational time dilation is closely related to gravitational redshift:[4] the closer a body (emitting light of constant frequency) is to a gravitating body, the more its time is slowed by gravitational time dilation, and the lower (more "redshifted") would seem the frequency of the light it emits, as measured by a fixed observer.

Definition
Clocks that are far from massive bodies (or at higher gravitational potentials) run more quickly, and clocks close to massive bodies (or at lower gravitational potentials) run more slowly. For example, considered over the total time-span of Earth (4.6 billion years), a clock set in a geostationary position at an altitude of 9,000 meters above sea level, such as perhaps at the top of Mount Everest (prominence 8,848 m), would be about 39 hours ahead of a clock set at sea level.[5][6] This is because gravitational time dilation is manifested in accelerated frames of reference or, by virtue of the equivalence principle, in the gravitational field of massive objects.[7]

According to general relativity, inertial mass and gravitational mass are the same, and all accelerated reference frames (such as a uniformly rotating reference frame with its proper time dilation) are physically equivalent to a gravitational field of the same strength.[8]
$
10
Question 1: What is the effect of gravitational time dilation on time as one gets closer to the source of gravitation?
A: Time speeds up
B: Time remains the same
C: Time reverses
D: Time slows down
E: Time becomes unpredictable
Answer: D

Question 2: How was gravitational time dilation originally predicted?
A: By Albert Einstein's theory of relativity
B: By Isaac Newton's laws of motion
C: By the Pound–Rebka experiment
D: By Quantum Mechanics
E: By the equivalence principle
Answer: A

Question 3: How are differences in time due to gravitational time dilation on Earth typically measured?
A: Minutes
B: Hours
C: Days
D: Nanoseconds
E: Milliseconds
Answer: D

Question 4: Relative to Earth's age in billions of years, how much younger is Earth's core compared to its surface?
A: 5 years
B: 2.5 years
C: 10 years
D: 6 months
E: 1 year
Answer: B

Question 5: Gravitational time dilation was first described by Albert Einstein in which year?
A: 1920
B: 1910
C: 1907
D: 1959
E: 1935
Answer: C

Question 6: Which experiment first directly confirmed the existence of gravitational time dilation?
A: Gravity Probe A
B: Michelson–Morley experiment
C: Pound–Rebka experiment
D: Einstein's theory experiment
E: Mount Everest Clock experiment
Answer: C

Question 7: Gravitational time dilation is closely related to which of the following phenomena?
A: Gravitational blue shift
B: Special relativity
C: Nuclear reactions
D: Gravitational redshift
E: Quantum entanglement
Answer: D

Question 8: For a clock set at sea level compared to one at 9,000 meters above sea level (like at the top of Mount Everest), the higher altitude clock would be how much time ahead over Earth's total lifespan?
A: 24 hours
B: 48 hours
C: 39 hours
D: 50 hours
E: 60 hours
Answer: C

Question 9: According to general relativity, what are considered the same?
A: Acceleration and speed
B: Gravitational mass and speed of light
C: Inertial mass and gravitational mass
D: Time and space
E: Gravity and electromagnetic force
Answer: C

Question 10: All accelerated reference frames are physically equivalent to a gravitational field of what kind of strength?
A: Strong nuclear force
B: Weak gravitational pull
C: The same strength
D: Incomparable strength
E: Double the strength
Answer: C
@
Mass is an intrinsic property of a body. It was traditionally believed to be related to the quantity of matter in a physical body, until the discovery of the atom and particle physics. It was found that different atoms and different elementary particles, theoretically with the same amount of matter, have nonetheless different masses. Mass in modern physics has multiple definitions which are conceptually distinct, but physically equivalent. Mass can be experimentally defined as a measure of the body's inertia, meaning the resistance to acceleration (change of velocity) when a net force is applied.[1] The object's mass also determines the strength of its gravitational attraction to other bodies.

The SI base unit of mass is the kilogram (kg). In physics, mass is not the same as weight, even though mass is often determined by measuring the object's weight using a spring scale, rather than balance scale comparing it directly with known masses. An object on the Moon would weigh less than it does on Earth because of the lower gravity, but it would still have the same mass. This is because weight is a force, while mass is the property that (along with gravity) determines the strength of this force.

Definitions
In physical science, one may distinguish conceptually between at least seven different aspects of mass, or seven physical notions that involve the concept of mass.[5] Every experiment to date has shown these seven values to be proportional, and in some cases equal, and this proportionality gives rise to the abstract concept of mass. There are a number of ways mass can be measured or operationally defined:

Inertial mass is a measure of an object's resistance to acceleration when a force is applied. It is determined by applying a force to an object and measuring the acceleration that results from that force. An object with small inertial mass will accelerate more than an object with large inertial mass when acted upon by the same force. One says the body of greater mass has greater inertia.
Active gravitational mass[note 4] is a measure of the strength of an object's gravitational flux (gravitational flux is equal to the surface integral of gravitational field over an enclosing surface). Gravitational field can be measured by allowing a small "test object" to fall freely and measuring its free-fall acceleration. For example, an object in free-fall near the Moon is subject to a smaller gravitational field, and hence accelerates more slowly, than the same object would if it were in free-fall near the Earth. The gravitational field near the Moon is weaker because the Moon has less active gravitational mass.
Passive gravitational mass is a measure of the strength of an object's interaction with a gravitational field. Passive gravitational mass is determined by dividing an object's weight by its free-fall acceleration. Two objects within the same gravitational field will experience the same acceleration; however, the object with a smaller passive gravitational mass will experience a smaller force (less weight) than the object with a larger passive gravitational mass.
According to relativity, mass is nothing else than the rest energy of a system of particles, meaning the energy of that system in a reference frame where it has zero momentum. Mass can be converted into other forms of energy according to the principle of mass–energy equivalence. This equivalence is exemplified in a large number of physical processes including pair production, beta decay and nuclear fusion. Pair production and nuclear fusion are processes in which measurable amounts of mass are converted to kinetic energy or vice versa.
Curvature of spacetime is a relativistic manifestation of the existence of mass. Such curvature is extremely weak and difficult to measure. For this reason, curvature was not discovered until after it was predicted by Einstein's theory of general relativity. Extremely precise atomic clocks on the surface of the Earth, for example, are found to measure less time (run slower) when compared to similar clocks in space. This difference in elapsed time is a form of curvature called gravitational time dilation. Other forms of curvature have been measured using the Gravity Probe B satellite.
Quantum mass manifests itself as a difference between an object's quantum frequency and its wave number. The quantum mass of a particle is proportional to the inverse Compton wavelength and can be determined through various forms of spectroscopy. In relativistic quantum mechanics, mass is one of the irreducible representation labels of the Poincaré group.
$
10

Question 1: What discovery led to the understanding that different atoms and elementary particles with the same amount of matter have different masses?
A: Discovery of relativity
B: Discovery of the kilogram
C: Discovery of the atom and particle physics
D: Discovery of gravitational fields
E: Discovery of the Poincaré group
Answer: C

Question 2: How is mass in modern physics defined?
A: As a property of weight
B: As a measure of the body's inertia
C: As the amount of space an object occupies
D: As a measure of the body's volume
E: As a measure of the body's force
Answer: B

Question 3: How is weight different from mass?
A: Weight is the same as mass but in different units
B: Weight varies based on the material of the object, mass doesn't
C: Weight is a force, while mass determines the strength of this force
D: Weight is determined only in vacuum, mass isn't
E: Weight and mass are interchangeable terms
Answer: C

Question 4: What is the SI base unit for mass?
A: Meter
B: Newton
C: Liter
D: Kilogram
E: Joule
Answer: D

Question 5: What does inertial mass measure?
A: An object's gravitational flux
B: An object's resistance to acceleration when a force is applied
C: The amount of space an object occupies
D: An object's interaction with a gravitational field
E: The energy of a system of particles in a stationary frame
Answer: B

Question 6: How can one measure active gravitational mass?
A: By dividing an object's weight by its volume
B: By converting mass into kinetic energy
C: By measuring an object's resistance to force
D: By allowing a "test object" to fall freely and measuring its free-fall acceleration
E: By comparing the time of atomic clocks on Earth to those in space
Answer: D

Question 7: How can one determine passive gravitational mass?
A: By measuring its quantum frequency
B: By converting its energy into mass using mass–energy equivalence
C: By dividing an object's weight by its free-fall acceleration
D: By allowing an object to float in space
E: By measuring its inertial mass
Answer: C

Question 8: According to relativity, what is mass?
A: The measure of weight in a gravitational field
B: The rest energy of a system of particles in a frame where it has zero momentum
C: The force exerted by an object in a gravitational field
D: The amount of space an object occupies
E: The frequency of a particle in motion
Answer: B

Question 9: What form of curvature is manifested when extremely precise atomic clocks on Earth run slower than similar clocks in space?
A: Quantum curvature
B: Inertial curvature
C: Spacetime deviation
D: Gravitational time dilation
E: Gravitational redshift
Answer: D

Question 10: In which process are measurable amounts of mass converted to kinetic energy or vice versa?
A: Compton scattering
B: Nuclear fusion
C: Photoelectric effect
D: Electromagnetic induction
E: Resonance frequency
Answer: B
@
In physics, spacetime is any mathematical model that fuses the three dimensions of space and the one dimension of time into a single four-dimensional continuum. Spacetime diagrams are useful in visualizing and understanding relativistic effects such as how different observers perceive where and when events occur.

Until the turn of the 20th century, the assumption had been that the three-dimensional geometry of the universe (its description in terms of locations, shapes, distances, and directions) was distinct from time (the measurement of when events occur within the universe). However, space and time took on new meanings with the Lorentz transformation and special theory of relativity.

In 1908, Hermann Minkowski presented a geometric interpretation of special relativity that fused time and the three spatial dimensions of space into a single four-dimensional continuum now known as Minkowski space. This interpretation proved vital to the general theory of relativity, wherein spacetime is curved by mass and energy.

Fundamentals

Definitions
Non-relativistic classical mechanics treats time as a universal quantity of measurement which is uniform throughout space, and separate from space. Classical mechanics assumes that time has a constant rate of passage, independent of the observer's state of motion, or anything external.[1] Furthermore, it assumes that space is Euclidean; it assumes that space follows the geometry of common sense.[2]

In the context of special relativity, time cannot be separated from the three dimensions of space, because the observed rate at which time passes for an object depends on the object's velocity relative to the observer.[3]: 214–217  General relativity also provides an explanation of how gravitational fields can slow the passage of time for an object as seen by an observer outside the field.

In ordinary space, a position is specified by three numbers, known as dimensions. In the Cartesian coordinate system, these are called x, y, and z. A position in spacetime is called an event, and requires four numbers to be specified: the three-dimensional location in space, plus the position in time (Fig. 1). An event is represented by a set of coordinates x, y, z and t. Spacetime is thus four dimensional.

Unlike the analogies used in popular writings to explain events, such as firecrackers or sparks, mathematical events have zero duration and represent a single point in spacetime.[4] Although it is possible to be in motion relative to the popping of a firecracker or a spark, it is not possible for an observer to be in motion relative to an event.

The path of a particle through spacetime can be considered to be a succession of events. The series of events can be linked together to form a line which represents a particle's progress through spacetime. That line is called the particle's world line.[5]: 105 

Mathematically, spacetime is a manifold, which is to say, it appears locally "flat" near each point in the same way that, at small enough scales, a globe appears flat.[6] A scale factor, 
�
c (conventionally called the speed-of-light) relates distances measured in space with distances measured in time. The magnitude of this scale factor (nearly 300,000 kilometres or 190,000 miles in space being equivalent to one second in time), along with the fact that spacetime is a manifold, implies that at ordinary, non-relativistic speeds and at ordinary, human-scale distances, there is little that humans might observe which is noticeably different from what they might observe if the world were Euclidean. It was only with the advent of sensitive scientific measurements in the mid-1800s, such as the Fizeau experiment and the Michelson–Morley experiment, that puzzling discrepancies began to be noted between observation versus predictions based on the implicit assumption of Euclidean space.[7]
$
10
Question 1: What does spacetime in physics refer to?
A: A model that separates three dimensions of space and one dimension of time
B: A single four-dimensional continuum that combines three dimensions of space and one dimension of time
C: A model that includes only the three dimensions of space
D: A model that includes only the dimension of time
E: A method to measure distances in the universe
Answer: B

Question 2: Who presented the geometric interpretation of special relativity that combined time and the three spatial dimensions into a single continuum?
A: Albert Einstein
B: Isaac Newton
C: Michelson-Morley
D: Hermann Minkowski
E: Lorenz
Answer: D

Question 3: Before the 20th century, how was time viewed in relation to the three-dimensional geometry of the universe?
A: As being dependent on motion
B: As a dimension separate from locations, shapes, distances, and directions
C: As being the same as the three dimensions
D: As being curved by mass and energy
E: As a variable entity depending on the observer's perspective
Answer: B

Question 4: In the context of special relativity, why can't time be separated from the three dimensions of space?
A: Because time is a universal constant
B: Because the rate at which time passes depends on an object's velocity relative to the observer
C: Because time is a human construct
D: Because time is the same everywhere in the universe
E: Because of the gravitational pull on time
Answer: B

Question 5: How many numbers are required to specify a position in spacetime?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: D

Question 6: How is the path of a particle through spacetime described?
A: As a constant point
B: As a succession of events represented by the particle's world line
C: As a circular loop
D: By its velocity
E: As a zigzag pattern in spacetime
Answer: B

Question 7: What is spacetime, mathematically speaking?
A: A linear equation
B: A Euclidean space
C: A manifold
D: A parabolic curve
E: A singular point
Answer: C

Question 8: How does a scale factor,
�
c (typically the speed of light), relate to spacetime?
A: It represents the speed of time
B: It divides the three dimensions of space
C: It determines the gravity of spacetime
D: It relates distances measured in space with distances measured in time
E: It determines the curvature of space
Answer: D

Question 9: How do events in spacetime differ from the analogies used in popular writings like firecrackers?
A: They last for a longer duration
B: They can be observed in motion relative to the observer
C: They have zero duration and represent a single point in spacetime
D: They spread out over multiple points in spacetime
E: They occur only in non-Euclidean space
Answer: C

Question 10: Why did discrepancies start to be noted between observations and predictions based on the assumption of Euclidean space in the mid-1800s?
A: Because of advanced computational models
B: Due to the discovery of black holes
C: Because of sensitive scientific measurements like the Fizeau experiment and the Michelson–Morley experiment
D: Due to the advent of quantum mechanics
E: Because of the discovery of the Lorentz transformation
Answer: C
@
Subject:
Time is the continued sequence of existence and events that occurs in an apparently irreversible succession from the past, through the present, into the future.[1][2][3] It is a component quantity of various measurements used to sequence events, to compare the duration of events or the intervals between them, and to quantify rates of change of quantities in material reality or in the conscious experience.[4][5][6][7] Time is often referred to as a fourth dimension, along with three spatial dimensions.[8]

Time is one of the seven fundamental physical quantities in both the International System of Units (SI) and International System of Quantities. The SI base unit of time is the second, which is defined by measuring the electronic transition frequency of caesium atoms. General relativity is the primary framework for understanding how spacetime works.[9] Through advances in both theoretical and experimental investigations of spacetime, it has been shown that time can be distorted and dilated, particularly at the edges of black holes.

Throughout history, time has been an important subject of study in religion, philosophy, and science. Temporal measurement has occupied scientists and technologists and was a prime motivation in navigation and astronomy. Time is also of significant social importance, having economic value ("time is money") as well as personal value, due to an awareness of the limited time in each day and in human life spans.

Definition
Defining time in a manner applicable to all fields without circularity has consistently eluded scholars.[7][10] Nevertheless, diverse fields such as business, industry, sports, the sciences, and the performing arts all incorporate some notion of time into their respective measuring systems.[11][12][13] In physics, time is used to define other quantities, such as velocity, so defining time in terms of such quantities would result in circularity of definition.[14]

Time in physics is operationally defined as "what a clock reads".[6][15][16] This operational definition of time, wherein one says that observing a certain number of repetitions of one or another standard cyclical event constitutes one standard unit such as the second, is highly useful in the conduct of both advanced experiments and everyday affairs of life. There are many systems for determining what time it is. Periodic events and periodic motion have long served as standards for units of time. Examples include the apparent motion of the sun across the sky, the phases of the moon, and the passage of a free-swinging pendulum. More modern systems include the Global Positioning System, other satellite systems, Coordinated Universal Time and mean solar time. In general, the numbers obtained from different time systems differ from one another, but with careful measurements they can be synchronized.

The operational definition of time does not address what the fundamental nature of time is. Investigations into the relationship between space and time led physicists to define the spacetime continuum, where every event is assigned four numbers representing its time and position (the event's coordinates). Examples of events are the collision of two particles, the explosion of a supernova, or the arrival of a rocket ship. General relativity explains why the observed time of an event may be different for different observers. In general relativity, the question of what time it is now only has meaning relative to a particular observer. Distance and time are intimately related, and the time required for light to travel a specific distance is the same for all observers, as first publicly demonstrated by Michelson and Morley. Events can be separated in many directions in space, but if two events are separated by time, then one event must precede the other, and all observers will agree on this. General relativity does not address the nature of time for extremely small intervals where quantum mechanics holds. In quantum mechanics, time is treated as a universal and absolute parameter, differing from general relativity's notion of independent clocks. Reconciling these two theories is known as the problem of time. As of 2023, there is no generally accepted theory of quantum general relativity.[17]
$
10
Question 1: How is time often referred to, alongside spatial dimensions?
A: A first dimension
B: A secondary dimension
C: A fifth dimension
D: A scalar dimension
E: A fourth dimension
Answer: E

Question 2: What is the SI base unit of time?
A: Hour
B: Day
C: Minute
D: Second
E: Millennium
Answer: D

Question 3: How is the SI base unit of time, the second, defined?
A: By observing the phases of the moon
B: By the transition of helium atoms
C: By measuring the electronic transition frequency of caesium atoms
D: By the motion of the sun
E: By the tick of a standard clock
Answer: C

Question 4: In the context of general relativity, where can time be significantly distorted?
A: In spaces with high oxygen levels
B: At the edges of black holes
C: Near powerful magnets
D: In areas with low gravity
E: In the center of galaxies
Answer: B

Question 5: How has time been historically significant?
A: Only in philosophy
B: Primarily in navigation and astronomy
C: As a topic in religion, philosophy, and science
D: Just as a measurement in technology
E: Exclusively in the context of science fiction
Answer: C

Question 6: What challenge has scholars faced when defining time universally?
A: It is too abstract
B: It is a physical entity
C: It can be easily measured with clocks
D: Avoiding circularity in the definition
E: It is considered as a mere illusion
Answer: D

Question 7: How is time operationally defined in physics?
A: By observing the sun's movement
B: What a calendar indicates
C: What a clock reads
D: The intervals between events
E: The rate of change of quantities
Answer: C

Question 8: What are the examples given for events?
A: A rocket ship leaving Earth, an apple falling from a tree, and a supernova's explosion
B: A supernova's explosion, two atoms colliding, and a rocket ship's arrival
C: A particle accelerating, a tree growing, and the sun setting
D: A planet orbiting, a pendulum swinging, and a star forming
E: The tick of a clock, an eclipse occurring, and a comet's passing
Answer: B

Question 9: How is time viewed in quantum mechanics compared to general relativity?
A: As a dependent and relational parameter in both
B: As a universal and absolute parameter in quantum mechanics and as independent clocks in general relativity
C: As a localized entity in both theories
D: As a fourth dimension in both
E: As an irrelevant concept in quantum mechanics
Answer: B

Question 10: What remains unresolved as of 2023 concerning time in the field of physics?
A: The concept of spacetime in general relativity
B: The definition of a second in quantum mechanics
C: The integration of quantum mechanics and general relativity's notions of time
D: The ability to measure time accurately in black holes
E: The true nature of time's flow
Answer: C
@
Life expectancy is a statistical measure of the estimate of the span of a life. The most commonly used measure is life expectancy at birth (LEB), which can be defined in two ways. Cohort LEB is the mean length of life of a birth cohort (in this case, all individuals born in a given year) and can be computed only for cohorts born so long ago that all their members have died. Period LEB is the mean length of life of a hypothetical cohort[1][2] assumed to be exposed, from birth through death, to the mortality rates observed at a given year.[3] National LEB figures reported by national agencies and international organizations for human populations are estimates of period LEB.

In the Bronze Age and the Iron Age, human LEB was 26 years; in 2010, world LEB was 67.2 years. In recent years, LEB in Eswatini (formerly Swaziland) is 49, while LEB in Japan is 83. The combination of high infant mortality and deaths in young adulthood from accidents, epidemics, plagues, wars, and childbirth, before modern medicine was widely available, significantly lowers LEB. For example, a society with a LEB of 40 would have relatively few people dying at exactly 40: most will die before 30 or after 55. In populations with high infant mortality rates, LEB is highly sensitive to the rate of death in the first few years of life. Because of this sensitivity, LEB can be grossly misinterpreted, leading to the belief that a population with a low LEB would have a small proportion of older people.[4] A different measure, such as life expectancy at age 5 (e5), can be used to exclude the effect of infant mortality to provide a simple measure of overall mortality rates other than in early childhood. For instance, in a society with a life expectancy of 30, it may nevertheless be common to have a 40-year remaining timespan at age 5 (but perhaps not a 60-year one).

Until the middle of the 20th century, infant mortality was approximately 40–60% of the total mortality. Excluding child mortality, the average life expectancy during the 12th–19th centuries was approximately 55 years. If a person survived childhood, they had about a 50% chance of living 50–55 years, instead of only 25–40 years.[5] As of 2016, the overall worldwide life expectancy had reached the highest level that has been measured in modern times.[6]

Aggregate population measures—such as the proportion of the population in various age groups—are also used alongside individual-based measures—such as formal life expectancy—when analyzing population structure and dynamics. Pre-modern societies had universally higher mortality rates and lower life expectancies at every age for both males and females. This example is relatively rare.

Life expectancy, longevity, and maximum lifespan are not synonymous. Longevity refers to the relatively long lifespan of some members of a population. Maximum lifespan is the age at death for the longest-lived individual of a species. Mathematically, life expectancy is denoted 
�
�
e_{x} [a] and is the mean number of years of life remaining at a given age 
�
x, with a particular mortality.[7] Because life expectancy is an average, a particular person may die many years before or after the expected survival.

Life expectancy is also used in plant or animal ecology,[8] and in life tables (also known as actuarial tables). The concept of life expectancy may also be used in the context of manufactured objects,[9] though the related term[dubious – discuss] shelf life is commonly used for consumer products, and the terms "mean time to breakdown" and "mean time between failures" are used in engineering.
$
10
Question 1: Which of the following best defines Cohort LEB?
A: The mean length of life of individuals born in different years.
B: The mean length of life of a hypothetical cohort.
C: The mean length of life of a birth cohort born in a specific year and can be computed once all its members have died.
D: The average life expectancy of an entire population.
E: The mean length of life of a cohort exposed to the mortality rates of the current year.
Answer: C

Question 2: During which age was human LEB recorded at 26 years?
A: Middle Ages
B: Modern Age
C: Stone Age
D: Bronze Age and the Iron Age
E: Industrial Age
Answer: D

Question 3: What is Eswatini's Life Expectancy at Birth (LEB) compared to Japan's?
A: 26 and 83 respectively
B: 49 and 67.2 respectively
C: 67.2 and 26 respectively
D: 49 and 83 respectively
E: 55 and 40 respectively
Answer: D

Question 4: How can life expectancy at age 5 (e5) be particularly useful?
A: To incorporate the effect of infant mortality.
B: To exclude the effect of infant mortality and get an overall mortality rate other than in early childhood.
C: To measure the life expectancy of the elderly population.
D: To determine the maximum lifespan of a species.
E: To compute the longevity of a population.
Answer: B

Question 5: Which century recorded an average life expectancy of approximately 55 years, excluding child mortality?
A: 20th century
B: 16th century
C: Between 12th–19th centuries
D: 10th century
E: 21st century
Answer: C

Question 6: How do pre-modern societies compare in terms of mortality rates and life expectancies?
A: Lower mortality rates and higher life expectancies
B: Higher mortality rates and lower life expectancies
C: Similar mortality rates and life expectancies to modern societies
D: There's no significant difference between the two
E: Mortality rates varied, but life expectancies were higher
Answer: B

Question 7: Which of the following is NOT synonymous with life expectancy?
A: Longevity
B: Maximum lifespan
C: Average lifespan
D: Both A and B
E: None of the above
Answer: D

Question 8: In which context is the term "shelf life" commonly used?
A: Human populations
B: Engineering
C: Animal ecology
D: Plant ecology
E: Consumer products
Answer: E

Question 9: In relation to life expectancy, what does
�
�
e_{x} [a] denote mathematically?
A: The mortality rate at a given age
B: The mean number of years of life remaining at a given age
�
x
C: The mean lifespan of a hypothetical cohort
D: The total life expectancy of a species
E: The average mortality rate in a cohort
Answer: B

Question 10: Until the middle of the 20th century, approximately what percentage of total mortality was infant mortality?
A: 10-30%
B: 30-40%
C: 40-60%
D: 70-80%
E: 60-70%
Answer: C
@
Ecology (from Ancient Greek οἶκος (oîkos) 'house', and -λογία (-logía) 'study of')[A] is the study of the relationships among living organisms, including humans, and their physical environment. Ecology considers organisms at the individual, population, community, ecosystem, and biosphere level. Ecology overlaps with the closely related sciences of biogeography, evolutionary biology, genetics, ethology, and natural history. Ecology is a branch of biology, and it is not synonymous with environmentalism.

Among other things, ecology is the study of abundance, biomass, and distribution of organisms in the context of the environment; life processes, interactions, and adaptations; movement of materials and energy through living communities; successional development of ecosystems; cooperation, competition, and predation within and between species; and patterns of biodiversity and its effect on ecosystem processes.

Ecology has practical applications in conservation biology, wetland management, natural resource management (agroecology, agriculture, forestry, agroforestry, fisheries, mining, tourism), urban planning (urban ecology), community health, economics, basic and applied science, and human social interaction (human ecology).

The word ecology (German: Ökologie) was coined in 1866 by the German scientist Ernst Haeckel. The science of ecology as we know it today began with a group of American botanists in the 1890s.[1] Evolutionary concepts relating to adaptation and natural selection are cornerstones of modern ecological theory.

Ecosystems are dynamically interacting systems of organisms, the communities they make up, and the non-living (abiotic) components of their environment. Ecosystem processes, such as primary production, nutrient cycling, and niche construction, regulate the flux of energy and matter through an environment. Ecosystems have biophysical feedback mechanisms that moderate processes acting on living (biotic) and abiotic components of the planet. Ecosystems sustain life-supporting functions and provide ecosystem services like biomass production (food, fuel, fiber, and medicine), the regulation of climate, global biogeochemical cycles, water filtration, soil formation, erosion control, flood protection, and many other natural features of scientific, historical, economic, or intrinsic value.
$
10
Question 1: What is the main focus of ecology?
A: The study of individual organisms.
B: The study of relationships among living organisms and their environment.
C: The study of the evolutionary history of organisms.
D: The study of environmental protection.
E: The study of human interaction with nature.
Answer: B

Question 2: Which of the following is NOT synonymous with ecology?
A: Environmentalism
B: Biogeography
C: Ethology
D: Natural history
E: Evolutionary biology
Answer: A

Question 3: Ecology encompasses the study of organisms at which levels?
A: Individual, family, and community
B: Individual, community, and biosphere
C: Individual, population, community, ecosystem, and biosphere
D: Community, ecosystem, and universe
E: Population, ecosystem, and universe
Answer: C

Question 4: Who coined the word 'ecology'?
A: Charles Darwin
B: Albert Einstein
C: Ernst Haeckel
D: Gregor Mendel
E: Isaac Newton
Answer: C

Question 5: In which decade did modern ecological theory, as we recognize it today, start to form?
A: 1760s
B: 1840s
C: 1890s
D: 1920s
E: 1970s
Answer: C

Question 6: Which of the following best describes an ecosystem?
A: Only the living components of the environment.
B: Only the abiotic components of the environment.
C: A static system of organisms and their environment.
D: Dynamically interacting systems of organisms and both biotic and abiotic components.
E: A system only involving the interactions between different species.
Answer: D

Question 7: Which processes regulate the flux of energy and matter in an environment?
A: Photosynthesis and respiration
B: Evolution and natural selection
C: Primary production, nutrient cycling, and niche construction
D: Cooperation and competition
E: Adaptation and mutation
Answer: C

Question 8: What is one of the ecosystem services provided by ecosystems?
A: Atmospheric pressure regulation
B: Nutrient absorption in humans
C: Water filtration
D: Heat generation
E: Oxygen production in enclosed spaces
Answer: C

Question 9: Which concept is a cornerstone of modern ecological theory?
A: Ecosystem fragmentation
B: Symbiotic relationships
C: Adaptation and natural selection
D: Cellular respiration
E: Population dispersion
Answer: C

Question 10: In what field does ecology have a practical application?
A: Mathematics
B: Literature
C: Music
D: Wetland management
E: Political science
Answer: D
@
Genetics is the study of genes, genetic variation, and heredity in organisms.[1][2][3] It is an important branch in biology because heredity is vital to organisms' evolution. Gregor Mendel, a Moravian Augustinian friar working in the 19th century in Brno, was the first to study genetics scientifically. Mendel studied "trait inheritance", patterns in the way traits are handed down from parents to offspring over time. He observed that organisms (pea plants) inherit traits by way of discrete "units of inheritance". This term, still used today, is a somewhat ambiguous definition of what is referred to as a gene.

Trait inheritance and molecular inheritance mechanisms of genes are still primary principles of genetics in the 21st century, but modern genetics has expanded to study the function and behavior of genes. Gene structure and function, variation, and distribution are studied within the context of the cell, the organism (e.g. dominance), and within the context of a population. Genetics has given rise to a number of subfields, including molecular genetics, epigenetics and population genetics. Organisms studied within the broad field span the domains of life (archaea, bacteria, and eukarya).

Genetic processes work in combination with an organism's environment and experiences to influence development and behavior, often referred to as nature versus nurture. The intracellular or extracellular environment of a living cell or organism may increase or decrease gene transcription. A classic example is two seeds of genetically identical corn, one placed in a temperate climate and one in an arid climate (lacking sufficient waterfall or rain). While the average height of the two corn stalks may be genetically determined to be equal, the one in the arid climate only grows to half the height of the one in the temperate climate due to lack of water and nutrients in its environment.

The observation that living things inherit traits from their parents has been used since prehistoric times to improve crop plants and animals through selective breeding.[7][8] The modern science of genetics, seeking to understand this process, began with the work of the Augustinian friar Gregor Mendel in the mid-19th century.[9]

Prior to Mendel, Imre Festetics, a Hungarian noble, who lived in Kőszeg before Mendel, was the first who used the word "genetic" in hereditarian context. He described several rules of biological inheritance in his works The genetic laws of the Nature (Die genetischen Gesetze der Natur, 1819).[10] His second law is the same as what Mendel published.[11] In his third law, he developed the basic principles of mutation (he can be considered a forerunner of Hugo de Vries).[12] Festetics argued that changes observed in the generation of farm animals, plants, and humans are the result of scientific laws.[13] Festetics empirically deduced that organisms inherit their characteristics, not acquire them. He recognized recessive traits and inherent variation by postulating that traits of past generations could reappear later, and organisms could produce progeny with different attributes.[14] These observations represent an important prelude to Mendel's theory of particulate inheritance insofar as it features a transition of heredity from its status as myth to that of a scientific discipline, by providing a fundamental theoretical basis for genetics in the twentieth century.[10][15]


Blending inheritance leads to the averaging out of every characteristic, which as the engineer Fleeming Jenkin pointed out, makes evolution by natural selection impossible.
Other theories of inheritance preceded Mendel's work. A popular theory during the 19th century, and implied by Charles Darwin's 1859 On the Origin of Species, was blending inheritance: the idea that individuals inherit a smooth blend of traits from their parents.[16] Mendel's work provided examples where traits were definitely not blended after hybridization, showing that traits are produced by combinations of distinct genes rather than a continuous blend. Blending of traits in the progeny is now explained by the action of multiple genes with quantitative effects. Another theory that had some support at that time was the inheritance of acquired characteristics: the belief that individuals inherit traits strengthened by their parents. This theory (commonly associated with Jean-Baptiste Lamarck) is now known to be wrong—the experiences of individuals do not affect the genes they pass to their children.[17] Other theories included Darwin's pangenesis (which had both acquired and inherited aspects) and Francis Galton's reformulation of pangenesis as both particulate and inherited.[18]
$
10
Question 1: What is genetics primarily concerned with?
A: The study of environments.
B: The study of cells.
C: The study of genes, genetic variation, and heredity.
D: The study of animals.
E: The study of plants.
Answer: C

Question 2: Who is considered the first to study genetics scientifically?
A: Jean-Baptiste Lamarck
B: Francis Galton
C: Imre Festetics
D: Charles Darwin
E: Gregor Mendel
Answer: E

Question 3: How did Mendel describe the way traits are passed down?
A: Through blending of traits.
B: Through acquired characteristics.
C: Through discrete "units of inheritance".
D: Through environmental factors.
E: Through pangenesis.
Answer: C

Question 4: Which of the following is NOT a subfield of genetics?
A: Molecular genetics
B: Epigenetics
C: Genetic blending
D: Population genetics
E: Nature versus nurture
Answer: C

Question 5: What affects the height of genetically identical corn seeds when placed in different climates?
A: Genetic mutations.
B: Different types of genes.
C: External factors like water and nutrients.
D: The type of soil only.
E: The age of the seed.
Answer: C

Question 6: Who first used the word "genetic" in a hereditarian context before Mendel?
A: Fleeming Jenkin
B: Hugo de Vries
C: Imre Festetics
D: Jean-Baptiste Lamarck
E: Francis Galton
Answer: C

Question 7: Which theory suggested that individuals inherit a blend of traits from their parents?
A: Pangenesis
B: Particulate inheritance
C: Blending inheritance
D: Inheritance of acquired characteristics
E: Dominance inheritance
Answer: C

Question 8: According to the text, what did blending inheritance lead to?
A: Distinct genes being passed down.
B: Averaging out of every characteristic.
C: A mix of acquired and inherited traits.
D: Traits being produced by the environment.
E: Discrete units of inheritance.
Answer: B

Question 9: Who is associated with the theory of the inheritance of acquired characteristics?
A: Francis Galton
B: Jean-Baptiste Lamarck
C: Imre Festetics
D: Gregor Mendel
E: Charles Darwin
Answer: B

Question 10: Mendel's work was different from the blending inheritance theory because he provided examples where:
A: Traits strengthened by parents were inherited.
B: Traits were inherited through pangenesis.
C: Traits were definitely not blended after hybridization.
D: Traits were determined by the environment.
E: Traits were a continuous blend from both parents.
Answer: C
@
Bacteria (/bækˈtɪəriə/ i; sg: bacterium) are ubiquitous, mostly free-living organisms often consisting of one biological cell. They constitute a large domain of prokaryotic microorganisms. Typically a few micrometres in length, bacteria were among the first life forms to appear on Earth, and are present in most of its habitats. Bacteria inhabit soil, water, acidic hot springs, radioactive waste, and the deep biosphere of Earth's crust. Bacteria play a vital role in many stages of the nutrient cycle by recycling nutrients and the fixation of nitrogen from the atmosphere. The nutrient cycle includes the decomposition of dead bodies; bacteria are responsible for the putrefaction stage in this process. In the biological communities surrounding hydrothermal vents and cold seeps, extremophile bacteria provide the nutrients needed to sustain life by converting dissolved compounds, such as hydrogen sulphide and methane, to energy. Bacteria also live in symbiotic and parasitic relationships with plants and animals. Most bacteria have not been characterised and there are many species that cannot be grown in the laboratory. The study of bacteria is known as bacteriology, a branch of microbiology.

Humans and most other animals carry vast numbers (approximately 1013 to 1014) of bacteria.[2] Most are in the gut, and there are many on the skin. Most of the bacteria in and on the body are harmless or rendered so by the protective effects of the immune system, and many are beneficial,[3] particularly the ones in the gut. However, several species of bacteria are pathogenic and cause infectious diseases, including cholera, syphilis, anthrax, leprosy, tuberculosis, tetanus and bubonic plague. The most common fatal bacterial diseases are respiratory infections. Antibiotics are used to treat bacterial infections and are also used in farming, making antibiotic resistance a growing problem. Bacteria are important in sewage treatment and the breakdown of oil spills, the production of cheese and yogurt through fermentation, the recovery of gold, palladium, copper and other metals in the mining sector, as well as in biotechnology, and the manufacture of antibiotics and other chemicals.

Once regarded as plants constituting the class Schizomycetes ("fission fungi"), bacteria are now classified as prokaryotes. Unlike cells of animals and other eukaryotes, bacterial cells do not contain a nucleus and rarely harbour membrane-bound organelles. Although the term bacteria traditionally included all prokaryotes, the scientific classification changed after the discovery in the 1990s that prokaryotes consist of two very different groups of organisms that evolved from an ancient common ancestor. These evolutionary domains are called Bacteria and Archaea.[4]
$
10
Question 1: Which term best describes bacteria?
A: Eukaryotic multicellular organisms.
B: Mostly free-living single-celled organisms.
C: Large eukaryotic cells.
D: Prokaryotic multicellular organisms.
E: Fungal organisms.
Answer: B

Question 2: In which location are bacteria NOT found?
A: Acidic hot springs
B: Radioactive waste
C: Deep biosphere of Earth's crust
D: Outer space
E: Soil
Answer: D

Question 3: What role do bacteria play in the nutrient cycle?
A: Creation of oxygen
B: Production of minerals
C: Fixation of nitrogen from the atmosphere
D: Formation of rain clouds
E: Conversion of sunlight to energy
Answer: C

Question 4: The study of bacteria is referred to as:
A: Mycology
B: Biotechnology
C: Archaeology
D: Bacteriology
E: Microtechnology
Answer: D

Question 5: What is the approximate number of bacteria most animals carry?
A: 10^7 to 10^8
B: 10^9 to 10^10
C: 10^11 to 10^12
D: 10^13 to 10^14
E: 10^15 to 10^16
Answer: D

Question 6: Which of the following diseases is NOT caused by bacteria?
A: Cholera
B: Syphilis
C: Tuberculosis
D: Malaria
E: Anthrax
Answer: D

Question 7: Which application does NOT involve bacteria?
A: Sewage treatment
B: Production of cheese
C: Breakdown of oil spills
D: Photosynthesis
E: Manufacture of antibiotics
Answer: D

Question 8: Bacteria were once regarded as a class of:
A: Eukaryotes
B: Animals
C: Plants named Schizomycetes
D: Archaea
E: Protozoa
Answer: C

Question 9: Which statement about bacterial cells is true?
A: They contain a nucleus.
B: They often harbour many membrane-bound organelles.
C: They are classified as eukaryotes.
D: They do not contain a nucleus.
E: They are the largest cells on Earth.
Answer: D

Question 10: After a discovery in the 1990s, prokaryotes were divided into two evolutionary domains named:
A: Eukaryotes and Prokaryotes
B: Bacteria and Fungi
C: Animals and Plants
D: Bacteria and Viruses
E: Bacteria and Archaea
Answer: E
@
Radioactive waste is a type of hazardous waste that contains radioactive material. Radioactive waste is a result of many activities, including nuclear medicine, nuclear research, nuclear power generation, nuclear decommissioning, rare-earth mining, and nuclear weapons reprocessing.[1] The storage and disposal of radioactive waste is regulated by government agencies in order to protect human health and the environment.

Radioactive waste is broadly classified into low-level waste (LLW), such as paper, rags, tools, clothing, which contain small amounts of mostly short-lived radioactivity, intermediate-level waste (ILW), which contains higher amounts of radioactivity and requires some shielding, and high-level waste (HLW), which is highly radioactive and hot due to decay heat, so requires cooling and shielding.

In nuclear reprocessing plants about 96% of spent nuclear fuel is recycled back into uranium-based and mixed-oxide (MOX) fuels. The residual 4% is minor actinides and fission products the latter of which are a mixture of stable and quickly decaying (most likely already having decayed in the spent fuel pool) elements, medium lived fission products such as strontium-90 and caesium-137 and finally seven long-lived fission products with half lives in the hundreds of thousands to millions of years. The minor actinides meanwhile are heavy elements other than uranium and plutonium which are created by neutron capture. Their half lives range from years to millions of years and as alpha emitters they are particularly radiotoxic. While there are proposed - and to a much lesser extent current - uses of all those elements, commercial scale reprocessing using the PUREX-process disposes of them as waste together with the fission products. The waste is subsequently converted into a glass-like ceramic for storage in a deep geological repository.

The time radioactive waste must be stored for depends on the type of waste and radioactive isotopes it contains. Short-term approaches to radioactive waste storage have been segregation and storage on the surface or near-surface. Burial in a deep geological repository is a favored solution for long-term storage of high-level waste, while re-use and transmutation are favored solutions for reducing the HLW inventory. Boundaries to recycling of spent nuclear fuel are regulatory and economic as well as the issue of radioactive contamination if chemical separation processes cannot achieve a very high purity. Furthermore, elements may be present in both useful and troublesome isotopes, which would require costly and energy intensive isotope separation for their use - a currently uneconomic prospect.

A summary of the amounts of radioactive waste and management approaches for most developed countries are presented and reviewed periodically as part of a joint convention of the International Atomic Energy Agency (IAEA).[2]
$
10
Question 1: What is radioactive waste a result of?
A: Solar energy production.
B: Wind turbine operations.
C: Nuclear medicine and research.
D: Organic farming.
E: Electric vehicle production.
Answer: C

Question 2: Which agency regulates the storage and disposal of radioactive waste?
A: World Health Organization (WHO)
B: United Nations (UN)
C: International Renewable Energy Agency (IREA)
D: International Atomic Energy Agency (IAEA)
E: Environmental Protection Agency (EPA)
Answer: D

Question 3: Which of the following is NOT a classification of radioactive waste?
A: Intermediate-level waste (ILW)
B: Very high-level waste (VHLW)
C: Low-level waste (LLW)
D: High-level waste (HLW)
E: Medium-level waste (MLW)
Answer: B

Question 4: What percentage of spent nuclear fuel is recycled back into fuels in nuclear reprocessing plants?
A: 50%
B: 75%
C: 96%
D: 4%
E: 85%
Answer: C

Question 5: Minor actinides are:
A: Light elements derived from hydrogen.
B: Heavy elements other than uranium and plutonium.
C: Elements resulting from the decay of uranium.
D: Elements with short half-lives.
E: Elements recycled in nuclear reprocessing plants.
Answer: B

Question 6: What happens to the radioactive waste after reprocessing using the PUREX-process?
A: It is reused in nuclear reactors.
B: It is stored on the surface.
C: It is converted into a glass-like ceramic for storage.
D: It is released into the environment.
E: It is transmuted to a harmless state.
Answer: C

Question 7: What is a favored solution for long-term storage of high-level waste?
A: Storage on the surface.
B: Reuse and transmutation.
C: Release into the environment.
D: Conversion into a gas form.
E: Burial in a deep geological repository.
Answer: E

Question 8: What challenges the recycling of spent nuclear fuel?
A: Wind resistance and atmospheric decay.
B: Radioactive contamination and high purity requirements.
C: Solar radiation and climate change.
D: Water absorption and dilution.
E: Natural decomposition and disintegration.
Answer: B

Question 9: Which type of radioactive waste contains small amounts of mostly short-lived radioactivity?
A: Intermediate-level waste (ILW)
B: Medium-level waste (MLW)
C: Very high-level waste (VHLW)
D: High-level waste (HLW)
E: Low-level waste (LLW)
Answer: E

Question 10: The joint convention reviewing the amounts of radioactive waste and management approaches for developed countries is presented by:
A: World Health Organization (WHO)
B: United Nations (UN)
C: International Renewable Energy Agency (IREA)
D: International Atomic Energy Agency (IAEA)
E: Environmental Protection Agency (EPA)
Answer: D
@
A nuclear weapon[a] is an explosive device that derives its destructive force from nuclear reactions, either fission (fission bomb) or a combination of fission and fusion reactions (thermonuclear bomb), producing a nuclear explosion. Both bomb types release large quantities of energy from relatively small amounts of matter.

The first test of a fission ("atomic") bomb released an amount of energy approximately equal to 20,000 tons of TNT (84 TJ).[1] The first thermonuclear ("hydrogen") bomb test released energy approximately equal to 10 million tons of TNT (42 PJ). Nuclear bombs have had yields between 10 tons TNT (the W54) and 50 megatons for the Tsar Bomba (see TNT equivalent). A thermonuclear weapon weighing as little as 600 pounds (270 kg) can release energy equal to more than 1.2 megatonnes of TNT (5.0 PJ).[2]

A nuclear device no larger than a conventional bomb can devastate an entire city by blast, fire, and radiation. Since they are weapons of mass destruction, the proliferation of nuclear weapons is a focus of international relations policy. Nuclear weapons have been deployed twice in war, by the United States against the Japanese cities of Hiroshima and Nagasaki in 1945 during World War II.

Testing and deployment
Nuclear weapons have only twice been used in warfare, both times by the United States against Japan at the end of World War II. On August 6, 1945, the United States Army Air Forces (USAAF) detonated a uranium gun-type fission bomb nicknamed "Little Boy" over the Japanese city of Hiroshima; three days later, on August 9, the USAAF detonated a plutonium implosion-type fission bomb nicknamed "Fat Man" over the Japanese city of Nagasaki. These bombings caused injuries that resulted in the deaths of approximately 200,000 civilians and military personnel.[3] The ethics of these bombings and their role in Japan's surrender are subjects of debate.

Since the atomic bombings of Hiroshima and Nagasaki, nuclear weapons have been detonated over 2,000 times for testing and demonstration. Only a few nations possess such weapons or are suspected of seeking them. The only countries known to have detonated nuclear weapons—and acknowledge possessing them—are (chronologically by date of first test) the United States, the Soviet Union (succeeded as a nuclear power by Russia), the United Kingdom, France, China, India, Pakistan, and North Korea. Israel is believed to possess nuclear weapons, though, in a policy of deliberate ambiguity, it does not acknowledge having them. Germany, Italy, Turkey, Belgium and the Netherlands are nuclear weapons sharing states.[4][5][b] South Africa is the only country to have independently developed and then renounced and dismantled its nuclear weapons.[6]

The Treaty on the Non-Proliferation of Nuclear Weapons aims to reduce the spread of nuclear weapons, but its effectiveness has been questioned. Modernisation of weapons continues to this day.[7]
$
10
Question 1: From which reactions does a nuclear weapon derive its destructive force?
A: Chemical reactions
B: Biological reactions
C: Fission or a combination of fission and fusion
D: Combustion reactions
E: Evaporative reactions
Answer: C

Question 2: How much energy did the first test of a fission ("atomic") bomb release?
A: Equivalent to 1,000 tons of TNT
B: Equivalent to 10 million tons of TNT
C: Equivalent to 10 tons of TNT
D: Equivalent to 20,000 tons of TNT
E: Equivalent to 50 megatons of TNT
Answer: D

Question 3: What is the TNT equivalent energy release for a thermonuclear weapon weighing 600 pounds?
A: 10 tons
B: 20,000 tons
C: More than 1.2 megatonnes
D: 10 million tons
E: 50 megatons
Answer: C

Question 4: How many times have nuclear weapons been used in warfare?
A: Twice
B: Once
C: Three times
D: Four times
E: Never
Answer: A

Question 5: Which cities were bombed with nuclear weapons during World War II?
A: Tokyo and Kyoto
B: Hiroshima and Nagasaki
C: Berlin and London
D: Moscow and Beijing
E: Paris and Rome
Answer: B

Question 6: What was the nickname of the uranium gun-type fission bomb detonated over Hiroshima?
A: Fat Man
B: Big Boy
C: Little Boy
D: Tsar Bomba
E: H-Bomb
Answer: C

Question 7: How many times have nuclear weapons been detonated for testing and demonstration since the atomic bombings?
A: 200 times
B: 500 times
C: 1,000 times
D: Over 2,000 times
E: Under 100 times
Answer: D

Question 8: Which country is known to have independently developed and then renounced its nuclear weapons?
A: Brazil
B: Iran
C: South Korea
D: Canada
E: South Africa
Answer: E

Question 9: What is the primary aim of the Treaty on the Non-Proliferation of Nuclear Weapons?
A: To modernize nuclear weapons.
B: To allow all countries to have nuclear weapons.
C: To reduce the spread of nuclear weapons.
D: To increase nuclear testing.
E: To create stronger nuclear bombs.
Answer: C

Question 10: Which country detonated a nuclear weapon nicknamed "Fat Man"?
A: Russia
B: France
C: China
D: United States
E: North Korea
Answer: D
@
A thermonuclear weapon, fusion weapon or hydrogen bomb (H bomb) is a second-generation nuclear weapon design. Its greater sophistication affords it vastly greater destructive power than first-generation nuclear bombs, a more compact size, a lower mass, or a combination of these benefits. Characteristics of nuclear fusion reactions make possible the use of non-fissile depleted uranium as the weapon's main fuel, thus allowing more efficient use of scarce fissile material such as uranium-235 (235
U
) or plutonium-239 (239
Pu
). The first full-scale thermonuclear test was carried out by the United States in 1952; the concept has since been employed by most of the world's nuclear powers in the design of their weapons.[1]

Modern fusion weapons consist essentially of two main components: a nuclear fission primary stage (fueled by 235
U
 or 239
Pu
) and a separate nuclear fusion secondary stage containing thermonuclear fuel: the heavy hydrogen isotopes deuterium and tritium, or in modern weapons lithium deuteride. For this reason, thermonuclear weapons are often colloquially called hydrogen bombs or H-bombs.[note 1]

A fusion explosion begins with the detonation of the fission primary stage. Its temperature soars past approximately 100 million kelvin, causing it to glow intensely with thermal X-rays. These X-rays flood the void (the "radiation channel" often filled with polystyrene foam) between the primary and secondary assemblies placed within an enclosure called a radiation case, which confines the X-ray energy and resists its outward pressure. The distance separating the two assemblies ensures that debris fragments from the fission primary (which move much more slowly than X-ray photons) cannot disassemble the secondary before the fusion explosion runs to completion.

The secondary fusion stage—consisting of outer pusher/tamper, fusion fuel filler and central plutonium spark plug—is imploded by the X-ray energy impinging on its pusher/tamper. This compresses the entire secondary stage and drives up the density of the plutonium spark plug. The density of the plutonium fuel rises to such an extent that the spark plug is driven into a supercritical state, and it begins a nuclear fission chain reaction. The fission products of this chain reaction heat the highly compressed, and thus super dense, thermonuclear fuel surrounding the spark plug to around 300 million kelvin, igniting fusion reactions between fusion fuel nuclei. In modern weapons fueled by lithium deuteride, the fissioning plutonium spark plug also emits free neutrons that collide with lithium nuclei and supply the tritium component of the thermonuclear fuel.

The secondary's relatively massive tamper (which resists outward expansion as the explosion proceeds) also serves as a thermal barrier to keep the fusion fuel filler from becoming too hot, which would spoil the compression. If made of uranium, enriched uranium or plutonium, the tamper captures fast fusion neutrons and undergoes fission itself, increasing the overall explosive yield. Additionally, in most designs the radiation case is also constructed of a fissile material that undergoes fission driven by fast thermonuclear neutrons. Such bombs are classified as two stage weapons, and most current Teller–Ulam designs are such fission-fusion-fission weapons. Fast fission of the tamper and radiation case is the main contribution to the total yield and is the dominant process that produces radioactive fission product fallout.[2][3]

Before Ivy Mike, the first U.S. test of a weapon design, Operation Greenhouse in 1951 was the first American nuclear test series to test principles that led to the development of thermonuclear weapons. Sufficient fission was achieved to boost the associated fusion device, and enough was learned to achieve a full-scale device within a year. The design of all modern thermonuclear weapons in the United States is known as the Teller–Ulam configuration for its two chief contributors, Edward Teller and Stanisław Ulam, who developed it in 1951[4] for the United States, with certain concepts developed with the contribution of physicist John von Neumann. Similar devices were developed by the Soviet Union, United Kingdom, France, China and India.[5] The thermonuclear Tsar Bomba was the most powerful bomb ever detonated.[6]

As thermonuclear weapons represent the most efficient design for weapon energy yield in weapons with yields above 50 kilotons of TNT (210 TJ), virtually all the nuclear weapons of this size deployed by the five nuclear-weapon states under the Non-Proliferation Treaty today are thermonuclear weapons using the Teller–Ulam design.[7]
$
10
Question 1: Which of the following is another name for a thermonuclear weapon?
A: F-bomb
B: G-bomb
C: A-bomb
D: H-bomb
E: X-bomb
Answer: D

Question 2: What main benefit does the sophistication of a thermonuclear weapon provide?
A: Lower cost
B: Simplicity of design
C: Longer detonation time
D: Vastly greater destructive power
E: Less radioactive fallout
Answer: D

Question 3: The first full-scale thermonuclear test was carried out by which country?
A: Russia
B: China
C: United States
D: United Kingdom
E: India
Answer: C

Question 4: What elements are typically found in the nuclear fusion secondary stage of a fusion weapon?
A: Oxygen and Nitrogen
B: Carbon and Argon
C: Deuterium and Tritium
D: Helium and Neon
E: Krypton and Xenon
Answer: C

Question 5: Which process initiates a fusion explosion in a thermonuclear weapon?
A: Implosion of the fusion fuel filler
B: Expansion of the plutonium spark plug
C: Detonation of the fission primary stage
D: Compression of the secondary tamper
E: Reaction within the radiation case
Answer: C

Question 6: At approximately what temperature does the fission primary stage glow with thermal X-rays?
A: 10 million kelvin
B: 100 million kelvin
C: 1 billion kelvin
D: 100,000 kelvin
E: 50 million kelvin
Answer: B

Question 7: Which two scientists are credited with developing the Teller–Ulam configuration?
A: Richard Feynman and Niels Bohr
B: Albert Einstein and Robert Oppenheimer
C: Edward Teller and Stanisław Ulam
D: Enrico Fermi and John von Neumann
E: Andrei Sakharov and Lev Landau
Answer: C

Question 8: Which bomb is considered the most powerful ever detonated?
A: Ivy Mike
B: Fat Man
C: Tsar Bomba
D: Little Boy
E: Bravo
Answer: C

Question 9: Operation Greenhouse in 1951 was a test for which country?
A: Soviet Union
B: China
C: United Kingdom
D: France
E: United States
Answer: E

Question 10: Virtually all nuclear weapons with yields above how many kilotons of TNT use the Teller–Ulam design?
A: 10 kilotons
B: 25 kilotons
C: 50 kilotons
D: 100 kilotons
E: 200 kilotons
Answer: C
@
John von Neumann (/vɒn ˈnɔɪmən/ von NOY-mən; Hungarian: Neumann János Lajos [ˈnɒjmɒn ˈjaːnoʃ ˈlɒjoʃ]; December 28, 1903 – February 8, 1957) was a Hungarian-American mathematician, physicist, computer scientist, engineer and polymath. He was regarded as having perhaps the widest coverage of any mathematician of his time[13] and was said to have been "the last representative of the great mathematicians who were equally at home in both pure and applied mathematics".[14][15] He integrated pure and applied sciences.

Von Neumann made major contributions to many fields, including mathematics (mathematical logic, measure theory, functional analysis, ergodic theory, group theory, lattice theory, representation theory, operator algebras, matrix theory, geometry, and numerical analysis), physics (quantum mechanics, hydrodynamics & ballistics, nuclear physics and quantum statistical mechanics), economics (game theory and general equilibrium theory), computing (Von Neumann architecture, linear programming, numerical meteorology, scientific computing, self-replicating machines, stochastic computing), and statistics. He was a pioneer of the application of operator theory to quantum mechanics in the development of functional analysis, and a key figure in the development of game theory and the concepts of cellular automata, the universal constructor and the digital computer.

Von Neumann published over 150 papers: about 60 in pure mathematics, 60 in applied mathematics, 20 in physics, and the remainder on special mathematical subjects or non-mathematical subjects.[16] His last work, an unfinished manuscript written while he was dying, was later published in book form as The Computer and the Brain.

His analysis of the structure of self-replication preceded the discovery of the structure of DNA. In a shortlist of facts about his life he submitted to the National Academy of Sciences, he wrote, "The part of my work I consider most essential is that on quantum mechanics, which developed in Göttingen in 1926, and subsequently in Berlin in 1927–1929. Also, my work on various forms of operator theory, Berlin 1930 and Princeton 1935–1939; on the ergodic theorem, Princeton, 1931–1932."[17]

During World War II, von Neumann worked on the Manhattan Project with theoretical physicist Edward Teller, mathematician Stanislaw Ulam and others, problem-solving key steps in the nuclear physics involved in thermonuclear reactions and the hydrogen bomb. He developed the mathematical models behind the explosive lenses used in the implosion-type nuclear weapon and coined the term "kiloton" (of TNT) as a measure of the explosive force generated.[18] During this time and after the war, he consulted for a vast number of organizations including the Office of Scientific Research and Development, the Army's Ballistic Research Laboratory, the Armed Forces Special Weapons Project and the Oak Ridge National Laboratory.[19]

At the peak of his influence in the 1950s, he was the chair for a number of critical Defense Department committees including the Strategic Missile Evaluation Committee and the ICBM Scientific Advisory Committee. He was also a member of the influential Atomic Energy Commission in charge of all atomic energy development in the country. He played a key role alongside Bernard Schriever and Trevor Gardner in contributing to the design and development of the United States' first ICBM programs.[20] During this time, he was considered the nation's foremost expert on nuclear weaponry and the leading defense scientist at the Pentagon. As a Hungarian émigré, concerned that the Soviets would achieve nuclear superiority, he designed and promoted the policy of mutually assured destruction to limit the arms race.[21]

In honor of his achievements and contributions to the modern world, he was named in 1999 the Financial Times Person of the Century, as a representative of the century's characteristic ideal that the power of the mind could shape the physical world, and of the "intellectual brilliance and human savagery" that defined the 20th century.[22][23][24]
$
10
Question 1: What nationality was John von Neumann before becoming an American?
A: French
B: British
C: Hungarian
D: German
E: Polish
Answer: C

Question 2: Which of the following areas did von Neumann NOT make major contributions to?
A: Botany
B: Game theory
C: Quantum mechanics
D: Ergodic theory
E: Numerical analysis
Answer: A

Question 3: Which architecture associated with computing was named after von Neumann?
A: Turing architecture
B: Gate architecture
C: Shannon architecture
D: Von Neumann architecture
E: Babbage architecture
Answer: D

Question 4: What was the title of von Neumann's unfinished manuscript published posthumously?
A: Quantum Realities
B: Ergodic Theories and Applications
C: The Computer and the Brain
D: Operator Theories in Depth
E: The Nature of Mathematics
Answer: C

Question 5: Who did von Neumann collaborate with on the Manhattan Project during World War II?
A: Niels Bohr
B: Werner Heisenberg
C: Richard Feynman
D: Edward Teller
E: Enrico Fermi
Answer: D

Question 6: Which term, related to measuring explosive force, did von Neumann coin?
A: Megawatt
B: Terabyte
C: Kiloton
D: Microgram
E: Decibel
Answer: C

Question 7: What committee did von Neumann chair in the 1950s that was critical to the Defense Department?
A: Nuclear Research Evaluation Committee
B: Strategic Missile Evaluation Committee
C: Atomic Energy Regulation Committee
D: Defense Mathematics Committee
E: Advanced Weaponry Development Committee
Answer: B

Question 8: In what capacity was von Neumann recognized by the Financial Times in 1999?
A: World's Top Mathematician
B: Person of the Millennium
C: Person of the Century
D: Scientist of the Decade
E: Influencer of the Year
Answer: C

Question 9: Which of these cities is NOT mentioned in connection with von Neumann's work on quantum mechanics and operator theory?
A: Paris
B: Göttingen
C: Berlin
D: Princeton
E: London
Answer: A

Question 10: What was von Neumann's stance on the arms race with the Soviets?
A: He was indifferent.
B: He was a strong advocate for disarmament.
C: He was against any form of nuclear armament.
D: He designed and promoted the policy of mutually assured destruction.
E: He believed in a unilateral nuclear superiority for the US.
Answer: D
@
John von Neumann's universal constructor is a self-replicating machine in a cellular automaton (CA) environment. It was designed in the 1940s, without the use of a computer. The fundamental details of the machine were published in von Neumann's book Theory of Self-Reproducing Automata, completed in 1966 by Arthur W. Burks after von Neumann's death.[2] While typically not as well known as von Neumann's other work[according to whom?], it is regarded as foundational for automata theory, complex systems, and artificial life.[3][4] Indeed, Nobel Laureate Sydney Brenner considered Von Neumann's work on self-reproducing automata (together with Turing's work on computing machines) central to biological theory as well, allowing us to "discipline our thoughts about machines, both natural and artificial."[5]

Von Neumann's goal, as specified in his lectures at the University of Illinois in 1949,[2] was to design a machine whose complexity could grow automatically akin to biological organisms under natural selection. He asked what is the threshold of complexity that must be crossed for machines to be able to evolve.[4] His answer was to specify an abstract machine which, when run, would replicate itself. In his design, the self-replicating machine consists of three parts: a "description" of ('blueprint' or program for) itself, a universal constructor mechanism that can read any description and construct the machine (sans description) encoded in that description, and a universal copy machine that can make copies of any description. After the universal constructor has been used to construct a new machine encoded in the description, the copy machine is used to create a copy of that description, and this copy is passed on to the new machine, resulting in a working replication of the original machine that can keep on reproducing. Some machines will do this backwards, copying the description and then building a machine. Crucially, the self-reproducing machine can evolve by accumulating mutations of the description, not the machine itself, thus gaining the ability to grow in complexity.[4][5]

To define his machine in more detail, von Neumann invented the concept of a cellular automaton. The one he used consists of a two-dimensional grid of cells, each of which can be in one of 29 states at any point in time. At each timestep, each cell updates its state depending on the states of the surrounding cells at the prior timestep. The rules governing these updates are identical for all cells.

The universal constructor is a certain pattern of cell states in this cellular automaton. It contains one line of cells that serve as the description (akin to Turing's tape), encoding a sequence of instructions that serve as a 'blueprint' for the machine. The machine reads these instructions one by one and performs the corresponding actions. The instructions direct the machine to use its 'construction arm' (another automaton that functions like an Operating System[4]) to build a copy of the machine, without the description tape, at some other location in the cell grid. The description cannot contain instructions to build an equally long description tape, just as a container cannot contain a container of the same size. Therefore, the machine includes the separate copy machine which reads the description tape and passes a copy to the newly constructed machine. The resulting new set of universal constructor and copy machines plus description tape is identical to the old one, and it proceeds to replicate again.
$
10
Question 1: In which environment does John von Neumann's universal constructor operate?
A: Quantum Environment
B: Computer Operating System
C: Cellular Automaton (CA) Environment
D: Virtual Reality Environment
E: Silicon Chip
Answer: C

Question 2: When was the fundamental details of the universal constructor machine published in von Neumann's book Theory of Self-Reproducing Automata?
A: 1940s
B: 1950s
C: 1966
D: 1970s
E: 1980s
Answer: C

Question 3: Who completed the book Theory of Self-Reproducing Automata after von Neumann's death?
A: Sydney Brenner
B: Turing
C: John Conway
D: Arthur W. Burks
E: Richard Feynman
Answer: D

Question 4: According to the text, what was one of von Neumann's goals with the universal constructor?
A: To create a new programming language
B: To design a machine whose complexity could grow like biological organisms
C: To replace traditional computers
D: To simulate human intelligence
E: To understand quantum physics
Answer: B

Question 5: The self-replicating machine von Neumann conceptualized comprises three main components. Which of the following is NOT one of those components?
A: Universal constructor mechanism
B: Universal memory storage
C: Universal copy machine
D: Description of itself
E: Construction arm
Answer: B

Question 6: How does the self-replicating machine evolve over time?
A: Through mutations in the machine itself
B: Through accumulation of mutations in the description
C: By changing its operating system
D: Through physical adaptation to its environment
E: By gaining more memory storage
Answer: B

Question 7: Von Neumann's cellular automaton consists of a grid of cells. How many possible states can each cell be in?
A: 10
B: 15
C: 29
D: 40
E: 50
Answer: C

Question 8: How does the universal constructor read the machine's instructions?
A: From a hard drive
B: From a quantum state
C: From a line of cells serving as the description
D: From a silicon chip
E: From an external source
Answer: C

Question 9: Why can't the description contain instructions to build an equally long description tape?
A: Because it exceeds the machine's memory limit
B: Because it would cause the machine to malfunction
C: Just as a container cannot contain a container of the same size
D: Because it is forbidden in the cellular automaton rules
E: Because it would take too long
Answer: C

Question 10: What does the 'construction arm' of the machine do?
A: It provides power to the machine
B: It acts as a defense mechanism against threats
C: It builds a copy of the machine at another location in the cell grid
D: It serves as the primary processing unit of the machine
E: It connects to external devices for additional resources
Answer: C
@
A self-replicating machine is a type of autonomous robot that is capable of reproducing itself autonomously using raw materials found in the environment, thus exhibiting self-replication in a way analogous to that found in nature. The concept of self-replicating machines has been advanced and examined by Homer Jacobson, Edward F. Moore, Freeman Dyson, John von Neumann, Konrad Zuse[1][2] and in more recent times by K. Eric Drexler in his book on nanotechnology, Engines of Creation (coining the term clanking replicator for such machines) and by Robert Freitas and Ralph Merkle in their review Kinematic Self-Replicating Machines[3] which provided the first comprehensive analysis of the entire replicator design space. The future development of such technology is an integral part of several plans involving the mining of moons and asteroid belts for ore and other materials, the creation of lunar factories, and even the construction of solar power satellites in space. The von Neumann probe[4] is one theoretical example of such a machine. Von Neumann also worked on what he called the universal constructor, a self-replicating machine that would be able to evolve and which he formalized in a cellular automata environment. Notably, Von Neumann's Self-Reproducing Automata scheme posited that open-ended evolution requires inherited information to be copied and passed to offspring separately from the self-replicating machine, an insight that preceded the discovery of the structure of the DNA molecule by Watson and Crick and how it is separately translated and replicated in the cell.[5][6]

A self-replicating machine is an artificial self-replicating system that relies on conventional large-scale technology and automation. Although suggested earlier than in the late 1940's by Von Neumann, no self-replicating machine has been seen until today.[citation needed] Certain idiosyncratic terms are occasionally found in the literature. For example, the term clanking replicator was once used by Drexler[7] to distinguish macroscale replicating systems from the microscopic nanorobots or "assemblers" that nanotechnology may make possible, but the term is informal and is rarely used by others in popular or technical discussions. Replicators have also been called "von Neumann machines" after John von Neumann, who first rigorously studied the idea. However, the term "von Neumann machine" is less specific and also refers to a completely unrelated computer architecture that von Neumann proposed and so its use is discouraged where accuracy is important.[3] Von Neumann himself used the term universal constructor to describe such self-replicating machines.

Historians of machine tools, even before the numerical control era, sometimes figuratively said that machine tools were a unique class of machines because they have the ability to "reproduce themselves"[8] by copying all of their parts. Implicit in these discussions is that a human would direct the cutting processes (later planning and programming the machines), and would then assemble the parts. The same is true for RepRaps, which are another class of machines sometimes mentioned in reference to such non-autonomous "self-replication". In contrast, machines that are truly autonomously self-replicating (like biological machines) are the main subject discussed here.
$
10
Question 1: Who first rigorously studied the idea of self-replicating machines?
A: Freeman Dyson
B: K. Eric Drexler
C: Robert Freitas
D: John von Neumann
E: Homer Jacobson
Answer: D

Question 2: What term did K. Eric Drexler coin to describe self-replicating machines in his book on nanotechnology?
A: Universal constructor
B: Von Neumann machine
C: Autonomous replicator
D: Clanking replicator
E: Kinematic machine
Answer: D

Question 3: Which of the following activities is NOT mentioned as a potential application for self-replicating machines in the future?
A: Mining of moons and asteroid belts
B: Creation of lunar factories
C: Construction of solar power satellites in space
D: Deep sea exploration
E: The von Neumann probe
Answer: D

Question 4: In Von Neumann's Self-Reproducing Automata scheme, what is required for open-ended evolution?
A: The machine to be able to mutate itself
B: The machine to be in a cellular automata environment
C: Inherited information to be copied and passed separately from the machine
D: The presence of a universal constructor
E: The machine to have a solar power source
Answer: C

Question 5: As of the provided text, have any self-replicating machines been created?
A: Yes, in the early 1940s
B: Yes, but only in the microscopic scale
C: No, they remain theoretical
D: Yes, but only by John von Neumann
E: Yes, using nanotechnology by Drexler
Answer: C

Question 6: What is a key difference between the machines historians refer to that "reproduce themselves" and truly autonomously self-replicating machines?
A: The former are based on nanotechnology
B: The former are directed by humans in the process
C: The latter cannot reproduce all of their parts
D: The latter were proposed by von Neumann
E: The former can evolve autonomously
Answer: B

Question 7: What term is discouraged for use due to its ambiguity and reference to another computer architecture proposed by von Neumann?
A: Clanking replicator
B: Universal constructor
C: Von Neumann machine
D: RepRaps
E: Self-replicating automata
Answer: C

Question 8: Who provided the first comprehensive analysis of the entire replicator design space?
A: Homer Jacobson and Edward F. Moore
B: John von Neumann and Konrad Zuse
C: Robert Freitas and Ralph Merkle
D: Freeman Dyson and K. Eric Drexler
E: Watson and Crick
Answer: C

Question 9: Which of the following describes the term "clanking replicator"?
A: A robot that makes noise when moving
B: A macroscale replicating system
C: A type of computer architecture
D: A microscopic nanorobot
E: An outdated term for self-replicating machines
Answer: B

Question 10: Who among the following did NOT advance and examine the concept of self-replicating machines?
A: Homer Jacobson
B: Freeman Dyson
C: Richard Feynman
D: Edward F. Moore
E: Konrad Zuse
Answer: C
@
An autonomous robot is a robot that acts without recourse to human control. The first autonomous robots environment were known as Elmer and Elsie, which were constructed in the late 1940s by W. Grey Walter. They were the first robots in history that were programmed to "think" the way biological brains do and meant to have free will.[1] Elmer and Elsie were often labeled as tortoises because of how they were shaped and the manner in which they moved. They were capable of phototaxis which is the movement that occurs in response to light stimulus.[citation needed]

Historic examples include space probes. Modern examples include self-driving vacuums and cars. Industrial robot arms that work on assembly lines inside factories may also be considered autonomous robots, though their autonomy is restricted due to a highly structured environment and their inability to locomote.

Components and criteria of robotic autonomy
Self-maintenance
The first requirement for complete physical autonomy is the ability for a robot to take care of itself. Many of the battery-powered robots on the market today can find and connect to a charging station, and some toys like Sony's Aibo are capable of self-docking to charge their batteries.

Self-maintenance is based on "proprioception", or sensing one's own internal status. In the battery charging example, the robot can tell proprioceptively that its batteries are low and it then seeks the charger. Another common proprioceptive sensor is for heat monitoring. Increased proprioception will be required for robots to work autonomously near people and in harsh environments. Common proprioceptive sensors include thermal, optical, and haptic sensing, as well as the Hall effect (electric).

Sensing the environment
Exteroception is sensing things about the environment. Autonomous robots must have a range of environmental sensors to perform their task and stay out of trouble. The autonomous robot can recognize sensor failures and minimize the impact on the performance caused by failures.[2]

Common exteroceptive sensors include the electromagnetic spectrum, sound, touch, chemical (smell, odor), temperature, range to various objects, and altitude.
Some robotic lawn mowers will adapt their programming by detecting the speed in which grass grows as needed to maintain a perfectly cut lawn, and some vacuum cleaning robots have dirt detectors that sense how much dirt is being picked up and use this information to tell them to stay in one area longer.

Task performance
The next step in autonomous behavior is to actually perform a physical task. A new area showing commercial promise is domestic robots, with a flood of small vacuuming robots beginning with iRobot and Electrolux in 2002. While the level of intelligence is not high in these systems, they navigate over wide areas and pilot in tight situations around homes using contact and non-contact sensors. Both of these robots use proprietary algorithms to increase coverage over simple random bounce.

The next level of autonomous task performance requires a robot to perform conditional tasks. For instance, security robots can be programmed to detect intruders and respond in a particular way depending upon where the intruder is. For example, Amazon (company) launched its Astro for home monitoring, security and eldercare in September 2021.[3]
$
10
Question 1: Who constructed the first autonomous robots known as Elmer and Elsie?
A: Elmer Grey
B: W. Grey Walter
C: Sony
D: iRobot
E: Electrolux
Answer: B

Question 2: Why were Elmer and Elsie often referred to as tortoises?
A: They were programmed with tortoise intelligence
B: They were built from tortoise shells
C: They were shaped like tortoises and had similar movement
D: They had slow processing speeds
E: They were built to explore tortoise habitats
Answer: C

Question 3: Which of the following is NOT an example of an autonomous robot as per the text?
A: Space probes
B: Self-driving cars
C: Robotic lawn mowers
D: Cellular phones
E: Self-driving vacuums
Answer: D

Question 4: What is proprioception in the context of robotic autonomy?
A: Sensing the environment externally
B: Sensing one's own internal status
C: Programming to perform a physical task
D: Responding to light stimulus
E: Detecting intruders
Answer: B

Question 5: Which of the following sensors is NOT mentioned under proprioceptive sensors?
A: Thermal
B: Optical
C: Sound
D: Haptic
E: Hall effect
Answer: C

Question 6: What is exteroception?
A: Sensing the external environment
B: Sensing one's own internal status
C: Adapting to the growing speed of grass
D: Docking to charge batteries
E: Recognizing sensor failures
Answer: A

Question 7: Which company launched its robot for home monitoring, security, and eldercare in September 2021?
A: Sony
B: Electrolux
C: iRobot
D: W. Grey Walter
E: Amazon
Answer: E

Question 8: What do some robotic lawn mowers detect to adapt their programming?
A: Amount of sunlight received
B: The speed at which grass grows
C: Level of soil moisture
D: Presence of pests
E: Type of grass
Answer: B

Question 9: Which of the following tasks is NOT mentioned as a performance of autonomous behavior in robots?
A: Vacuuming homes
B: Monitoring home security
C: Sending emails
D: Detecting intruders
E: Responding to intruder locations
Answer: C

Question 10: What do some vacuum cleaning robots use to decide if they should stay in one area longer?
A: Proximity to the docking station
B: Amount of battery left
C: Presence of obstacles
D: Amount of dirt being picked up
E: Size of the area
Answer: D
@
A robot is a machine—especially one programmable by a computer—capable of carrying out a complex series of actions automatically.[2] A robot can be guided by an external control device, or the control may be embedded within. Robots may be constructed to evoke human form, but most robots are task-performing machines, designed with an emphasis on stark functionality, rather than expressive aesthetics.

Robots can be autonomous or semi-autonomous and range from humanoids such as Honda's Advanced Step in Innovative Mobility (ASIMO) and TOSY's TOSY Ping Pong Playing Robot (TOPIO) to industrial robots, medical operating robots, patient assist robots, dog therapy robots, collectively programmed swarm robots, UAV drones such as General Atomics MQ-1 Predator, and even microscopic nano robots. By mimicking a lifelike appearance or automating movements, a robot may convey a sense of intelligence or thought of its own. Autonomous things are expected to proliferate in the future, with home robotics and the autonomous car as some of the main drivers.[3]

The branch of technology that deals with the design, construction, operation, and application of robots,[4] as well as computer systems for their control, sensory feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, or cognition. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics. These robots have also created a newer branch of robotics: soft robotics.

From the time of ancient civilization, there have been many accounts of user-configurable automated devices and even automata resembling humans and other animals, such as animatronics, designed primarily as entertainment. As mechanical techniques developed through the Industrial age, there appeared more practical applications such as automated machines, remote-control and wireless remote-control.

The term comes from a Slavic root, robot-, with meanings associated with labor. The word 'robot' was first used to denote a fictional humanoid in a 1920 Czech-language play R.U.R. (Rossumovi Univerzální Roboti – Rossum's Universal Robots) by Karel Čapek, though it was Karel's brother Josef Čapek who was the word's true inventor.[5][6][7] Electronics evolved into the driving force of development with the advent of the first electronic autonomous robots created by William Grey Walter in Bristol, England in 1948, as well as Computer Numerical Control (CNC) machine tools in the late 1940s by John T. Parsons and Frank L. Stulen.

The first modern digital and programmable robot was invented by George Devol in 1954 and spawned his seminal robotics company, Unimation. The first Unimate was sold to General Motors in 1961 where it lifted pieces of hot metal from die casting machines at the Inland Fisher Guide Plant in the West Trenton section of Ewing Township, New Jersey.[8]

Robots have replaced humans[9] in performing repetitive and dangerous tasks which humans prefer not to do, or are unable to do because of size limitations, or which take place in extreme environments such as outer space or the bottom of the sea. There are concerns about the increasing use of robots and their role in society. Robots are blamed for rising technological unemployment as they replace workers in increasing numbers of functions.[10] The use of robots in military combat raises ethical concerns. The possibilities of robot autonomy and potential repercussions have been addressed in fiction and may be a realistic concern in the future.
$
10
Question 1: What is a robot primarily designed for in terms of aesthetics?
A: Expressive aesthetics
B: Stark functionality
C: Lifelike appearance
D: Resembling humans
E: Entertainment
Answer: B

Question 2: Which company developed the humanoid robot ASIMO?
A: TOSY
B: General Atomics
C: Unimation
D: Honda
E: Karel Čapek
Answer: D

Question 3: What is the branch of technology that handles the design, construction, and operation of robots called?
A: Robonomics
B: Robography
C: Robology
D: Robocraft
E: Robotics
Answer: E

Question 4: In which play was the word 'robot' first used to denote a fictional humanoid?
A: Robots of Bristol
B: Robots of England
C: R.U.R. (Rossum's Universal Robots)
D: Rise of Robots
E: Robots Unleashed
Answer: C

Question 5: Who invented the first modern digital and programmable robot?
A: John T. Parsons
B: Karel Čapek
C: George Devol
D: Frank L. Stulen
E: William Grey Walter
Answer: C

Question 6: To which company was the first Unimate sold?
A: Bristol Robotics
B: Unimation
C: Honda
D: TOSY
E: General Motors
Answer: E

Question 7: Robots have taken the place of humans in environments that are:
A: Adventurous
B: Exciting
C: Artistic
D: Extreme
E: Ordinary
Answer: D

Question 8: Who is credited with the invention of the word 'robot'?
A: Karel Čapek
B: William Grey Walter
C: George Devol
D: Josef Čapek
E: John T. Parsons
Answer: D

Question 9: Robots replacing workers in various tasks has led to concerns about:
A: Technical advancements
B: Decreased production costs
C: Technological unemployment
D: Reduced work hours for humans
E: Increased safety
Answer: C

Question 10: Which of the following is NOT a type of robot mentioned in the text?
A: Lawn mowing robot
B: UAV drones
C: Medical operating robots
D: Patient assist robots
E: Dog therapy robots
Answer: A
@
A machine is a physical system using power to apply forces and control movement to perform an action. The term is commonly applied to artificial devices, such as those employing engines or motors, but also to natural biological macromolecules, such as molecular machines. Machines can be driven by animals and people, by natural forces such as wind and water, and by chemical, thermal, or electrical power, and include a system of mechanisms that shape the actuator input to achieve a specific application of output forces and movement. They can also include computers and sensors that monitor performance and plan movement, often called mechanical systems.

Renaissance natural philosophers identified six simple machines which were the elementary devices that put a load into motion, and calculated the ratio of output force to input force, known today as mechanical advantage.[1]

Modern machines are complex systems that consist of structural elements, mechanisms and control components and include interfaces for convenient use. Examples include: a wide range of vehicles, such as trains, automobiles, boats and airplanes; appliances in the home and office, including computers, building air handling and water handling systems; as well as farm machinery, machine tools and factory automation systems and robots.

Mechanisms
The mechanism of a mechanical system is assembled from components called machine elements. These elements provide structure for the system and control its movement.

The structural components are, generally, the frame members, bearings, splines, springs, seals, fasteners and covers. The shape, texture and color of covers provide a styling and operational interface between the mechanical system and its users.

The assemblies that control movement are also called "mechanisms."[46][54] Mechanisms are generally classified as gears and gear trains, which includes belt drives and chain drives, cam and follower mechanisms, and linkages, though there are other special mechanisms such as clamping linkages, indexing mechanisms, escapements and friction devices such as brakes and clutches.

The number of degrees of freedom of a mechanism, or its mobility, depends on the number of links and joints and the types of joints used to construct the mechanism. The general mobility of a mechanism is the difference between the unconstrained freedom of the links and the number of constraints imposed by the joints. It is described by the Chebychev-Grübler-Kutzbach criterion.
$
10
Question 1: Which term describes a physical system that uses power to apply forces and achieve a certain action?
A: Computer
B: Mechanism
C: Machine
D: Interface
E: Linkage
Answer: C

Question 2: If a device employs engines or motors, and also might function due to chemical, thermal, or electrical power, it can be referred to as a:
A: Battery
B: Machine
C: Gear train
D: Fastener
E: Structure
Answer: B

Question 3: What foundational machines were identified by scholars during the Renaissance era that serve as the building blocks for complex machinery?
A: Compound machines
B: Simple machines
C: Mobile machines
D: Nano machines
E: Complex mechanisms
Answer: B

Question 4: What can be an essential feature of modern machines to help in monitoring performance and deciding on movement?
A: External power source
B: Molecular structure
C: Computers and sensors
D: Aesthetic design
E: Simple tools
Answer: C

Question 5: If you were to examine a mechanical system and focus on the components responsible for its structural foundation and movement control, you'd be looking at:
A: Control systems
B: Machine interfaces
C: Actuator systems
D: Machine elements
E: Power sources
Answer: D

Question 6: Which component often serves as the visible and tactile point of contact between a mechanical system and its user, potentially influencing user experience and system aesthetics?
A: Bearings
B: Actuators
C: Covers
D: Splines
E: Springs
Answer: C

Question 7: Which of the following systems has mechanisms commonly classified as gears, cam and follower mechanisms, and linkages?
A: Electrical circuits
B: Mechanical systems
C: Hydraulic systems
D: Optical systems
E: Thermal systems
Answer: B

Question 8: If you were trying to figure out the range of motion or movement possibilities of a mechanism, which criterion might you consult?
A: Newton's laws
B: Chebychev-Grübler-Kutzbach criterion
C: Maxwell's equations
D: Bernoulli's principle
E: Archimedes' principle
Answer: B

Question 9: What type of machine element might be crucial in enabling two rotating components to align and transfer motion?
A: Seal
B: Fastener
C: Bearing
D: Cover
E: Spring
Answer: C

Question 10: In a scenario where components interact to produce motion in a system, but are not driven by any external source, these components together form a:
A: Battery
B: Mechanism
C: Interface
D: Sensor
E: Computer
Answer: B
@
A computer is a machine that can be programmed to carry out sequences of arithmetic or logical operations (computation) automatically. Modern digital electronic computers can perform generic sets of operations known as programs. These programs enable computers to perform a wide range of tasks. A computer system is a nominally complete computer that includes the hardware, operating system (main software), and peripheral equipment needed and used for full operation. This term may also refer to a group of computers that are linked and function together, such as a computer network or computer cluster.

A broad range of industrial and consumer products use computers as control systems. Simple special-purpose devices like microwave ovens and remote controls are included, as are factory devices like industrial robots and computer-aided design, as well as general-purpose devices like personal computers and mobile devices like smartphones. Computers power the Internet, which links billions of other computers and users.

Early computers were meant to be used only for calculations. Simple manual instruments like the abacus have aided people in doing calculations since ancient times. Early in the Industrial Revolution, some mechanical devices were built to automate long, tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s. The speed, power and versatility of computers have been increasing dramatically ever since then, with transistor counts increasing at a rapid pace (as predicted by Moore's law), leading to the Digital Revolution during the late 20th to early 21st centuries.

Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU) in the form of a microprocessor, along with some type of computer memory, typically semiconductor memory chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved.
$
10
Question 1: What is the primary function of a computer?
A: Perform logical operations
B: Display information
C: Store data only
D: Connect to the internet
E: Drive mechanical machines
Answer: A

Question 2: In the context of a "computer system", what does the term potentially encompass?
A: Only the hardware
B: Only the software
C: Both hardware and operating system
D: Only peripheral equipment
E: The computer network only
Answer: C

Question 3: Which of the following can be considered a simple special-purpose device that uses a computer as a control system?
A: The Internet
B: Microwave oven
C: Computer network
D: Moore's law
E: Central processing unit
Answer: B

Question 4: Early computers were primarily designed for:
A: Internet browsing
B: Logical operations
C: Gaming
D: Calculations
E: Word processing
Answer: D

Question 5: Which manual instrument has historically assisted humans with calculations?
A: Keyboard
B: Mouse
C: MOSFET
D: Abacus
E: Joystick
Answer: D

Question 6: What was a major development in the realm of computers during World War II?
A: Invention of the Internet
B: Development of touchscreen
C: Creation of the silicon-based MOSFET
D: Development of the first digital electronic calculating machines
E: Establishment of Moore's law
Answer: D

Question 7: The silicon-based MOSFET and monolithic integrated circuit chip technologies in the late 1950s paved the way for which revolution in the 1970s?
A: Analog Revolution
B: Industrial Revolution
C: Microcomputer Revolution
D: Touchscreen Revolution
E: Digital Analog Revolution
Answer: C

Question 8: Which element typically carries out the arithmetic and logical operations in a modern computer?
A: Peripheral device
B: Monitor screen
C: Central processing unit (CPU)
D: Touchscreen
E: Joystick
Answer: C

Question 9: What is the primary function of peripheral devices in a computer system?
A: Carry out arithmetic operations
B: Store primary data
C: Allow information retrieval and save results of operations
D: Act as the main processing unit
E: Increase the speed of the computer
Answer: C

Question 10: Which of the following devices can both input data and display results?
A: Keyboard
B: Mouse
C: Monitor screen
D: Printer
E: Touchscreen
Answer: E
@
The history of computing is longer than the history of computing hardware and modern computing technology and includes the history of methods intended for pen and paper or for chalk and slate, with or without the aid of tables.

Concrete devices
Digital computing is intimately tied to the representation of numbers.[1] But long before abstractions like the number arose, there were mathematical concepts to serve the purposes of civilization. These concepts are implicit in concrete practices such as:

One-to-one correspondence,[2] a rule to count how many items, e.g. on a tally stick, eventually abstracted into numbers.
Comparison to a standard,[3] a method for assuming reproducibility in a measurement, for example, the number of coins.
The 3-4-5 right triangle was a device for assuring a right angle, using ropes with 12 evenly spaced knots, for example.[4][failed verification]

Numbers
Eventually, the concept of numbers became concrete and familiar enough for counting to arise, at times with sing-song mnemonics to teach sequences to others. All known human languages, except the Piraha language, have words for at least "one" and "two", and even some animals like the blackbird can distinguish a surprising number of items.[5]

Advances in the numeral system and mathematical notation eventually led to the discovery of mathematical operations such as addition, subtraction, multiplication, division, squaring, square root, and so forth. Eventually the operations were formalized, and concepts about the operations became understood well enough to be stated formally, and even proven. See, for example, Euclid's algorithm for finding the greatest common divisor of two numbers.

Early computation
Mathematical statements need not be abstract only; when a statement can be illustrated with actual numbers, the numbers can be communicated and a community can arise. This allows the repeatable, verifiable statements which are the hallmark of mathematics and science. These kinds of statements have existed for thousands of years, and across multiple civilizations, as shown below:

The earliest known tool for use in computation is the Sumerian abacus, and it was thought to have been invented in Babylon c. 2700–2300 BC. Its original style of usage was by lines drawn in sand with pebbles. Abaci, of a more modern design, are still used as calculation tools today. This was the first known calculator and most advanced system of calculation known to date - preceding Archimedes by 2,000 years.

In c. 1050–771 BC, the south-pointing chariot was invented in ancient China. It was the first known geared mechanism to use a differential gear, which was later used in analog computers. The Chinese also invented a more sophisticated abacus from around the 2nd century BC known as the Chinese abacus.[8]

In the 5th century BC in ancient India, the grammarian Pāṇini formulated the grammar of Sanskrit in 3959 rules known as the Ashtadhyayi which was highly systematized and technical. Panini used metarules, transformations and recursions.[9]

In the 3rd century BC, Archimedes used the mechanical principle of balance (see Archimedes Palimpsest § The Method of Mechanical Theorems) to calculate mathematical problems, such as the number of grains of sand in the universe (The sand reckoner), which also required a recursive notation for numbers (e.g., the myriad myriad).

The Antikythera mechanism is believed to be the earliest known mechanical analog computer.[10] It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC.

Digital electronic computers
In an 1886 letter, Charles Sanders Peirce described how logical operations could be carried out by electrical switching circuits.[30] During 1880-81 he showed that NOR gates alone (or alternatively NAND gates alone) can be used to reproduce the functions of all the other logic gates, but this work on it was unpublished until 1933.[31] The first published proof was by Henry M. Sheffer in 1913, so the NAND logical operation is sometimes called Sheffer stroke; the logical NOR is sometimes called Peirce's arrow.[32] Consequently, these gates are sometimes called universal logic gates.[33]

Eventually, vacuum tubes replaced relays for logic operations. Lee De Forest's modification, in 1907, of the Fleming valve can be used as a logic gate. Ludwig Wittgenstein introduced a version of the 16-row truth table as proposition 5.101 of Tractatus Logico-Philosophicus (1921). Walther Bothe, inventor of the coincidence circuit, got part of the 1954 Nobel Prize in physics, for the first modern electronic AND gate in 1924. Konrad Zuse designed and built electromechanical logic gates for his computer Z1 (from 1935 to 1938).

The first recorded idea of using digital electronics for computing was the 1931 paper "The Use of Thyratrons for High Speed Automatic Counting of Physical Phenomena" by C. E. Wynn-Williams.[34] From 1934 to 1936, NEC engineer Akira Nakashima, Claude Shannon, and Victor Shestakov published papers introducing switching circuit theory, using digital electronics for Boolean algebraic operations.[35][36][37][38]

In 1935 Alan Turing wrote his seminal paper On Computable Numbers, with an Application to the Entscheidungsproblem[39] in which he modeled computation in terms of a one-dimensional storage tape, leading to the idea of the Universal Turing machine and Turing-complete systems.[citation needed]

The first digital electronic computer was developed in the period April 1936 - June 1939, in the IBM Patent Department, Endicott, New York by Arthur Halsey Dickinson.[40][41][42] In this computer IBM introduced for the first time, a calculating device with keyboard, processor and electronic output (display). Competitor to IBM was the digital electronic computer NCR3566, developed in NCR, Dayton, Ohio by Joseph Desch and Robert Mumma in the period April 1939 - August 1939.[43][44] The IBM and NCR machines were decimal, executing addition and subtraction in binary position code.
$
10
Question 1: What was one of the earliest known tools used for computation?
A: Archimedes' balance
B: The south-pointing chariot
C: The Chinese abacus
D: The Sumerian abacus
E: The Antikythera mechanism
Answer: D

Question 2: In the context of ancient computational methods, what did the 3-4-5 right triangle assure?
A: A perfect circle
B: A repeating decimal
C: A recursive notation
D: A right angle
E: An infinite sequence
Answer: D

Question 3: Which civilization formulated the grammar of Sanskrit in highly systematized and technical rules in the 5th century BC?
A: Chinese
B: Greek
C: Sumerian
D: Indian
E: Babylonian
Answer: D

Question 4: Which mechanism is believed to be the earliest known mechanical analog computer?
A: The Ashtadhyayi
B: Archimedes' balance
C: The south-pointing chariot
D: The Antikythera mechanism
E: The Chinese abacus
Answer: D

Question 5: Charles Sanders Peirce described how logical operations could be carried out using which component in an 1886 letter?
A: Transistors
B: Electromechanical logic gates
C: NOR gates
D: Vacuum tubes
E: Electrical switching circuits
Answer: E

Question 6: Who got part of the 1954 Nobel Prize in physics for the first modern electronic AND gate?
A: Konrad Zuse
B: Akira Nakashima
C: Alan Turing
D: Walther Bothe
E: Arthur Halsey Dickinson
Answer: D

Question 7: Who wrote the seminal paper "On Computable Numbers, with an Application to the Entscheidungsproblem"?
A: C. E. Wynn-Williams
B: Ludwig Wittgenstein
C: Charles Sanders Peirce
D: Alan Turing
E: Archimedes
Answer: D

Question 8: Which computer was developed by Arthur Halsey Dickinson in the IBM Patent Department?
A: NCR3566
B: Computer Z1
C: Universal Turing machine
D: The first digital electronic computer
E: Antikythera mechanism
Answer: D

Question 9: The Chinese invented a more sophisticated abacus around when?
A: 5th century BC
B: 3rd century BC
C: 2nd century BC
D: 1st century BC
E: 1st century AD
Answer: C

Question 10: Who introduced the concept of using digital electronics for Boolean algebraic operations?
A: Akira Nakashima, Claude Shannon, and Victor Shestakov
B: C. E. Wynn-Williams
C: Walther Bothe
D: Konrad Zuse
E: Archimedes
Answer: A
@
A number is a mathematical object used to count, measure, and label. The original examples are the natural numbers 1, 2, 3, 4, and so forth.[1] Numbers can be represented in language with number words. More universally, individual numbers can be represented by symbols, called numerals; for example, "5" is a numeral that represents the number five. As only a relatively small number of symbols can be memorized, basic numerals are commonly organized in a numeral system, which is an organized way to represent any number. The most common numeral system is the Hindu–Arabic numeral system, which allows for the representation of any number using a combination of ten fundamental numeric symbols, called digits.[2][a] In addition to their use in counting and measuring, numerals are often used for labels (as with telephone numbers), for ordering (as with serial numbers), and for codes (as with ISBNs). In common usage, a numeral is not clearly distinguished from the number that it represents.

In mathematics, the notion of number has been extended over the centuries to include zero (0),[3] negative numbers,[4] rational numbers such as one half 
(
1
2
)
{\displaystyle \left({\tfrac {1}{2}}\right)}, real numbers such as the square root of 2 
(
2
)
{\displaystyle \left({\sqrt {2}}\right)} and π,[5] and complex numbers[6] which extend the real numbers with a square root of −1 (and its combinations with real numbers by adding or subtracting its multiples).[4] Calculations with numbers are done with arithmetical operations, the most familiar being addition, subtraction, multiplication, division, and exponentiation. Their study or usage is called arithmetic, a term which may also refer to number theory, the study of the properties of numbers.

Besides their practical uses, numbers have cultural significance throughout the world.[7][8] For example, in Western society, the number 13 is often regarded as unlucky, and "a million" may signify "a lot" rather than an exact quantity.[7] Though it is now regarded as pseudoscience, belief in a mystical significance of numbers, known as numerology, permeated ancient and medieval thought.[9] Numerology heavily influenced the development of Greek mathematics, stimulating the investigation of many problems in number theory which are still of interest today.[9]

During the 19th century, mathematicians began to develop many different abstractions which share certain properties of numbers, and may be seen as extending the concept. Among the first were the hypercomplex numbers, which consist of various extensions or modifications of the complex number system. In modern mathematics, number systems are considered important special examples of more general algebraic structures such as rings and fields, and the application of the term "number" is a matter of convention, without fundamental significance.[10]
$
10
Question 1: Which of the following is a numeral that represents the number five?
A: Seven
B: Five
C: 5
D: F
E: IV
Answer: C

Question 2: What is the most common numeral system?
A: Greek Numeral System
B: Roman Numeral System
C: Babylonian Numeral System
D: Egyptian Numeral System
E: Hindu–Arabic Numeral System
Answer: E

Question 3: How many fundamental numeric symbols are used in the Hindu–Arabic numeral system to represent any number?
A: Five
B: Ten
C: Twenty
D: Sixteen
E: Twelve
Answer: B

Question 4: Which of the following is NOT a purpose for which numerals are often used?
A: Counting
B: Drawing
C: Measuring
D: Labeling
E: Ordering
Answer: B

Question 5: In Western society, which number is often regarded as unlucky?
A: 7
B: 10
C: 13
D: 12
E: 11
Answer: C

Question 6: What term refers to the belief in the mystical significance of numbers?
A: Arithmetic
B: Algebra
C: Trigonometry
D: Numerology
E: Geometry
Answer: D

Question 7: Which mathematical operations are most commonly associated with numbers?
A: Integration, differentiation
B: Tangent, sine, cosine
C: Addition, subtraction, multiplication, division, exponentiation
D: Matrix, vector
E: Circle, square, triangle
Answer: C

Question 8: Which of the following numbers extends the real numbers by adding a square root of -1?
A: Rational numbers
B: Natural numbers
C: Complex numbers
D: Whole numbers
E: Prime numbers
Answer: C

Question 9: In modern mathematics, what are considered important special examples of more general algebraic structures such as rings and fields?
A: Shapes
B: Equations
C: Graphs
D: Number systems
E: Constants
Answer: D

Question 10: Which concept, regarded now as pseudoscience, heavily influenced the development of Greek mathematics?
A: Geometry
B: Calculus
C: Trigonometry
D: Algebra
E: Numerology
Answer: E
@
Mathematics is an area of knowledge that includes the topics of numbers, formulas and related structures, shapes and the spaces in which they are contained, and quantities and their changes. These topics are represented in modern mathematics with the major subdisciplines of number theory,[1] algebra,[2] geometry,[1] and analysis,[3][4] respectively. There is no general consensus among mathematicians about a common definition for their academic discipline.

Most mathematical activity involves the discovery of properties of abstract objects and the use of pure reason to prove them. These objects consist of either abstractions from nature or—in modern mathematics—entities that are stipulated to have certain properties, called axioms. A proof consists of a succession of applications of deductive rules to already established results. These results include previously proved theorems, axioms, and—in case of abstraction from nature—some basic properties that are considered true starting points of the theory under consideration.[5]

Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent from any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics), but often later find practical applications.[6][7] The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks.

Historically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements.[8] Since its beginning, mathematics was essentially divided into geometry and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra[a] and infinitesimal calculus were introduced as new areas. Since then, the interaction between mathematical innovations and scientific discoveries has led to a rapid lockstep increase in the development of both.[9] At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method,[10] which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than 60 first-level areas of mathematics.

Areas of mathematics
Before the Renaissance, mathematics was divided into two main areas: arithmetic, regarding the manipulation of numbers, and geometry, regarding the study of shapes.[18] Some types of pseudoscience, such as numerology and astrology, were not then clearly distinguished from mathematics.[19]

During the Renaissance, two more areas appeared. Mathematical notation led to algebra which, roughly speaking, consists of the study and the manipulation of formulas. Calculus, consisting of the two subfields differential calculus and integral calculus, is the study of continuous functions, which model the typically nonlinear relationships between varying quantities, as represented by variables. This division into four main areas–arithmetic, geometry, algebra, calculus[20]–endured until the end of the 19th century. Areas such as celestial mechanics and solid mechanics were then studied by mathematicians, but now are considered as belonging to physics.[21] The subject of combinatorics has been studied for much of recorded history, yet did not become a separate branch of mathematics until the seventeenth century.[22]

At the end of the 19th century, the foundational crisis in mathematics and the resulting systematization of the axiomatic method led to an explosion of new areas of mathematics.[23][10] The 2020 Mathematics Subject Classification contains no less than sixty-three first-level areas.[24] Some of these areas correspond to the older division, as is true regarding number theory (the modern name for higher arithmetic) and geometry. Several other first-level areas have "geometry" in their names or are otherwise commonly considered part of geometry. Algebra and calculus do not appear as first-level areas but are respectively split into several first-level areas. Other first-level areas emerged during the 20th century or had not previously been considered as mathematics, such as mathematical logic and foundations.[25]
$
10
Question 1: Which of the following is NOT listed as a major subdiscipline of modern mathematics in the subject text?
A: Probability
B: Number theory
C: Geometry
D: Algebra
E: Analysis
Answer: A

Question 2: How do mathematical proofs derive their results?
A: Intuition
B: Scientific experimentation
C: Deductive rules applied to already established results
D: Purely from axioms
E: Through observation of nature
Answer: C

Question 3: The foundational truths of mathematics are:
A: Dependent on scientific experimentation.
B: Subject to change based on practical applications.
C: Independent from any scientific experimentation.
D: Derived purely from observation.
E: Based on axioms and conjectures.
Answer: C

Question 4: Which area of mathematics dates back to Euclid in 300 BC and later found a practical application in the security of computer networks?
A: Statistics
B: Game theory
C: Integer factorization
D: Geometry
E: Calculus
Answer: C

Question 5: The concept of a mathematical proof and its associated rigour first emerged in:
A: Renaissance mathematics
B: 19th century mathematics
C: Greek mathematics
D: Modern mathematics
E: Medieval mathematics
Answer: C

Question 6: Before the Renaissance, mathematics was essentially divided into which two areas?
A: Algebra and calculus
B: Arithmetic and geometry
C: Analysis and algebra
D: Geometry and calculus
E: Arithmetic and analysis
Answer: B

Question 7: During the Renaissance, which two areas of mathematics appeared?
A: Combinatorics and analysis
B: Differential calculus and integral calculus
C: Geometry and number theory
D: Algebra and calculus
E: Arithmetic and mathematical logic
Answer: D

Question 8: Combinatorics became a separate branch of mathematics in which century?
A: Fifteenth century
B: Seventeenth century
C: Nineteenth century
D: Twentieth century
E: Eighteenth century
Answer: B

Question 9: How many first-level areas of mathematics are listed in the 2020 Mathematics Subject Classification?
A: Four
B: Twenty
C: Sixty-three
D: Forty-five
E: Fifty
Answer: C

Question 10: Which of the following areas emerged during the 20th century or was not previously considered as mathematics?
A: Geometry
B: Algebra
C: Mathematical logic and foundations
D: Arithmetic
E: Calculus
Answer: C
@
Abstraction in mathematics is the process of extracting the underlying structures, patterns or properties of a mathematical concept, removing any dependence on real world objects with which it might originally have been connected, and generalizing it so that it has wider applications or matching among other abstract descriptions of equivalent phenomena.[1][2][3] Two of the most highly abstract areas of modern mathematics are category theory and model theory.

Description
Many areas of mathematics began with the study of real world problems, before the underlying rules and concepts were identified and defined as abstract structures. For example, geometry has its origins in the calculation of distances and areas in the real world, and algebra started with methods of solving problems in arithmetic.

Abstraction is an ongoing process in mathematics and the historical development of many mathematical topics exhibits a progression from the concrete to the abstract. For example, the first steps in the abstraction of geometry were historically made by the ancient Greeks, with Euclid's Elements being the earliest extant documentation of the axioms of plane geometry—though Proclus tells of an earlier axiomatisation by Hippocrates of Chios.[4] In the 17th century, Descartes introduced Cartesian co-ordinates which allowed the development of analytic geometry. Further steps in abstraction were taken by Lobachevsky, Bolyai, Riemann and Gauss, who generalised the concepts of geometry to develop non-Euclidean geometries. Later in the 19th century, mathematicians generalised geometry even further, developing such areas as geometry in n dimensions, projective geometry, affine geometry and finite geometry. Finally Felix Klein's "Erlangen program" identified the underlying theme of all of these geometries, defining each of them as the study of properties invariant under a given group of symmetries. This level of abstraction revealed connections between geometry and abstract algebra.[5]

In mathematics, abstraction can be advantageous in the following ways:

It reveals deep connections between different areas of mathematics.
Known results in one area can suggest conjectures in another related area.
Techniques and methods from one area can be applied to prove results in other related areas.
Patterns from one mathematical object can be generalized to other similar objects in the same class.
On the other hand, abstraction can also be disadvantageous in that highly abstract concepts can be difficult to learn.[6] A degree of mathematical maturity and experience may be needed for conceptual assimilation of abstractions.

Bertrand Russell, in The Scientific Outlook (1931), writes that "Ordinary language is totally unsuited for expressing what physics really asserts, since the words of everyday life are not sufficiently abstract. Only mathematics and mathematical logic can say as little as the physicist means to say."[7]
$
10
Question 1: What is the primary aim of abstraction in mathematics?
A: To connect mathematics with real-world problems
B: To derive new mathematical operations
C: To extract underlying structures or patterns of a concept
D: To redefine basic mathematical principles
E: To make mathematics more complex
Answer: C

Question 2: Which two areas are noted as the most abstract in modern mathematics?
A: Geometry and algebra
B: Algebra and arithmetic
C: Category theory and model theory
D: Arithmetic and category theory
E: Geometry and category theory
Answer: C

Question 3: Geometry originally began with the study of:
A: Abstract shapes
B: Real-world distances and areas
C: Theoretical points and lines
D: Algebraic equations
E: Logical patterns
Answer: B

Question 4: Who is credited with the earliest documentation of the axioms of plane geometry?
A: Lobachevsky
B: Descartes
C: Euclid
D: Bolyai
E: Gauss
Answer: C

Question 5: In the 17th century, which mathematical concept allowed the development of analytic geometry?
A: Non-Euclidean geometries
B: Axioms of plane geometry
C: Cartesian co-ordinates
D: Finite geometry
E: Erlangen program
Answer: C

Question 6: Which mathematician or mathematicians are credited with the generalization of geometry to develop non-Euclidean geometries?
A: Euclid and Descartes
B: Lobachevsky and Bolyai
C: Gauss and Riemann
D: Hippocrates and Proclus
E: Klein and Gauss
Answer: B

Question 7: Felix Klein's "Erlangen program" defined geometries as studies of properties invariant under what?
A: Axiomatic principles
B: Analytic structures
C: Mathematical abstractions
D: Given group of symmetries
E: Cartesian coordinates
Answer: D

Question 8: One of the advantages of mathematical abstraction is that it:
A: Simplifies all mathematical concepts
B: Makes every mathematical topic concrete
C: Reveals connections between different areas of mathematics
D: Ensures all mathematical problems have solutions
E: Creates a one-size-fits-all approach to mathematical problems
Answer: C

Question 9: What challenge can arise from highly abstract mathematical concepts?
A: They are always incorrect
B: They are universally simple to understand
C: They are too interconnected with real-world applications
D: They can be difficult to learn
E: They avoid patterns and structures
Answer: D

Question 10: According to Bertrand Russell in "The Scientific Outlook", why is mathematics essential in expressing what physics asserts?
A: Mathematics is always accurate
B: Only mathematics can be applied in real-world scenarios
C: Ordinary language is not sufficiently abstract
D: Physics cannot exist without mathematical proofs
E: All physicists are also mathematicians
Answer: C
@
Geometry (from Ancient Greek γεωμετρία (geōmetría) 'land measurement'; from γῆ (gê) 'earth, land', and μέτρον (métron) 'a measure')[1] is a branch of mathematics concerned with properties of space such as the distance, shape, size, and relative position of figures.[2] Geometry is, along with arithmetic, one of the oldest branches of mathematics. A mathematician who works in the field of geometry is called a geometer. Until the 19th century, geometry was almost exclusively devoted to Euclidean geometry,[a] which includes the notions of point, line, plane, distance, angle, surface, and curve, as fundamental concepts.[3]

Originally developed to model the physical world, geometry has applications in almost all sciences, and also in art, architecture, and other activities that are related to graphics.[4] Geometry also has applications in areas of mathematics that are apparently unrelated. For example, methods of algebraic geometry are fundamental in Wiles's proof of Fermat's Last Theorem, a problem that was stated in terms of elementary arithmetic, and remained unsolved for several centuries.

During the 19th century several discoveries enlarged dramatically the scope of geometry. One of the oldest such discoveries is Carl Friedrich Gauss' Theorema Egregium ("remarkable theorem") that asserts roughly that the Gaussian curvature of a surface is independent from any specific embedding in a Euclidean space. This implies that surfaces can be studied intrinsically, that is, as stand-alone spaces, and has been expanded into the theory of manifolds and Riemannian geometry. Later in the 19th century, it appeared that geometries without the parallel postulate (non-Euclidean geometries) can be developed without introducing any contradiction. The geometry that underlies general relativity is a famous application of non-Euclidean geometry.

Since the late 19th century, the scope of geometry has been greatly expanded, and the field has been split in many subfields that depend on the underlying methods—differential geometry, algebraic geometry, computational geometry, algebraic topology, discrete geometry (also known as combinatorial geometry), etc.—or on the properties of Euclidean spaces that are disregarded—projective geometry that consider only alignment of points but not distance and parallelism, affine geometry that omits the concept of angle and distance, finite geometry that omits continuity, and others. This enlargement of the scope of geometry led to a change of meaning of the word "space", which originally referred to the three-dimensional space of the physical world and its model provided by Euclidean geometry; presently a geometric space, or simply a space is a mathematical structure on which some geometry is defined.
$
10
Question 1: What is the primary concern of geometry?
A: The study of arithmetic sequences
B: Properties of space such as distance, shape, size, and position of figures
C: The study of algebraic equations
D: Analysis of large data sets
E: Examination of the laws of physics
Answer: B

Question 2: Along with which other branch is geometry one of the oldest branches of mathematics?
A: Algebra
B: Analysis
C: Arithmetic
D: Trigonometry
E: Calculus
Answer: C

Question 3: What is a mathematician specializing in geometry called?
A: Algebraist
B: Analyst
C: Arithmetician
D: Geometer
E: Topologist
Answer: D

Question 4: Which geometry was almost exclusively studied until the 19th century?
A: Riemannian geometry
B: Non-Euclidean geometry
C: Euclidean geometry
D: Algebraic geometry
E: Projective geometry
Answer: C

Question 5: Geometry was originally developed to model which domain?
A: Abstract mathematical spaces
B: Linguistic structures
C: Computer algorithms
D: The physical world
E: Historical events
Answer: D

Question 6: Which theorem by Carl Friedrich Gauss suggested that surfaces can be studied as standalone spaces?
A: Parallel Postulate
B: Fermat's Last Theorem
C: Theorema Egregium
D: Pythagorean Theorem
E: Golden Ratio Principle
Answer: C

Question 7: Which geometry underlies general relativity?
A: Euclidean geometry
B: Differential geometry
C: Non-Euclidean geometry
D: Algebraic geometry
E: Discrete geometry
Answer: C

Question 8: What does projective geometry primarily focus on?
A: Distance and angle
B: Curvature and surface
C: Point alignment without considering distance and parallelism
D: Computational calculations
E: Finite measurements
Answer: C

Question 9: The term "space" in the context of geometry originally referred to what?
A: Two-dimensional paper surfaces
B: Abstract mathematical constructions
C: The three-dimensional space of the physical world
D: Digital realms
E: Non-Euclidean dimensions
Answer: C

Question 10: In modern geometry, what is a "space"?
A: Any physical area
B: Only three-dimensional areas
C: A mathematical structure where geometry is defined
D: Virtual reality domains
E: Areas studied in differential calculus
Answer: C
@
Arithmetic (from Ancient Greek ἀριθμός (arithmós) 'number', and τική [τέχνη] (tikḗ [tékhnē]) 'art, craft') is an elementary part of mathematics that consists of the study of the properties of the traditional operations on numbers—addition, subtraction, multiplication, division, exponentiation, and extraction of roots. In the 19th century, Italian mathematician Giuseppe Peano formalized arithmetic with his Peano axioms,[disputed – discuss] which are highly important to the field of mathematical logic today.

History
Main article: History of arithmetic
The prehistory of arithmetic is limited to a small number of artifacts that may indicate the conception of addition and subtraction; the best-known is the Ishango bone from central Africa, dating from somewhere between 20,000 and 18,000 BC, although its interpretation is disputed.[1]

The earliest written records indicate the Egyptians and Babylonians used all the elementary arithmetic operations: addition, subtraction, multiplication, and division, as early as 2000 BC. These artifacts do not always reveal the specific process used for solving problems, but the characteristics of the particular numeral system strongly influence the complexity of the methods. The hieroglyphic system for Egyptian numerals, like the later Roman numerals, descended from tally marks used for counting. In both cases, this origin resulted in values that used a decimal base but did not include positional notation. Complex calculations with Roman numerals required the assistance of a counting board (or the Roman abacus) to obtain the results.

Early number systems that included positional notation were not decimal; these include the sexagesimal (base 60) system for Babylonian numerals and the vigesimal (base 20) system that defined Maya numerals. Because of the place-value concept, the ability to reuse the same digits for different values contributed to simpler and more efficient methods of calculation.

The continuous historical development of modern arithmetic starts with the Hellenistic period of ancient Greece; it originated much later than the Babylonian and Egyptian examples. Prior to the works of Euclid around 300 BC, Greek studies in mathematics overlapped with philosophical and mystical beliefs. Nicomachus is an example of this viewpoint, using the earlier Pythagorean approach to numbers and their relationships to each other in his work, Introduction to Arithmetic.

Greek numerals were used by Archimedes, Diophantus, and others in a positional notation not very different from modern notation. The ancient Greeks lacked a symbol for zero until the Hellenistic period, and they used three separate sets of symbols as digits: one set for the units place, one for the tens place, and one for the hundreds. For the thousands place, they would reuse the symbols for the units place, and so on. Their addition algorithm was identical to the modern method, and their multiplication algorithm was only slightly different. Their long division algorithm was the same, and the digit-by-digit square root algorithm, popularly used as recently as the 20th century, was known to Archimedes (who may have invented it). He preferred it to Hero's method of successive approximation because, once computed, a digit does not change, and the square roots of perfect squares, such as 7485696, terminate immediately as 2736. For numbers with a fractional part, such as 546.934, they used negative powers of 60 instead of negative powers of 10 for the fractional part 0.934.[2]

The ancient Chinese had advanced arithmetic studies dating from the Shang Dynasty and continuing through the Tang Dynasty, from basic numbers to advanced algebra. The ancient Chinese used a positional notation similar to that of the Greeks. Since they also lacked a symbol for zero, they had one set of symbols for the units place and a second set for the tens place. For the hundreds place, they then reused the symbols for the units place, and so on. Their symbols were based on ancient counting rods. The exact time when the Chinese started calculating with positional representation is unknown, though it is known that the adoption started before 400 BC.[3] The ancient Chinese were the first to meaningfully discover, understand, and apply negative numbers. This is explained in the Nine Chapters on the Mathematical Art (Jiuzhang Suanshu), which was written by Liu Hui and dates back to the 2nd century BC.

The gradual development of the Hindu–Arabic numeral system independently devised the place-value concept and positional notation, which combined the simpler methods for computations with a decimal base and the use of a digit representing 0. This allowed the system to consistently represent both large and small integers—an approach that eventually replaced all other systems. In the early 6th century AD, the Indian mathematician Aryabhata incorporated an existing version of this system into his work and experimented with different notations. In the 7th century, Brahmagupta established the use of 0 as a separate number and determined the results for multiplication, division, addition, and subtraction of zero and all other numbers—except for the result of division by zero. His contemporary, the Syriac bishop Severus Sebokht (650 AD) said, "Indians possess a method of calculation that no word can praise enough. Their rational system of mathematics, or of their method of calculation. I mean the system using nine symbols."[4] The Arabs also learned this new method and called it hesab.


Leibniz's Stepped Reckoner was the first calculator that could perform all four arithmetic operations.
Although the Codex Vigilanus described an early form of Arabic numerals (omitting 0) by 976 AD, Leonardo of Pisa (Fibonacci) was primarily responsible for spreading their use throughout Europe after the publication of his book Liber Abaci in 1202. He wrote, "The method of the Indians (Latin Modus Indorum) surpasses any known method to compute. It's a marvelous method. They do their computations using nine figures and symbol zero".[5]

In the Middle Ages, arithmetic was one of the seven liberal arts taught in universities.

The flourishing of algebra in the medieval Islamic world, and also in Renaissance Europe, was an outgrowth of the enormous simplification of computation through decimal notation.

Various types of tools have been invented and widely used to assist in numeric calculations. Before Renaissance, they were various types of abaci. More recent examples include slide rules, nomograms and mechanical calculators, such as Pascal's calculator. At present, they have been supplanted by electronic calculators and computers.
$
10
Question 1: What does arithmetic primarily study?
A: The traditional operations on numbers
B: Ancient mathematical tools
C: Greek mythology
D: Algebraic equations
E: Medieval arts
Answer: A

Question 2: Who formalized arithmetic in the 19th century with the Peano axioms?
A: Archimedes
B: Aryabhata
C: Fibonacci
D: Giuseppe Peano
E: Brahmagupta
Answer: D

Question 3: The Ishango bone, possibly indicating concepts of addition and subtraction, originated from where?
A: Ancient Greece
B: Medieval Europe
C: Central Africa
D: Babylon
E: Egypt
Answer: C

Question 4: Which ancient civilizations were known to use basic arithmetic operations like addition, subtraction, multiplication, and division around 2000 BC?
A: Greeks and Romans
B: Babylonians and Egyptians
C: Mayans and Chinese
D: Indians and Persians
E: Hellenistic and Greek
Answer: B

Question 5: Which number system was used by the Babylonians?
A: Decimal (base 10)
B: Vigesimal (base 20)
C: Sexagesimal (base 60)
D: Binary (base 2)
E: Tetradecimal (base 14)
Answer: C

Question 6: Which historical figure is associated with the digit-by-digit square root algorithm?
A: Leonardo of Pisa (Fibonacci)
B: Brahmagupta
C: Aryabhata
D: Archimedes
E: Euclid
Answer: D

Question 7: The Chinese were known to use which type of counting tool?
A: Abacus
B: Slide rule
C: Counting rods
D: Nomograms
E: Decimal charts
Answer: C

Question 8: Which numeral system incorporated the use of a digit representing 0 and allowed for a consistent representation of both large and small integers?
A: Roman numerals
B: Greek numerals
C: Babylonian numerals
D: Egyptian numerals
E: Hindu–Arabic numeral system
Answer: E

Question 9: Which mathematician of the early 6th century AD incorporated the place-value concept into his work?
A: Archimedes
B: Aryabhata
C: Leonardo of Pisa (Fibonacci)
D: Giuseppe Peano
E: Euclid
Answer: B

Question 10: During the Middle Ages, arithmetic was considered one of how many liberal arts taught in universities?
A: Five
B: Six
C: Seven
D: Eight
E: Nine
Answer: C
@
Mathematical logic is the study of formal logic within mathematics. Major subareas include model theory, proof theory, set theory, and recursion theory (also known as computability theory). Research in mathematical logic commonly addresses the mathematical properties of formal systems of logic such as their expressive or deductive power. However, it can also include uses of logic to characterize correct mathematical reasoning or to establish foundations of mathematics.

Since its inception, mathematical logic has both contributed to and been motivated by the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.

Subfields and scope
The Handbook of Mathematical Logic[1] in 1977 makes a rough division of contemporary mathematical logic into four areas:

set theory
model theory
recursion theory, and
proof theory and constructive mathematics (considered as parts of a single area).
Additionally, sometimes the field of computational complexity theory is also included as part of mathematical logic.[2] Each area has a distinct focus, although many techniques and results are shared among multiple areas. The borderlines amongst these fields, and the lines separating mathematical logic and other fields of mathematics, are not always sharp. Gödel's incompleteness theorem marks not only a milestone in recursion theory and proof theory, but has also led to Löb's theorem in modal logic. The method of forcing is employed in set theory, model theory, and recursion theory, as well as in the study of intuitionistic mathematics.

The mathematical field of category theory uses many formal axiomatic methods, and includes the study of categorical logic, but category theory is not ordinarily considered a subfield of mathematical logic. Because of its applicability in diverse fields of mathematics, mathematicians including Saunders Mac Lane have proposed category theory as a foundational system for mathematics, independent of set theory. These foundations use toposes, which resemble generalized models of set theory that may employ classical or nonclassical logic.

History
Mathematical logic emerged in the mid-19th century as a subfield of mathematics, reflecting the confluence of two traditions: formal philosophical logic and mathematics.[3] "Mathematical logic, also called 'logistic', 'symbolic logic', the 'algebra of logic', and, more recently, simply 'formal logic', is the set of logical theories elaborated in the course of the last Nineteenth Century with the aid of an artificial notation and a rigorously deductive method."[4] Before this emergence, logic was studied with rhetoric, with calculationes,[5] through the syllogism, and with philosophy. The first half of the 20th century saw an explosion of fundamental results, accompanied by vigorous debate over the foundations of mathematics.
$
10
Question 1: What does mathematical logic study?
A: The philosophy of mathematics
B: Historical events of mathematical figures
C: Formal logic within mathematics
D: The rhetoric of mathematics
E: The psychology behind mathematical reasoning
Answer: C

Question 2: Which of these is NOT a major subarea of mathematical logic?
A: Algebra
B: Set theory
C: Proof theory
D: Model theory
E: Recursion theory
Answer: A

Question 3: Who was a key figure in the early 20th century that aimed to prove the consistency of foundational theories?
A: Saunders Mac Lane
B: Aristotle
C: David Hilbert
D: Gerhard Gentzen
E: Löb
Answer: C

Question 4: Which theorem provided clarity on issues involved in proving consistency?
A: Löb's theorem
B: Hilbert's foundation theorem
C: Gödel's incompleteness theorem
D: Mac Lane's theorem
E: The theorem of rhetoric
Answer: C

Question 5: What does the field of computational complexity theory sometimes fall under?
A: Algebra
B: Geometry
C: Mathematical logic
D: Arithmetic
E: Physics
Answer: C

Question 6: Which method is employed in both set theory and model theory?
A: Deductive method
B: Forcing method
C: Syllogism method
D: Categorical logic
E: Rhetoric method
Answer: B

Question 7: Which field of mathematics uses many formal axiomatic methods but isn't typically seen as a subfield of mathematical logic?
A: Set theory
B: Computational complexity theory
C: Category theory
D: Proof theory
E: Recursion theory
Answer: C

Question 8: Who proposed category theory as a foundational system for mathematics?
A: Löb
B: Aristotle
C: Gödel
D: Saunders Mac Lane
E: David Hilbert
Answer: D

Question 9: Before emerging as a subfield of mathematics, how was logic traditionally studied?
A: Through the lens of calculus
B: With computational systems
C: With rhetoric and the syllogism
D: With recursion theory
E: Through category theory
Answer: C

Question 10: What was a distinct feature of mathematical logic as it emerged in the mid-19th century?
A: Use of modern computational tools
B: A shift away from formal methods
C: Integration of an artificial notation and a rigorously deductive method
D: Heavy reliance on philosophy
E: Primarily focused on set theory
Answer: C
@
In theoretical computer science and mathematics, computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.

A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying their computational complexity, i.e., the amount of resources needed to solve them, such as time and storage. Other measures of complexity are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. The P versus NP problem, one of the seven Millennium Prize Problems, is dedicated to the field of computational complexity.[1]

Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kinds of problems can, in principle, be solved algorithmically.

Problem instances
A computational problem can be viewed as an infinite collection of instances together with a set (possibly empty) of solutions for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of primality testing. The instance is a number (e.g., 15) and the solution is "yes" if the number is prime and "no" otherwise (in this case, 15 is not prime and the answer is "no"). Stated another way, the instance is a particular input to the problem, and the solution is the output corresponding to the given input.

To further highlight the difference between a problem and an instance, consider the following instance of the decision version of the traveling salesman problem: Is there a route of at most 2000 kilometres passing through all of Germany's 15 largest cities? The quantitative answer to this particular problem instance is of little use for solving other instances of the problem, such as asking for a round trip through all sites in Milan whose total length is at most 10 km. For this reason, complexity theory addresses computational problems and not particular problem instances.

Representing problem instances
When considering computational problems, a problem instance is a string over an alphabet. Usually, the alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are bitstrings. As in a real-world computer, mathematical objects other than bitstrings must be suitably encoded. For example, integers can be represented in binary notation, and graphs can be encoded directly via their adjacency matrices, or by encoding their adjacency lists in binary.

Even though some proofs of complexity-theoretic theorems regularly assume some concrete choice of input encoding, one tries to keep the discussion abstract enough to be independent of the choice of encoding. This can be achieved by ensuring that different representations can be transformed into each other efficiently.
$
10
Question 1: What is the primary focus of computational complexity theory?
A: Designing new algorithms
B: Building efficient computer hardware
C: Classifying computational problems by resource usage
D: Creating new programming languages
E: Fixing software bugs
Answer: C

Question 2: Which of the following best describes a computational problem?
A: A software malfunction
B: A hardware malfunction
C: A task solved by human intervention
D: A task solved by a computer
E: A mathematical theory
Answer: D

Question 3: If a problem requires significant resources to solve, it is considered:
A: Easily solvable
B: Inherently difficult
C: Unimportant
D: Unsolvable
E: Trivial
Answer: B

Question 4: What is NOT a measure of computational complexity?
A: Time
B: Storage
C: Communication
D: Processor temperature
E: Number of gates in a circuit
Answer: D

Question 5: Which Millennium Prize Problem is dedicated to the field of computational complexity?
A: Fermat's Last Theorem
B: Poincaré Conjecture
C: P versus NP problem
D: Birch and Swinnerton-Dyer Conjecture
E: Navier–Stokes existence and smoothness
Answer: C

Question 6: How does analysis of algorithms differ from computational complexity theory?
A: Analysis of algorithms focuses on all possible algorithms; computational complexity focuses on one specific algorithm.
B: Analysis of algorithms focuses on the amount of resources needed by a specific algorithm; computational complexity theory asks about all possible algorithms.
C: They are the same.
D: Analysis of algorithms is a subset of computational complexity theory.
E: Analysis of algorithms is not related to computer science.
Answer: B

Question 7: In computational complexity theory, a problem instance is:
A: The answer to a computational problem.
B: The same as the computational problem itself.
C: A particular input for a decision problem.
D: A type of algorithm.
E: A specific type of computer.
Answer: C

Question 8: When discussing the problem of primality testing, if the instance is the number 15, what is the solution?
A: Yes
B: Maybe
C: No
D: Prime
E: 15
Answer: C

Question 9: How is a problem instance typically represented in computational problems?
A: As a sequence of algorithms
B: As a string over an alphabet
C: As a geometric figure
D: As a set of mathematical formulas
E: As a flowchart
Answer: B

Question 10: When encoding mathematical objects in computational problems, which of the following is commonly used as the default alphabet?
A: The Latin alphabet
B: The Greek alphabet
C: The Cyrillic alphabet
D: The ternary alphabet
E: The binary alphabet
Answer: E
@
The travelling salesman problem (TSP) asks the following question: "Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city?" It is an NP-hard problem in combinatorial optimization, important in theoretical computer science and operations research.

The travelling purchaser problem and the vehicle routing problem are both generalizations of TSP.

In the theory of computational complexity, the decision version of the TSP (where given a length L, the task is to decide whether the graph has a tour of at most L) belongs to the class of NP-complete problems. Thus, it is possible that the worst-case running time for any algorithm for the TSP increases superpolynomially (but no more than exponentially) with the number of cities.

The problem was first formulated in 1930 and is one of the most intensively studied problems in optimization. It is used as a benchmark for many optimization methods. Even though the problem is computationally difficult, many heuristics and exact algorithms are known, so that some instances with tens of thousands of cities can be solved completely and even problems with millions of cities can be approximated within a small fraction of 1%.[1]

The TSP has several applications even in its purest formulation, such as planning, logistics, and the manufacture of microchips. Slightly modified, it appears as a sub-problem in many areas, such as DNA sequencing. In these applications, the concept city represents, for example, customers, soldering points, or DNA fragments, and the concept distance represents travelling times or cost, or a similarity measure between DNA fragments. The TSP also appears in astronomy, as astronomers observing many sources will want to minimize the time spent moving the telescope between the sources; in such problems, the TSP can be embedded inside an optimal control problem. In many applications, additional constraints such as limited resources or time windows may be imposed.

Description
As a graph problem
TSP can be modelled as an undirected weighted graph, such that cities are the graph's vertices, paths are the graph's edges, and a path's distance is the edge's weight. It is a minimization problem starting and finishing at a specified vertex after having visited each other vertex exactly once. Often, the model is a complete graph (i.e., each pair of vertices is connected by an edge). If no path exists between two cities, adding a sufficiently long edge will complete the graph without affecting the optimal tour.

Asymmetric and symmetric
In the symmetric TSP, the distance between two cities is the same in each opposite direction, forming an undirected graph. This symmetry halves the number of possible solutions. In the asymmetric TSP, paths may not exist in both directions or the distances might be different, forming a directed graph. Traffic collisions, one-way streets, and airfares for cities with different departure and arrival fees are examples of how this symmetry could break down.

Related problems
An equivalent formulation in terms of graph theory is: Given a complete weighted graph (where the vertices would represent the cities, the edges would represent the roads, and the weights would be the cost or distance of that road), find a Hamiltonian cycle with the least weight.
The requirement of returning to the starting city does not change the computational complexity of the problem, see Hamiltonian path problem.
Another related problem is the bottleneck travelling salesman problem (bottleneck TSP): Find a Hamiltonian cycle in a weighted graph with the minimal weight of the weightiest edge. For example, avoiding narrow streets with big buses.[14] The problem is of considerable practical importance, apart from evident transportation and logistics areas. A classic example is in printed circuit manufacturing: scheduling of a route of the drill machine to drill holes in a PCB. In robotic machining or drilling applications, the "cities" are parts to machine or holes (of different sizes) to drill, and the "cost of travel" includes time for retooling the robot (single machine job sequencing problem).[15]
The generalized travelling salesman problem, also known as the "travelling politician problem", deals with "states" that have (one or more) "cities" and the salesman has to visit exactly one "city" from each "state". One application is encountered in ordering a solution to the cutting stock problem in order to minimize knife changes. Another is concerned with drilling in semiconductor manufacturing, see e.g., U.S. Patent 7,054,798. Noon and Bean demonstrated that the generalized travelling salesman problem can be transformed into a standard TSP with the same number of cities, but a modified distance matrix.
The sequential ordering problem deals with the problem of visiting a set of cities where precedence relations between the cities exist.
A common interview question at Google is how to route data among data processing nodes; routes vary by time to transfer the data, but nodes also differ by their computing power and storage, compounding the problem of where to send data.
The travelling purchaser problem deals with a purchaser who is charged with purchasing a set of products. He can purchase these products in several cities, but at different prices and not all cities offer the same products. The objective is to find a route between a subset of the cities that minimizes total cost (travel cost + purchasing cost) and enables the purchase of all required products.
$
10
uestion 1: What is the primary objective of the travelling salesman problem (TSP)?
A: To find the longest route between cities.
B: To visit each city multiple times.
C: To find the shortest route that visits each city once and returns to the start.
D: To decide the number of cities.
E: To calculate the total number of routes possible.
Answer: C

Question 2: Which classification does the TSP belong to in terms of computational difficulty?
A: P-hard
B: NP-easy
C: NP-hard
D: Simple
E: Linear
Answer: C

Question 3: How does the decision version of the TSP relate to NP-complete problems?
A: It is unrelated.
B: The decision version of the TSP is an NP-complete problem.
C: The decision version of the TSP is simpler than NP-complete problems.
D: All NP-complete problems are decision versions of TSP.
E: Only some instances of TSP are NP-complete.
Answer: B

Question 4: When was the TSP first formulated?
A: 1900
B: 1920
C: 1950
D: 2000
E: 1930
Answer: E

Question 5: In the graph representation of the TSP, what do cities represent?
A: Edges
B: Weights
C: Vertices
D: Paths
E: Distance
Answer: C

Question 6: In a symmetric TSP, what is true about the distance between two cities?
A: There's no path between them.
B: The distance differs in each direction.
C: The distance is the same in both directions.
D: Only one path exists between them.
E: The distance is immeasurable.
Answer: C

Question 7: In the context of the TSP, what does the bottleneck travelling salesman problem focus on?
A: Minimizing the total distance.
B: Visiting each city multiple times.
C: Maximizing the weight of the weightiest edge.
D: Minimizing the weight of the weightiest edge.
E: Avoiding cities with bottlenecks.
Answer: D

Question 8: The generalized travelling salesman problem requires the salesman to:
A: Visit multiple cities from each state.
B: Avoid states completely.
C: Visit exactly one city from each state.
D: Visit the capital city of each state.
E: Visit states based on population.
Answer: C

Question 9: In the sequential ordering problem, what additional constraint is introduced?
A: Distance between cities.
B: Precedence relations between the cities.
C: Symmetry of routes.
D: Cost of travel.
E: Number of cities to be visited.
Answer: B

Question 10: In the travelling purchaser problem, what is the main objective?
A: To visit the most cities.
B: To minimize travel distance only.
C: To minimize total cost considering both travel and purchasing.
D: To purchase the most products.
E: To only visit cities offering unique products.
Answer: C
@
NP-hardness

Article
Talk
Read
Edit
View history

Tools
From Wikipedia, the free encyclopedia
For a gentler introduction, see P versus NP problem.
Euler diagram for P, NP, NP-complete, and NP-hard set of problems.
Euler diagram for P, NP, NP-complete, and NP-hard set of problems. The left side is valid under the assumption that P≠NP, while the right side is valid under the assumption that P=NP (except that the empty language and its complement are never NP-complete)
In computational complexity theory, NP-hardness (non-deterministic polynomial-time hardness) is the defining property of a class of problems that are informally "at least as hard as the hardest problems in NP". A simple example of an NP-hard problem is the subset sum problem.

A more precise specification is: a problem H is NP-hard when every problem L in NP can be reduced in polynomial time to H; that is, assuming a solution for H takes 1 unit time, H's solution can be used to solve L in polynomial time.[1][2] As a consequence, finding a polynomial time algorithm to solve any NP-hard problem would give polynomial time algorithms for all the problems in NP. As it is suspected that P≠NP, it is unlikely that such an algorithm exists.[3]

It is suspected that there are no polynomial-time algorithms for NP-hard problems, but that has not been proven.[4] Moreover, the class P, in which all problems can be solved in polynomial time, is contained in the NP class.[5]

Definition
A decision problem H is NP-hard when for every problem L in NP, there is a polynomial-time many-one reduction from L to H.[1]: 80  An equivalent definition is to require that every problem L in NP can be solved in polynomial time by an oracle machine with an oracle for H.[6] Informally, an algorithm can be thought of that calls such an oracle machine as a subroutine for solving H and solves L in polynomial time if the subroutine call takes only one step to compute.

Another definition is to require that there be a polynomial-time reduction from an NP-complete problem G to H.[1]: 91  As any problem L in NP reduces in polynomial time to G, L reduces in turn to H in polynomial time so this new definition implies the previous one. It does not restrict the class NP-hard to decision problems, and it also includes search problems or optimization problems.

Consequences
If P ≠ NP, then NP-hard problems could not be solved in polynomial time.

Some NP-hard optimization problems can be polynomial-time approximated up to some constant approximation ratio (in particular, those in APX) or even up to any approximation ratio (those in PTAS or FPTAS).
$
10
Question 1: What is the informal definition of NP-hardness?
A: Easier than the easiest problems in NP.
B: As hard as the average problems in NP.
C: At least as hard as the hardest problems in NP.
D: Exactly as hard as NP-complete problems.
E: Solvable in polynomial time.
Answer: C

Question 2: Which of the following is a simple example of an NP-hard problem?
A: Hamiltonian cycle problem.
B: Subset sum problem.
C: Prim's algorithm.
D: Shortest path problem.
E: Maximum flow problem.
Answer: B

Question 3: If a solution for a problem H takes 1 unit time and H's solution can solve L in polynomial time, then what is true about H?
A: H is easier than L.
B: H is an NP-easy problem.
C: H is NP-hard.
D: L is NP-hard.
E: L and H are both NP-complete.
Answer: C

Question 4: Which of the following statements about P and NP classes is true?
A: P is not contained in NP.
B: NP is a subset of P.
C: All problems in P can be solved in polynomial time and P is contained in NP.
D: P and NP are mutually exclusive.
E: NP problems are always solvable in polynomial time.
Answer: C

Question 5: What does it mean when a decision problem H is NP-hard?
A: Only some problems in NP can be reduced to H in polynomial time.
B: Every problem in NP can be solved in exponential time by H.
C: No problem in NP can be reduced to H.
D: Every problem in NP can be reduced in polynomial time to H.
E: H is solvable in polynomial time.
Answer: D

Question 6: How can an algorithm use an oracle machine for solving problem H?
A: By using it as a subroutine and solving L in exponential time.
B: By using it as a main function and H as a subroutine.
C: By using it as a subroutine and solving L in polynomial time with a one-step computation for the subroutine call.
D: By ignoring the oracle machine completely.
E: By using the oracle machine only at the end of computation.
Answer: C

Question 7: If there's a polynomial-time reduction from an NP-complete problem G to H, what does it signify about H?
A: H is not NP-hard.
B: H is easier than G.
C: H is NP-hard.
D: G is NP-hard but H is not.
E: Both G and H are solvable in polynomial time.
Answer: C

Question 8: Are NP-hard problems restricted only to decision problems?
A: Yes, only decision problems.
B: Yes, but sometimes they include search problems.
C: No, they also include optimization problems.
D: No, they include neither search nor optimization problems.
E: It is unknown.
Answer: C

Question 9: If P ≠ NP, what can be inferred about NP-hard problems?
A: They can be solved in polynomial time.
B: They cannot be solved in polynomial time.
C: They are equivalent to NP-complete problems.
D: They are always easier than NP problems.
E: There is always an approximation for them.
Answer: B

Question 10: What is possible for some NP-hard optimization problems?
A: They can always be solved in polynomial time.
B: They cannot be approximated.
C: They can be polynomial-time approximated up to some constant approximation ratio.
D: They are unrelated to approximation schemes.
E: They always have an exact solution in polynomial time.
Answer: C
@
In computability theory and computational complexity theory, a decision problem is a computational problem that can be posed as a yes–no question of the input values. An example of a decision problem is deciding by means of an algorithm whether a given natural number is prime. Another is the problem "given two numbers x and y, does x evenly divide y?". The answer is either 'yes' or 'no' depending upon the values of x and y. A method for solving a decision problem, given in the form of an algorithm, is called a decision procedure for that problem. A decision procedure for the decision problem "given two numbers x and y, does x evenly divide y?" would give the steps for determining whether x evenly divides y. One such algorithm is long division. If the remainder is zero the answer is 'yes', otherwise it is 'no'. A decision problem which can be solved by an algorithm is called decidable.

Decision problems typically appear in mathematical questions of decidability, that is, the question of the existence of an effective method to determine the existence of some object or its membership in a set; some of the most important problems in mathematics are undecidable.

The field of computational complexity categorizes decidable decision problems by how difficult they are to solve. "Difficult", in this sense, is described in terms of the computational resources needed by the most efficient algorithm for a certain problem. The field of recursion theory, meanwhile, categorizes undecidable decision problems by Turing degree, which is a measure of the noncomputability inherent in any solution.

Definition
A decision problem is a yes-or-no question on an infinite set of inputs. It is traditional to define the decision problem as the set of possible inputs together with the set of inputs for which the answer is yes.[1]

These inputs can be natural numbers, but can also be values of some other kind, like binary strings or strings over some other alphabet. The subset of strings for which the problem returns "yes" is a formal language, and often decision problems are defined as formal languages.

Using an encoding such as Gödel numbering, any string can be encoded as a natural number, via which a decision problem can be defined as a subset of the natural numbers. Therefore, the algorithm of a decision problem is to compute the characteristic function of a subset of the natural numbers.

Examples
A classic example of a decidable decision problem is the set of prime numbers. It is possible to effectively decide whether a given natural number is prime by testing every possible nontrivial factor. Although much more efficient methods of primality testing are known, the existence of any effective method is enough to establish decidability.

Decidability
Main articles: Undecidable problem and Decidability (logic)
A decision problem is decidable or effectively solvable if the set of inputs (or natural numbers) for which the answer is yes is a recursive set. A problem is partially decidable, semidecidable, solvable, or provable if the set of inputs (or natural numbers) for which the answer is yes is a recursively enumerable set. Problems that are not decidable are undecidable. For those it is not possible to create an algorithm, efficient or otherwise, that solves them.

The halting problem is an important undecidable decision problem; for more examples, see list of undecidable problems.
$
10
Question 1: What kind of answer does a decision problem provide?
A: A numerical solution.
B: A descriptive answer.
C: A yes or no answer.
D: An approximation.
E: A percentage.
Answer: C

Question 2: What is a decision procedure in the context of decision problems?
A: A method for posing a question.
B: A method for defining a problem.
C: A method to avoid a problem.
D: A method for deriving multiple solutions.
E: A method for solving a decision problem.
Answer: E

Question 3: How can you determine if "x evenly divides y" using long division?
A: If the result is a prime number.
B: If the quotient is non-zero.
C: If the remainder is zero.
D: If the result is a natural number.
E: If the quotient is odd.
Answer: C

Question 4: Which of the following best describes the questions that decision problems typically appear in?
A: Computation ability.
B: Algorithm efficiency.
C: Decidability.
D: Complexity.
E: Functionality.
Answer: C

Question 5: How is "difficult" described in the field of computational complexity?
A: In terms of the algorithm's design.
B: In terms of the computational resources needed.
C: In terms of the problem's popularity.
D: In terms of the number of inputs.
E: In terms of the problem's age.
Answer: B

Question 6: What is a formal language in the context of decision problems?
A: A type of encoding.
B: A specific algorithm.
C: The subset of strings for which the problem returns "yes".
D: The set of all possible inputs.
E: The software used to solve decision problems.
Answer: C

Question 7: How is a decision problem traditionally defined?
A: As the most efficient algorithm.
B: As the set of possible inputs together with the set of inputs for which the answer is yes.
C: As the decision procedure used.
D: As the difficulty of the problem.
E: As the type of encoding used.
Answer: B

Question 8: If a decision problem is decidable, what does it mean about the set of inputs for which the answer is yes?
A: It is a recursive set.
B: It is an undecidable set.
C: It is a finite set.
D: It is a non-recursive set.
E: It is a non-enumerable set.
Answer: A

Question 9: How can a decision problem be classified if the set of inputs for which the answer is yes is a recursively enumerable set?
A: Undecidable.
B: Decidable.
C: Semidecidable.
D: Non-computable.
E: Approximable.
Answer: C

Question 10: What is the halting problem in the context of decision problems?
A: A decidable problem.
B: A problem that determines the efficiency of an algorithm.
C: An important undecidable decision problem.
D: A problem that can be solved with enough computational resources.
E: A problem related to prime numbers.
Answer: C
@
In computability theory, the halting problem is the problem of determining, from a description of an arbitrary computer program and an input, whether the program will finish running, or continue to run forever. The halting problem is undecidable, meaning that no general algorithm exists that solves the halting problem for all possible program–input pairs.

A key part of the formal statement of the problem is a mathematical definition of a computer and program, usually via a Turing machine. The proof then shows, for any program f that might determine whether programs halt, that a "pathological" program g, called with some input, can pass its own source and its input to f and then specifically do the opposite of what f predicts g will do. No f can exist that handles this case, thus showing undecidability. This proof is significant to practical computing efforts, defining a class of applications which no programming invention can possibly perform perfectly.

Background
The halting problem is a decision problem about properties of computer programs on a fixed Turing-complete model of computation, i.e., all programs that can be written in some given programming language that is general enough to be equivalent to a Turing machine. The problem is to determine, given a program and an input to the program, whether the program will eventually halt when run with that input. In this abstract framework, there are no resource limitations on the amount of memory or time required for the program's execution; it can take arbitrarily long and use an arbitrary amount of storage space before halting. The question is simply whether the given program will ever halt on a particular input.

For example, in pseudocode, the program

while (true) continue
does not halt; rather, it goes on forever in an infinite loop. On the other hand, the program

print "Hello, world!"
does halt.

While deciding whether these programs halt is simple, more complex programs prove problematic. One approach to the problem might be to run the program for some number of steps and check if it halts. However, as long as the program is running, it is unknown whether it will eventually halt or run forever. Turing proved no algorithm exists that always correctly decides whether, for a given arbitrary program and input, the program halts when run with that input. The essence of Turing's proof is that any such algorithm can be made to produce contradictory output and therefore cannot be correct.
$
10
Question 1: What is the primary challenge of the halting problem?
A: Determining the efficiency of a computer program.
B: Checking if a program will finish running or continue indefinitely.
C: Determining the best programming language for a task.
D: Deciding if a program will print "Hello, world!".
E: Deciding the number of steps a program will run.
Answer: B

Question 2: How is the halting problem categorized in terms of decidability?
A: Solvable.
B: Decidable.
C: Undecidable.
D: Partially decidable.
E: Recursively enumerable.
Answer: C

Question 3: What is typically used for the formal statement of the halting problem to define a computer and program?
A: Python code.
B: C++ programming.
C: A Turing machine.
D: Pseudocode.
E: Quantum computation.
Answer: C

Question 4: What happens when the "pathological" program 
�
g is called with some input?
A: It optimizes itself.
B: It runs faster than any other program.
C: It goes into an infinite loop.
D: It does the opposite of what 
�
f predicts 
�
g will do.
E: It halts immediately.
Answer: D

Question 5: What does the halting problem define in terms of practical computing efforts?
A: A class of applications that always run efficiently.
B: A class of applications that always halt.
C: A class of applications no programming invention can perform perfectly.
D: A class of applications that can be solved by Turing machines.
E: A class of applications that are always decidable.
Answer: C

Question 6: The halting problem focuses on computer programs on a fixed Turing-complete model of computation, which refers to:
A: Programs written only in a specific programming language.
B: Programs that are restricted by memory and time.
C: All programs written in a language equivalent to a Turing machine.
D: Programs that can run on quantum computers.
E: Programs that will halt under any circumstances.
Answer: C

Question 7: In the abstract framework of the halting problem, what resource limitations are there?
A: Limited memory and time.
B: Limited steps for execution.
C: Limited Turing machines available.
D: No resource limitations.
E: Limited number of inputs.
Answer: D

Question 8: Which of the following pseudocode examples halts?
A: while (true) continue
B: print "Running indefinitely!"
C: print "Hello, world!"
D: for i in range(infinity) print i
E: while (1) print "Running"
Answer: C

Question 9: What is the essence of Turing's proof regarding the halting problem?
A: Any algorithm can always determine the halting of a program.
B: Any algorithm can be optimized to solve the halting problem.
C: Any algorithm can be made to produce contradictory output.
D: The efficiency of an algorithm can be enhanced with a Turing machine.
E: Every program will eventually halt.
Answer: C

Question 10: What is the result if one tries to run a program for a certain number of steps to determine if it halts?
A: It will always halt before the steps are completed.
B: It will always determine the halting status correctly.
C: As long as it's running, one can't know if it will halt or run forever.
D: The program will request more steps.
E: The program will produce an error message.
Answer: C
@
A programming language is a system of notation for writing computer programs.[1] Most programming languages are text-based formal languages, but they may also be graphical. They are a kind of computer language.

The description of a programming language is usually split into the two components of syntax (form) and semantics (meaning), which are usually defined by a formal language. Some languages are defined by a specification document (for example, the C programming language is specified by an ISO Standard) while other languages (such as Perl) have a dominant implementation that is treated as a reference. Some languages have both, with the basic language defined by a standard and extensions taken from the dominant implementation being common.

Programming language theory is the subfield of computer science that studies the design, implementation, analysis, characterization, and classification of programming languages.

Definitions
There are many considerations when defining what constitutes a programming language.

Computer languages vs programming languages
The term computer language is sometimes used interchangeably with programming language.[2] However, the usage of both terms varies among authors, including the exact scope of each. One usage describes programming languages as a subset of computer languages.[3] Similarly, languages used in computing that have a different goal than expressing computer programs are generically designated computer languages. For instance, markup languages are sometimes referred to as computer languages to emphasize that they are not meant to be used for programming.[4] One way of classifying computer languages is by the computations they are capable of expressing, as described by the theory of computation. The majority of practical programming languages are Turing complete,[5] and all Turing complete languages can implement the same set of algorithms. ANSI/ISO SQL-92 and Charity are examples of languages that are not Turing complete, yet are often called programming languages.[6][7] However, some authors restrict the term "programming language" to Turing complete languages.[1][8]

Another usage regards programming languages as theoretical constructs for programming abstract machines and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources.[9] John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.[10]
$
10
Question 1: Which of the following is NOT always a characteristic of a programming language?
A: Text-based.
B: Graphical.
C: System of notation.
D: Turing complete.
E: Used for writing computer programs.
Answer: B

Question 2: What are the two main components that describe a programming language?
A: Syntax and Script.
B: Syntax and Semantics.
C: Semantics and Script.
D: Semantics and Sequence.
E: Sequence and Syntax.
Answer: B

Question 3: How is the C programming language defined?
A: By a dominant implementation.
B: By an ISO Standard.
C: By its syntax alone.
D: By community consensus.
E: By the dominant extensions.
Answer: B

Question 4: Which field of computer science studies the design and classification of programming languages?
A: Software Engineering.
B: Artificial Intelligence.
C: Hardware Design.
D: Programming language theory.
E: Computational Theory.
Answer: D

Question 5: The term computer language can sometimes be used as a synonym for which of the following?
A: Hardware Language.
B: Semantic Language.
C: Programming Language.
D: Graphical Language.
E: Natural Language.
Answer: C

Question 6: Markup languages are emphasized as not being meant for what primary purpose?
A: Database management.
B: Graphics design.
C: Programming.
D: Communication.
E: Web designing.
Answer: C

Question 7: The majority of practical programming languages are what?
A: Practical complete.
B: Hardware complete.
C: Turing complete.
D: Semantically complete.
E: Syntax complete.
Answer: C

Question 8: ANSI/ISO SQL-92 is an example of a language that is what?
A: Turing incomplete.
B: Programming language.
C: Graphical language.
D: Execution language.
E: Fully standardized.
Answer: A

Question 9: John C. Reynolds suggests that formal specification languages can also be considered as what?
A: Hardware languages.
B: Abstract machines.
C: Execution languages.
D: Programming languages.
E: Communication languages.
Answer: D

Question 10: Which of the following is a potential reason for flaws in input formats according to John C. Reynolds?
A: Limited hardware resources.
B: Lack of standardized specifications.
C: Ignorance of programming language concepts.
D: Overemphasis on graphical representation.
E: Absence of a dominant implementation.
Answer: C
@
In computability theory, a system of data-manipulation rules (such as a model of computation, a computer's instruction set, a programming language, or a cellular automaton) is said to be Turing-complete or computationally universal if it can be used to simulate any Turing machine (devised by English mathematician and computer scientist Alan Turing). This means that this system is able to recognize or decide other data-manipulation rule sets. Turing completeness is used as a way to express the power of such a data-manipulation rule set. Virtually all programming languages today are Turing-complete.

A related concept is that of Turing equivalence – two computers P and Q are called equivalent if P can simulate Q and Q can simulate P. The Church–Turing thesis conjectures that any function whose values can be computed by an algorithm can be computed by a Turing machine, and therefore that if any real-world computer can simulate a Turing machine, it is Turing equivalent to a Turing machine. A universal Turing machine can be used to simulate any Turing machine and by extension the computational aspects of any possible real-world computer.[NB 1]

To show that something is Turing-complete, it is enough to show that it can be used to simulate some Turing-complete system. No physical system can have infinite memory, but if the limitation of finite memory is ignored, most programming languages are otherwise Turing-complete.

Non-mathematical usage
In colloquial usage, the terms "Turing-complete" and "Turing-equivalent" are used to mean that any real-world general-purpose computer or computer language can approximately simulate the computational aspects of any other real-world general-purpose computer or computer language. In real life, this leads to the practical concepts of computing virtualization and emulation.[citation needed]

Real computers constructed so far can be functionally analyzed like a single-tape Turing machine (the "tape" corresponding to their memory); thus the associated mathematics can apply by abstracting their operation far enough. However, real computers have limited physical resources, so they are only linear bounded automaton complete. In contrast, a universal computer is defined as a device with a Turing-complete instruction set, infinite memory, and infinite available time.

Formal definitions
In computability theory, several closely related terms are used to describe the computational power of a computational system (such as an abstract machine or programming language):

Turing completeness
A computational system that can compute every Turing-computable function is called Turing-complete (or Turing-powerful). Alternatively, such a system is one that can simulate a universal Turing machine.
Turing equivalence
A Turing-complete system is called Turing-equivalent if every function it can compute is also Turing-computable; i.e., it computes precisely the same class of functions as do Turing machines. Alternatively, a Turing-equivalent system is one that can simulate, and be simulated by, a universal Turing machine. (All known physically-implementable Turing-complete systems are Turing-equivalent, which adds support to the Church–Turing thesis.[citation needed])
(Computational) universality
A system is called universal with respect to a class of systems if it can compute every function computable by systems in that class (or can simulate each of those systems). Typically, the term 'universality' is tacitly used with respect to a Turing-complete class of systems. The term "weakly universal" is sometimes used to distinguish a system (e.g. a cellular automaton) whose universality is achieved only by modifying the standard definition of Turing machine so as to include input streams with infinitely many 1s.
$
10
Question 1: Which of the following is considered Turing-complete?
A: A system that can only simulate specific Turing machines.
B: A system that can simulate any Turing machine.
C: A system that has infinite memory.
D: A system that cannot decide other data-manipulation rule sets.
E: A system that is only linear bounded automaton complete.
Answer: B

Question 2: What does Turing completeness express about a system of data-manipulation rules?
A: Its compatibility with Turing machines.
B: Its memory capacity.
C: Its execution speed.
D: The power of such a system.
E: Its practical application.
Answer: D

Question 3: If two computers, P and Q, can simulate each other, they are said to be:
A: Turing-powerful.
B: Turing-complete.
C: Turing-equivalent.
D: Universally complete.
E: Weakly universal.
Answer: C

Question 4: Which machine can simulate any Turing machine and the computational aspects of any possible real-world computer?
A: Linear Bounded Automaton.
B: Universal Computer.
C: Turing-Equivalent Machine.
D: Universal Turing Machine.
E: Computationally Universal Machine.
Answer: D

Question 5: What is the primary difference between real computers and a universal computer?
A: Real computers can simulate Turing machines.
B: Universal computers cannot be Turing-equivalent.
C: Real computers have limited physical resources.
D: Universal computers have limited computational power.
E: Real computers cannot compute every Turing-computable function.
Answer: C

Question 6: What is a common colloquial usage of the term "Turing-complete"?
A: A system that has an infinite memory.
B: A machine that can process data at an infinite speed.
C: Any real-world general-purpose computer can simulate the computational aspects of another.
D: A system that is linear bounded automaton complete.
E: A system that can compute only specific algorithms.
Answer: C

Question 7: How is a Turing-complete system described in relation to a universal Turing machine?
A: A system that is limited by memory.
B: A system that can simulate a universal Turing machine.
C: A system that cannot be simulated by any Turing machine.
D: A system that can only compute non-Turing-computable functions.
E: A system that has infinite time.
Answer: B

Question 8: What does the term "weakly universal" imply about a system?
A: It can simulate any Turing machine.
B: It requires infinite memory to function.
C: Its universality is based on a modified definition of Turing machine with specific input streams.
D: It has limited computational power.
E: It is equivalent to a linear bounded automaton.
Answer: C

Question 9: A system that can compute every function by other systems in a certain class is referred to as:
A: Turing-powerful.
B: Turing-comparable.
C: Linearly universal.
D: Turing-complete.
E: Universal with respect to that class of systems.
Answer: E

Question 10: Which term describes a system that computes the exact same class of functions as Turing machines?
A: Turing completeness.
B: Turing equivalence.
C: Computational universality.
D: Weak universality.
E: Turing variability.
Answer: B
@
In physics, gravity (from Latin gravitas 'weight'[1]) is a fundamental interaction which causes mutual attraction between all things that have mass. Gravity is, by far, the weakest of the four fundamental interactions, approximately 1038 times weaker than the strong interaction, 1036 times weaker than the electromagnetic force and 1029 times weaker than the weak interaction. As a result, it has no significant influence at the level of subatomic particles.[2] However, gravity is the most significant interaction between objects at the macroscopic scale, and it determines the motion of planets, stars, galaxies, and even light.

On Earth, gravity gives weight to physical objects, and the Moon's gravity is responsible for sublunar tides in the oceans (the corresponding antipodal tide is caused by the inertia of the Earth and Moon orbiting one another). Gravity also has many important biological functions, helping to guide the growth of plants through the process of gravitropism and influencing the circulation of fluids in multicellular organisms.

The gravitational attraction between the original gaseous matter in the universe caused it to coalesce and form stars which eventually condensed into galaxies, so gravity is responsible for many of the large-scale structures in the universe. Gravity has an infinite range, although its effects become weaker as objects get farther away.

Gravity is most accurately described by the general theory of relativity (proposed by Albert Einstein in 1915), which describes gravity not as a force, but as the curvature of spacetime, caused by the uneven distribution of mass, and causing masses to move along geodesic lines. The most extreme example of this curvature of spacetime is a black hole, from which nothing—not even light—can escape once past the black hole's event horizon.[3] However, for most applications, gravity is well approximated by Newton's law of universal gravitation, which describes gravity as a force causing any two bodies to be attracted toward each other, with magnitude proportional to the product of their masses and inversely proportional to the square of the distance between them.

Current models of particle physics imply that the earliest instance of gravity in the universe, possibly in the form of quantum gravity, supergravity or a gravitational singularity, along with ordinary space and time, developed during the Planck epoch (up to 10−43 seconds after the birth of the universe), possibly from a primeval state, such as a false vacuum, quantum vacuum or virtual particle, in a currently unknown manner.[4] Scientists are currently working to develop a theory of gravity consistent with quantum mechanics, a quantum gravity theory,[5] which would allow gravity to be united in a common mathematical framework (a theory of everything) with the other three fundamental interactions of physics.
$
10
Question 1: Which fundamental interaction causes mutual attraction between objects with mass?
A: Electromagnetic force.
B: Strong interaction.
C: Weak interaction.
D: Gravity.
E: Quantum mechanics.
Answer: D

Question 2: Compared to the strong interaction, how much weaker is gravity?
A: 10^29 times weaker.
B: 10^36 times weaker.
C: 10^38 times weaker.
D: 10^43 times weaker.
E: 10^45 times weaker.
Answer: C

Question 3: At what scale is gravity the most significant interaction between objects?
A: Subatomic level.
B: Quantum level.
C: Molecular level.
D: Planetary level.
E: Macroscopic scale.
Answer: E

Question 4: What on Earth is responsible for causing sublunar tides in the oceans?
A: Earth's rotation.
B: Electromagnetic force.
C: The Moon's gravity.
D: The strong interaction.
E: The curvature of spacetime.
Answer: C

Question 5: What process helps guide the growth of plants due to gravity?
A: Photosynthesis.
B: Cellular respiration.
C: Gravitropism.
D: Magnetism.
E: Bioluminescence.
Answer: C

Question 6: How does the general theory of relativity describe gravity?
A: As a force between two objects.
B: As a result of the electromagnetic field.
C: As the curvature of spacetime due to the uneven distribution of mass.
D: As a quantum interaction.
E: As a strong interaction between particles.
Answer: C

Question 7: From which type of astronomical object can nothing, including light, escape once past its event horizon?
A: Supernova.
B: Neutron star.
C: Red giant.
D: Pulsar.
E: Black hole.
Answer: E

Question 8: In Newton's law of universal gravitation, the force of gravity between two objects is inversely proportional to:
A: The sum of their masses.
B: The product of their masses.
C: The square root of the distance between them.
D: The square of the distance between them.
E: The cube of the distance between them.
Answer: D

Question 9: The earliest instance of gravity in the universe, possibly in forms like quantum gravity or supergravity, developed during which epoch?
A: Electromagnetic epoch.
B: Inflationary epoch.
C: Planck epoch.
D: Hadron epoch.
E: Lepton epoch.
Answer: C

Question 10: What are scientists currently aiming to achieve regarding the theory of gravity?
A: Disprove Newton's law of gravitation.
B: Show that gravity is the strongest force.
C: Develop a theory of gravity consistent with quantum mechanics.
D: Separate gravity from the other fundamental interactions.
E: Prove that gravity has a finite range.
Answer: C
@
In physics, the fundamental interactions or fundamental forces are the interactions that do not appear to be reducible to more basic interactions. There are four fundamental interactions known to exist:[1]

gravity
electromagnetism
weak interaction
strong interaction
The gravitational and electromagnetic interactions produce long-range forces whose effects can be seen directly in everyday life. The strong and weak interactions produce forces at minuscule, subatomic distances and govern nuclear interactions inside atoms.

Some scientists hypothesize that a fifth force might exist, but these hypotheses remain speculative.[2][3][4]

Each of the known fundamental interactions can be described mathematically as a field. The gravitational force is attributed to the curvature of spacetime, described by Einstein's general theory of relativity. The other three are discrete quantum fields, and their interactions are mediated by elementary particles described by the Standard Model of particle physics.[5]

Within the Standard Model, the strong interaction is carried by a particle called the gluon and is responsible for quarks binding together to form hadrons, such as protons and neutrons. As a residual effect, it creates the nuclear force that binds the latter particles to form atomic nuclei. The weak interaction is carried by particles called W and Z bosons, and also acts on the nucleus of atoms, mediating radioactive decay. The electromagnetic force, carried by the photon, creates electric and magnetic fields, which are responsible for the attraction between orbital electrons and atomic nuclei which holds atoms together, as well as chemical bonding and electromagnetic waves, including visible light, and forms the basis for electrical technology. Although the electromagnetic force is far stronger than gravity, it tends to cancel itself out within large objects, so over large (astronomical) distances gravity tends to be the dominant force, and is responsible for holding together the large scale structures in the universe, such as planets, stars, and galaxies.

Many theoretical physicists believe these fundamental forces to be related and to become unified into a single force at very high energies on a minuscule scale, the Planck scale,[6] but particle accelerators cannot produce the enormous energies required to experimentally probe this. Devising a common theoretical framework that would explain the relation between the forces in a single theory is perhaps the greatest goal of today's theoretical physicists. The weak and electromagnetic forces have already been unified with the electroweak theory of Sheldon Glashow, Abdus Salam, and Steven Weinberg, for which they received the 1979 Nobel Prize in physics.[7][8][9] Some physicists seek to unite the electroweak and strong fields within what is called a Grand Unified Theory (GUT). An even bigger challenge is to find a way to quantize the gravitational field, resulting in a theory of quantum gravity (QG) which would unite gravity in a common theoretical framework with the other three forces. Some theories, notably string theory, seek both QG and GUT within one framework, unifying all four fundamental interactions along with mass generation within a theory of everything (ToE).
$
10
Question 1: How many fundamental interactions are known to exist in physics?
A: Two
B: Three
C: Four
D: Five
E: Six
Answer: C

Question 2: Which two of the fundamental interactions produce long-range forces observable in everyday life?
A: Strong and weak interaction
B: Electromagnetism and gravity
C: Strong interaction and gravity
D: Weak interaction and electromagnetism
E: Electromagnetism and strong interaction
Answer: B

Question 3: What mediates the strong interaction, helping quarks bind together?
A: W and Z bosons
B: Electrons
C: Photons
D: Gluons
E: Neutrinos
Answer: D

Question 4: Which particle is responsible for the electromagnetic force?
A: Neutron
B: Gluon
C: Photon
D: Quark
E: Boson
Answer: C

Question 5: The electromagnetic force tends to cancel itself out within large objects, but which force becomes dominant over large distances?
A: Weak interaction
B: Electromagnetism
C: Strong interaction
D: Gravity
E: Nuclear force
Answer: D

Question 6: The electroweak theory unified which two forces?
A: Strong and weak interaction
B: Electromagnetic and gravity
C: Weak and electromagnetic forces
D: Electromagnetic and strong interaction
E: Gravity and weak interaction
Answer: C

Question 7: Which theory seeks to unify all four fundamental interactions along with mass generation?
A: Quantum Field Theory
B: General Relativity
C: Theory of Everything (ToE)
D: Electroweak Theory
E: Grand Unified Theory (GUT)
Answer: C

Question 8: What is the primary challenge of devising a common theoretical framework for the fundamental forces?
A: Proving the existence of black holes
B: Separating weak and electromagnetic forces
C: Experimentally probing the Planck scale
D: Detecting the fifth force
E: Observing gravity on a subatomic level
Answer: C

Question 9: Who are the recipients of the 1979 Nobel Prize in physics for the electroweak theory?
A: Isaac Newton, James Clerk Maxwell, and Richard Feynman
B: Sheldon Glashow, Abdus Salam, and Steven Weinberg
C: Niels Bohr, Werner Heisenberg, and Paul Dirac
D: Stephen Hawking, Brian Greene, and Leonard Susskind
E: Albert Einstein, Marie Curie, and Max Planck
Answer: B

Question 10: Which force is responsible for the radioactive decay in atomic nuclei?
A: Strong interaction
B: Electromagnetic force
C: Weak interaction
D: Gravity
E: Nuclear force
Answer: C
@
In nuclear physics and particle physics, the weak interaction, which is also often called the weak force or weak nuclear force, is one of the four known fundamental interactions, with the others being electromagnetism, the strong interaction, and gravitation. It is the mechanism of interaction between subatomic particles that is responsible for the radioactive decay of atoms: The weak interaction participates in nuclear fission and nuclear fusion. The theory describing its behaviour and effects is sometimes called quantum flavourdynamics (QFD); however, the term QFD is rarely used, because the weak force is better understood by electroweak theory (EWT).[1]

The effective range of the weak force is limited to subatomic distances and is less than the diameter of a proton.[2]

Background
The Standard Model of particle physics provides a uniform framework for understanding electromagnetic, weak, and strong interactions. An interaction occurs when two particles (typically, but not necessarily, half-integer spin fermions) exchange integer-spin, force-carrying bosons. The fermions involved in such exchanges can be either elementary (e.g. electrons or quarks) or composite (e.g. protons or neutrons), although at the deepest levels, all weak interactions ultimately are between elementary particles.

In the weak interaction, fermions can exchange three types of force carriers, namely W+, W−, and Z bosons. The masses of these bosons are far greater than the mass of a proton or neutron, which is consistent with the short range of the weak force.[3] In fact, the force is termed weak because its field strength over any set distance is typically several orders of magnitude less than that of the electromagnetic force, which itself is further orders of magnitude less than the strong nuclear force.

The weak interaction is the only fundamental interaction that breaks parity symmetry, and similarly, but far more rarely, the only interaction to break charge–parity symmetry.

Quarks, which make up composite particles like neutrons and protons, come in six "flavours" – up, down, strange, charm, top and bottom – which give those composite particles their properties. The weak interaction is unique in that it allows quarks to swap their flavour for another. The swapping of those properties is mediated by the force carrier bosons. For example, during beta-minus decay, a down quark within a neutron is changed into an up quark, thus converting the neutron to a proton and resulting in the emission of an electron and an electron antineutrino.

Weak interaction is important in the fusion of hydrogen into helium in a star. This is because it can convert a proton (hydrogen) into a neutron to form deuterium which is important for the continuation of nuclear fusion to form helium. The accumulation of neutrons facilitates the buildup of heavy nuclei in a star.[3]

Most fermions decay by a weak interaction over time. Such decay makes radiocarbon dating possible, as carbon-14 decays through the weak interaction to nitrogen-14. It can also create radioluminescence, commonly used in tritium luminescence, and in the related field of betavoltaics[4] (but not similar radium luminescence).

The electroweak force is believed to have separated into the electromagnetic and weak forces during the quark epoch of the early universe.
$
10
Question 1: Which of the following is responsible for the radioactive decay of atoms?
A: Electromagnetic force
B: Strong nuclear force
C: Gravitation
D: Weak interaction
E: Quantum force
Answer: D

Question 2: What is the range of the weak force in relation to the diameter of a proton?
A: Greater than the diameter
B: Equal to the diameter
C: Less than the diameter
D: Twice the diameter
E: Ten times the diameter
Answer: C

Question 3: Which interaction is the only one that breaks parity symmetry?
A: Electromagnetic interaction
B: Gravitational interaction
C: Strong interaction
D: Weak interaction
E: Quantum interaction
Answer: D

Question 4: How many types of force carriers can fermions exchange in the weak interaction?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: C

Question 5: Which of the following "flavours" is NOT a type of quark?
A: Top
B: Electron
C: Strange
D: Down
E: Up
Answer: B

Question 6: What happens to a down quark during beta-minus decay?
A: It remains unchanged.
B: It changes into a strange quark.
C: It changes into an electron.
D: It changes into an up quark.
E: It changes into a W+ boson.
Answer: D

Question 7: Why is the weak interaction important in the fusion of hydrogen into helium in a star?
A: It separates protons from electrons.
B: It prevents neutron formation.
C: It changes gamma rays into X-rays.
D: It can convert a proton into a neutron.
E: It accelerates the fusion process by increasing heat.
Answer: D

Question 8: What does radiocarbon dating rely upon?
A: Proton decay
B: Electromagnetic radiation
C: The strong nuclear force
D: The decay of carbon-14 through the weak interaction
E: The luminosity of stars
Answer: D

Question 9: What can result from the weak interaction in the context of luminescence?
A: Gravitational luminescence
B: Radium luminescence
C: Tritium luminescence
D: Proton luminescence
E: Quark luminescence
Answer: C

Question 10: During which epoch is the electroweak force believed to have separated into the electromagnetic and weak forces?
A: Fusion epoch
B: Radiation epoch
C: Hadron epoch
D: Lepton epoch
E: Quark epoch
Answer: E
@
In nuclear physics and particle physics, the strong interaction, which is also often called the strong force or strong nuclear force, is a fundamental interaction that confines quarks into proton, neutron, and other hadron particles. The strong interaction also binds neutrons and protons to create atomic nuclei, where it is called the nuclear force.

Most of the mass of a common proton or neutron is the result of the strong interaction energy; the individual quarks provide only about 1% of the mass of a proton. At the range of 10−15 m (1 femtometer, slightly more than the radius of a nucleon), the strong force is approximately 100 times as strong as electromagnetism, 106 times as strong as the weak interaction, and 1038 times as strong as gravitation.[1]

The strong interaction is observable at two ranges and mediated by two force carriers. On a larger scale (of about 1 to 3 fm), it is the force (carried by mesons) that binds protons and neutrons (nucleons) together to form the nucleus of an atom. On the smaller scale (less than about 0.8 fm, the radius of a nucleon), it is the force (carried by gluons) that holds quarks together to form protons, neutrons, and other hadron particles.[2] In the latter context, it is often known as the color force. The strong force inherently has such a high strength that hadrons bound by the strong force can produce new massive particles. Thus, if hadrons are struck by high-energy particles, they give rise to new hadrons instead of emitting freely moving radiation (gluons). This property of the strong force is called color confinement, and it prevents the free "emission" of the strong force: instead, in practice, jets of massive particles are produced.

In the context of atomic nuclei, the same strong interaction force (that binds quarks within a nucleon) also binds protons and neutrons together to form a nucleus. In this capacity it is called the nuclear force (or residual strong force). So the residuum from the strong interaction within protons and neutrons also binds nuclei together.[2] As such, the residual strong interaction obeys a distance-dependent behavior between nucleons that is quite different from that when it is acting to bind quarks within nucleons. Additionally, distinctions exist in the binding energies of the nuclear force of nuclear fusion vs nuclear fission. Nuclear fusion accounts for most energy production in the Sun and other stars. Nuclear fission allows for decay of radioactive elements and isotopes, although it is often mediated by the weak interaction. Artificially, the energy associated with the nuclear force is partially released in nuclear power and nuclear weapons, both in uranium or plutonium-based fission weapons and in fusion weapons like the hydrogen bomb.[3][4]

The strong interaction is mediated by the exchange of massless particles called gluons that act between quarks, antiquarks, and other gluons. Gluons are thought to interact with quarks and other gluons by way of a type of charge called color charge. Color charge is analogous to electromagnetic charge, but it comes in three types (±red, ±green, and ±blue) rather than one, which results in a different type of force, with different rules of behavior. These rules are detailed in the theory of quantum chromodynamics (QCD), which is the theory of quark–gluon interactions.
$
10
Question 1: Which interaction confines quarks into protons, neutrons, and other hadron particles?
A: Electromagnetic interaction
B: Gravitation
C: Weak interaction
D: Strong interaction
E: Atomic interaction
Answer: D

Question 2: Approximately how much of a proton's mass is provided by individual quarks?
A: 1%
B: 10%
C: 25%
D: 50%
E: 100%
Answer: A

Question 3: At the range of 1 femtometer, the strong force is how many times stronger than electromagnetism?
A: 10 times
B: 100 times
C: 1,000 times
D: 10,000 times
E: 100,000 times
Answer: B

Question 4: Which force carrier holds quarks together to form hadron particles?
A: Mesons
B: Photons
C: Electrons
D: Gluons
E: Bosons
Answer: D

Question 5: The property of the strong force that prevents the free "emission" of the strong force is called:
A: Electromagnetic confinement
B: Gluonic blockage
C: Nucleonic retention
D: Color confinement
E: Quark containment
Answer: D

Question 6: In the context of atomic nuclei, what is the strong interaction force that binds protons and neutrons together also called?
A: Electromagnetic force
B: Atomic force
C: Residual strong force
D: Gravitational force
E: Quantum force
Answer: C

Question 7: Which process accounts for most of the energy production in the Sun and other stars?
A: Nuclear decay
B: Nuclear fission
C: Electromagnetic fusion
D: Atomic fusion
E: Nuclear fusion
Answer: E

Question 8: Which particles mediate the strong interaction between quarks, antiquarks, and other gluons?
A: Photons
B: Electrons
C: Neutrons
D: Gluons
E: Mesons
Answer: D

Question 9: The theory detailing the rules of quark-gluon interactions is called:
A: Quantum electrodynamics
B: Quantum flavordynamics
C: Quantum fusiondynamics
D: Quantum chromodynamics
E: Quark fusion theory
Answer: D

Question 10: How many types of color charge exist in the context of the strong interaction?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: C
@
A proton is a stable subatomic particle, symbol 
p
, H+, or 1H+ with a positive electric charge of +1 e (elementary charge). Its mass is slightly less than that of a neutron and 1,836 times the mass of an electron (the proton-to-electron mass ratio). Protons and neutrons, each with masses of approximately one atomic mass unit, are jointly referred to as "nucleons" (particles present in atomic nuclei).

One or more protons are present in the nucleus of every atom. They provide the attractive electrostatic central force that binds the atomic electrons. The number of protons in the nucleus is the defining property of an element, and is referred to as the atomic number (represented by the symbol Z). Since each element has a unique number of protons, each element has its own unique atomic number, which determines the number of atomic electrons and consequently the chemical characteristics of the element.

The word proton is Greek for "first", and this name was given to the hydrogen nucleus by Ernest Rutherford in 1920. In previous years, Rutherford had discovered that the hydrogen nucleus (known to be the lightest nucleus) could be extracted from the nuclei of nitrogen by atomic collisions.[10] Protons were therefore a candidate to be a fundamental or elementary particle, and hence a building block of nitrogen and all other heavier atomic nuclei.

Although protons were originally considered to be elementary particles, in the modern Standard Model of particle physics, protons are now known to be composite particles, containing three valence quarks, and together with neutrons are now classified as hadrons. Protons are composed of two up quarks of charge +
2
/
3
e and one down quark of charge −
1
/
3
e. The rest masses of quarks contribute only about 1% of a proton's mass.[11] The remainder of a proton's mass is due to quantum chromodynamics binding energy, which includes the kinetic energy of the quarks and the energy of the gluon fields that bind the quarks together. Because protons are not fundamental particles, they possess a measurable size; the root mean square charge radius of a proton is about 0.84–0.87 fm (1 fm = 10−15 m).[12][13] In 2019, two different studies, using different techniques, found this radius to be 0.833 fm, with an uncertainty of ±0.010 fm.[14][15]

Free protons occur occasionally on Earth: thunderstorms can produce protons with energies of up to several tens of MeV.[16][17] At sufficiently low temperatures and kinetic energies, free protons will bind to electrons. However, the character of such bound protons does not change, and they remain protons. A fast proton moving through matter will slow by interactions with electrons and nuclei, until it is captured by the electron cloud of an atom. The result is a protonated atom, which is a chemical compound of hydrogen. In a vacuum, when free electrons are present, a sufficiently slow proton may pick up a single free electron, becoming a neutral hydrogen atom, which is chemically a free radical. Such "free hydrogen atoms" tend to react chemically with many other types of atoms at sufficiently low energies. When free hydrogen atoms react with each other, they form neutral hydrogen molecules (H2), which are the most common molecular component of molecular clouds in interstellar space.

Free protons are routinely used for accelerators for proton therapy or various particle physics experiments, with the most powerful example being the Large Hadron Collider.
$
10
Question 1: What is the electric charge of a proton?
A: 0 e
B: -1 e
C: +2 e
D: +1 e
E: -2 e
Answer: D

Question 2: How does the mass of a proton compare to that of an electron?
A: It is equal.
B: It is half.
C: It is twice.
D: It is 1,836 times.
E: It is 918 times.
Answer: D

Question 3: What are protons and neutrons collectively referred to as?
A: Electrons
B: Atoms
C: Molecules
D: Nucleons
E: Quarks
Answer: D

Question 4: The number of protons in the nucleus determines an element's:
A: Atomic weight
B: Isotope number
C: Atomic radius
D: Electron count
E: Atomic number
Answer: E

Question 5: Who named the hydrogen nucleus as "proton"?
A: Albert Einstein
B: Niels Bohr
C: James Chadwick
D: Ernest Rutherford
E: Richard Feynman
Answer: D

Question 6: Originally, protons were considered to be:
A: Isotopes
B: Molecules
C: Hadrons
D: Elementary particles
E: Quarks
Answer: D

Question 7: What percentage of a proton's mass is contributed by the rest masses of quarks?
A: 50%
B: 25%
C: 10%
D: 5%
E: 1%
Answer: E

Question 8: What is the approximate root mean square charge radius of a proton?
A: 0.54-0.57 fm
B: 0.64-0.67 fm
C: 0.74-0.77 fm
D: 0.84-0.87 fm
E: 0.94-0.97 fm
Answer: D

Question 9: When free protons and electrons interact at sufficiently low temperatures and kinetic energies, what do they form?
A: Neutrons
B: Hydrogen molecules
C: Neutral hydrogen atoms
D: Positrons
E: Helium atoms
Answer: C

Question 10: What is the most powerful accelerator used for various particle physics experiments?
A: Small Atom Collider
B: Medium Proton Collider
C: Large Neutron Collider
D: Mega Electron Collider
E: Large Hadron Collider
Answer: E
@
The neutron is a subatomic particle, symbol 
n
 or 
n0
, which has a neutral (not positive or negative) charge, and a mass slightly greater than that of a proton. Protons and neutrons constitute the nuclei of atoms. Since protons and neutrons behave similarly within the nucleus, and each has a mass of approximately one dalton, they are both referred to as nucleons.[7] Their properties and interactions are described by nuclear physics. Protons and neutrons are not elementary particles; each is composed of three quarks.

The chemical properties of an atom are mostly determined by the configuration of electrons that orbit the atom's heavy nucleus. The electron configuration is determined by the charge of the nucleus, which is determined by the number of protons, or atomic number. The number of neutrons is the neutron number. Neutrons do not affect the electron configuration, but the sum of atomic and neutron numbers is the mass of the nucleus.

Atoms of a chemical element that differ only in neutron number are called isotopes. For example, carbon, with atomic number 6, has an abundant isotope carbon-12 with 6 neutrons and a rare isotope carbon-13 with 7 neutrons. Some elements occur in nature with only one stable isotope, such as fluorine. Other elements occur with many stable isotopes, such as tin with ten stable isotopes, or with no stable isotope, such as technetium.

The properties of an atomic nucleus depend on both atomic and neutron numbers. With their positive charge, the protons within the nucleus are repelled by the long-range electromagnetic force, but the much stronger, but short-range, nuclear force binds the nucleons closely together. Neutrons are required for the stability of nuclei, with the exception of the single-proton hydrogen nucleus. Neutrons are produced copiously in nuclear fission and fusion. They are a primary contributor to the nucleosynthesis of chemical elements within stars through fission, fusion, and neutron capture processes.

The neutron is essential to the production of nuclear power. In the decade after the neutron was discovered by James Chadwick in 1932,[8] neutrons were used to induce many different types of nuclear transmutations. With the discovery of nuclear fission in 1938,[9] it was quickly realized that, if a fission event produced neutrons, each of these neutrons might cause further fission events, in a cascade known as a nuclear chain reaction.[10] These events and findings led to the first self-sustaining nuclear reactor (Chicago Pile-1, 1942) and the first nuclear weapon (Trinity, 1945).

Dedicated neutron sources like neutron generators, research reactors and spallation sources produce free neutrons for use in irradiation and in neutron scattering experiments. A free neutron spontaneously decays to a proton, an electron, and an antineutrino, with a mean lifetime of about 15 minutes.[11] Free neutrons do not directly ionize atoms, but they do indirectly cause ionizing radiation, so they can be a biological hazard, depending on dose.[10] A small natural "neutron background" flux of free neutrons exists on Earth, caused by cosmic ray showers, and by the natural radioactivity of spontaneously fissionable elements in the Earth's crust.[12]
$
10
Question 1: What is the charge of a neutron?
A: Positive
B: Negative
C: Neutral
D: Half-positive
E: Half-negative
Answer: C

Question 2: Protons and neutrons in the nucleus of atoms are collectively referred to as:
A: Isotopes
B: Electrons
C: Atoms
D: Nucleons
E: Daltonions
Answer: D

Question 3: The chemical properties of an atom are primarily determined by the:
A: Number of neutrons
B: Mass of the nucleus
C: Configuration of electrons
D: Nuclear force
E: Fusion process
Answer: C

Question 4: Atoms of the same element that differ only in neutron number are called:
A: Ions
B: Molecules
C: Compounds
D: Isotopes
E: Radicals
Answer: D

Question 5: What is responsible for binding nucleons closely together despite the repulsion between protons?
A: Electromagnetic force
B: Nuclear force
C: Electron force
D: Atomic force
E: Gravitational force
Answer: B

Question 6: Who discovered the neutron?
A: Albert Einstein
B: Robert Oppenheimer
C: Niels Bohr
D: James Chadwick
E: Richard Feynman
Answer: D

Question 7: What significant event occurred as a result of discovering nuclear fission in 1938?
A: The creation of the first stable isotope
B: The formation of the first atomic bond
C: The realization of the nuclear chain reaction possibility
D: The invention of the neutron microscope
E: The discovery of electromagnetic radiation
Answer: C

Question 8: The first self-sustaining nuclear reactor was named:
A: Neutron-1
B: Chicago Proton-2
C: New York Fission-3
D: Trinity-1944
E: Chicago Pile-1
Answer: E

Question 9: A free neutron spontaneously decays into a proton, an electron, and:
A: A neutrino
B: A positron
C: An atom
D: An antineutrino
E: A quark
Answer: D

Question 10: Free neutrons do not directly ionize atoms, but can indirectly cause:
A: Nuclear fusion
B: Chemical reactions
C: Ionizing radiation
D: Electromagnetic force
E: Proton decay
Answer: C
@
The Large Hadron Collider (LHC) is the world's largest and highest-energy particle collider.[1][2] It was built by the European Organization for Nuclear Research (CERN) between 1998 and 2008 in collaboration with over 10,000 scientists and hundreds of universities and laboratories, as well as more than 100 countries.[3] It lies in a tunnel 27 kilometres (17 mi) in circumference and as deep as 175 metres (574 ft) beneath the France–Switzerland border near Geneva.

The first collisions were achieved in 2010 at an energy of 3.5 teraelectronvolts (TeV) per beam, about four times the previous world record.[4][5] The discovery of the Higgs boson at the LHC was announced in 2012. Between 2013 and 2015, the LHC was shut down and upgraded; after those upgrades it reached 6.8 TeV per beam (13.6 TeV total collision energy).[6][7][8][9] At the end of 2018, it was shut down for three years for further upgrades.

The collider has four crossing points where the accelerated particles collide. Nine detectors,[10] each designed to detect different phenomena, are positioned around the crossing points. The LHC primarily collides proton beams, but it can also accelerate beams of heavy ions: lead–lead collisions and proton–lead collisions are typically performed for one month a year.

The LHC's goal is to allow physicists to test the predictions of different theories of particle physics, including measuring the properties of the Higgs boson,[11] searching for the large family of new particles predicted by supersymmetric theories,[12] and other unresolved questions in particle physics.

Background
The term hadron refers to subatomic composite particles composed of quarks held together by the strong force (analogous to the way that atoms and molecules are held together by the electromagnetic force).[13] The best-known hadrons are the baryons such as protons and neutrons; hadrons also include mesons such as the pion and kaon, which were discovered during cosmic ray experiments in the late 1940s and early 1950s.[14]

A collider is a type of a particle accelerator that brings two opposing particle beams together such that the particles collide. In particle physics, colliders, though harder to construct, are a powerful research tool because they reach a much higher center of mass energy than fixed target setups. [1] Analysis of the byproducts of these collisions gives scientists good evidence of the structure of the subatomic world and the laws of nature governing it. Many of these byproducts are produced only by high-energy collisions, and they decay after very short periods of time. Thus many of them are hard or nearly impossible to study in other ways.[15]

Purpose
Many physicists hope that the Large Hadron Collider will help answer some of the fundamental open questions in physics, which concern the basic laws governing the interactions and forces among the elementary objects, the deep structure of space and time, and in particular the interrelation between quantum mechanics and general relativity.[16]

Data is also needed from high-energy particle experiments to suggest which versions of current scientific models are more likely to be correct – in particular to choose between the Standard Model and Higgsless model and to validate their predictions and allow further theoretical development.

Issues explored by LHC collisions include:[17][18]

Is the mass of elementary particles being generated by the Higgs mechanism via electroweak symmetry breaking?[19] It was expected that the collider experiments will either demonstrate or rule out the existence of the elusive Higgs boson, thereby allowing physicists to consider whether the Standard Model or its Higgsless alternatives are more likely to be correct.[20][21]
Is supersymmetry, an extension of the Standard Model and Poincaré symmetry, realized in nature, implying that all known particles have supersymmetric partners?[22][23][24]
Are there extra dimensions,[25] as predicted by various models based on string theory, and can we detect them?[26]
What is the nature of the dark matter that appears to account for 27% of the mass–energy of the universe?
Other open questions that may be explored using high-energy particle collisions:

It is already known that electromagnetism and the weak nuclear force are different manifestations of a single force called the electroweak force. The LHC may clarify whether the electroweak force and the strong nuclear force are similarly just different manifestations of one universal unified force, as predicted by various Grand Unification Theories.
Why is the fourth fundamental force (gravity) so many orders of magnitude weaker than the other three fundamental forces? See also Hierarchy problem.
Are there additional sources of quark flavour mixing, beyond those already present within the Standard Model?
Why are there apparent violations of the symmetry between matter and antimatter? See also CP violation.
What are the nature and properties of quark–gluon plasma, thought to have existed in the early universe and in certain compact and strange astronomical objects today? This will be investigated by heavy ion collisions, mainly in ALICE, but also in CMS, ATLAS and LHCb. First observed in 2010, findings published in 2012 confirmed the phenomenon of jet quenching in heavy-ion collisions.[27][28][29]
$
10
Question 1: Which organization constructed the Large Hadron Collider (LHC)?
A: NASA
B: ESA
C: CERN
D: MIT
E: IBM
Answer: C

Question 2: The LHC is situated beneath the border of which two countries?
A: USA and Canada
B: Germany and France
C: Italy and Switzerland
D: France and Switzerland
E: Germany and Austria
Answer: D

Question 3: How long did the LHC shut down for upgrades at the end of 2018?
A: 1 year
B: 2 years
C: 3 years
D: 4 years
E: 5 years
Answer: C

Question 4: How many detectors are positioned around the LHC's crossing points?
A: Three
B: Five
C: Seven
D: Nine
E: Eleven
Answer: D

Question 5: The term "hadron" refers to subatomic particles made up of:
A: Electrons
B: Neutrons
C: Quarks
D: Positrons
E: Bosons
Answer: C

Question 6: What is the primary objective of a collider?
A: To generate electrical energy
B: To cool particles
C: To oppose particle beams
D: To make particles travel in a straight line
E: To allow particles to collide with one another
Answer: E

Question 7: What was one of the primary questions the LHC hoped to explore regarding elementary particles?
A: Their speed in a vacuum
B: Their behavior under extreme heat
C: Their generation by the Higgs mechanism via electroweak symmetry breaking
D: Their ability to conduct electricity
E: Their resistance to gravitational forces
Answer: C

Question 8: Supersymmetry suggests that all known particles have:
A: Inverse partners
B: Supersymmetric partners
C: Electromagnetic partners
D: Isotopic partners
E: Atomic partners
Answer: B

Question 9: How much of the universe's mass–energy does dark matter appear to account for?
A: 5%
B: 13%
C: 27%
D: 50%
E: 72%
Answer: C

Question 10: The LHC might help clarify whether the electroweak force and the strong nuclear force are different manifestations of:
A: A universal duality force
B: A universal binary force
C: A universal unified force
D: A universal quantum force
E: A universal cosmic force
Answer: C
@
In particle physics, a hadron (/ˈhædrɒn/ i; Ancient Greek: ἁδρός, romanized: hadrós; "stout, thick") is a composite subatomic particle made of two or more quarks held together by the strong interaction. They are analogous to molecules that are held together by the electric force. Most of the mass of ordinary matter comes from two hadrons: the proton and the neutron, while most of the mass of the protons and neutrons is in turn due to the binding energy of their constituent quarks, due to the strong force.

Hadrons are categorized into two broad families: baryons, made of an odd number of quarks (usually three quarks) and mesons, made of an even number of quarks (usually two quarks: one quark and one antiquark).[1] Protons and neutrons (which make the majority of the mass of an atom) are examples of baryons; pions are an example of a meson. "Exotic" hadrons, containing more than three valence quarks, have been discovered in recent years. A tetraquark state (an exotic meson), named the Z(4430)−, was discovered in 2007 by the Belle Collaboration[2] and confirmed as a resonance in 2014 by the LHCb collaboration.[3] Two pentaquark states (exotic baryons), named P+
c(4380) and P+
c(4450), were discovered in 2015 by the LHCb collaboration.[4] There are several more exotic hadron candidates and other colour-singlet quark combinations that may also exist.

Almost all "free" hadrons and antihadrons (meaning, in isolation and not bound within an atomic nucleus) are believed to be unstable and eventually decay into other particles. The only known possible exception is free protons, which appear to be stable, or at least, take immense amounts of time to decay (order of 1034+ years). By way of comparison, free neutrons are the longest-lived unstable particle, and decay with a half-life of about 879 seconds.[a][5]

Hadron physics is studied by colliding hadrons, e.g. protons, with each other or the nuclei of dense, heavy elements, such as lead (Pb) or gold (Au), and detecting the debris in the produced particle showers. A similar process occurs in the natural environment, in the extreme upper-atmosphere, where muons and mesons such as pions are produced by the collisions of cosmic rays with rarefied gas particles in the outer atmosphere.[6]
$
10
Question 1: What is a hadron primarily composed of?
A: Electrons
B: Atoms
C: Neutrons
D: Quarks
E: Photons
Answer: D

Question 2: The strong interaction primarily binds which of the following?
A: Molecules
B: Atoms
C: Quarks in hadrons
D: Electrons in atoms
E: Photons and protons
Answer: C

Question 3: Which two hadrons mainly contribute to the mass of ordinary matter?
A: Electron and Neutrino
B: Pion and Muon
C: Proton and Electron
D: Proton and Neutron
E: Meson and Baryon
Answer: D

Question 4: How many quarks usually make up baryons?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: C

Question 5: What is the main difference between mesons and baryons based on their quark composition?
A: Mesons have an odd number and baryons have an even number of quarks.
B: Mesons have an even number and baryons have an odd number of quarks.
C: Mesons and baryons both have an even number of quarks.
D: Mesons and baryons both have an odd number of quarks.
E: There is no difference.
Answer: B

Question 6: The Z(4430)−, discovered in 2007, is an example of:
A: A pion
B: An exotic baryon
C: An ordinary meson
D: An exotic meson
E: A neutron
Answer: D

Question 7: What makes a hadron "exotic"?
A: They are made of atoms.
B: They have more than three valence quarks.
C: They are made of only one type of quark.
D: They contain neutrinos.
E: They are less dense than other hadrons.
Answer: B

Question 8: Which hadron is believed to be possibly stable and might not decay for immense amounts of time?
A: Neutron
B: Meson
C: Pion
D: Proton
E: Tetraquark
Answer: D

Question 9: What is the primary method used to study hadron physics?
A: Observing their natural occurrence in nature
B: Colliding hadrons with each other or with nuclei of dense elements
C: Using lasers to excite hadrons
D: Placing hadrons in strong magnetic fields
E: Cooling them to absolute zero and observing their behavior
Answer: B

Question 10: In the natural environment, where are muons and mesons such as pions mainly produced?
A: Deep within the Earth's crust
B: At the center of stars
C: In the outer atmosphere due to collisions of cosmic rays
D: Within the core of the Earth
E: At the bottom of oceans
Answer: C
@
The heart is a muscular organ in most animals. This organ pumps blood through the blood vessels of the circulatory system.[1] The pumped blood carries oxygen and nutrients to the body, while carrying metabolic waste such as carbon dioxide to the lungs.[2] In humans, the heart is approximately the size of a closed fist and is located between the lungs, in the middle compartment of the chest, called the mediastinum.[3]

In humans, other mammals, and birds, the heart is divided into four chambers: upper left and right atria and lower left and right ventricles.[4][5] Commonly the right atrium and ventricle are referred together as the right heart and their left counterparts as the left heart.[6] Fish, in contrast, have two chambers, an atrium and a ventricle, while most reptiles have three chambers.[5] In a healthy heart blood flows one way through the heart due to heart valves, which prevent backflow.[3] The heart is enclosed in a protective sac, the pericardium, which also contains a small amount of fluid. The wall of the heart is made up of three layers: epicardium, myocardium, and endocardium.[7]

The heart pumps blood with a rhythm determined by a group of pacemaker cells in the sinoatrial node. These generate an electric current that causes the heart to contract, traveling through the atrioventricular node and along the conduction system of the heart. In humans, deoxygenated blood enters the heart through the right atrium from the superior and inferior venae cavae and passes to the right ventricle. From here it is pumped into pulmonary circulation to the lungs, where it receives oxygen and gives off carbon dioxide. Oxygenated blood then returns to the left atrium, passes through the left ventricle and is pumped out through the aorta into systemic circulation, traveling through arteries, arterioles, and capillaries—where nutrients and other substances are exchanged between blood vessels and cells, losing oxygen and gaining carbon dioxide—before being returned to the heart through venules and veins.[8] The heart beats at a resting rate close to 72 beats per minute.[9] Exercise temporarily increases the rate, but lowers it in the long term, and is good for heart health.[10]

Cardiovascular diseases are the most common cause of death globally as of 2008, accounting for 30% of deaths.[11][12] Of these more than three-quarters are a result of coronary artery disease and stroke.[11] Risk factors include: smoking, being overweight, little exercise, high cholesterol, high blood pressure, and poorly controlled diabetes, among others.[13] Cardiovascular diseases do not frequently have symptoms but may cause chest pain or shortness of breath. Diagnosis of heart disease is often done by the taking of a medical history, listening to the heart-sounds with a stethoscope, as well as with ECG, echocardiogram, and ultrasound.[3] Specialists who focus on diseases of the heart are called cardiologists, although many specialties of medicine may be involved in treatment.[12]
$
10
Question 1: What is the primary function of the heart in most animals?
A: To digest food
B: To pump blood through the circulatory system
C: To filter toxins
D: To produce hormones
E: To regulate body temperature
Answer: B

Question 2: Where does the pumped blood carry metabolic waste such as carbon dioxide?
A: To the liver
B: To the kidneys
C: To the stomach
D: To the brain
E: To the lungs
Answer: E

Question 3: How many chambers does the heart of humans, other mammals, and birds typically have?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: D

Question 4: What prevents backflow of blood in a healthy heart?
A: Arteries
B: Veins
C: Heart valves
D: Capillaries
E: Pericardium
Answer: C

Question 5: Which of the following is NOT a layer of the heart's wall?
A: Endocardium
B: Epicardium
C: Pericardium
D: Myocardium
E: Atrium
Answer: C

Question 6: The rhythm of the heart is determined by pacemaker cells located in which part?
A: Atrioventricular node
B: Ventricles
C: Aorta
D: Sinoatrial node
E: Pulmonary circulation
Answer: D

Question 7: Where does deoxygenated blood enter the heart in humans?
A: Left ventricle
B: Left atrium
C: Right atrium
D: Right ventricle
E: Aorta
Answer: C

Question 8: At what approximate rate does the human heart beat when at rest?
A: 50 beats per minute
B: 60 beats per minute
C: 72 beats per minute
D: 80 beats per minute
E: 90 beats per minute
Answer: C

Question 9: Which of these is a major risk factor for cardiovascular diseases?
A: Consuming a lot of vitamin C
B: Regular exercise
C: High blood pressure
D: Drinking a lot of water
E: High calcium intake
Answer: C

Question 10: What do specialists who focus on diseases of the heart commonly called?
A: Oncologists
B: Neurologists
C: Cardiologists
D: Endocrinologists
E: Nephrologists
Answer: C
@
Animals are multicellular, eukaryotic organisms in the biological kingdom Animalia. With few exceptions, animals consume organic material, breathe oxygen, have myocytes and are able to move, can reproduce sexually, and grow from a hollow sphere of cells, the blastula, during embryonic development. As of 2022, 2.16 million living animal species have been described—of which around 1.05 million are insects, over 85,000 are molluscs, and around 65,000 are vertebrates. It has been estimated there are around 7.77 million animal species. Animals range in length from 8.5 micrometres (0.00033 in) to 33.6 metres (110 ft). They have complex interactions with each other and their environments, forming intricate food webs. The scientific study of animals is known as zoology.

Most living animal species are in Bilateria, a clade whose members have a bilaterally symmetric body plan. The Bilateria include the protostomes, containing animals such as nematodes, arthropods, flatworms, annelids and molluscs, and the deuterostomes, containing the echinoderms and the chordates, the latter including the vertebrates. Life forms interpreted as early animals were present in the Ediacaran biota of the late Precambrian. Many modern animal phyla became clearly established in the fossil record as marine species during the Cambrian explosion, which began around 539 million years ago. 6,331 groups of genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago.

Historically, Aristotle divided animals into those with blood and those without. Carl Linnaeus created the first hierarchical biological classification for animals in 1758 with his Systema Naturae, which Jean-Baptiste Lamarck expanded into 14 phyla by 1809. In 1874, Ernst Haeckel divided the animal kingdom into the multicellular Metazoa (now synonymous with Animalia) and the Protozoa, single-celled organisms no longer considered animals. In modern times, the biological classification of animals relies on advanced techniques, such as molecular phylogenetics, which are effective at demonstrating the evolutionary relationships between taxa.

Humans make use of many animal species, such as for food (including meat, milk, and eggs), for materials (such as leather and wool), as pets, and as working animals including for transport. Dogs have been used in hunting, as have birds of prey, while many terrestrial and aquatic animals were hunted for sports. Nonhuman animals have appeared in art since the earliest times and are featured in mythology and religion.

Etymology
The word "animal" comes from the Latin animalis, meaning 'having breath', 'having soul' or 'living being'.[4] The biological definition includes all members of the kingdom Animalia.[5] In colloquial usage, the term animal is often used to refer only to nonhuman animals.[6][7][8][9] The term "metazoa" is derived from the Ancient Greek μετα (meta, meaning "later") and ζῷᾰ (zōia, plural of ζῷον zōion, meaning animal).[10][11]

Characteristics
Animals have several characteristics that set them apart from other living things. Animals are eukaryotic and multicellular.[12] Unlike plants and algae, which produce their own nutrients,[13] animals are heterotrophic,[14][15] feeding on organic material and digesting it internally.[16] With very few exceptions, animals respire aerobically.[a][18] All animals are motile[19] (able to spontaneously move their bodies) during at least part of their life cycle, but some animals, such as sponges, corals, mussels, and barnacles, later become sessile. The blastula is a stage in embryonic development that is unique to animals, allowing cells to be differentiated into specialised tissues and organs.[20]
$
10
Question 1: What do animals predominantly consume for sustenance?
A: Photosynthesis
B: Minerals from the soil
C: Organic material
D: Carbon dioxide
E: Sunlight
Answer: C

Question 2: Which group has the highest number of described living species among animals?
A: Molluscs
B: Vertebrates
C: Insects
D: Flatworms
E: Arthropods
Answer: C

Question 3: What is the scientific study of animals called?
A: Botany
B: Meteorology
C: Anatomy
D: Zoology
E: Ethology
Answer: D

Question 4: The majority of living animal species are part of which clade having a bilaterally symmetric body plan?
A: Deuterostomes
B: Echinoderms
C: Bilateria
D: Chordates
E: Protozoa
Answer: C

Question 5: The term "animal" derives from the Latin word "animalis," which means:
A: Four-legged
B: Furry creature
C: Having breath or living being
D: Domesticated being
E: Land dweller
Answer: C

Question 6: Who historically divided animals into those with blood and those without?
A: Carl Linnaeus
B: Jean-Baptiste Lamarck
C: Ernst Haeckel
D: Aristotle
E: Charles Darwin
Answer: D

Question 7: What term is used to describe animals that produce their own nutrients?
A: Heterotrophic
B: Aerobic
C: Autotrophic
D: Eukaryotic
E: Sessile
Answer: C

Question 8: During which historical event did many modern animal phyla become established in the fossil record as marine species?
A: Jurassic Period
B: Ediacaran Biota
C: Cambrian Explosion
D: Permian Extinction
E: Cretaceous Period
Answer: C

Question 9: All animals are capable of movement:
A: Throughout their entire life
B: Only when they are predators
C: During at least part of their life cycle
D: Only during the night
E: Only during the day
Answer: C

Question 10: Which stage in embryonic development is unique to animals, allowing cells to differentiate into specialized tissues and organs?
A: Mitosis
B: Gestation
C: Fetus
D: Zygote
E: Blastula
Answer: E
@
A multicellular organism is an organism that consists of more than one cell, in contrast to unicellular organism.[1] All species of animals, land plants and most fungi are multicellular, as are many algae, whereas a few organisms are partially uni- and partially multicellular, like slime molds and social amoebae such as the genus Dictyostelium.[2][3]

Multicellular organisms arise in various ways, for example by cell division or by aggregation of many single cells.[4][3] Colonial organisms are the result of many identical individuals joining together to form a colony. However, it can often be hard to separate colonial protists from true multicellular organisms, because the two concepts are not distinct; colonial protists have been dubbed "pluricellular" rather than "multicellular".[5][6] There are also macroscopic organisms that are multinucleate though technically unicellular, such as the Xenophyophorea that can reach 20 cm.

Evolutionary history
Occurrence
Multicellularity has evolved independently at least 25 times in eukaryotes,[7][8] and also in some prokaryotes, like cyanobacteria, myxobacteria, actinomycetes, Magnetoglobus multicellularis or Methanosarcina.[3] However, complex multicellular organisms evolved only in six eukaryotic groups: animals, symbiomycotan fungi, brown algae, red algae, green algae, and land plants.[9] It evolved repeatedly for Chloroplastida (green algae and land plants), once for animals, once for brown algae, three times in the fungi (chytrids, ascomycetes, and basidiomycetes)[10] and perhaps several times for slime molds and red algae.[11] The first evidence of multicellular organization, which is when unicellular organisms coordinate behaviors and may be an evolutionary precursor to true multicellularity, is from cyanobacteria-like organisms that lived 3.0–3.5 billion years ago.[7] To reproduce, true multicellular organisms must solve the problem of regenerating a whole organism from germ cells (i.e., sperm and egg cells), an issue that is studied in evolutionary developmental biology. Animals have evolved a considerable diversity of cell types in a multicellular body (100–150 different cell types), compared with 10–20 in plants and fungi.[12]

Loss of multicellularity
Loss of multicellularity occurred in some groups.[13] Fungi are predominantly multicellular, though early diverging lineages are largely unicellular (e.g., Microsporidia) and there have been numerous reversions to unicellularity across fungi (e.g., Saccharomycotina, Cryptococcus, and other yeasts).[14][15] It may also have occurred in some red algae (e.g., Porphyridium), but it is possible that they are primitively unicellular.[16] Loss of multicellularity is also considered probable in some green algae (e.g., Chlorella vulgaris and some Ulvophyceae).[17][18] In other groups, generally parasites, a reduction of multicellularity occurred, in number or types of cells (e.g., the myxozoans, multicellular organisms, earlier thought to be unicellular, are probably extremely reduced cnidarians).[19]

Cancer
Multicellular organisms, especially long-living animals, face the challenge of cancer, which occurs when cells fail to regulate their growth within the normal program of development. Changes in tissue morphology can be observed during this process. Cancer in animals (metazoans) has often been described as a loss of multicellularity.[20] There is a discussion about the possibility of existence of cancer in other multicellular organisms[21][22] or even in protozoa.[23] For example, plant galls have been characterized as tumors,[24] but some authors argue that plants do not develop cancer.[25]

Separation of somatic and germ cells
In some multicellular groups, which are called Weismannists, a separation between a sterile somatic cell line and a germ cell line evolved. However, Weismannist development is relatively rare (e.g., vertebrates, arthropods, Volvox), as a great part of species have the capacity for somatic embryogenesis (e.g., land plants, most algae, many invertebrates).[26][10]
$
10
Question 1: What defines a multicellular organism?
A: An organism with multiple organs
B: An organism that consists of more than one cell
C: An organism with one specialized cell
D: An organism that consumes other cells
E: An organism with multiple nuclei in a single cell
Answer: B

Question 2: Which organisms are partially uni- and partially multicellular?
A: Animals and plants
B: Slime molds and Dictyostelium
C: Fungi and algae
D: Cyanobacteria and myxobacteria
E: Brown and red algae
Answer: B

Question 3: Which term describes colonial protists that aren't exactly multicellular?
A: Binucleate
B: Unicellular
C: Pluricellular
D: Colonial
E: Macronucleate
Answer: C

Question 4: How many times has multicellularity evolved independently in eukaryotes?
A: Once
B: Three times
C: Six times
D: Ten times
E: At least 25 times
Answer: E

Question 5: Complex multicellular organisms evolved only in six eukaryotic groups. Which among the options is NOT one of them?
A: Animals
B: Brown algae
C: Symbiomycotan fungi
D: Slime molds
E: Land plants
Answer: D

Question 6: True multicellular organisms face challenges in regenerating the entire organism from which type of cells?
A: Somatic cells
B: Germ cells
C: Pluricellular cells
D: Macronucleate cells
E: Sessile cells
Answer: B

Question 7: Which group predominantly reverted to unicellularity from multicellularity?
A: Brown algae
B: Animals
C: Fungi
D: Land plants
E: Red algae
Answer: C

Question 8: Cancer in animals can be described as:
A: A gain of multicellularity
B: A mutation in the germ cell
C: A loss of unicellularity
D: A loss of multicellularity
E: An increase in pluricellularity
Answer: D

Question 9: In which organisms has cancer been debated to possibly exist, aside from animals?
A: Symbiomycotan fungi
B: Protozoa
C: Slime molds
D: Brown algae
E: Volvox
Answer: B

Question 10: In Weismannists, what distinction evolved in multicellular groups?
A: A separation between pluricellular and unicellular lines
B: A separation between somatic and germ cell lines
C: A separation between cancerous and normal cells
D: A separation between algae and fungi cell lines
E: A separation between multicellular and colonial organisms
Answer: B
@
The cell is the basic structural and functional unit of all forms of life. Every cell consists of cytoplasm enclosed within a membrane, and contains many macromolecules such as proteins, DNA and RNA, as well as many small molecules of nutrients and metabolites.[1] The term comes from the Latin word cellula meaning 'small room'.[2]

Cells can acquire specified function and carry out various tasks within the cell such as replication, DNA repair, protein synthesis, and motility. Cells are capable of specialization and mobility within the cell.

Most plant and animal cells are only visible under a light microscope, with dimensions between 1 and 100 micrometres.[3] Electron microscopy gives a much higher resolution showing greatly detailed cell structure. Organisms can be classified as unicellular (consisting of a single cell such as bacteria) or multicellular (including plants and animals).[4] Most unicellular organisms are classed as microorganisms. The number of cells in plants and animals varies from species to species; it has been estimated that the human body contains around 37 trillion (3.72×1013) cells.[5] The human brain accounts for around 80 billion of these cells.[6]

The study of cells and how they work has led to many other studies in related areas of biology, including: discovery of DNA, cancer systems biology, aging and developmental biology.

Cell biology is the study of cells, which were discovered by Robert Hooke in 1665, who named them for their resemblance to cells inhabited by Christian monks in a monastery.[7][8] Cell theory, first developed in 1839 by Matthias Jakob Schleiden and Theodor Schwann, states that all organisms are composed of one or more cells, that cells are the fundamental unit of structure and function in all living organisms, and that all cells come from pre-existing cells.[9] Cells emerged on Earth about 4 billion years ago.[10][11][12][13]
$
10
Question 1: What is the primary structural and functional unit of all life forms?
A: Organ
B: Tissue
C: Organelle
D: Macromolecule
E: Cell
Answer: E

Question 2: Which of the following is NOT a macromolecule found in cells?
A: DNA
B: RNA
C: Protein
D: Lipid
E: Nutrient
Answer: E

Question 3: The term "cell" derives from a Latin word meaning:
A: Small life
B: Basic unit
C: Tiny being
D: Small room
E: Life brick
Answer: D

Question 4: Which process is NOT associated with a cell's function?
A: Replication
B: DNA repair
C: Protein synthesis
D: Photosynthesis
E: Motility
Answer: D

Question 5: What is the estimated number of cells present in the human body?
A: 80 billion
B: 100 billion
C: 37 trillion
D: 3.72 billion
E: 1 trillion
Answer: C

Question 6: How many of the estimated cells in the human body does the brain account for?
A: 100 billion
B: 80 billion
C: 37 billion
D: 50 billion
E: 10 billion
Answer: B

Question 7: Who is credited with the discovery of cells in 1665?
A: Matthias Jakob Schleiden
B: Theodor Schwann
C: Robert Hooke
D: Christian Monks
E: Charles Darwin
Answer: C

Question 8: Cell theory, developed in 1839, states all of the following EXCEPT:
A: All organisms are made of one or more cells
B: Cells are the fundamental unit of structure and function
C: Cells can only emerge from non-living matter
D: All cells come from pre-existing cells
E: Cells are the basic structure in all living organisms
Answer: C

Question 9: Approximately how long ago did cells emerge on Earth?
A: 2 billion years ago
B: 3 billion years ago
C: 4 billion years ago
D: 5 billion years ago
E: 6 billion years ago
Answer: C

Question 10: What did Robert Hooke name cells after due to their resemblance?
A: Small chambers in a castle
B: Brick structures
C: Cells in a monastery
D: Honeycombs in a beehive
E: Pockets in a honeycomb
Answer: C
@
In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ i) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation.[1] Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as "memory", "search" and "stimulus".[2]

In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.[3]

As an effective method, an algorithm can be expressed within a finite amount of space and time[4] and in a well-defined formal language[5] for calculating a function.[6] Starting from an initial state and initial input (perhaps empty),[7] the instructions describe a computation that, when executed, proceeds through a finite[8] number of well-defined successive states, eventually producing "output"[9] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[10]

Formalization
Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform—in a specific order—to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987), and Gurevich (2000):

Minsky: "But we will also maintain, with Turing ... that any procedure which could "naturally" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments ... in its favor are hard to refute".[41] Gurevich: "… Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine … according to Savage [1987], an algorithm is a computational process defined by a Turing machine".[42]

Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case—due to a major theorem of computability theory known as the halting problem.

Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.

For some of these computational processes, the algorithm must be rigorously defined: and specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case by case; the criteria for each case must be clear (and computable).

Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting "from the top" and going "down to the bottom"—an idea that is described more formally by flow of control.

So far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception—one which attempts to describe a task in discrete, "mechanical" means. Unique to this conception of formalized algorithms is the assignment operation, which sets the value of a variable. It derives from the intuition of "memory" as a scratchpad. An example of such an assignment can be found below.

For some alternate conceptions of what constitutes an algorithm, see functional programming and logic programming.
$
10
Question: What is an algorithm in mathematics and computer science?
A: A finite sequence of rigorous instructions.
B: A random set of guidelines.
C: A heuristic approach to problem solving.
D: A metaphorical descriptor for machines.
E: A non-terminating process.
Answer: A

Question: How do advanced algorithms achieve automation?
A: Through randomness.
B: By avoiding conditionals.
C: By not using any input data.
D: Through automated reasoning and decision-making.
E: By ignoring code execution.
Answer: D

Question: Which term did Alan Turing use metaphorically to describe machines?
A: Stimulus.
B: Algorithm.
C: Heuristic.
D: Optimization.
E: Turing-complete system.
Answer: A

Question: How does a heuristic differ from an algorithm?
A: A heuristic is always well-defined.
B: A heuristic always produces optimal results.
C: A heuristic does not necessarily guarantee correct or optimal results.
D: A heuristic is a type of advanced algorithm.
E: A heuristic uses automated reasoning.
Answer: C

Question: Which of the following is true about the expression of an algorithm?
A: It requires infinite space.
B: It doesn’t require any formal language.
C: It must be expressed over an infinite amount of time.
D: It can be expressed within a finite amount of space and time.
E: It always starts with a final ending state.
Answer: D

Question: What can some algorithms incorporate for their input?
A: Heuristic methods.
B: Only deterministic elements.
C: Random input.
D: Only fixed variables.
E: Non-functional steps.
Answer: C

Question: How are algorithms related to the way computers process data?
A: Algorithms detail the aesthetics of the software interface.
B: Algorithms only manage the storage of data.
C: They detail the specific instructions a computer should perform in a specific order.
D: They reduce the need for data structures.
E: Algorithms make computers Turing-incomplete.
Answer: C

Question: According to the provided text, why is the order of computation crucial for an algorithm?
A: Because it determines the speed of the algorithm.
B: Because it allows for more randomness.
C: Because the algorithm might not need memory.
D: Because an algorithm is a precise list of precise steps.
E: Because order determines the algorithm’s efficiency.
Answer: D

Question: In the context of the provided text, what does "flow of control" describe more formally?
A: The intuition of "memory".
B: The idea of listing instructions from the top to bottom.
C: The use of randomized algorithms.
D: The conception of functional programming.
E: The termination criteria of algorithms.
Answer: B

Question: Which programming concept attempts to describe a task in discrete, "mechanical" means?
A: Logic programming.
B: Functional programming.
C: Imperative programming.
D: Object-oriented programming.
E: Asynchronous programming.
Answer: C
@
According to the quark model,[8] the properties of hadrons are primarily determined by their so-called valence quarks. For example, a proton is composed of two up quarks (each with electric charge ++2⁄3, for a total of +4⁄3 together) and one down quark (with electric charge −+1⁄3). Adding these together yields the proton charge of +1. Although quarks also carry color charge, hadrons must have zero total color charge because of a phenomenon called color confinement. That is, hadrons must be "colorless" or "white". The simplest ways for this to occur are with a quark of one color and an antiquark of the corresponding anticolor, or three quarks of different colors. Hadrons with the first arrangement are a type of meson, and those with the second arrangement are a type of baryon.

Massless virtual gluons compose the overwhelming majority of particles inside hadrons, as well as the major constituents of its mass (with the exception of the heavy charm and bottom quarks; the top quark vanishes before it has time to bind into a hadron). The strength of the strong force gluons which bind the quarks together has sufficient energy (E) to have resonances composed of massive (m) quarks (E ≥ mc2). One outcome is that short-lived pairs of virtual quarks and antiquarks are continually forming and vanishing again inside a hadron. Because the virtual quarks are not stable wave packets (quanta), but an irregular and transient phenomenon, it is not meaningful to ask which quark is real and which virtual; only the small excess is apparent from the outside in the form of a hadron. Therefore, when a hadron or anti-hadron is stated to consist of (typically) 2 or 3 quarks, this technically refers to the constant excess of quarks vs. antiquarks.

Like all subatomic particles, hadrons are assigned quantum numbers corresponding to the representations of the Poincaré group: JPC (m), where J is the spin quantum number, P the intrinsic parity (or P-parity), C the charge conjugation (or C-parity), and m is the particle's mass. Note that the mass of a hadron has very little to do with the mass of its valence quarks; rather, due to mass–energy equivalence, most of the mass comes from the large amount of energy associated with the strong interaction. Hadrons may also carry flavor quantum numbers such as isospin (G parity), and strangeness. All quarks carry an additive, conserved quantum number called a baryon number (B), which is ++1⁄3 for quarks and −+1⁄3 for antiquarks. This means that baryons (composite particles made of three, five or a larger odd number of quarks) have B = 1 whereas mesons have B = 0.

Hadrons have excited states known as resonances. Each ground state hadron may have several excited states; several hundreds of resonances have been observed in experiments. Resonances decay extremely quickly (within about 10−24 seconds) via the strong nuclear force.

In other phases of matter the hadrons may disappear. For example, at very high temperature and high pressure, unless there are sufficiently many flavors of quarks, the theory of quantum chromodynamics (QCD) predicts that quarks and gluons will no longer be confined within hadrons, "because the strength of the strong interaction diminishes with energy". This property, which is known as asymptotic freedom, has been experimentally confirmed in the energy range between 1 GeV (gigaelectronvolt) and 1 TeV (teraelectronvolt).[9] All free hadrons except (possibly) the proton and antiproton are unstable.
$
10
Question: What primarily determines the properties of hadrons according to the quark model?
A: Their meson arrangements.
B: Their electric charge.
C: Their valence quarks.
D: Their color confinement.
E: Their interaction with gluons.
Answer: C

Question: How is the electric charge of a proton determined?
A: By adding two up quarks and subtracting one down quark.
B: By adding two down quarks and subtracting one up quark.
C: By the presence of virtual gluons.
D: By the total color charge of the quarks.
E: By the presence of mesons.
Answer: A

Question: Why must hadrons have zero total color charge?
A: Due to the law of energy conservation.
B: Because of the quark arrangement.
C: Because of a phenomenon called color confinement.
D: Due to the presence of mesons.
E: Because they interact with gluons.
Answer: C

Question: Which type of hadron consists of three quarks of different colors?
A: Meson.
B: Baryon.
C: Gluon.
D: Resonance.
E: Antiquark.
Answer: B

Question: What do massless virtual gluons compose the majority of inside hadrons?
A: Valence quarks.
B: Baryons.
C: Antiquarks.
D: Mesons.
E: Particles and its mass constituents.
Answer: E

Question: Which of the following is a quantum number corresponding to the representations of the Poincaré group?
A: Isospin.
B: Baryon number.
C: P-parity.
D: Meson arrangement.
E: Color charge.
Answer: C

Question: What does the baryon number of mesons equal to?
A: 1.
B: 2.
C: 3.
D: 0.
E: -1.
Answer: D

Question: How long do resonances typically last within hadrons?
A: Several minutes.
B: About 10 seconds.
C: Several hours.
D: About 10−24 seconds.
E: They are permanent.
Answer: D

Question: What does the theory of quantum chromodynamics (QCD) predict about quarks and gluons at very high temperature and pressure?
A: They become unstable.
B: They are confined within mesons.
C: They will no longer be confined within hadrons.
D: They transform into baryons.
E: They exhibit color confinement.
Answer: C

Question: Which property has been experimentally confirmed in the energy range between 1 GeV and 1 TeV?
A: Quantum confinement.
B: Asymptotic freedom.
C: P-parity conservation.
D: Baryon confinement.
E: Meson decay.
Answer: B
@
Elementary particles
Main article: Elementary particle
See Standard Model for the current consensus theory of these particles.
Elementary particles are particles with no measurable internal structure; that is, it is unknown whether they are composed of other particles.[1] They are the fundamental objects of quantum field theory. Many families and sub-families of elementary particles exist. Elementary particles are classified according to their spin. Fermions have half-integer spin while bosons have integer spin. All the particles of the Standard Model have been experimentally observed, including the Higgs boson in 2012.[2][3] Many other hypothetical elementary particles, such as the graviton, have been proposed, but not observed experimentally.

Fermions
Fermions are one of the two fundamental classes of particles, the other being bosons. Fermion particles are described by Fermi–Dirac statistics and have quantum numbers described by the Pauli exclusion principle. They include the quarks and leptons, as well as any composite particles consisting of an odd number of these, such as all baryons and many atoms and nuclei.

Fermions have half-integer spin; for all known elementary fermions this is 1⁄2. All known fermions except neutrinos, are also Dirac fermions; that is, each known fermion has its own distinct antiparticle. It is not known whether the neutrino is a Dirac fermion or a Majorana fermion.[4] Fermions are the basic building blocks of all matter. They are classified according to whether they interact via the strong interaction or not. In the Standard Model, there are 12 types of elementary fermions: six quarks and six leptons.

Quarks
Quarks are the fundamental constituents of hadrons and interact via the strong force. Quarks are the only known carriers of fractional charge, but because they combine in groups of three quarks (baryons) or in pairs of one quark and one antiquark (mesons), only integer charge is observed in nature. Their respective antiparticles are the antiquarks, which are identical except that they carry the opposite electric charge (for example the up quark carries charge +2⁄3, while the up antiquark carries charge −2⁄3), color charge, and baryon number. There are six flavors of quarks; the three positively charged quarks are called "up-type quarks" while the three negatively charged quarks are called "down-type quarks".

Quarks
Generation	Name	Symbol	Antiparticle	Spin	Charge
(e)	Mass (MeV/c2) [5]
1	up	u	
u
1⁄2	+2⁄3	2.2+0.6
−0.4
down	d	
d
1⁄2	−1⁄3	4.6+0.5
−0.4
2	charm	c	
c
1⁄2	+2⁄3	1280±30
strange	s	
s
1⁄2	−1⁄3	96+8
−4
3	top	t	
t
1⁄2	+2⁄3	173100±600
bottom	b	
b
1⁄2	−1⁄3	4180+40
−30
Leptons
Leptons do not interact via the strong interaction. Their respective antiparticles are the antileptons, which are identical, except that they carry the opposite electric charge and lepton number. The antiparticle of an electron is an antielectron, which is almost always called a "positron" for historical reasons. There are six leptons in total; the three charged leptons are called "electron-like leptons", while the neutral leptons are called "neutrinos". Neutrinos are known to oscillate, so that neutrinos of definite flavor do not have definite mass, rather they exist in a superposition of mass eigenstates. The hypothetical heavy right-handed neutrino, called a "sterile neutrino", has been omitted.

Leptons
Generation	Name	Symbol	Antiparticle	Spin	Charge
(e)	Mass (MeV/c2) [5]
1	electron	
e−

e+
1
/
2
−1	0.511[note 1]
electron neutrino	
ν
e	
ν
e	
1
/
2
0	< 0.0000022
2	muon	
μ−

μ+
1
/
2
−1	105.7[note 2]
muon neutrino	
ν
μ	
ν
μ	
1
/
2
0	< 0.170
3	tau	
τ−

τ+
1
/
2
−1	1776.86±0.12
tau neutrino	
ν
τ	
ν
τ	
1
/
2
0	< 15.5
 A precise value of the electron mass is 0.51099895000(15) MeV/c2.[6]
 A precise value of the muon mass is 105.6583755(23) MeV/c2.[7]
Bosons
Bosons are one of the two fundamental particles having integral spinclasses of particles, the other being fermions. Bosons are characterized by Bose–Einstein statistics and all have integer spins. Bosons may be either elementary, like photons and gluons, or composite, like mesons.

According to the Standard Model, the elementary bosons are:

Name	Symbol	Antiparticle	Spin	Charge (e)	Mass (GeV/c2) [5]	Interaction mediated	Observed
photon	γ	self	1	0	0	electromagnetism	Yes
W boson	
W−

W+
1	±1	80.385±0.015	weak interaction	Yes
Z boson	
Z
self	1	0	91.1875±0.0021	weak interaction	Yes
gluon	
g
self	1	0	0	strong interaction	Yes
Higgs boson	
H0
self	0	0	125.09±0.24	mass	Yes
The Higgs boson is postulated by the electroweak theory primarily to explain the origin of particle masses. In a process known as the "Higgs mechanism", the Higgs boson and the other gauge bosons in the Standard Model acquire mass via spontaneous symmetry breaking of the SU(2) gauge symmetry. The Minimal Supersymmetric Standard Model (MSSM) predicts several Higgs bosons. On 4 July 2012, the discovery of a new particle with a mass between 125 and 127 GeV/c2 was announced; physicists suspected that it was the Higgs boson. Since then, the particle has been shown to behave, interact, and decay in many of the ways predicted for Higgs particles by the Standard Model, as well as having even parity and zero spin, two fundamental attributes of a Higgs boson. This also means it is the first elementary scalar particle discovered in nature.

Elementary bosons responsible for the four fundamental forces of nature are called force particles (gauge bosons). Strong interaction is mediated by the gluon, weak interaction is mediated by the W and Z bosons.
$
10
Question: Which particles have no measurable internal structure?
A: Bosons
B: Fermions
C: Hadrons
D: Elementary particles
E: Leptons
Answer: D: Elementary particles

Question: Fermions and bosons differ primarily in their:
A: Interactions
B: Quantum numbers
C: Spin
D: Charge
E: Composition
Answer: C: Spin

Question: The Higgs boson was primarily postulated to explain the origin of:
A: Spin
B: Electric charge
C: Color charge
D: Particle masses
E: Quantum numbers
Answer: D: Particle masses

Question: Which of the following particles do not interact via the strong force?
A: Quarks
B: Hadrons
C: Gluons
D: Leptons
E: Mesons
Answer: D: Leptons

Question: How many types of elementary fermions are there in the Standard Model?
A: 6
B: 12
C: 3
D: 8
E: 4
Answer: B: 12

Question: The antiparticle of an electron is also known as a:
A: Positron
B: Neutrino
C: Muon
D: Tau
E: Proton
Answer: A: Positron

Question: Which of the following particles is responsible for mediating the strong interaction?
A: W boson
B: Z boson
C: Photon
D: Gluon
E: Higgs boson
Answer: D: Gluon

Question: In terms of charge, what is unique about quarks compared to other particles?
A: They carry no charge
B: They carry positive charge only
C: They carry fractional charge
D: They carry integer charge only
E: They carry double charge
Answer: C: They carry fractional charge

Question: Which particle is described by the Pauli exclusion principle?
A: Bosons
B: Leptons
C: Gluons
D: Fermions
E: Hadrons
Answer: D: Fermions

Question: How many flavors of quarks are there?
A: 2
B: 4
C: 6
D: 8
E: 10
Answer: C: 6
@
tic nervous system is activated when organisms are in a relaxed state. The enteric nervous system functions to control the gastrointestinal system. Both autonomic and enteric nervous systems function involuntarily. Nerves that exit from the cranium are called cranial nerves while those exiting from the spinal cord are called spinal nerves.

At the cellular level, the nervous system is defined by the presence of a special type of cell, called the neuron. Neurons have special structures that allow them to send signals rapidly and precisely to other cells. They send these signals in the form of electrochemical impulses traveling along thin fibers called axons, which can be directly transmitted to neighboring cells through electrical synapses or cause chemicals called neurotransmitters to be released at chemical synapses. A cell that receives a synaptic signal from a neuron may be excited, inhibited, or otherwise modulated. The connections between neurons can form neural pathways, neural circuits, and larger networks that generate an organism's perception of the world and determine its behavior. Along with neurons, the nervous system contains other specialized cells called glial cells (or simply glia), which provide structural and metabolic support. Many of the cells and vasculature channels within the nervous system make up the neurovascular unit, which regulates cerebral blood flow in order to rapidly satisfy the high energy demands of activated neurons.[2]

Nervous systems are found in most multicellular animals, but vary greatly in complexity.[3] The only multicellular animals that have no nervous system at all are sponges, placozoans, and mesozoans, which have very simple body plans. The nervous systems of the radially symmetric organisms ctenophores (comb jellies) and cnidarians (which include anemones, hydras, corals and jellyfish) consist of a diffuse nerve net. All other animal species, with the exception of a few types of worm, have a nervous system containing a brain, a central cord (or two cords running in parallel), and nerves radiating from the brain and central cord. The size of the nervous system ranges from a few hundred cells in the simplest worms, to around 300 billion cells in African elephants.[4]

The central nervous system functions to send signals from one cell to others, or from one part of the body to others and to receive feedback. Malfunction of the nervous system can occur as a result of genetic defects, physical damage due to trauma or toxicity, infection, or simply senescence. The medical specialty of neurology studies disorders of the nervous system and looks for interventions that can prevent or treat them. In the peripheral nervous system, the most common problem is the failure of nerve conduction, which can be due to different causes including diabetic neuropathy and demyelinating disorders such as multiple sclerosis and amyotrophic lateral sclerosis. Neuroscience is the field of science that focuses on the study of the nervous system.
$
10
Question: What does the nervous system coordinate?
A: Digestion
B: Blood circulation
C: Actions and sensory information
D: Respiration
E: Muscle growth
Answer: C: Actions and sensory information

Question: Which system does the nervous system work in tandem with to respond to environmental changes?
A: Digestive system
B: Circulatory system
C: Respiratory system
D: Endocrine system
E: Muscular system
Answer: D: Endocrine system

Question: The central nervous system (CNS) consists of what?
A: Brain and liver
B: Brain and spinal cord
C: Heart and brain
D: Spinal cord and nerves
E: Liver and spinal cord
Answer: B: Brain and spinal cord

Question: What are the nerves that transmit signals from the brain called?
A: Afferent nerves
B: Spinal nerves
C: Motor nerves
D: Somatic nerves
E: Enteric nerves
Answer: C: Motor nerves

Question: Which of the following systems is NOT a subsystem of the peripheral nervous system (PNS)?
A: Enteric nervous system
B: Somatic nervous system
C: Sympathetic nervous system
D: Parasympathetic nervous system
E: Central nervous system
Answer: E: Central nervous system

Question: What is activated when an organism is in a relaxed state?
A: Somatic nervous system
B: Sympathetic nervous system
C: Enteric nervous system
D: Parasympathetic nervous system
E: Autonomic nervous system
Answer: D: Parasympathetic nervous system

Question: What type of cell is defined by the presence in the nervous system and can send signals rapidly and precisely to other cells?
A: Glial cells
B: Blood cells
C: Epithelial cells
D: Muscle cells
E: Neuron
Answer: E: Neuron

Question: Which of the following is a function of glial cells?
A: Sending electrochemical impulses
B: Providing structural and metabolic support
C: Releasing neurotransmitters
D: Regulating blood flow
E: Generating perception of the world
Answer: B: Providing structural and metabolic support

Question: Which animals lack a nervous system?
A: Worms
B: Elephants
C: Sponges
D: Birds
E: Mammals
Answer: C: Sponges

Question: What is the medical specialty that studies disorders of the nervous system?
A: Cardiology
B: Endocrinology
C: Gastroenterology
D: Oncology
E: Neurology
Answer: E: Neurology
@
The parasympathetic nervous system (PSNS) is one of the three divisions of the autonomic nervous system, the others being the sympathetic nervous system and the enteric nervous system.[1][2] The enteric nervous system is sometimes considered part of the autonomic nervous system, and sometimes considered an independent system.[3]

The autonomic nervous system is responsible for regulating the body's unconscious actions. The parasympathetic system is responsible for stimulation of "rest-and-digest" or "feed and breed"[4] activities that occur when the body is at rest, especially after eating, including sexual arousal, salivation, lacrimation (tears), urination, digestion, and defecation. Its action is described as being complementary to that of the sympathetic nervous system, which is responsible for stimulating activities associated with the fight-or-flight response.

Nerve fibres of the parasympathetic nervous system arise from the central nervous system. Specific nerves include several cranial nerves, specifically the oculomotor nerve, facial nerve, glossopharyngeal nerve, and vagus nerve. Three spinal nerves in the sacrum (S2–4), commonly referred to as the pelvic splanchnic nerves, also act as parasympathetic nerves.

Owing to its location, the parasympathetic system is commonly referred to as having "craniosacral outflow", which stands in contrast to the sympathetic nervous system, which is said to have "thoracolumbar outflow".[5]

Structure
The parasympathetic nerves are autonomic or visceral [6][7] branches of the peripheral nervous system (PNS). Parasympathetic nerve supply arises through three primary areas:

Certain cranial nerves in the cranium, namely the preganglionic parasympathetic nerves (CN III, CN VII, CN IX and CN X) usually arise from specific nuclei in the central nervous system (CNS) and synapse at one of four parasympathetic ganglia: ciliary, pterygopalatine, otic, or submandibular. From these four ganglia the parasympathetic nerves complete their journey to target tissues via trigeminal branches (ophthalmic nerve, maxillary nerve, mandibular nerve).
The vagus nerve does not participate in these cranial ganglia as most of its parasympathetic fibers are destined for a broad array of ganglia on or near thoracic viscera (esophagus, trachea, heart, lungs) and abdominal viscera (stomach, pancreas, liver, kidneys, small intestine, and about half of the large intestine). The vagus innervation ends at the junction between the midgut and hindgut, just before the splenic flexure of the transverse colon.
The pelvic splanchnic efferent preganglionic nerve cell bodies reside in the lateral gray horn of the spinal cord at the T12–L1 vertebral levels (the spinal cord terminates at the L1–L2 vertebrae with the conus medullaris), and their axons exit the vertebral column as S2–S4 spinal nerves through the sacral foramina.[8] Their axons continue away from the CNS to synapse at an autonomic ganglion. The parasympathetic ganglion where these preganglionic neurons synapse will be close to the organ of innervation. This differs from the sympathetic nervous system, where synapses between pre- and post-ganglionic efferent nerves in general occur at ganglia that are farther away from the target organ.
As in the sympathetic nervous system, efferent parasympathetic nerve signals are carried from the central nervous system to their targets by a system of two neurons. The first neuron in this pathway is referred to as the preganglionic or presynaptic neuron. Its cell body sits in the central nervous system and its axon usually extends to synapse with the dendrites of a postganglionic neuron somewhere else in the body. The axons of presynaptic parasympathetic neurons are usually long, extending from the CNS into a ganglion that is either very close to or embedded in their target organ. As a result, the postsynaptic parasympathetic nerve fibers are very short.[9]: 42 
$
10
Question: Which nervous system is responsible for the "rest-and-digest" activities?
A: Sympathetic nervous system
B: Central nervous system
C: Enteric nervous system
D: Parasympathetic nervous system
E: Peripheral nervous system
Answer: D: Parasympathetic nervous system

Question: What other term is associated with the activities of the parasympathetic system besides "rest-and-digest"?
A: "React and respond"
B: "Feed and breed"
C: "Fight and flight"
D: "Engage and escape"
E: "Activate and assimilate"
Answer: B: "Feed and breed"

Question: Where do the nerve fibers of the parasympathetic nervous system arise from?
A: Peripheral nervous system
B: Enteric nervous system
C: Central nervous system
D: Autonomic ganglion
E: Sympathetic nervous system
Answer: C: Central nervous system

Question: The parasympathetic system is described as having what kind of outflow?
A: Thoracolumbar outflow
B: Cranial outflow
C: Craniosacral outflow
D: Sacral outflow
E: Spinal outflow
Answer: C: Craniosacral outflow

Question: Which cranial nerve does not participate in the cranial ganglia for the parasympathetic nerve supply?
A: Facial nerve
B: Oculomotor nerve
C: Glossopharyngeal nerve
D: Vagus nerve
E: Trigeminal nerve
Answer: D: Vagus nerve

Question: The vagus nerve provides parasympathetic fibers to all of the following EXCEPT:
A: Liver
B: Lungs
C: Heart
D: Stomach
E: Splenic flexure of the transverse colon
Answer: E: Splenic flexure of the transverse colon

Question: The pelvic splanchnic efferent preganglionic nerve cell bodies are located at which vertebral levels?
A: C1–C3
B: T4–T6
C: T12–L1
D: S1–S3
E: L3–L5
Answer: C: T12–L1

Question: Which statement about the parasympathetic nervous system is true?
A: It synapses at ganglia far away from the target organ.
B: Its postsynaptic nerve fibers are usually long.
C: The parasympathetic ganglion where these preganglionic neurons synapse is far from the organ of innervation.
D: It is part of the enteric nervous system.
E: The axons of presynaptic parasympathetic neurons are usually long.
Answer: E: The axons of presynaptic parasympathetic neurons are usually long.

Question: What is the role of the autonomic nervous system?
A: Stimulating voluntary actions.
B: Regulating the body's conscious thoughts.
C: Controlling voluntary muscle movements.
D: Regulating the body's unconscious actions.
E: Directing the body's fight-or-flight response.
Answer: D: Regulating the body's unconscious actions.

Question: Which of the following cranial nerves is NOT associated with the parasympathetic nervous system?
A: Optic nerve
B: Oculomotor nerve
C: Facial nerve
D: Glossopharyngeal nerve
E: Vagus nerve
Answer: A: Optic nerve
@
The enteric nervous system (ENS) or intrinsic nervous system is one of the main divisions of the autonomic nervous system (ANS) and consists of a mesh-like system of neurons that governs the function of the gastrointestinal tract.[1] It is capable of acting independently of the sympathetic and parasympathetic nervous systems, although it may be influenced by them. The ENS is nicknamed the "second brain".[2][3] It is derived from neural crest cells.[4][5]

The enteric nervous system is capable of operating independently of the brain and spinal cord,[6] but does rely on innervation from the vagus nerve and prevertebral ganglia in healthy subjects. However, studies have shown that the system is operable with a severed vagus nerve.[7] The neurons of the enteric nervous system control the motor functions of the system, in addition to the secretion of gastrointestinal enzymes. These neurons communicate through many neurotransmitters similar to the CNS, including acetylcholine, dopamine, and serotonin. The large presence of serotonin and dopamine in the gut are key areas of research for neurogastroenterologists.[8][9][10]

Structure
The enteric nervous system in humans consists of some 500 million neurons[11] (including the various types of Dogiel cells),[1][12] 0.5% of the number of neurons in the brain, five times as many as the one hundred million neurons in the human spinal cord,[13] and about 2⁄3 as many as in the whole nervous system of a cat. The enteric nervous system is embedded in the lining of the gastrointestinal system, beginning in the esophagus and extending down to the anus.[13]

The neurons of the ENS are collected into two types of ganglia: myenteric (Auerbach's) and submucosal (Meissner's) plexuses.[14] Myenteric plexuses are located between the inner and outer layers of the muscularis externa, while submucosal plexuses are located in the submucosa.

Auerbach's plexus
Main article: Auerbach's plexus
Auerbach's plexus, also known as the myenteric plexus, is a collection of fibers and postganglionic autonomic cell bodies that lie between the circular and longitudinal layers of the muscularis externa in the gastrointestinal tract.[citation needed] It was discovered and named by German neuropathologist Leopold Auerbach. These neurons provide motor inputs to both layers of the muscularis externa and provide both parasympathetic and sympathetic input. The anatomy of the plexus is similar to the anatomy of the central nervous system. The plexus includes sensory receptors, such as chemoreceptors and mechanoreceptors, that are used to provide sensory input to the interneurons in the enteric nervous system. The plexus is the parasympathetic nucleus of origin for the vagus nerve and communicates with the medulla oblongata through both the anterior and posterior vagal nerves.

Submucosal plexus
Main article: Submucosal plexus
The submucosal plexus (also known as Meissner's plexus) is found in the submucosal layer of the gastrointestinal tract.[15] It was discovered and named by German physiologist Georg Meissner. It functions as a pathway for the innervation in the mucosa layer of the gastrointestinal wall.

Function
The ENS is capable of autonomous functions[16] like the coordination of reflexes; although it receives considerable innervation from the autonomic nervous system, it can and does operate independently of the brain and the spinal cord.[17] Its study is the focus of neurogastroenterology.
$
10
Question: What is the ENS also known as due to its capability to act independently?
A: "First brain"
B: "Central nerve"
C: "Intrinsic nerve"
D: "Second brain"
E: "Primary controller"
Answer: D: "Second brain"

Question: From what cells is the enteric nervous system derived?
A: Muscle cells
B: Neural crest cells
C: Epithelial cells
D: Blood cells
E: Bone marrow cells
Answer: B: Neural crest cells

Question: Which neurotransmitters do the neurons of the ENS communicate through, similar to the CNS?
A: Epinephrine, norepinephrine, and GABA
B: Histamine, norepinephrine, and glycine
C: Acetylcholine, dopamine, and serotonin
D: GABA, epinephrine, and histamine
E: Glycine, serotonin, and norepinephrine
Answer: C: Acetylcholine, dopamine, and serotonin

Question: Approximately how many neurons does the human enteric nervous system consist of?
A: One hundred million
B: Two billion
C: Five hundred million
D: One billion
E: Fifty million
Answer: C: Five hundred million

Question: Where is the Auerbach's plexus located?
A: In the submucosa
B: Between the inner and outer layers of the muscularis externa
C: Within the CNS
D: On the surface of the esophagus
E: Around the spinal cord
Answer: B: Between the inner and outer layers of the muscularis externa

Question: Who discovered the Auerbach's plexus?
A: Georg Meissner
B: Leopold Auerbach
C: Albert Einstein
D: Werner Heisenberg
E: Alfred Nobel
Answer: B: Leopold Auerbach

Question: Which plexus functions as a pathway for innervation in the mucosa layer of the gastrointestinal wall?
A: Auerbach's plexus
B: Dorsal root plexus
C: Brachial plexus
D: Submucosal plexus
E: Cervical plexus
Answer: D: Submucosal plexus

Question: Which of the following statements is true regarding the function of the ENS?
A: It cannot operate without the brain and spinal cord.
B: It is incapable of autonomous functions.
C: It doesn't coordinate reflexes.
D: It can operate independently of the brain and the spinal cord.
E: It solely relies on the CNS for its functions.
Answer: D: It can operate independently of the brain and the spinal cord.

Question: The study of the ENS falls under which scientific field?
A: Cardiology
B: Endocrinology
C: Neurogastroenterology
D: Nephrology
E: Rheumatology
Answer: C: Neurogastroenterology

Question: Where does the enteric nervous system begin and end in the gastrointestinal system?
A: Stomach to small intestine
B: Esophagus to stomach
C: Duodenum to ileum
D: Esophagus to anus
E: Liver to colon
Answer: D: Esophagus to anus
@
Information is an abstract concept that refers to that which has the power to inform. At the most fundamental level, information pertains to the interpretation (perhaps formally) of that which may be sensed, or their abstractions. Any natural process that is not completely random and any observable pattern in any medium can be said to convey some amount of information. Whereas digital signals and other data use discrete signs to convey information, other phenomena and artefacts such as analogue signals, poems, pictures, music or other sounds, and currents convey information in a more continuous form.[1] Information is not knowledge itself, but the meaning that may be derived from a representation through interpretation.[2]

The concept of information is relevant or connected to various concepts,[3] including constraint, communication, control, data, form, education, knowledge, meaning, understanding, mental stimuli, pattern, perception, proposition, representation, and entropy.

Information is often processed iteratively: Data available at one step are processed into information to be interpreted and processed at the next step. For example, in written text each symbol or letter conveys information relevant to the word it is part of, each word conveys information relevant to the phrase it is part of, each phrase conveys information relevant to the sentence it is part of, and so on until at the final step information is interpreted and becomes knowledge in a given domain. In a digital signal, bits may be interpreted into the symbols, letters, numbers, or structures that convey the information available at the next level up. The key characteristic of information is that it is subject to interpretation and processing.

The derivation of information from a signal or message may be thought of as the resolution of ambiguity or uncertainty that arises during the interpretation of patterns within the signal or message.[4]

Information may be structured as data. Redundant data can be compressed up to an optimal size, which is the theoretical limit of compression.

The information available through a collection of data may be derived by analysis. For example, a restaurant collects data from every customer order. That information may be analyzed to produce knowledge that is put to use when the business subsequently wants to identify the most popular or least popular dish.[5]

Information can be transmitted in time, via data storage, and space, via communication and telecommunication.[6] Information is expressed either as the content of a message or through direct or indirect observation. That which is perceived can be construed as a message in its own right, and in that sense, all information is always conveyed as the content of a message.

Information can be encoded into various forms for transmission and interpretation (for example, information may be encoded into a sequence of signs, or transmitted via a signal). It can also be encrypted for safe storage and communication.

The uncertainty of an event is measured by its probability of occurrence. Uncertainty is inversely proportional to the probability of occurrence. Information theory takes advantage of this by concluding that more uncertain events require more information to resolve their uncertainty. The bit is a typical unit of information. It is 'that which reduces uncertainty by half'.[7] Other units such as the nat may be used. For example, the information encoded in one "fair" coin flip is log2(2/1) = 1 bit, and in two fair coin flips is log2(4/1) = 2 bits. A 2011 Science article estimates that 97% of technologically stored information was already in digital bits in 2007 and that the year 2002 was the beginning of the digital age for information storage (with digital storage capacity bypassing analogue for the first time).[8]

Exact definition of information and digital application
Information can be defined exactly by set theory:

"Information is selection from the domain of information".

The "domain of information" is a set that the sender and receiver of information must know before exchanging information. Digital information, for example, consists of building blocks that are all number sequences. Each number sequence represents a selection from its domain. The sender and receiver of digital information (number sequences) must know the domain and binary format of each number sequence before exchanging information.

By defining number sequences online, this would be systematically and universally usable. Before the exchanged digital number sequence, an efficient unique link to its online definition can be set. This online defined digital information (number sequence) would be globally comparable and globally searchable.[9]
$
10
Question: What is the fundamental nature of information?
A: Random processes
B: Observable patterns
C: Discrete signs only
D: Knowledge itself
E: Both random processes and observable patterns
Answer: B: Observable patterns

Question: How is information different from knowledge?
A: Information is knowledge.
B: Information is the interpretation of data.
C: Knowledge is the interpretation of data.
D: Information is the meaning derived from a representation.
E: Information and knowledge are the same.
Answer: D: Information is the meaning derived from a representation.

Question: What role does interpretation play in the concept of information?
A: It removes redundancy.
B: It encrypts the data.
C: It is subject to analysis.
D: It is subject to processing and interpretation.
E: It is a byproduct of information.
Answer: D: It is subject to processing and interpretation.

Question: Which of the following can be thought of as the resolution of ambiguity in information?
A: Encryption
B: Redundancy
C: Interpretation of patterns
D: Telecommunication
E: Compression
Answer: C: Interpretation of patterns

Question: What can be said about redundant data?
A: It can be compressed up to a theoretical limit.
B: It is always encrypted.
C: It always contains new information.
D: It is the primary form of data storage.
E: It is not useful in the concept of information.
Answer: A: It can be compressed up to a theoretical limit.

Question: How is the uncertainty of an event related to its probability of occurrence?
A: Directly proportional
B: Not related
C: Inversely proportional
D: Equal in measure
E: Uncertainty doesn't relate to probability
Answer: C: Inversely proportional

Question: Which unit measures information by reducing uncertainty by half?
A: Nat
B: Byte
C: Hertz
D: Bit
E: Megabit
Answer: D: Bit

Question: When was the beginning of the digital age for information storage, according to a 2011 Science article?
A: 1999
B: 2000
C: 2001
D: 2002
E: 2007
Answer: D: 2002

Question: How is information defined in set theory?
A: Information is always encrypted.
B: Information is the same as knowledge.
C: Information is selection from the domain of information.
D: Information is derived from its binary format.
E: Information is always in a sequence.
Answer: C: Information is selection from the domain of information.

Question: For exchanging digital information, what must the sender and receiver know?
A: The encryption method
B: The compression rate
C: The domain and binary format
D: The size of the data
E: The telecommunication medium
Answer: C: The domain and binary format
@
A digital signal is a signal that represents data as a sequence of discrete values; at any given time it can only take on, at most, one of a finite number of values.[1][2][3] This contrasts with an analog signal, which represents continuous values; at any given time it represents a real number within a continuous range of values.

Simple digital signals represent information in discrete bands of analog levels. All levels within a band of values represent the same information state.[1] In most digital circuits, the signal can have two possible valid values; this is called a binary signal or logic signal.[4] They are represented by two voltage bands: one near a reference value (typically termed as ground or zero volts), and the other a value near the supply voltage. These correspond to the two values "zero" and "one" (or "false" and "true") of the Boolean domain, so at any given time a binary signal represents one binary digit (bit). Because of this discretization, relatively small changes to the analog signal levels do not leave the discrete envelope, and as a result are ignored by signal state sensing circuitry. As a result, digital signals have noise immunity; electronic noise, provided it is not too great, will not affect digital circuits, whereas noise always degrades the operation of analog signals to some degree.[5]

Digital signals having more than two states are occasionally used; circuitry using such signals is called multivalued logic. For example, signals that can assume three possible states are called three-valued logic.

In a digital signal, the physical quantity representing the information may be a variable electric current or voltage, the intensity, phase or polarization of an optical or other electromagnetic field, acoustic pressure, the magnetization of a magnetic storage media, etcetera. Digital signals are used in all digital electronics, notably computing equipment and data transmission.

In digital electronics, a digital signal is a pulse train (a pulse amplitude modulated signal), i.e. a sequence of fixed-width square wave electrical pulses or light pulses, each occupying one of a discrete number of levels of amplitude.[6][7] A special case is a logic signal or a binary signal, which varies between a low and a high signal level.

The pulse trains in digital circuits are typically generated by metal–oxide–semiconductor field-effect transistor (MOSFET) devices, due to their rapid on–off electronic switching speed and large-scale integration (LSI) capability.[8][9] In contrast, BJT transistors more slowly generate analog signals resembling sine waves.[8]

In digital signal processing, a digital signal is a representation of a physical signal that is sampled and quantized. A digital signal is an abstraction that is discrete in time and amplitude. The signal's value only exists at regular time intervals, since only the values of the corresponding physical signal at those sampled moments are significant for further digital processing. The digital signal is a sequence of codes drawn from a finite set of values.[10] The digital signal may be stored, processed or transmitted physically as a pulse-code modulation (PCM) signal.

In digital communications, a digital signal is a continuous-time physical signal, alternating between a discrete number of waveforms,[3] representing a bitstream. The shape of the waveform depends the transmission scheme, which may be either a line coding scheme allowing baseband transmission; or a digital modulation scheme, allowing passband transmission over long wires or over a limited radio frequency band. Such a carrier-modulated sine wave is considered a digital signal in literature on digital communications and data transmission,[11] but considered as a bit stream converted to an analog signal in electronics and computer networking.[12]

In communications, sources of interference are usually present, and noise is frequently a significant problem. The effects of interference are typically minimized by filtering off interfering signals as much as possible and by using data redundancy. The main advantages of digital signals for communications are often considered to be noise immunity, and the ability, in many cases such as with audio and video data, to use data compression to greatly decrease the bandwidth that is required on the communication media.
$
10
Question: What type of signal represents data as a sequence of discrete values?
A: Analog signal
B: Continuous signal
C: Digital signal
D: Multivalued logic
E: Sine wave
Answer: C: Digital signal

Question: How does an analog signal differ from a digital one?
A: It represents a finite number of values.
B: It represents discrete values.
C: It represents continuous values.
D: It represents a single value.
E: It represents values in binary.
Answer: C: It represents continuous values.

Question: In most digital circuits, how many valid values can a signal have?
A: One
B: Two
C: Three
D: Four
E: Infinite
Answer: B: Two

Question: What do the two voltage bands in a binary signal correspond to in the Boolean domain?
A: Positive and Negative
B: High and Low
C: Zero and One
D: True and Maybe
E: False and Indeterminate
Answer: C: Zero and One

Question: Why do digital signals have noise immunity?
A: Because they can handle more data.
B: Because they are continuous in nature.
C: Small changes to the analog signal levels do not leave the discrete envelope.
D: Because they are always transmitted in a straight line.
E: Because they are always in binary.
Answer: C: Small changes to the analog signal levels do not leave the discrete envelope.

Question: What kind of logic has signals that can assume three possible states?
A: Binary logic
B: Dual logic
C: Three-valued logic
D: Multivalued logic
E: Ternary logic
Answer: C: Three-valued logic

Question: In digital electronics, what kind of signal varies between a low and a high signal level?
A: Analog signal
B: Continuous signal
C: Logic signal
D: Ternary signal
E: Multivalued signal
Answer: C: Logic signal

Question: Which device typically generates the pulse trains in digital circuits?
A: BJT transistors
B: Diodes
C: Inductors
D: MOSFET devices
E: Resistors
Answer: D: MOSFET devices

Question: In digital signal processing, at what intervals does a digital signal's value exist?
A: At random time intervals
B: At continuous time intervals
C: Only at regular time intervals
D: In infinite intervals
E: Only at the start and end of transmission
Answer: C: Only at regular time intervals

Question: What is one of the main advantages of digital signals for communications?
A: Signal boosting
B: Noise amplification
C: Ability to increase bandwidth
D: Noise immunity
E: Limited data redundancy
Answer: D: Noise immunity
@
Magnetic storage or magnetic recording is the storage of data on a magnetized medium. Magnetic storage uses different patterns of magnetisation in a magnetizable material to store data and is a form of non-volatile memory. The information is accessed using one or more read/write heads.

Magnetic storage media, primarily hard disks, are widely used to store computer data as well as audio and video signals. In the field of computing, the term magnetic storage is preferred and in the field of audio and video production, the term magnetic recording is more commonly used. The distinction is less technical and more a matter of preference. Other examples of magnetic storage media include floppy disks, magnetic tape, and magnetic stripes on credit cards.[citation needed]

History
Magnetic storage in the form of wire recording—audio recording on a wire—was publicized by Oberlin Smith in the Sept 8, 1888 issue of Electrical World.[1] Smith had previously filed a patent in September, 1878 but found no opportunity to pursue the idea as his business was machine tools. The first publicly demonstrated (Paris Exposition of 1900) magnetic recorder was invented by Valdemar Poulsen in 1898. Poulsen's device recorded a signal on a wire wrapped around a drum. In 1928, Fritz Pfleumer developed the first magnetic tape recorder. Early magnetic storage devices were designed to record analog audio signals. Computers and now most audio and video magnetic storage devices record digital data.[citation needed]

In computers, magnetic storage was also used for primary storage in a form of magnetic drum, or core memory, core rope memory, thin film memory, twistor memory or bubble memory. Unlike modern computers, magnetic tape was also often used for secondary storage.

Design
Information is written to and read from the storage medium as it moves past devices called read-and-write heads that operate very close (often tens of nanometers) over the magnetic surface. The read-and-write head is used to detect and modify the magnetisation of the material immediately under it. There are two magnetic polarities, each of which is used to represent either 0 or 1.[citation needed]

The magnetic surface is conceptually divided into many small sub-micrometer-sized magnetic regions, referred to as magnetic domains, (although these are not magnetic domains in a rigorous physical sense), each of which has a mostly uniform magnetisation. Due to the polycrystalline nature of the magnetic material, each of these magnetic regions is composed of a few hundred magnetic grains. Magnetic grains are typically 10 nm in size and each form a single true magnetic domain. Each magnetic region in total forms a magnetic dipole which generates a magnetic field. In older hard disk drive (HDD) designs the regions were oriented horizontally and parallel to the disk surface, but beginning about 2005, the orientation was changed to perpendicular to allow for closer magnetic domain spacing.[citation needed]
$
10
Question: What does magnetic storage use to store data?
A: Electrical pulses
B: Different patterns of magnetization in a material
C: Optical laser beams
D: Tiny physical pits on a surface
E: Radio frequencies
Answer: B

Question: Which of the following is a form of non-volatile memory?
A: RAM
B: Cache memory
C: Magnetic storage
D: Register
E: DRAM
Answer: C

Question: Which medium is commonly used to store computer data?
A: CDs
B: Hard disks
C: Optical strips
D: Paper tapes
E: Photographic film
Answer: B

Question: Who publicized magnetic storage in the form of wire recording in 1888?
A: Fritz Pfleumer
B: Oberlin Smith
C: Valdemar Poulsen
D: Nikola Tesla
E: Thomas Edison
Answer: B

Question: Who invented the first publicly demonstrated magnetic recorder?
A: Fritz Pfleumer
B: Oberlin Smith
C: Valdemar Poulsen
D: Nikola Tesla
E: John Logie Baird
Answer: C

Question: What did early magnetic storage devices primarily record?
A: Digital data
B: Analog audio signals
C: Video signals
D: Radio frequencies
E: Encrypted data
Answer: B

Question: In computers, what was often used for secondary storage?
A: Magnetic tape
B: Optical disc
C: SSD
D: RAM
E: ROM
Answer: A

Question: What is used to detect and modify the magnetisation of a magnetic storage medium?
A: Laser beam
B: Electric coil
C: Read-and-write head
D: Magnetometer
E: Capacitor
Answer: C

Question: Each magnetic region in a storage medium forms what?
A: Magnetic coil
B: Magnetic pulse
C: Magnetic grain
D: Magnetic domain
E: Magnetic dipole
Answer: E

Question: In older HDD designs, how were the magnetic regions oriented?
A: Vertically and perpendicular to the disk surface
B: Diagonally to the disk surface
C: Horizontally and parallel to the disk surface
D: In a random pattern
E: In concentric circles
Answer: C
@
In common usage and statistics, data (US: /ˈdætə/; UK: /ˈdeɪtə/) is a collection of discrete or continuous values that convey information, describing the quantity, quality, fact, statistics, other basic units of meaning, or simply sequences of symbols that may be further interpreted formally. A datum is an individual value in a collection of data. Data is usually organized into structures such as tables that provide additional context and meaning, and which may themselves be used as data in larger structures. Data may be used as variables in a computational process.[1][2] Data may represent abstract ideas or concrete measurements.[3] Data is commonly used in scientific research, economics, and in virtually every other form of human organizational activity. Examples of data sets include price indices (such as consumer price index), unemployment rates, literacy rates, and census data. In this context, data represents the raw facts and figures from which useful information can be extracted.

Data is collected using techniques such as measurement, observation, query, or analysis, and is typically represented as numbers or characters which may be further processed. Field data is data that is collected in an uncontrolled in-situ environment. Experimental data is data that is generated in the course of a controlled scientific experiment. Data is analyzed using techniques such as calculation, reasoning, discussion, presentation, visualization, or other forms of post-analysis. Prior to analysis, raw data (or unprocessed data) is typically cleaned: Outliers are removed and obvious instrument or data entry errors are corrected.

Data can be seen as the smallest units of factual information that can be used as a basis for calculation, reasoning, or discussion. Data can range from abstract ideas to concrete measurements, including, but not limited to, statistics. Thematically connected data presented in some relevant context can be viewed as information. Contextually connected pieces of information can then be described as data insights or intelligence. The stock of insights and intelligence that accumulates over time resulting from the synthesis of data into information, can then be described as knowledge. Data has been described as "the new oil of the digital economy".[4][5] Data, as a general concept, refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing.

Advances in computing technologies have led to the advent of big data, which usually refers to very large quantities of data, usually at the petabyte scale. Using traditional data analysis methods and computing, working with such large (and growing) datasets is difficult, even impossible. (Theoretically speaking, infinite data would yield infinite information, which would render extracting insights or intelligence impossible.) In response, the relatively new field of data science uses machine learning (and other artificial intelligence (AI)) methods that allow for efficient applications of analytic methods to big data.

Etymology and terminology
Further information: Data (word)
The Latin word data is the plural of datum, "(thing) given", neuter past participle of dare, "to give".[6] The first English use of the word "data" is from the 1640s. The word "data" was first used to mean "transmissible and storable computer information" in 1946. The expression "data processing" was first used in 1954.[6]

When "data" is used more generally as a synonym for "information", it is treated as a mass noun in singular form. This usage is common in everyday language and in technical and scientific fields such as software development and computer science. One example of this usage is the term "big data". When used more specifically to refer to the processing and analysis of sets of data, the term retains its plural form. This usage is common in natural sciences, life sciences, social sciences, software development and computer science, and grew in popularity in the 20th and 21st centuries. Some style guides do not recognize the different meanings of the term, and simply recommend the form that best suits the target audience of the guide. For example, APA style as of the 7th edition requires "data" to be treated as a plural form.[7]
$
10
Question: What does a datum refer to?
A: A technique for collecting values
B: An individual value in a collection of data
C: The analysis of a data set
D: A type of software used for data analysis
E: The presentation of data in a visualization
Answer: B

Question: What does field data refer to?
A: Data organized in tables
B: Data generated from a controlled experiment
C: Data collected in an uncontrolled environment
D: Data from a literary source
E: Data from computer simulations
Answer: C

Question: How is raw data typically treated before analysis?
A: Augmented with extra information
B: Stored without any modifications
C: Removed of outliers and corrected for errors
D: Compressed for storage efficiency
E: Encrypted for security reasons
Answer: C

Question: Which term describes the stock of insights and intelligence accumulating over time from data synthesis?
A: Data oil
B: Data insights
C: Data intelligence
D: Knowledge
E: Data economy
Answer: D

Question: How has data been described in the context of the digital economy?
A: The new currency
B: The backbone of technology
C: The new oil
D: The future of business
E: The source of power
Answer: C

Question: What challenge does "big data" present?
A: Difficulty in organizing the data
B: Difficulty in transmission of data over the internet
C: Difficulty in analyzing with traditional methods due to its size
D: Difficulty in ensuring the security of the data
E: Difficulty in converting it into readable formats
Answer: C

Question: What field has emerged to address challenges with big data using AI methods?
A: Data visualization
B: Data compression
C: Data entry
D: Data mining
E: Data science
Answer: E

Question: What is the origin of the word "data"?
A: Greek
B: French
C: Latin
D: German
E: Spanish
Answer: C

Question: When was the term "data processing" first used?
A: 1640s
B: 1900s
C: 1946
D: 1954
E: 21st century
Answer: D

Question: In which edition does the APA style require "data" to be treated as a plural form?
A: 3rd edition
B: 5th edition
C: 6th edition
D: 7th edition
E: 8th edition
Answer: D
@
Statistics (from German: Statistik, orig. "description of a state, a country")[1][2] is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data.[3][4][5] In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as "all people living in a country" or "every atom composing a crystal". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.[6]

When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.

Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).[7] Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.

A standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a "false positive") and Type II errors (null hypothesis fails to be rejected and an actual relationship between populations is missed giving a "false negative").[8] Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.[7]

Statistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.

Statistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data,[9] or as a branch of mathematics.[10] Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is concerned with the use of data in the context of uncertainty and decision-making in the face of uncertainty.[11][12]

In applying statistics to a problem, it is common practice to start with a population or process to be studied. Populations can be diverse topics, such as "all people living in a country" or "every atom composing a crystal". Ideally, statisticians compile data about the entire population (an operation called a census). This may be organized by governmental statistical institutes. Descriptive statistics can be used to summarize the population data. Numerical descriptors include mean and standard deviation for continuous data (like income), while frequency and percentage are more useful in terms of describing categorical data (like education).

When a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, drawing the sample contains an element of randomness; hence, the numerical descriptors from the sample are also prone to uncertainty. To draw meaningful conclusions about the entire population, inferential statistics are needed. It uses patterns in the sample data to draw inferences about the population represented while accounting for randomness. These inferences may take the form of answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation), and modeling relationships within the data (for example, using regression analysis). Inference can extend to the forecasting, prediction, and estimation of unobserved values either in or associated with the population being studied. It can include extrapolation and interpolation of time series or spatial data, as well as data mining.
$
10
Question: What does statistics primarily concern itself with?
A: The philosophy of numbers
B: The collection, organization, and presentation of data
C: Mathematical equations without real-world applications
D: Advanced computational methods
E: The history of numbers and their significance
Answer: B

Question: What kind of group can a statistical population refer to?
A: Only groups of people
B: Only groups of objects
C: Either groups of people or objects
D: Only groups of plants
E: Only groups of animals
Answer: C

Question: What is the difference between an experimental study and an observational study?
A: Experimental studies use larger data sets
B: Observational studies involve manipulating the system
C: Experimental studies involve manipulating the system
D: Observational studies always give accurate results
E: Experimental studies never involve measurements
Answer: C

Question: Descriptive statistics often focus on two main properties of a distribution. What are they?
A: Size and shape
B: Height and depth
C: Complexity and simplicity
D: Central tendency and dispersion
E: Range and median
Answer: D

Question: Under what framework are inferences on mathematical statistics made?
A: The framework of algebra
B: The framework of geometry
C: The framework of trigonometry
D: The framework of calculus
E: The framework of probability theory
Answer: E

Question: In hypothesis testing, what are the two primary errors recognized?
A: Type A and Type B
B: Type I and Type II
C: Error X and Error Y
D: Error 1 and Error 2
E: Main Error and Secondary Error
Answer: B

Question: Statistics can be regarded as what?
A: A distinct mathematical science
B: Only a branch of physics
C: A form of literature
D: A modern philosophy
E: Only a branch of chemistry
Answer: A

Question: What is the term for the study of the entire population?
A: Sampling
B: Summarizing
C: Experimenting
D: Census
E: Questionnaire
Answer: D

Question: Which term refers to a subset of the population that is studied when a census isn't feasible?
A: Subset
B: Segment
C: Portion
D: Slice
E: Sample
Answer: E

Question: What is used to draw inferences about an entire population from a sample due to the element of randomness in the sample?
A: Descriptive statistics
B: Inferential statistics
C: Direct observations
D: Speculative statistics
E: Absolute certainty
Answer: B
@

A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process.[1] When referring specifically to probabilities, the corresponding term is probabilistic model.

A statistical model is usually specified as a mathematical relationship between one or more random variables and other non-random variables. As such, a statistical model is "a formal representation of a theory" (Herman Adèr quoting Kenneth Bollen).[2]

All statistical hypothesis tests and all statistical estimators are derived via statistical models. More generally, statistical models are part of the foundation of statistical inference.

Introduction
Informally, a statistical model can be thought of as a statistical assumption (or set of statistical assumptions) with a certain property: that the assumption allows us to calculate the probability of any event. As an example, consider a pair of ordinary six-sided dice. We will study two different statistical assumptions about the dice.

The first statistical assumption is this: for each of the dice, the probability of each face (1, 2, 3, 4, 5, and 6) coming up is 
1
/
6
. From that assumption, we can calculate the probability of both dice coming up 5:  
1
/
6
 × 
1
/
6
 = 
1
/
36
.  More generally, we can calculate the probability of any event: e.g. (1 and 2) or (3 and 3) or (5 and 6).

The alternative statistical assumption is this: for each of the dice, the probability of the face 5 coming up is 
1
/
8
 (because the dice are weighted). From that assumption, we can calculate the probability of both dice coming up 5:  
1
/
8
 × 
1
/
8
 = 
1
/
64
.  We cannot, however, calculate the probability of any other nontrivial event, as the probabilities of the other faces are unknown.

The first statistical assumption constitutes a statistical model: because with the assumption alone, we can calculate the probability of any event. The alternative statistical assumption does not constitute a statistical model: because with the assumption alone, we cannot calculate the probability of every event.

In the example above, with the first assumption, calculating the probability of an event is easy. With some other examples, though, the calculation can be difficult, or even impractical (e.g. it might require millions of years of computation). For an assumption to constitute a statistical model, such difficulty is acceptable: doing the calculation does not need to be practicable, just theoretically possible.
$
10
Question: What does a statistical model represent?
A: A theory of relativity
B: A method of data storage
C: The process of hypothesis testing
D: The data-generating process in an idealized form
E: A practical method of calculation
Answer: D

Question: When a statistical model specifically pertains to probabilities, what is the term used?
A: Assumptive model
B: Practical model
C: Probability estimator
D: Random variable model
E: Probabilistic model
Answer: E

Question: A statistical model is often described as a formal representation of what?
A: A data set
B: A theory
C: A hypothesis
D: An experiment
E: An estimator
Answer: B

Question: From which foundation are all statistical hypothesis tests and estimators derived?
A: Probability functions
B: Estimation theories
C: Statistical models
D: Random variables
E: Probabilistic events
Answer: C

Question: Informally, a statistical model is a statistical assumption that allows for the calculation of what?
A: Only specific events
B: Any event's probability
C: Only nontrivial events
D: The mean of a sample
E: The variance of a sample
Answer: B

Question: In the example of the dice, what is the probability of both dice coming up as 5, given the first statistical assumption?
A: 1/64
B: 1/36
C: 1/8
D: 1/12
E: 1/72
Answer: B

Question: Which of the two dice assumptions mentioned in the text constitutes a statistical model?
A: The one where the probability of face 5 is 1/8
B: The one where the probability of each face is 1/6
C: Both assumptions
D: Neither of the assumptions
E: The assumption that dice are always fair
Answer: B

Question: For an assumption to be a statistical model, what is required about the calculation of an event's probability?
A: It should be easily practicable
B: It should be quick and efficient
C: It should only be valid for common events
D: It doesn't need to be practicable, just theoretically possible
E: It must always produce a probability of less than 0.5
Answer: D

Question: In the alternative dice assumption, why can't it be considered a statistical model?
A: Because it doesn't involve random variables
B: Because it only allows for the calculation of specific events
C: Because it is based on a biased dice
D: Because it doesn't allow for calculating the probability of every event
E: Because it is based on a practical example
Answer: D

Question: What is the primary characteristic of a statistical model in relation to sample data generation?
A: It provides a purely random representation
B: It always involves two or more random variables
C: It embodies a set of statistical assumptions
D: It always results in an even distribution
E: It guarantees accurate predictions
Answer: C
@
In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment.[1][2] It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space).[3]

For instance, if X is used to denote the outcome of a coin toss ("the experiment"), then the probability distribution of X would take the value 0.5 (1 in 2 or 1/2) for X = heads, and 0.5 for X = tails (assuming that the coin is fair). More commonly, probability distributions are used to compare the relative occurrence of many different random values.

Probability distributions can be defined in different ways and for discrete or for continuous variables. Distributions with special properties or for especially important applications are given specific names..

Introduction
A probability distribution is a mathematical description of the probabilities of events, subsets of the sample space. The sample space, often denoted by 
Ω\Omega , is the set of all possible outcomes of a random phenomenon being observed; it may be any set: a set of real numbers, a set of vectors, a set of arbitrary non-numerical values, etc. For example, the sample space of a coin flip would be Ω = {heads, tails}.

To define probability distributions for the specific case of random variables (so the sample space can be seen as a numeric set), it is common to distinguish between discrete and absolutely continuous random variables. In the discrete case, it is sufficient to specify a probability mass function 
�
p assigning a probability to each possible outcome: for example, when throwing a fair dice, each of the six values 1 to 6 has the probability 1/6. The probability of an event is then defined to be the sum of the probabilities of the outcomes that satisfy the event; for example, the probability of the event "the die rolls an even value" is

�
(
2
)
+
�
(
4
)
+
�
(
6
)
=
1
/
6
+
1
/
6
+
1
/
6
=
1
/
2.
{\displaystyle p(2)+p(4)+p(6)=1/6+1/6+1/6=1/2.}
In contrast, when a random variable takes values from a continuum then typically, any individual outcome has probability zero and only events that include infinitely many outcomes, such as intervals, can have positive probability. For example, consider measuring the weight of a piece of ham in the supermarket, and assume the scale has many digits of precision. The probability that it weighs exactly 500 g is zero, as it will most likely have some non-zero decimal digits. Nevertheless, one might demand, in quality control, that a package of "500 g" of ham must weigh between 490 g and 510 g with at least 98% probability, and this demand is less sensitive to the accuracy of measurement instruments.


The left graph shows a probability density function. The right graph shows the cumulative distribution function, for which the value at a equals the area under the probability density curve to the left of a.
Absolutely continuous probability distributions can be described in several ways. The probability density function describes the infinitesimal probability of any given value, and the probability that the outcome lies in a given interval can be computed by integrating the probability density function over that interval.[4] An alternative description of the distribution is by means of the cumulative distribution function, which describes the probability that the random variable is no larger than a given value (i.e., 
�
(
�
<
�
)
{\displaystyle P(X<x)} for some 
�
x). The cumulative distribution function is the area under the probability density function from 
−
∞-\infty  to 
�
x, as described by the picture to the right.[5]
$
10
Question: What is the function of a probability distribution in statistics?
A: To calculate the mean of an experiment
B: To determine the accuracy of an outcome
C: To give the probabilities of occurrence of different possible outcomes of an experiment
D: To display the sample space of an event
E: To provide the variance of a sample
Answer: C

Question: If 
�
X is used to represent a coin toss, what is the probability distribution of 
�
X resulting in tails for a fair coin?
A: 0.25
B: 0.75
C: 1
D: 0.5
E: 0
Answer: D

Question: Probability distributions can be defined for which types of variables?
A: Only discrete variables
B: Only continuous variables
C: Discrete or continuous variables
D: Neither discrete nor continuous variables
E: Only for non-numerical values
Answer: C

Question: What symbol typically denotes the sample space?
A: 
�
X
B: 
�
p
C: 
Ω
Ω
D: 
�
δ
E: 
�
α
Answer: C

Question: For a fair dice roll, what is the probability associated with each individual outcome (like rolling a 1, 2, 3, etc.)?
A: 1/3
B: 1/4
C: 1/5
D: 1
E: 1/6
Answer: E

Question: In the case of continuous variables, what is the typical probability of an individual outcome?
A: 1
B: 0.5
C: 0
D: It varies based on the variable
E: Equal to the sample size
Answer: C

Question: When measuring the weight of an item like ham in the supermarket with many digits of precision, what is the probability it weighs exactly 500g?
A: 0
B: 0.5
C: 1
D: It depends on the scale used
E: 0.1
Answer: A

Question: What function describes the infinitesimal probability of any given value in an absolutely continuous probability distribution?
A: Probability density function
B: Cumulative distribution function
C: Probability mass function
D: Probability outcome function
E: Continuous outcome function
Answer: A

Question: What does the cumulative distribution function describe?
A: The mean value of a sample
B: The variance of an outcome
C: The probability of a given range of outcomes
D: The probability that a random variable is no larger than a certain value
E: The total sample size of a given event
Answer: D

Question: For an absolutely continuous distribution, the area under the probability density function up to a certain value 
�
x can be represented by what?
A: The average distribution function
B: The variance function
C: The sample space
D: The cumulative distribution function
E: The probability mass function
Answer: D
@
Economics (/ˌɛkəˈnɒmɪks, ˌiːkə-/)[1] is a social science that studies the production, distribution, and consumption of goods and services.[2][3]

Economics focuses on the behaviour and interactions of economic agents and how economies work. Microeconomics analyzes what's viewed as basic elements in the economy, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyzes the economy as a system where production, consumption, saving, and investment interact, and factors affecting it: employment of the resources of labour, capital, and land, currency inflation, economic growth, and public policies that have impact on these elements.

Other broad distinctions within economics include those between positive economics, describing "what is", and normative economics, advocating "what ought to be";[4] between economic theory and applied economics; between rational and behavioural economics; and between mainstream economics and heterodox economics.[5]

Economic analysis can be applied throughout society, including business,[6] finance, cybersecurity,[7] health care,[8] engineering[9] and government.[10] It is also applied to such diverse subjects as crime,[11] education,[12] the family,[13] feminism,[14] law,[15] philosophy,[16] politics, religion,[17] social institutions, war,[18] science,[19] and the environment.[20]

Definitions of economics over time
The earlier term for the discipline was 'political economy', but since the late 19th century, it has commonly been called 'economics'.[21] The term is ultimately derived from Ancient Greek οἰκονομία (oikonomia) which is a term for the "way (nomos) to run a household (oikos)", or in other words the know-how of an οἰκονομικός (oikonomikos), or "household or homestead manager". Derived terms such as "economy" can therefore often mean "frugal" or "thrifty".[22][23][24][25] By extension then, "political economy" was the way to manage a polis or state.

There are a variety of modern definitions of economics; some reflect evolving views of the subject or different views among economists.[26][27] Scottish philosopher Adam Smith (1776) defined what was then called political economy as "an inquiry into the nature and causes of the wealth of nations", in particular as:

a branch of the science of a statesman or legislator [with the twofold objectives of providing] a plentiful revenue or subsistence for the people ... [and] to supply the state or commonwealth with a revenue for the publick services.[28]

Jean-Baptiste Say (1803), distinguishing the subject matter from its public-policy uses, defined it as the science of production, distribution, and consumption of wealth.[29] On the satirical side, Thomas Carlyle (1849) coined "the dismal science" as an epithet for classical economics, in this context, commonly linked to the pessimistic analysis of Malthus (1798).[30] John Stuart Mill (1844) delimited the subject matter further:

The science which traces the laws of such of the phenomena of society as arise from the combined operations of mankind for the production of wealth, in so far as those phenomena are not modified by the pursuit of any other object.[31]

Alfred Marshall provided a still widely cited definition in his textbook Principles of Economics (1890) that extended analysis beyond wealth and from the societal to the microeconomic level:

Economics is a study of man in the ordinary business of life. It enquires how he gets his income and how he uses it. Thus, it is on the one side, the study of wealth and on the other and more important side, a part of the study of man.[32]

Lionel Robbins (1932) developed implications of what has been termed "[p]erhaps the most commonly accepted current definition of the subject":[27]

Economics is the science which studies human behaviour as a relationship between ends and scarce means which have alternative uses.[33]

Robbins described the definition as not classificatory in "pick[ing] out certain kinds of behaviour" but rather analytical in "focus[ing] attention on a particular aspect of behaviour, the form imposed by the influence of scarcity."[34] He affirmed that previous economists have usually centred their studies on the analysis of wealth: how wealth is created (production), distributed, and consumed; and how wealth can grow.[35] But he said that economics can be used to study other things, such as war, that are outside its usual focus. This is because war has as the goal winning it (as a sought after end), generates both cost and benefits; and, resources (human life and other costs) are used to attain the goal. If the war is not winnable or if the expected costs outweigh the benefits, the deciding actors (assuming they are rational) may never go to war (a decision) but rather explore other alternatives. Economics cannot be defined as the science that studies wealth, war, crime, education, and any other field economic analysis can be applied to; but, as the science that studies a particular common aspect of each of those subjects (they all use scarce resources to attain a sought after end).

Some subsequent comments criticized the definition as overly broad in failing to limit its subject matter to analysis of markets. From the 1960s, however, such comments abated as the economic theory of maximizing behaviour and rational-choice modelling expanded the domain of the subject to areas previously treated in other fields.[36] There are other criticisms as well, such as in scarcity not accounting for the macroeconomics of high unemployment.[37]

Gary Becker, a contributor to the expansion of economics into new areas, described the approach he favoured as "combin[ing the] assumptions of maximizing behaviour, stable preferences, and market equilibrium, used relentlessly and unflinchingly."[38] One commentary characterizes the remark as making economics an approach rather than a subject matter but with great specificity as to the "choice process and the type of social interaction that [such] analysis involves." The same source reviews a range of definitions included in principles of economics textbooks and concludes that the lack of agreement need not affect the subject-matter that the texts treat. Among economists more generally, it argues that a particular definition presented may reflect the direction toward which the author believes economics is evolving, or should evolve.[27]

Many economists including nobel prize winners James M. Buchanan and Ronald Coase reject the method-based definition of Robbins and continue to prefer definitions like those of Say, in terms of its subject matter.[36] Ha-Joon Chang has for example argued that the definition of Robbins would make economics very peculiar because all other sciences define themselves in terms of the area of inquiry or object of inquiry rather than the methodology. In the biology department, they do not say that all biology should be studied with DNA analysis. People study living organisms in many different ways, so some people will do DNA analysis, others might do anatomy, and still others might build game theoretic models of animal behavior. But they are all called biology because they all study living organisms. According to Ha Joon Chang, this view that the economy can and should be studied in only one way (for example by studying only rational choices), and going even one step further and basically redefining economics as a theory of everything, is very peculiar.[39]
$
10
Question: Which science primarily focuses on the production, distribution, and consumption of goods and services?
A: Chemistry
B: Biology
C: Physics
D: Economics
E: Sociology
Answer: D

Question: What does microeconomics primarily analyze?
A: Global economic patterns
B: Interactions of galaxies
C: Interactions of individual agents and markets
D: National policies and their implications
E: Migratory patterns of animals
Answer: C

Question: Which of the following is NOT typically analyzed under macroeconomics?
A: Currency inflation
B: Economic growth
C: Individual buyer preferences
D: Employment of resources
E: Public policies affecting the economy
Answer: C

Question: Thomas Carlyle coined the term "the dismal science" for:
A: Physics
B: Chemistry
C: Biology
D: Classical economics
E: Sociology
Answer: D

Question: Which term was used for the discipline before it was commonly referred to as 'economics'?
A: Market analysis
B: Production study
C: Financial science
D: Political economy
E: Social wealth
Answer: D

Question: The term "economy" can often be synonymous with which of the following words?
A: Expensive
B: Wealthy
C: Frugal
D: Abundant
E: Extravagant
Answer: C

Question: How did Alfred Marshall describe economics in his textbook "Principles of Economics"?
A: A study of global patterns
B: A study of man in the ordinary business of life
C: A study of market fluctuations
D: A science of scarcity and demand
E: A discipline focusing on rational choices
Answer: B

Question: What central concept does Lionel Robbins' definition of economics focus on?
A: Abundance
B: Scarcity
C: Wealth
D: Production
E: Consumption
Answer: B

Question: Who described the favored approach to economics as "combin[ing the] assumptions of maximizing behaviour, stable preferences, and market equilibrium"?
A: Adam Smith
B: Jean-Baptiste Say
C: John Stuart Mill
D: Gary Becker
E: Ronald Coase
Answer: D

Question: According to Ha-Joon Chang, why is Robbins' method-based definition of economics seen as peculiar by some economists?
A: It focuses on the historic roots of economics
B: It emphasizes the role of governments in economics
C: It defines economics based on methodology rather than the subject of inquiry
D: It is too broad and includes all sciences
E: It dismisses the role of scarcity in economic decisions
Answer: C
@
Social science is one of the branches of science, devoted to the study of societies and the relationships among individuals within those societies. The term was formerly used to refer to the field of sociology, the original "science of society", established in the 19th century. In addition to sociology, it now encompasses a wide array of academic disciplines, including anthropology, archaeology, economics, human geography, linguistics, management science, communication science and political science.[1]

Positivist social scientists use methods resembling those of the natural sciences as tools for understanding society, and so define science in its stricter modern sense. Interpretivist social scientists, by contrast, may use social critique or symbolic interpretation rather than constructing empirically falsifiable theories, and thus treat science in its broader sense. In modern academic practice, researchers are often eclectic, using multiple methodologies (for instance, by combining both quantitative and qualitative research). The term social research has also acquired a degree of autonomy as practitioners from various disciplines share the same goals and methods.

History
The history of the social sciences has origin in the common stock of Western philosophy and shares various precursors, but began most intentionally in the early 19th century with the positivist philosophy of science. Since the mid-20th century, the term "social science" has come to refer more generally, not just to sociology, but to all those disciplines which analyze society and culture; from anthropology to psychology to media studies.

The idea that society may be studied in a standardized and objective manner, with scholarly rules and methodology, is comparatively recent. While there is evidence of early sociology in medieval Islam, and while philosophers such as Confucius had long since theorised on topics such as social roles, the scientific analysis of human society is peculiar to the intellectual break away from the Age of Enlightenment and toward the discourses of Modernity. Social sciences came forth from the moral philosophy of the time and was influenced by the Age of Revolutions, such as the Industrial revolution and the French revolution.[1] The beginnings of the social sciences in the 18th century are reflected in the grand encyclopedia of Diderot, with articles from Rousseau and other pioneers.

Around the start of the 20th century, Enlightenment philosophy was challenged in various quarters. After the use of classical theories since the end of the scientific revolution, various fields substituted mathematics studies for experimental studies and examining equations to build a theoretical structure. The development of social science subfields became very quantitative in methodology. Conversely, the interdisciplinary and cross-disciplinary nature of scientific inquiry into human behavior and social and environmental factors affecting it made many of the natural sciences interested in some aspects of social science methodology.[2] Examples of boundary blurring include emerging disciplines like social studies of medicine, biocultural anthropology, neuropsychology, and the history and sociology of science. Increasingly, quantitative and qualitative methods are being integrated in the study of human action and its implications and consequences. In the first half of the 20th century, statistics became a free-standing discipline of applied mathematics. Statistical methods were used confidently.

In the contemporary period, there continues to be little movement toward consensus on what methodology might have the power and refinement to connect a proposed "grand theory" with the various midrange theories that, with considerable success, continue to provide usable frameworks for massive, growing data banks. See consilience.
$
10
Question 1: Which branch of science is devoted to the study of societies and the relationships among individuals within those societies?
A: Natural science
B: Physics
C: Life science
D: Social science
E: Chemical science
Answer: D

Question 2: Which "science of society" was established in the 19th century?
A: Anthropology
B: Economics
C: Psychology
D: Sociology
E: Archaeology
Answer: D

Question 3: Which type of social scientists use methods resembling those of natural sciences to understand society?
A: Interpretivist
B: Positivist
C: Symbolic
D: Critical
E: Constructivist
Answer: B

Question 4: In the early 19th century, the social sciences began with the philosophy of what?
A: Interpretivist philosophy
B: Naturalist philosophy
C: Positivist philosophy of science
D: Enlightenment philosophy
E: Constructivist philosophy
Answer: C

Question 5: The scientific analysis of human society became significant after which period?
A: Medieval era
B: Renaissance
C: Age of Enlightenment
D: Classical era
E: Ancient era
Answer: C

Question 6: Which revolution had an influence on the emergence of social sciences?
A: American revolution
B: Russian revolution
C: Industrial revolution
D: Spanish revolution
E: Glorious revolution
Answer: C

Question 7: Who wrote articles in the grand encyclopedia of Diderot that reflect the beginnings of the social sciences in the 18th century?
A: Plato
B: Socrates
C: Aristotle
D: Rousseau
E: Kant
Answer: D

Question 8: By the 20th century, various fields began using what type of studies instead of experimental studies?
A: Observational studies
B: Mathematics studies
C: Qualitative studies
D: Philosophical studies
E: Ethnographic studies
Answer: B

Question 9: In the first half of the 20th century, which discipline became a free-standing discipline of applied mathematics?
A: Algebra
B: Geometry
C: Trigonometry
D: Calculus
E: Statistics
Answer: E

Question 10: What is the term for the idea of linking a proposed "grand theory" with various midrange theories to provide frameworks for data?
A: Quantitative research
B: Integration
C: Methodology
D: Boundary blurring
E: Consilience
Answer: E
@
The Age of Enlightenment or the Enlightenment,[note 2] also known as the Age of Reason, was an intellectual and philosophical movement that occurred in Europe, especially Western Europe, in the 17th and 18th centuries, with global influences and effects.[2][3] The Enlightenment included a range of ideas centered on the value of human happiness, the pursuit of knowledge obtained by means of reason and the evidence of the senses, and ideals such as natural law, liberty, progress, toleration, fraternity, constitutional government, and separation of church and state.[4][5]

The Enlightenment was preceded by the Scientific Revolution and the work of Francis Bacon and John Locke, among others. Some date the beginning of the Enlightenment to the publication of René Descartes' Discourse on the Method in 1637, featuring his famous dictum, Cogito, ergo sum ("I think, therefore I am"). Others cite the publication of Isaac Newton's Principia Mathematica (1687) as the culmination of the Scientific Revolution and the beginning of the Enlightenment. European historians traditionally date its beginning with the death of Louis XIV of France in 1715 and its end with the 1789 outbreak of the French Revolution. Many historians now date the end of the Enlightenment as the start of the 19th century, with the latest proposed year being the death of Immanuel Kant in 1804.[6]

Philosophers and scientists of the period widely circulated their ideas through meetings at scientific academies, Masonic lodges, literary salons, coffeehouses and in printed books, journals,[7] and pamphlets. The ideas of the Enlightenment undermined the authority of the monarchy and the Catholic Church and paved the way for the political revolutions of the 18th and 19th centuries. A variety of 19th-century movements, including liberalism, socialism[8] and neoclassicism, trace their intellectual heritage to the Enlightenment.[9]

The central doctrines of the Enlightenment were individual liberty and religious tolerance, in opposition to an absolute monarchy and the fixed dogmas of the Church. The concepts of utility and sociability were also crucial in the dissemination of information that would better society as a whole. The Enlightenment was marked by an increasing awareness of the relationship between the mind and the everyday media of the world,[10] and by an emphasis on the scientific method and reductionism, along with increased questioning of religious orthodoxy—an attitude captured by Kant's essay Answering the Question: What is Enlightenment, where the phrase Sapere aude (Dare to know) can be found.[11]
$
10
Question 1: During which centuries did the Age of Enlightenment primarily occur?
A: 14th and 15th centuries
B: 15th and 16th centuries
C: 16th and 17th centuries
D: 17th and 18th centuries
E: 18th and 19th centuries
Answer: D

Question 2: Which movement preceded the Enlightenment and included the work of Francis Bacon and John Locke?
A: The Romantic Movement
B: The Industrial Revolution
C: The Reformation
D: The Scientific Revolution
E: The Age of Discovery
Answer: D

Question 3: Whose publication, "Discourse on the Method", is considered by some as marking the beginning of the Enlightenment?
A: Isaac Newton
B: Immanuel Kant
C: John Locke
D: René Descartes
E: Francis Bacon
Answer: D

Question 4: The end of the Enlightenment is often associated with the outbreak of which event?
A: American Revolution
B: Napoleonic Wars
C: World War I
D: French Revolution
E: The Renaissance
Answer: D

Question 5: Where did philosophers and scientists of the period commonly circulate their ideas?
A: Schools and universities
B: Churches
C: Town halls
D: Literary salons and coffeehouses
E: Countryside gatherings
Answer: D

Question 6: The ideas of the Enlightenment primarily undermined the authority of which two entities?
A: The bourgeoisie and the proletariat
B: The army and the navy
C: The monarchy and the Catholic Church
D: Merchants and craftsmen
E: The senate and the parliament
Answer: C

Question 7: Which two doctrines were central to the Enlightenment?
A: Wealth and prosperity
B: War and peace
C: Individual liberty and religious tolerance
D: Art and music
E: Exploration and discovery
Answer: C

Question 8: Which method was emphasized during the Enlightenment, showcasing the focus on systematic observation and experimentation?
A: The artistic method
B: The qualitative method
C: The intuitive method
D: The scientific method
E: The philosophical method
Answer: D

Question 9: Which essay by Kant captures the attitude of the Enlightenment and includes the phrase "Dare to know"?
A: The Metaphysics of Morals
B: Critique of Pure Reason
C: Prolegomena to Any Future Metaphysics
D: Grounding for the Metaphysics of Morals
E: Answering the Question: What is Enlightenment
Answer: E

Question 10: What does the Latin phrase "Sapere aude" translate to in English?
A: Seek the truth
B: Knowledge is power
C: Think and prosper
D: Dare to know
E: Wisdom is freedom
Answer: D
@
Science played an important role in Enlightenment discourse and thought. Many Enlightenment writers and thinkers had backgrounds in the sciences and associated scientific advancement with the overthrow of religion and traditional authority in favour of the development of free speech and thought.[30] There were immediate practical results. The experiments of Antoine Lavoisier were used to create the first modern chemical plants in Paris, and the experiments of the Montgolfier brothers enabled them to launch the first manned flight in a hot air balloon in 1783.[31]

Broadly speaking, Enlightenment science greatly valued empiricism and rational thought and was embedded with the Enlightenment ideal of advancement and progress. The study of science, under the heading of natural philosophy, was divided into physics and a conglomerate grouping of chemistry and natural history, which included anatomy, biology, geology, mineralogy, and zoology.[32] As with most Enlightenment views, the benefits of science were not seen universally: Rousseau criticized the sciences for distancing man from nature and not operating to make people happier.[33]

Science during the Enlightenment was dominated by scientific societies and academies, which had largely replaced universities as centres of scientific research and development. Societies and academies were also the backbone of the maturation of the scientific profession. Scientific academies and societies grew out of the Scientific Revolution as the creators of scientific knowledge, in contrast to the scholasticism of the university.[34] Some societies created or retained links to universities, but contemporary sources distinguished universities from scientific societies by claiming that the university's utility was in the transmission of knowledge while societies functioned to create knowledge.[35] As the role of universities in institutionalized science began to diminish, learned societies became the cornerstone of organized science. Official scientific societies were chartered by the state to provide technical expertise.[36]

Most societies were granted permission to oversee their own publications, control the election of new members and the administration of the society.[37] In the 18th century, a tremendous number of official academies and societies were founded in Europe, and by 1789 there were over 70 official scientific societies. In reference to this growth, Bernard de Fontenelle coined the term "the Age of Academies" to describe the 18th century.[38]

Another important development was the popularization of science among an increasingly literate population. Philosophes introduced the public to many scientific theories, most notably through the Encyclopédie and the popularization of Newtonianism by Voltaire and Émilie du Châtelet. Some historians have marked the 18th century as a drab period in the history of science.[39] The century saw significant advancements in the practice of medicine, mathematics, and physics; the development of biological taxonomy; a new understanding of magnetism and electricity; and the maturation of chemistry as a discipline, which established the foundations of modern chemistry.

The influence of science began appearing more commonly in poetry and literature. Some poetry became infused with scientific metaphor and imagery, while other poems were written directly about scientific topics. Richard Blackmore committed the Newtonian system to verse in Creation, a Philosophical Poem in Seven Books (1712). After Newton's death in 1727, poems were composed in his honour for decades.[40] James Thomson penned his "Poem to the Memory of Newton", which mourned the loss of Newton and praised his science and legacy.[41]
$
10
Question 1: Which Enlightenment figures were known for their backgrounds in the sciences?
A: Philosophers and writers
B: Religious figures and priests
C: Military leaders and generals
D: Artists and musicians
E: Monarchs and rulers
Answer: A

Question 2: What practical achievement was a result of the experiments by the Montgolfier brothers in 1783?
A: The creation of the first steam engine
B: The invention of the telescope
C: The launch of the first manned flight in a hot air balloon
D: The discovery of the law of gravity
E: The development of the first microscope
Answer: C

Question 3: During the Enlightenment, under which category did physics, chemistry, and natural history fall?
A: Classical arts
B: Metaphysical philosophy
C: Natural philosophy
D: Social sciences
E: Literary studies
Answer: C

Question 4: Who criticized the sciences for distancing man from nature?
A: Voltaire
B: Bernard de Fontenelle
C: Newton
D: Rousseau
E: Émilie du Châtelet
Answer: D

Question 5: What significant role did scientific societies and academies take on during the Enlightenment?
A: Replacing universities as centres of religious studies
B: Replacing universities as centres of scientific research and development
C: Serving as entertainment hubs for the elite
D: Managing trade and commerce
E: Organizing military strategies
Answer: B

Question 6: Which term did Bernard de Fontenelle use to describe the 18th century, referencing the growth of official academies and societies?
A: The Age of Discovery
B: The Age of Science
C: The Golden Era
D: The Age of Revolutions
E: The Age of Academies
Answer: E

Question 7: Who were instrumental in introducing the public to Newtonianism?
A: Rousseau and Montesquieu
B: Voltaire and Émilie du Châtelet
C: Antoine Lavoisier and Montgolfier brothers
D: Descartes and Bacon
E: Locke and Hume
Answer: B

Question 8: In which field did the 18th century see significant advancements that established the foundations of its modern counterpart?
A: Astronomy
B: Chemistry
C: Physics
D: Literature
E: Philosophy
Answer: B

Question 9: Who committed the Newtonian system to verse in "Creation, a Philosophical Poem in Seven Books"?
A: James Thomson
B: Voltaire
C: Richard Blackmore
D: Bernard de Fontenelle
E: Rousseau
Answer: C

Question 10: Following Newton's death, what form of artistic expression was used to honor him for decades?
A: Painting
B: Statues
C: Plays
D: Poems
E: Novels
Answer: D
@
Locke, one of the most influential Enlightenment thinkers,[53] based his governance philosophy in social contract theory, a subject that permeated Enlightenment political thought. English philosopher Thomas Hobbes ushered in this new debate with his work Leviathan in 1651. Hobbes also developed some of the fundamentals of European liberal thought: the right of the individual, the natural equality of all men, the artificial character of the political order (which led to the later distinction between civil society and the state), the view that all legitimate political power must be "representative" and based on the consent of the people, and a liberal interpretation of law which leaves people free to do whatever the law does not explicitly forbid.[54]

Both Locke and Rousseau developed social contract theories in Two Treatises of Government and Discourse on Inequality, respectively. While quite different works, Locke, Hobbes, and Rousseau agreed that a social contract, in which the government's authority lies in the consent of the governed,[55] is necessary for man to live in civil society. Locke defines the state of nature as a condition in which humans are rational and follow natural law, in which all men are born equal and with the right to life, liberty, and property. However, when one citizen breaks the law of nature both the transgressor and the victim enter into a state of war, from which it is virtually impossible to break free. Therefore, Locke said that individuals enter into civil society to protect their natural rights via an "unbiased judge" or common authority, such as courts. In contrast, Rousseau's conception relies on the supposition that "civil man" is corrupted, while "natural man" has no want he cannot fulfill himself. Natural man is only taken out of the state of nature when the inequality associated with private property is established.[56] Rousseau said that people join into civil society via the social contract to achieve unity while preserving individual freedom. This is embodied in the sovereignty of the general will, the moral and collective legislative body constituted by citizens.

Locke is known for his statement that individuals have a right to "Life, Liberty, and Property," and his belief that the natural right to property is derived from labor. Tutored by Locke, Anthony Ashley-Cooper, 3rd Earl of Shaftesbury, wrote in 1706: "There is a mighty Light which spreads its self over the world especially in those two free Nations of England and Holland; on whom the Affairs of Europe now turn."[57] Locke's theory of natural rights has influenced many political documents, including the U.S. Declaration of Independence and the French National Constituent Assembly's Declaration of the Rights of Man and of the Citizen.

The philosophes argued that the establishment of a contractual basis of rights would lead to the market mechanism and capitalism, the scientific method, religious tolerance, and the organization of states into self-governing republics through democratic means. In this view, the tendency of the philosophes in particular to apply rationality to every problem is considered the essential change.[58]

Although much of Enlightenment political thought was dominated by social contract theorists, Hume and Ferguson criticized this camp. Hume's essay Of the Original Contract argues that governments derived from consent are rarely seen and civil government is grounded in a ruler's habitual authority and force. It is precisely because of the ruler's authority over-and-against the subject that the subject tacitly consents, and Hume says that the subjects would "never imagine that their consent made him sovereign", rather the authority did so.[59] Similarly, Ferguson did not believe citizens built the state, rather polities grew out of social development. In his 1767 An Essay on the History of Civil Society, Ferguson uses the four stages of progress, a theory that was popular in Scotland at the time, to explain how humans advance from a hunting and gathering society to a commercial and civil society without agreeing to a social contract.

Both Rousseau's and Locke's social contract theories rest on the presupposition of natural rights, which are not a result of law or custom but are things that all men have in pre-political societies and are therefore universal and inalienable. The most famous natural right formulation comes from Locke's Second Treatise, when he introduces the state of nature. For Locke, the law of nature is grounded on mutual security or the idea that one cannot infringe on another's natural rights, as every man is equal and has the same inalienable rights. These natural rights include perfect equality and freedom, as well as the right to preserve life and property.

Locke argues against indentured servitude on the basis that enslaving oneself goes against the law of nature because a person cannot surrender their own rights: freedom is absolute, and no one can take it away. Locke argues that one person cannot enslave another because it is morally reprehensible, although he introduces a caveat by saying that enslavement of a lawful captive in time of war would not go against one's natural rights. As a spill-over of the Enlightenment, nonsecular beliefs expressed first by Quakers and then by Protestant evangelicals in Britain and the United States emerged. To these groups, slavery became "repugnant to our religion" and a "crime in the sight of God".[60] These ideas added to those expressed by Enlightenment thinkers, leading many in Britain to believe that slavery was "not only morally wrong and economically inefficient, but also politically unwise." This ideals eventually led to the abolition of slavery in Britain and the United States.[61]
$
10
Question 1: Who initiated the new debate on social contract theory with the work "Leviathan"?
A: John Locke
B: Jean-Jacques Rousseau
C: Thomas Hobbes
D: David Hume
E: Adam Ferguson
Answer: C

Question 2: Locke believed that the state of nature is a condition where humans are:
A: Irrational and lawless
B: Rational and follow natural law
C: Corrupted and selfish
D: Governed by a monarch
E: In a constant state of war
Answer: B

Question 3: Which of the following did Rousseau believe pulled man out of the state of nature?
A: The establishment of democratic governments
B: The rise of scientific advancements
C: The inequality associated with private property
D: The formation of international alliances
E: The desire for knowledge and enlightenment
Answer: C

Question 4: Who wrote, "There is a mighty Light which spreads its self over the world especially in those two free Nations of England and Holland; on whom the Affairs of Europe now turn"?
A: Voltaire
B: David Hume
C: Anthony Ashley-Cooper, 3rd Earl of Shaftesbury
D: John Locke
E: Jean-Jacques Rousseau
Answer: C

Question 5: Which political document was influenced by Locke's theory of natural rights?
A: Magna Carta
B: The Communist Manifesto
C: U.S. Declaration of Independence
D: The Treaty of Versailles
E: The English Bill of Rights
Answer: C

Question 6: Which thinker argued that governments rarely derive from consent and that civil government is rooted in a ruler's habitual authority and force?
A: Voltaire
B: Jean-Jacques Rousseau
C: John Locke
D: David Hume
E: Adam Smith
Answer: D

Question 7: Who wrote "An Essay on the History of Civil Society" in 1767?
A: David Hume
B: John Locke
C: Adam Ferguson
D: Jean-Jacques Rousseau
E: Thomas Hobbes
Answer: C

Question 8: The most notable expression of natural rights comes from which of Locke's works?
A: Leviathan
B: First Treatise of Government
C: An Essay Concerning Human Understanding
D: Discourse on Inequality
E: Second Treatise
Answer: E

Question 9: Why did Locke argue against indentured servitude?
A: Because it was economically inefficient
B: Because it went against the law of nature
C: Because it was a political tool for monarchs
D: Because it was supported by the church
E: Because it was not a widespread practice
Answer: B

Question 10: Which groups in Britain began to express the idea that slavery was "repugnant to our religion" and a "crime in the sight of God"?
A: Catholics and Orthodox Christians
B: Philosophes and Enlightenment thinkers
C: Quakers and Protestant evangelicals
D: Monarchs and lords
E: Merchants and traders
Answer: C
@
A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning into Moral Subjects (1739–40) is a book by Scottish philosopher David Hume, considered by many to be Hume's most important work and one of the most influential works in the history of philosophy.[1] The Treatise is a classic statement of philosophical empiricism, scepticism, and naturalism. In the introduction Hume presents the idea of placing all science and philosophy on a novel foundation: namely, an empirical investigation into human nature. Impressed by Isaac Newton's achievements in the physical sciences, Hume sought to introduce the same experimental method of reasoning into the study of human psychology, with the aim of discovering the "extent and force of human understanding". Against the philosophical rationalists, Hume argues that the passions, rather than reason, cause human behaviour. He introduces the famous problem of induction, arguing that inductive reasoning and our beliefs regarding cause and effect cannot be justified by reason; instead, our faith in induction and causation is caused by mental habit and custom. Hume defends a sentimentalist account of morality, arguing that ethics is based on sentiment and the passions rather than reason, and famously declaring that "reason is, and ought only to be the slave to the passions". Hume also offers a sceptical theory of personal identity and a compatibilist account of free will.

Contemporary philosophers have written of Hume that "no man has influenced the history of philosophy to a deeper or more disturbing degree",[2] and that Hume's Treatise is "the founding document of cognitive science"[3] and the "most important philosophical work written in English". However, the public in Britain at the time did not agree, nor in the end did Hume himself agree, reworking the material in both An Enquiry Concerning Human Understanding (1748) and An Enquiry Concerning the Principles of Morals (1751). In the Author's introduction to the former, Hume wrote:

Most of the principles, and reasonings, contained in this volume, were published in a work in three volumes, called A Treatise of Human Nature: a work which the Author had projected before he left College, and which he wrote and published not long after. But not finding it successful, he was sensible of his error in going to the press too early, and he cast the whole anew in the following pieces, where some negligences in his former reasoning and more in the expression, are, he hopes, corrected. Yet several writers who have honoured the Author's Philosophy with answers, have taken care to direct all their batteries against that juvenile work, which the author never acknowledged, and have affected to triumph in any advantages, which, they imagined, they had obtained over it: A practice very contrary to all rules of candour and fair-dealing, and a strong instance of those polemical artifices which a bigotted zeal thinks itself authorized to employ. Henceforth, the Author desires, that the following Pieces may alone be regarded as containing his philosophical sentiments and principles.

Regarding An Enquiry Concerning the Principles of Morals, Hume said: "of all my writings, historical, philosophical, or literary, incomparably the best".[4]
$
10
Question 1: Which book is considered by many as David Hume's most significant work?
A: An Enquiry Concerning Human Understanding
B: A Treatise of Human Nature
C: An Enquiry Concerning the Principles of Morals
D: The History of England
E: Dialogues Concerning Natural Religion
Answer: B

Question 2: In the Treatise, Hume presents the idea of basing all science and philosophy on what foundation?
A: Pure reason
B: Divine intervention
C: Empirical investigation into human nature
D: Ethical determinism
E: Cultural relativism
Answer: C

Question 3: Who inspired Hume with achievements in the physical sciences?
A: John Locke
B: Rene Descartes
C: Immanuel Kant
D: Isaac Newton
E: George Berkeley
Answer: D

Question 4: Hume argues that what, rather than reason, cause human behaviour?
A: Genetics
B: Environmental factors
C: The passions
D: Divine predestination
E: Economic conditions
Answer: C

Question 5: According to Hume, our belief in induction and causation is caused by:
A: Pure reason
B: Mental habit and custom
C: Scientific evidence
D: Divine intervention
E: Societal norms
Answer: B

Question 6: Hume's stance on morality is that ethics are based on:
A: Logic and reason
B: Sentiment and the passions
C: Divine commandments
D: Cultural norms
E: Social contracts
Answer: B

Question 7: Hume's view on personal identity can best be described as:
A: Dogmatic
B: Sentimentalist
C: Compatibilist
D: Sceptical
E: Rationalist
Answer: D

Question 8: Hume revised the material from "A Treatise of Human Nature" into which work?
A: The History of England
B: Dialogues Concerning Natural Religion
C: An Enquiry Concerning Human Understanding
D: Moral Sentiments
E: Philosophical Foundations
Answer: C

Question 9: How did Hume describe "An Enquiry Concerning the Principles of Morals" in relation to his other writings?
A: His least significant work
B: A reiteration of the Treatise
C: Incomparably the best
D: A work for the common people
E: A necessary evil
Answer: C

Question 10: Critics of Hume often targeted which of his works for criticism, despite him not acknowledging it later in life?
A: An Enquiry Concerning the Principles of Morals
B: An Enquiry Concerning Human Understanding
C: Dialogues Concerning Natural Religion
D: A Treatise of Human Nature
E: The History of England
Answer: D
@
Flight or flying is the process by which an object moves through a space without contacting any planetary surface, either within an atmosphere (i.e. air flight or aviation) or through the vacuum of outer space (i.e. spaceflight). This can be achieved by generating aerodynamic lift associated with gliding or propulsive thrust, aerostatically using buoyancy, or by ballistic movement.

Many things can fly, from animal aviators such as birds, bats and insects, to natural gliders/parachuters such as patagial animals, anemochorous seeds and ballistospores, to human inventions like aircraft (airplanes, helicopters, airships, balloons, etc.) and rockets which may propel spacecraft and spaceplanes.

The engineering aspects of flight are the purview of aerospace engineering which is subdivided into aeronautics, the study of vehicles that travel through the atmosphere, and astronautics, the study of vehicles that travel through space, and ballistics, the study of the flight of projectiles.

Mechanical flight is the use of a machine to fly. These machines include aircraft such as airplanes, gliders, helicopters, autogyros, airships, balloons, ornithopters as well as spacecraft. Gliders are capable of unpowered flight. Another form of mechanical flight is para-sailing, where a parachute-like object is pulled by a boat. In an airplane, lift is created by the wings; the shape of the wings of the airplane are designed specially for the type of flight desired. There are different types of wings: tempered, semi-tempered, sweptback, rectangular and elliptical. An aircraft wing is sometimes called an airfoil, which is a device that creates lift when air flows across it.

Supersonic
Main article: Supersonic speed
Supersonic flight is flight faster than the speed of sound. Supersonic flight is associated with the formation of shock waves that form a sonic boom that can be heard from the ground,[10] and is frequently startling. This shockwave takes quite a lot of energy to create and this makes supersonic flight generally less efficient than subsonic flight at about 85% of the speed of sound.

Hypersonic
Main article: Hypersonic speed
Hypersonic flight is very high speed flight where the heat generated by the compression of the air due to the motion through the air causes chemical changes to the air. Hypersonic flight is achieved primarily by reentering spacecraft such as the Space Shuttle and Soyuz.


The International Space Station in Earth orbit
Ballistic
Main article: Ballistics
Atmospheric
Some things generate little or no lift and move only or mostly under the action of momentum, gravity, air drag and in some cases thrust. This is termed ballistic flight. Examples include balls, arrows, bullets, fireworks etc.

Spaceflight
Main article: Spaceflight
Essentially an extreme form of ballistic flight, spaceflight is the use of space technology to achieve the flight of spacecraft into and through outer space. Examples include ballistic missiles, orbital spaceflight, etc.

Spaceflight is used in space exploration, and also in commercial activities like space tourism and satellite telecommunications. Additional non-commercial uses of spaceflight include space observatories, reconnaissance satellites and other Earth observation satellites.

A spaceflight typically begins with a rocket launch, which provides the initial thrust to overcome the force of gravity and propels the spacecraft from the surface of the Earth.[11] Once in space, the motion of a spacecraft—both when unpropelled and when under propulsion—is covered by the area of study called astrodynamics. Some spacecraft remain in space indefinitely, some disintegrate during atmospheric reentry, and others reach a planetary or lunar surface for landing or impact.

Solid-state propulsion
In 2018, researchers at Massachusetts Institute of Technology (MIT) managed to fly an aeroplane with no moving parts, powered by an "ionic wind" also known as electroaerodynamic thrust.[12][13]
$
10
Question 1: Flight can be described as the movement of an object through space without:
A: Generating lift
B: Using fuel
C: Contacting any planetary surface
D: Making noise
E: Causing turbulence
Answer: C

Question 2: What is responsible for creating lift in an airplane?
A: The engine's thrust
B: The pilot's skill
C: The speed of the plane
D: The wings
E: The aircraft's weight
Answer: D

Question 3: Which of the following animals is not mentioned as an example of natural fliers?
A: Fish
B: Birds
C: Bats
D: Insects
E: None of the above
Answer: A

Question 4: Aerospace engineering can be subdivided into all of the following EXCEPT:
A: Aeronautics
B: Ballistics
C: Astrobiology
D: Astronautics
E: Aerostatics
Answer: C

Question 5: Supersonic flight is characterized by:
A: Being slower than the speed of sound
B: The formation of shock waves resulting in a sonic boom
C: The lack of sound production
D: Being at about 85% of the speed of sound
E: Being efficient due to low energy consumption
Answer: B

Question 6: In what type of flight does the compression of air due to motion result in chemical changes to the air?
A: Subsonic
B: Supersonic
C: Ballistic
D: Hypersonic
E: Transonic
Answer: D

Question 7: Ballistic flight involves objects that:
A: Always generate significant lift
B: Always have propulsion systems
C: Move mainly due to momentum, gravity, and air drag
D: Are always intended for space exploration
E: Are always used for commercial activities
Answer: C

Question 8: What event typically initiates a spaceflight?
A: An aircraft taking off
B: A rocket launch
C: The release of a parachute
D: The turning on of a spacecraft's internal engine
E: The propulsion of an aircraft to hypersonic speeds
Answer: B

Question 9: What unique type of propulsion was utilized by researchers at MIT in 2018 for an airplane?
A: Jet propulsion
B: Supersonic thrust
C: Electroaerodynamic thrust
D: Combustion thrust
E: Gravitational pull
Answer: C

Question 10: Gliders are distinct because they:
A: Require fuel to fly
B: Are capable of supersonic flight
C: Have multiple engines
D: Are capable of unpowered flight
E: Cannot fly at high altitudes
Answer: D
@
The history of aviation extends for more than two thousand years, from the earliest forms of aviation such as kites and attempts at tower jumping to supersonic and hypersonic flight by powered, heavier-than-air jets.

Kite flying in China dates back to several hundred years BC and slowly spread around the world. It is thought to be the earliest example of man-made flight. Leonardo da Vinci's 15th-century dream of flight found expression in several rational designs, but which relied on poor science.

In the late 18th century the Montgolfier brothers invented the hot-air balloon and began manned flights. At almost the same time, the discovery of hydrogen gas led to the invention of the hydrogen balloon.[1] Various theories in mechanics by physicists during the same period of time, notably fluid dynamics and Newton's laws of motion, led to the foundation of modern aerodynamics, most notably by Sir George Cayley. Balloons, both free-flying and tethered, began to be used for military purposes from the end of the 18th century, with the French government establishing Balloon Companies during the Revolution.[2]

Experiments with gliders provided the groundwork for learning the dynamics of heavier-than-air craft, most notably by Cayley, Otto Lilienthal, and Octave Chanute. By the early 20th century, advances in engine technology and aerodynamics made controlled, powered flight possible for the first time. In 1903, following their pioneering research and experiments with wing design and aircraft control, the Wright brothers successfully incorporated all of the required elements to create and fly the first airplane. The basic configuration with its characteristic tail was established by 1909, followed by rapid design and performance improvements aided by the development of more powerful engines.

The first great ships of the air were the rigid dirigible balloons pioneered by Ferdinand von Zeppelin, which soon became synonymous with airships and dominated long-distance flight until the 1930s, when large flying boats became popular. After World War II, the flying boats were in their turn replaced by land planes, and the new and immensely powerful jet engine revolutionised both air travel and military aviation.

In the latter part of the 20th century, the advent of digital electronics produced great advances in flight instrumentation and "fly-by-wire" systems. The 21st century saw the large-scale use of pilotless drones for military, civilian and leisure use. With digital controls, inherently unstable aircraft such as flying wings became possible.
$
10
Question 1: Which activity is believed to be the earliest example of man-made flight?
A: Hot-air balloon flights
B: Tower jumping
C: Kite flying in China
D: Gliders
E: Aircraft with jet engines
Answer: C

Question 2: Leonardo da Vinci's designs for flight were:
A: Based on solid scientific principles
B: Inspired by Chinese kite flying
C: The first to use hydrogen gas
D: Derived from the Montgolfier brothers' work
E: Rational but relied on poor science
Answer: E

Question 3: Who pioneered the rigid dirigible balloons that dominated long-distance flight until the 1930s?
A: Otto Lilienthal
B: The Wright brothers
C: Ferdinand von Zeppelin
D: Octave Chanute
E: Sir George Cayley
Answer: C

Question 4: What discovery in the late 18th century led to the invention of the hydrogen balloon?
A: Hot air
B: Helium gas
C: Hydrogen gas
D: Oxygen
E: Aerodynamics
Answer: C

Question 5: Who successfully incorporated all of the required elements to create and fly the first airplane?
A: Ferdinand von Zeppelin
B: Leonardo da Vinci
C: The Wright brothers
D: Sir George Cayley
E: Octave Chanute
Answer: C

Question 6: Which factor was NOT mentioned as a significant advancement in aviation in the latter part of the 20th century?
A: Improved engine technology
B: Rigid dirigible balloons
C: Digital electronics
D: "Fly-by-wire" systems
E: Pilotless drones
Answer: B

Question 7: Experiments with what provided the groundwork for understanding the dynamics of heavier-than-air craft?
A: Balloons
B: Drones
C: Flying wings
D: Gliders
E: Hydrogen balloons
Answer: D

Question 8: After World War II, what replaced the flying boats in popularity?
A: Digital drones
B: Gliders
C: Hot-air balloons
D: Land planes
E: Rigid dirigible balloons
Answer: D

Question 9: Who made notable contributions to the foundation of modern aerodynamics during the late 18th century?
A: Wright brothers
B: Ferdinand von Zeppelin
C: Octave Chanute
D: Otto Lilienthal
E: Sir George Cayley
Answer: E

Question 10: What recent advancement in the 21st century allowed for the creation of inherently unstable aircraft?
A: Hot-air balloon technology
B: More powerful engines
C: Rigid dirigible balloons
D: Digital controls
E: Fly-by-wire systems
Answer: D
@
Stealth technology, also termed low observable technology (LO technology), is a sub-discipline of military tactics and passive and active electronic countermeasures,[1] which covers a range of methods used to make personnel, aircraft, ships, submarines, missiles, satellites, and ground vehicles less visible (ideally invisible) to radar, infrared,[2] sonar and other detection methods. It corresponds to military camouflage for these parts of the electromagnetic spectrum (i.e., multi-spectral camouflage).

Development of modern stealth technologies in the United States began in 1958,[3][4] where earlier attempts to prevent radar tracking of its U-2 spy planes during the Cold War by the Soviet Union had been unsuccessful.[5] Designers turned to developing a specific shape for planes that tended to reduce detection by redirecting electromagnetic radiation waves from radars.[6] Radiation-absorbent material was also tested and made to reduce or block radar signals that reflect off the surfaces of aircraft. Such changes to shape and surface composition comprise stealth technology as currently used on the Northrop Grumman B-2 Spirit "Stealth Bomber".[4]

The concept of stealth is to operate or hide while giving enemy forces no indication as to the presence of friendly forces. This concept was first explored through camouflage to make an object's appearance blend into the visual background. As the potency of detection and interception technologies (radar, infrared search and tracking, surface-to-air missiles, etc.) have increased, so too has the extent to which the design and operation of military personnel and vehicles have been affected in response. Some military uniforms are treated with chemicals to reduce their infrared signature. A modern stealth vehicle is designed from the outset to have a chosen spectral signature. The degree of stealth embodied in a given design is chosen according to the projected threats of detection.

Principles
Stealth technology (or LO for low observability) is not one technology. It is a set of technologies, used in combinations, that can greatly reduce the distances at which a person or vehicle can be detected; more so radar cross-section reductions, but also acoustic, thermal, and other aspects.

Radar cross-section (RCS) reductions
Almost since the invention of radar, various methods have been tried to minimize detection. Rapid development of radar during World War II led to equally rapid development of numerous counter radar measures during the period; a notable example of this was the use of chaff. Modern methods include Radar jamming and deception.

The term stealth in reference to reduced radar signature aircraft became popular during the late eighties when the Lockheed Martin F-117 stealth fighter became widely known. The first large scale (and public) use of the F-117 was during the Gulf War in 1991. However, F-117A stealth fighters were used for the first time in combat during Operation Just Cause, the United States invasion of Panama in 1989.[25]
$
10
Question 1: Stealth technology aims to make military assets less visible to what types of detection methods?
A: Sonar and light
B: Radar, infrared, and sonar
C: Visual and acoustic
D: Thermal and visual
E: Light and acoustic
Answer: B

Question 2: Stealth technology can be seen as a form of camouflage for which of the following?
A: Physical appearance only
B: The electromagnetic spectrum
C: Sound waves only
D: Infrared spectrum only
E: Visible light only
Answer: B

Question 3: When did the development of modern stealth technologies begin in the United States?
A: During World War II
B: 1975
C: 1989
D: 1958
E: 1991
Answer: D

Question 4: Why were stealth technologies developed for the U-2 spy planes?
A: To increase speed
B: To increase altitude
C: To prevent radar tracking by the Soviet Union
D: To improve fuel efficiency
E: To reduce acoustic noise
Answer: C

Question 5: What is a characteristic of the Northrop Grumman B-2 Spirit?
A: It is a reconnaissance aircraft
B: It uses stealth technology
C: It is a supersonic jet
D: It is used for transporting troops
E: It is a drone
Answer: B

Question 6: The initial concept of stealth was to:
A: Improve vehicle speed
B: Introduce radar jamming techniques
C: Blend an object's appearance into the visual background
D: Develop new materials for aircraft
E: Increase aircraft altitude
Answer: C

Question 7: Some military uniforms are treated to:
A: Make them visually brighter
B: Increase their weight
C: Reflect radar signals
D: Reduce their infrared signature
E: Increase thermal detection
Answer: D

Question 8: Stealth technology (or LO) can be best described as:
A: A single technology focused on visual camouflage
B: A material that absorbs all radar signals
C: A set of technologies aimed at reducing detectability
D: A software system used in military aircraft
E: A method used only for underwater vehicles
Answer: C

Question 9: Chaff was used as a countermeasure to:
A: Infrared detection
B: Visual detection
C: Acoustic detection
D: Radar
E: Sonar
Answer: D

Question 10: The Lockheed Martin F-117 stealth fighter was first used in large scale combat during:
A: World War II
B: The Gulf War in 1991
C: Operation Just Cause in 1989
D: The Cold War
E: The Vietnam War
Answer: B
@
The Lockheed Martin F-35 Lightning II is an American family of single-seat, single-engine, all-weather stealth multirole combat aircraft that is intended to perform both air superiority and strike missions. It is also able to provide electronic warfare and intelligence, surveillance, and reconnaissance capabilities. Lockheed Martin is the prime F-35 contractor, with principal partners Northrop Grumman and BAE Systems. The aircraft has three main variants: the conventional takeoff and landing (CTOL) F-35A, the short take-off and vertical-landing (STOVL) F-35B, and the carrier-based (CV/CATOBAR) F-35C.

The aircraft descends from the Lockheed Martin X-35, which in 2001 beat the Boeing X-32 to win the Joint Strike Fighter (JSF) program. Its development is principally funded by the United States, with additional funding from program partner countries from the North Atlantic Treaty Organization (NATO) and close U.S. allies, including the United Kingdom, Australia, Canada, Italy, Norway, Denmark, the Netherlands, and formerly Turkey.[5][6][7] Several other countries have also ordered, or are considering ordering, the aircraft. The program has drawn much scrutiny and criticism for its unprecedented size, complexity, ballooning costs, and much-delayed deliveries.[8][N 1] The acquisition strategy of concurrent production of the aircraft while it was still in development and testing led to expensive design changes and retrofits.[10][11]

The F-35 first flew in 2006 and entered service with the U.S. Marine Corps F-35B in July 2015, followed by the U.S. Air Force F-35A in August 2016 and the U.S. Navy F-35C in February 2019.[1][2][3] The aircraft was first used in combat in 2018 by the Israeli Air Force.[12] The U.S. plans to buy 2,456 F-35s through 2044, which will represent the bulk of the crewed tactical aviation of the U.S. Air Force, Navy, and Marine Corps for several decades; the aircraft is planned to be a cornerstone of NATO and U.S.-allied air power and to operate until 2070.[13][14]
$
10
Question 1: What type of engine does the Lockheed Martin F-35 Lightning II have?
A: Twin-engine
B: Single-engine, dual-seat
C: Single-engine, single-seat
D: Electric-engine
E: Twin-engine, dual-seat
Answer: C

Question 2: The F-35 is designed to perform which of the following missions?
A: Only air superiority
B: Only electronic warfare
C: Air superiority and strike missions
D: Only strike missions
E: Only intelligence and reconnaissance
Answer: C

Question 3: Which company is the prime contractor for the F-35?
A: Boeing
B: Northrop Grumman
C: BAE Systems
D: Lockheed Martin
E: Raytheon
Answer: D

Question 4: Which variant of the F-35 is designed for short take-off and vertical-landing?
A: F-35A
B: F-35B
C: F-35C
D: F-35D
E: F-35E
Answer: B

Question 5: The F-35 descends from which Lockheed Martin aircraft that won the Joint Strike Fighter program?
A: X-30
B: X-31
C: X-32
D: X-33
E: X-35
Answer: E

Question 6: Principal funding for the F-35's development comes from which country?
A: Australia
B: United Kingdom
C: Canada
D: United States
E: Italy
Answer: D

Question 7: In what year did the F-35 first fly?
A: 2001
B: 2006
C: 2010
D: 2015
E: 2018
Answer: B

Question 8: Which branch of the U.S. military first entered service with the F-35?
A: U.S. Air Force
B: U.S. Army
C: U.S. Navy
D: U.S. Marine Corps
E: U.S. Coast Guard
Answer: D

Question 9: When was the F-35 first used in combat?
A: 2015
B: 2016
C: 2017
D: 2018
E: 2019
Answer: D

Question 10: How many F-35s does the U.S. plan to purchase through 2044?
A: 1,234
B: 2,456
C: 3,567
D: 4,678
E: 5,789
Answer: B
@
Sometimes dubbed the Golden Age of Aviation,[1] the period in the history of aviation between the end of World War I (1918) and the beginning of World War II (1939) was characterised by a progressive change from the slow wood-and-fabric biplanes of World War I to fast, streamlined metal monoplanes, creating a revolution in both commercial and military aviation. By the outbreak of World War II in 1939 the biplane was all but obsolete. This revolution was made possible by the continuing development of lightweight aero engines of increasing power. The jet engine also began development during the 1930s but would not see operational use until later.


"Map of Air Routes and Landing Places in Great Britain, as temporarily arranged by the Air Ministry for civilian flying", published in 1919, showing Hounslow, near London, as the hub
During this period civil aviation became widespread and many daring and dramatic feats took place such as round-the-world flights, air races and barnstorming displays.[2] Many commercial airlines were started during this period. Long-distance flights for the luxury traveller became possible for the first time; the early services used airships, but, after the Hindenburg disaster, airships fell out of use and the flying boat came to dominate.

In military aviation, the fast all-metal monoplane equipped with retractable landing gear — first placed into production by the Soviet Union with the Polikarpov I-16 of 1934 — emerged in such classic designs as the German Messerschmitt Bf 109 and the British Supermarine Spitfire, which would go on to see service in the coming war.

Aeronautical advances
During the late 1920s and early 1930s the available power from aero engines increased significantly, making possible the adoption of the fast cantilever-wing monoplane, originally pioneered as far back as late 1915. The ability to handle the high mechanical stresses imposed by this advanced form of airframe design philosophy suited the all-metal aircraft construction techniques pioneered by some earlier designers, and the increasing availability of high strength-to-weight aluminum alloys — first used by Hugo Junkers in 1916-17 as duralumin for his all-metal airframe designs — made it practical, allowing the earliest all-metal airliners like the Ford Trimotor designed by William Stout, and Junkers' own pioneering airliners like the Junkers F.13 to be built and accepted into service. When Andrei Tupolev likewise used the Junkers firm's techniques for all-metal aircraft construction, his designs ranged in size to the enormous, 63 meter (206 ft) wingspan eight-engined Soviet Maksim Gorki, the largest aircraft built anywhere before World War II.

The de Havilland DH.88 Comet racer of 1934 was one of the first designs to incorporate all the features of the modern fast monoplane, including; stressed-skin construction, a thin, clean, low-drag cantilever wing, retractable undercarriage, landing flaps, variable-pitch propeller and enclosed cockpit. Unusually for such a highly stressed wing at that time it was still made of wood, with the thin stressed-skin design made possible by the appearance of new high-strength synthetic resin adhesives.[5]

The Comet was powered by two race-tuned but otherwise standard production de Havilland Gipsy Six engines with a combined output of 460 hp (344 kW). This compares for example on the one hand to the single 180 hp engine fitted to the Junkers CL.I all-metal monoplane of 1918 and to the 1,172 hp Rolls-Royce Merlin C development engine which powered the prototype Spitfire in 1936.[6]

In the 1930s development of the jet engine began in Germany and in England. In England Frank Whittle patented a design for a jet engine in 1930 and towards the end of the decade began developing an engine. In Germany Hans von Ohain patented his version of a jet engine in 1936 and began developing a similar engine. The two men were unaware of the other's work, and both Germany and Britain would go on to develop jet aircraft by the end of World War II. In Hungary, György Jendrassik began construction of the world's first turboprop engine.[7]

Military aviation
In military aviation, the fast all-metal monoplane emerged slowly. During the 1920s the high-wing parasol monoplane vied with the traditional biplane. It was not until the arrival of the American Boeing P-26 Peashooter in 1932 — nearly fifteen years after the first low-wing fighter to enter limited military service, the all-metal airframe Junkers D.I had entered service with the Luftstreitkräfte in 1918 — that the low-wing monoplane began to gain favour, reaching its classic form in such designs. These were pioneered in late 1933 by the Soviet Union with the Polikarpov I-16 fighter, powered initially with an American Wright Cyclone nine-cylinder radial engine. Within only a few years after the I-16's first flights, the German Messerschmitt Bf 109 of 1935 and the British Supermarine Spitfire of 1936 were also flying, powered by new and powerful liquid-cooled vee-twelve engines respectively from Daimler-Benz and Rolls-Royce. The rotary engines common in the First World War quickly fell out of favour, being replaced by more powerful stationary air-cooled radial engines such as the Pratt and Whitney Wasp series.
$
10
Question 1: What is the period between the end of World War I and the beginning of World War II in the history of aviation commonly known as?
A: The Dark Age of Aviation
B: The Modern Era of Aviation
C: The Jet Age of Aviation
D: The Golden Age of Aviation
E: The Early Age of Aviation
Answer: D

Question 2: By the start of World War II, which type of plane was becoming obsolete?
A: Monoplane
B: Jet plane
C: Biplane
D: Triplane
E: Flying boat
Answer: C

Question 3: What event led to a decline in the use of airships for commercial services?
A: Boeing 747 crash
B: Hindenburg disaster
C: Challenger explosion
D: Comet racer accident
E: Ford Trimotor mishap
Answer: B

Question 4: Which country first put the fast all-metal monoplane with retractable landing gear into production?
A: United States
B: Britain
C: Germany
D: Soviet Union
E: France
Answer: D

Question 5: The de Havilland DH.88 Comet racer introduced several modern features, but what material was its highly stressed wing made of?
A: Aluminum
B: Duralumin
C: Steel
D: Wood
E: Carbon-fiber
Answer: D

Question 6: In which two countries did the development of the jet engine notably begin during the 1930s?
A: France and Japan
B: Soviet Union and Italy
C: Germany and England
D: United States and Canada
E: Australia and New Zealand
Answer: C

Question 7: Who patented a design for a jet engine in England in the 1930s?
A: Hans von Ohain
B: György Jendrassik
C: Hugo Junkers
D: Frank Whittle
E: Andrei Tupolev
Answer: D

Question 8: Which was the first low-wing fighter to enter limited military service after World War I?
A: Boeing P-26 Peashooter
B: Polikarpov I-16
C: Junkers D.I
D: Supermarine Spitfire
E: Messerschmitt Bf 109
Answer: C

Question 9: What type of engine replaced the rotary engines common in World War I?
A: Liquid-cooled vee-twelve engines
B: Jet engines
C: Stationary air-cooled radial engines
D: Hydrogen-fueled engines
E: Electric engines
Answer: C

Question 10: What was the main material used by Hugo Junkers for his all-metal airframe designs in 1916-17?
A: Steel
B: Duralumin
C: Titanium
D: Wood
E: Iron
Answer: B
@
Apollo 11 (July 16–24, 1969) was the American spaceflight that first landed humans on the Moon. Commander Neil Armstrong and Lunar Module Pilot Buzz Aldrin landed the Apollo Lunar Module Eagle on July 20, 1969, at 20:17 UTC, and Armstrong became the first person to step onto the Moon's surface six hours and 39 minutes later, on July 21 at 02:56 UTC. Aldrin joined him 19 minutes later, and they spent about two and a quarter hours together exploring the site they had named Tranquility Base upon landing. Armstrong and Aldrin collected 47.5 pounds (21.5 kg) of lunar material to bring back to Earth as pilot Michael Collins flew the Command Module Columbia in lunar orbit, and were on the Moon's surface for 21 hours, 36 minutes before lifting off to rejoin Columbia.

Apollo 11 was launched by a Saturn V rocket from Kennedy Space Center on Merritt Island, Florida, on July 16 at 13:32 UTC, and it was the fifth crewed mission of NASA's Apollo program. The Apollo spacecraft had three parts: a command module (CM) with a cabin for the three astronauts, the only part that returned to Earth; a service module (SM), which supported the command module with propulsion, electrical power, oxygen, and water; and a lunar module (LM) that had two stages—a descent stage for landing on the Moon and an ascent stage to place the astronauts back into lunar orbit.

After being sent to the Moon by the Saturn V's third stage, the astronauts separated the spacecraft from it and traveled for three days until they entered lunar orbit. Armstrong and Aldrin then moved into Eagle and landed in the Sea of Tranquility on July 20. The astronauts used Eagle's ascent stage to lift off from the lunar surface and rejoin Collins in the command module. They jettisoned Eagle before they performed the maneuvers that propelled Columbia out of the last of its 30 lunar orbits onto a trajectory back to Earth.[9] They returned to Earth and splashed down in the Pacific Ocean on July 24 after more than eight days in space.

Armstrong's first step onto the lunar surface was broadcast on live TV to a worldwide audience. He described the event as "one small step for [a] man, one giant leap for mankind."[a][15] Apollo 11 effectively proved U.S. victory in the Space Race to demonstrate spaceflight superiority, by fulfilling a national goal proposed in 1961 by President John F. Kennedy, "before this decade is out, of landing a man on the Moon and returning him safely to the Earth."[16]
$
10
Question 1: When did Apollo 11 land humans on the Moon?
A: July 15, 1969
B: July 20, 1969
C: July 24, 1969
D: July 30, 1969
E: August 1, 1969
Answer: B

Question 2: How long after landing did Neil Armstrong step onto the Moon's surface?
A: 2 hours
B: 6 hours
C: 6 hours and 39 minutes
D: 12 hours
E: 24 hours
Answer: C

Question 3: What was the name of the site where Armstrong and Aldrin landed?
A: Sea of Tranquility Base
B: Tranquility Landing
C: Eagle Base
D: Tranquility Base
E: Armstrong Base
Answer: D

Question 4: Who remained in the Command Module Columbia in lunar orbit while the others landed on the Moon?
A: Neil Armstrong
B: Buzz Aldrin
C: Michael Collins
D: John F. Kennedy
E: None of the above
Answer: C

Question 5: From which location was Apollo 11 launched?
A: Houston Space Center, Texas
B: Cape Canaveral, Florida
C: Kennedy Space Center on Merritt Island, Florida
D: Johnson Space Center, Texas
E: Edwards Air Force Base, California
Answer: C

Question 6: How many crewed missions of NASA's Apollo program had occurred before Apollo 11?
A: 3
B: 4
C: 5
D: 6
E: 7
Answer: B

Question 7: What part of the Apollo spacecraft returned to Earth?
A: Service module
B: Lunar module
C: Command module
D: Descent stage
E: Ascent stage
Answer: C

Question 8: Where did Armstrong and Aldrin land on the Moon?
A: Mare Imbrium
B: Sea of Serenity
C: Sea of Tranquility
D: Ocean of Storms
E: Fra Mauro Highlands
Answer: C

Question 9: How did Armstrong describe his first step onto the lunar surface?
A: "A moment in history."
B: "Man's greatest achievement."
C: "One small step for [a] man, one giant leap for mankind."
D: "The final frontier conquered."
E: "For all of humanity."
Answer: C

Question 10: Who proposed the national goal of landing a man on the Moon and returning him safely to the Earth before the decade was out?
A: Richard Nixon
B: Lyndon B. Johnson
C: Neil Armstrong
D: Buzz Aldrin
E: John F. Kennedy
Answer: E
@
The Space Race was a 20th-century competition between two Cold War rivals, the United States and the Soviet Union, to achieve superior spaceflight capability. It had its origins in the ballistic missile-based nuclear arms race between the two nations following World War II. The technological advantage demonstrated by spaceflight achievement was seen as necessary for national security and became part of the symbolism and ideology of the time. The Space Race brought pioneering launches of artificial satellites, robotic space probes to the Moon, Venus, and Mars, and human spaceflight in low Earth orbit and ultimately to the Moon.[1]

Public interest in space travel originated in the 1951 publication of a Soviet youth magazine and was promptly picked up by US magazines.[2] The competition began on July 30, 1955, when the United States announced its intent to launch artificial satellites for the International Geophysical Year. Four days later, the Soviet Union responded by declaring they would also launch a satellite "in the near future". The launching of satellites was enabled by developments in ballistic missile capabilities since the end of World War II.[3] The competition gained Western public attention with the "Sputnik crisis", when the USSR achieved the first successful satellite launch, Sputnik 1, on October 4, 1957. It gained momentum when the USSR sent the first human, Yuri Gagarin, into space with the orbital flight of Vostok 1 on April 12, 1961. These were followed by a string of other early firsts achieved by the Soviets over the next few years.[4]

Gagarin's flight led US president John F. Kennedy to raise the stakes on May 25, 1961, by asking the US Congress to commit to the goal of "landing a man on the Moon and returning him safely to the Earth" before the end of the decade.[5] Both countries began developing super heavy-lift launch vehicles, with the US successfully deploying the Saturn V, which was large enough to send a three-person orbiter and two-person lander to the Moon. Kennedy's Moon landing goal was achieved in July 1969, with the flight of Apollo 11,[6][7][8] a remarkable achievement that many Americans believed overshadowed all Soviet achievements. However, such an opinion is generally contentious globally, with others attributing the first man in space as being a larger achievement.[9][10] The USSR pursued two crewed lunar programs but did not succeed with its N1 rocket to launch and land on the Moon before the US and eventually canceled it to concentrate on Salyut, the first space station program, and the first landings on Venus and on Mars. Meanwhile, the US landed five more Apollo crews on the Moon[11] and continued exploration of other extraterrestrial bodies robotically.

A period of détente followed with the April 1972 agreement on a cooperative Apollo–Soyuz Test Project (ASTP), resulting in the July 1975 rendezvous in Earth orbit of a US astronaut crew with a Soviet cosmonaut crew and joint development of an international docking standard APAS-75. Being considered as the final act of the Space Race,[10] the competition was only gradually replaced with cooperation.[12] The collapse of the Soviet Union eventually allowed the US and the newly founded Russian Federation to end their Cold War competition also in space, by agreeing in 1993 on the Shuttle–Mir and International Space Station programs.[13][14]
$
10
Question 1: Which two countries were the primary competitors in the Space Race?
A: UK and France
B: United States and China
C: United States and Soviet Union
D: Germany and Italy
E: Japan and South Korea
Answer: C

Question 2: What was the first successful satellite launch by the USSR called?
A: Luna 1
B: Vostok 1
C: Soyuz 1
D: Sputnik 1
E: Explorer 1
Answer: D

Question 3: Which president asked the US Congress to commit to landing a man on the Moon before the end of the 1960s?
A: Richard Nixon
B: Dwight D. Eisenhower
C: Lyndon B. Johnson
D: Ronald Reagan
E: John F. Kennedy
Answer: E

Question 4: What was the name of the first human to orbit the Earth?
A: Alan Shepard
B: John Glenn
C: Neil Armstrong
D: Yuri Gagarin
E: Buzz Aldrin
Answer: D

Question 5: In which year did the US achieve the Moon landing goal set by President Kennedy?
A: 1967
B: 1968
C: 1969
D: 1970
E: 1971
Answer: C

Question 6: Which rocket did the USSR develop for their crewed lunar programs?
A: Saturn V
B: Proton
C: Vostok
D: Soyuz
E: N1
Answer: E

Question 7: After the Space Race, the US and USSR initiated a cooperative project that saw a rendezvous in Earth orbit in 1975. What was this project called?
A: Luna-Soyuz
B: Shuttle-Mir
C: Apollo-Venus
D: Apollo–Soyuz Test Project
E: International Docking
Answer: D

Question 8: The competition of the Space Race was gradually replaced with what?
A: Rivalry
B: Cooperation
C: Domination
D: Isolation
E: Expansion
Answer: B

Question 9: Which event allowed the US and the Russian Federation to end their Cold War competition in space?
A: Signing of the Outer Space Treaty
B: Collapse of the Soviet Union
C: Moon landing of Apollo 11
D: Launch of Sputnik 1
E: Yuri Gagarin's flight
Answer: B

Question 10: Which program resulted in the US and the newly founded Russian Federation's agreement in 1993?
A: Apollo 12
B: Shuttle-Columbia
C: Shuttle–Mir and International Space Station
D: Apollo–Soyuz Reunion
E: Soyuz–Saturn
Answer: C
@
A ballistic missile is a type of missile that uses projectile motion to deliver warheads on a target. These weapons are powered only during relatively brief periods—most of the flight is unpowered. Short-range ballistic missiles stay within the Earth's atmosphere, while intercontinental ballistic missiles (ICBMs) are launched on a sub-orbital flight.

These weapons are in a distinct category from cruise missiles, which are aerodynamically guided in powered flight. Unlike cruise missiles, which are restricted to the atmosphere, it is advantageous for ballistic missiles to avoid the denser parts of the atmosphere and they may travel above the atmosphere into outer space.

The earliest form of ballistic missile dates from the 13th century with its use derived from the history of rockets. In the 14th century, the Ming Chinese navy used an early form of a ballistic missile weapon called the Huolongchushui in naval battles against enemy ships.[1]

One modern pioneer ballistic missile was the A-4,[2] commonly known as the V-2 developed by Nazi Germany in the 1930s and 1940s under the direction of Wernher von Braun. The first successful launch of a V-2 was on October 3, 1942, and it began operation on September 6, 1944, against Paris, followed by an attack on London two days later. By the end of World War II in Europe in May 1945, more than 3,000 V-2s had been launched.[3]

The R-7 Semyorka was the first intercontinental ballistic missile.[4]

An intercontinental ballistic missile trajectory consists of three parts: the powered flight portion; the free-flight portion, which constitutes most of the flight time; and the re-entry phase, where the missile re-enters the Earth's atmosphere. The flight phases for shorter-range ballistic missiles are essentially the first two phases of the ICBM, as some ballistic categories do not leave the atmosphere.[5]

Ballistic missiles can be launched from fixed sites or mobile launchers, including vehicles (e.g., transporter erector launchers), aircraft, ships, and submarines. The powered flight portion can last from a few tenths of seconds to several minutes and can consist of multiple rocket stages.[5]

When the fuel is exhausted, no more thrust is provided and the missile enters free flight. In order to cover large distances, ballistic missiles are usually launched into a high sub-orbital spaceflight; for intercontinental missiles, the highest altitude (apogee) reached during free-flight is about 4,500 kilometers (2,800 mi).[6]

The re-entry stage begins at an altitude where atmospheric drag plays a significant part in missile trajectory, and lasts until missile impact.[5] Re-entry vehicles re-enter the Earth's atmosphere at very high velocities, on the order of 6–8 kilometers per second (22,000–29,000 km/h; 13,000–18,000 mph) at ICBM ranges.[7]
$
10
Question 1: Which missile is powered only during short durations of its flight and delivers warheads on targets?
A: Cruise missile
B: Guided missile
C: Ballistic missile
D: Atmospheric missile
E: Drone missile
Answer: C

Question 2: Short-range ballistic missiles travel within which boundary?
A: Outer space
B: Ozone layer
C: Earth's atmosphere
D: Stratosphere
E: Exosphere
Answer: C

Question 3: In what significant way do ballistic missiles differ from cruise missiles?
A: Ballistic missiles can travel into outer space.
B: Cruise missiles are longer in range.
C: Ballistic missiles are shorter.
D: Cruise missiles can carry heavier warheads.
E: Ballistic missiles are aerodynamically guided.
Answer: A

Question 4: The Huolongchushui, an early form of ballistic missile, was used by which country in the 14th century?
A: Mongol Empire
B: Ottoman Empire
C: Japan
D: Ming China
E: India
Answer: D

Question 5: Who directed the development of the V-2 missile for Nazi Germany?
A: Adolf Hitler
B: Hermann Göring
C: Albert Speer
D: Wernher von Braun
E: Heinz Guderian
Answer: D

Question 6: Which missile was the first intercontinental ballistic missile?
A: A-4
B: R-6
C: R-7 Semyorka
D: V-2
E: V-3
Answer: C

Question 7: How many phases does an ICBM trajectory typically consist of?
A: One
B: Two
C: Three
D: Four
E: Five
Answer: C

Question 8: From which of these can ballistic missiles be launched?
A: Fixed sites
B: Mobile launchers
C: Aircraft
D: Submarines
E: All of the above
Answer: E

Question 9: Approximately how high is the apogee reached during free-flight for intercontinental missiles?
A: 2,500 kilometers
B: 3,500 kilometers
C: 4,000 kilometers
D: 4,500 kilometers
E: 5,000 kilometers
Answer: D

Question 10: At ICBM ranges, re-entry vehicles re-enter the Earth's atmosphere at velocities of approximately how many kilometers per second?
A: 3–5 km/s
B: 4–6 km/s
C: 5–7 km/s
D: 6–8 km/s
E: 7–9 km/s
Answer: D
@
In fluid dynamics, drag (sometimes called fluid resistance) is a force acting opposite to the relative motion of any object moving with respect to a surrounding fluid.[1] This can exist between two fluid layers (or surfaces) or between a fluid and a solid surface.

Unlike other resistive forces, such as dry friction, which are nearly independent of velocity, the drag force depends on velocity.[2][3] Drag force is proportional to the velocity for low-speed flow and the squared velocity for high speed flow, where the distinction between low and high speed is measured by the Reynolds number.

Drag forces always tend to decrease fluid velocity relative to the solid object in the fluid's path.

Examples of drag include the component of the net aerodynamic or hydrodynamic force acting opposite to the direction of movement of a solid object such as cars (automobile drag coefficient), aircraft[3] and boat hulls; or acting in the same geographical direction of motion as the solid, as for sails attached to a down wind sail boat, or in intermediate directions on a sail depending on points of sail.[4][5][6] In the case of viscous drag of fluid in a pipe, drag force on the immobile pipe decreases fluid velocity relative to the pipe.[7][8]

In the physics of sports, the drag force is necessary to explain the motion of balls, javelins, arrows and frisbees and the performance of runners and swimmers.[9]

Types of drag are generally divided into the following categories:

form drag or pressure drag due to the size and shape of a body
skin friction drag or viscous drag due to the friction between the fluid and a surface which may be the outside of an object or inside such as the bore of a pipe
The effect of streamlining on the relative proportions of skin friction and form drag is shown for two different body sections, an airfoil, which is a streamlined body, and a cylinder, which is a bluff body. Also shown is a flat plate illustrating the effect that orientation has on the relative proportions of skin friction and pressure difference between front and back. A body is known as bluff (or blunt) if the source of drag is dominated by pressure forces and streamlined if the drag is dominated by viscous forces. Road vehicles are bluff bodies.[10] For aircraft, pressure and friction drag are included in the definition of parasitic drag. Parasite drag is often expressed in terms of a hypothetical (in so far as there is no edge spillage drag[11]) "equivalent parasite drag area" which is the area of a flat plate perpendicular to the flow. It is used for comparing the drag of different aircraft. For example, the Douglas DC-3 has an equivalent parasite area of 23.7 sq ft and the McDonnell Douglas DC-9, with 30 years of advancement in aircraft design, an area of 20.6 sq ft although it carried five times as many passengers.[12]

lift-induced drag appears with wings or a lifting body in aviation and with semi-planing or planing hulls for watercraft
wave drag (aerodynamics) is caused by the presence of shockwaves and first appears at subsonic aircraft speeds when local flow velocities become supersonic. The wave drag of the supersonic Concorde prototype aircraft was reduced at Mach 2 by 1.8% by applying the area rule which extended the rear fuselage 3.73m on the production aircraft.[13]
wave resistance (ship hydrodynamics) or wave drag occurs when a solid object is moving along a fluid boundary and making surface waves
boat-tail drag on an aircraft is caused by the angle with which the rear fuselage, or engine nacelle, narrows to the engine exhaust diameter.[14]
$
10

Question 5: Skin friction drag is a result of the friction between a fluid and what?
A: Another fluid
B: Air particles
C: An exterior surface or the inside like the bore of a pipe
D: Magnetic fields
E: Gravity waves
Answer: C

Question 6: An object is considered streamlined if the source of drag is primarily due to which force?
A: Magnetic
B: Pressure forces
C: Viscous forces
D: Gravitational
E: Elastic
Answer: C

Question 7: Which of the following is a measure used for comparing the drag of different aircraft?
A: Lift ratio
B: Pressure coefficient
C: Equivalent parasite drag area
D: Thrust area
E: Velocity ratio
Answer: C

Question 8: Which type of drag is associated with wings in aviation?
A: Wave resistance
B: Lift-induced drag
C: Boat-tail drag
D: Wave drag
E: Form drag
Answer: B

Question 9: The wave drag of which aircraft was reduced by applying the area rule?
A: Boeing 747
B: Airbus A320
C: Concorde prototype
D: Douglas DC-3
E: McDonnell Douglas DC-9
Answer: C

Question 10: Boat-tail drag on an aircraft is caused by what specific factor?
A: The aircraft's weight
B: The angle between the wings
C: Turbulence caused by other aircraft
D: The angle at which the rear fuselage narrows to the engine exhaust diameter
E: The height of the aircraft above sea level
Answer: D
@
In fluid mechanics, the Reynolds number (Re) is a dimensionless quantity that helps predict fluid flow patterns in different situations by measuring the ratio between inertial and viscous forces. At low Reynolds numbers, flows tend to be dominated by laminar (sheet-like) flow, while at high Reynolds numbers, flows tend to be turbulent. The turbulence results from differences in the fluid's speed and direction, which may sometimes intersect or even move counter to the overall direction of the flow (eddy currents). These eddy currents begin to churn the flow, using up energy in the process, which for liquids increases the chances of cavitation.

The Reynolds number has wide applications, ranging from liquid flow in a pipe to the passage of air over an aircraft wing. It is used to predict the transition from laminar to turbulent flow and is used in the scaling of similar but different-sized flow situations, such as between an aircraft model in a wind tunnel and the full-size version. The predictions of the onset of turbulence and the ability to calculate scaling effects can be used to help predict fluid behavior on a larger scale, such as in local or global air or water movement, and thereby the associated meteorological and climatological effects.

The concept was introduced by George Stokes in 1851,[2] but the Reynolds number was named by Arnold Sommerfeld in 1908[3] after Osborne Reynolds (1842–1912), who popularized its use in 1883.[4][5]

The Reynolds number is the ratio of inertial forces to viscous forces within a fluid that is subjected to relative internal movement due to different fluid velocities. A region where these forces change behavior is known as a boundary layer, such as the bounding surface in the interior of a pipe. A similar effect is created by the introduction of a stream of high-velocity fluid into a low-velocity fluid, such as the hot gases emitted from a flame in air. This relative movement generates fluid friction, which is a factor in developing turbulent flow. Counteracting this effect is the viscosity of the fluid, which tends to inhibit turbulence. The Reynolds number quantifies the relative importance of these two types of forces for given flow conditions, and is a guide to when turbulent flow will occur in a particular situation.[6]

This ability to predict the onset of turbulent flow is an important design tool for equipment such as piping systems or aircraft wings, but the Reynolds number is also used in scaling of fluid dynamics problems and is used to determine dynamic similitude between two different cases of fluid flow, such as between a model aircraft, and its full-size version. Such scaling is not linear and the application of Reynolds numbers to both situations allows scaling factors to be developed.
$
10
Question 1: The Reynolds number is a measure of what in fluid mechanics?
A: The velocity of fluid flow
B: The temperature of the fluid
C: The ratio between inertial and viscous forces
D: The viscosity of the fluid
E: The boundary layer thickness
Answer: C

Question 2: At low Reynolds numbers, the flow tends to be:
A: Turbulent
B: Chaotic
C: Laminar
D: Irregular
E: Unpredictable
Answer: C

Question 3: What can turbulence in liquids increase the chances of?
A: Freezing
B: Evaporation
C: Cavitation
D: Condensation
E: Solidification
Answer: C

Question 4: Where does the Reynolds number find applications?
A: Only in air movement over aircraft wings
B: Only in liquid flow in a pipe
C: In predicting electrical currents
D: In the passage of air over an aircraft wing and liquid flow in a pipe
E: In predicting sound waves in fluids
Answer: D

Question 5: Who introduced the concept of the Reynolds number?
A: George Stokes
B: Arnold Sommerfeld
C: Osborne Reynolds
D: Isaac Newton
E: Albert Einstein
Answer: A

Question 6: Who popularized the use of the Reynolds number in 1883?
A: George Stokes
B: Arnold Sommerfeld
C: Osborne Reynolds
D: Isaac Newton
E: Albert Einstein
Answer: C

Question 7: A region where inertial forces and viscous forces change behavior is termed as:
A: Viscous layer
B: Turbulent zone
C: Boundary layer
D: Eddy current
E: Laminar sheet
Answer: C

Question 8: What effect does the introduction of a high-velocity fluid into a low-velocity fluid have?
A: Reduces turbulence
B: Generates fluid friction
C: Increases viscosity
D: Decreases Reynolds number
E: Generates electric current
Answer: B

Question 9: What is the primary purpose of the Reynolds number in equipment design?
A: Predict the material strength
B: Determine the heat transfer
C: Predict the onset of turbulent flow
D: Calculate the weight of the equipment
E: Measure the fluid temperature
Answer: C

Question 10: The Reynolds number helps determine dynamic similitude between two different cases of fluid flow. What is an example of this application?
A: Between a rock and its shadow
B: Between two different gases
C: Between an aircraft engine and its exhaust
D: Between a model aircraft and its full-size version
E: Between the moon and the Earth
Answer: D
@
A dimensionless quantity (also known as a bare quantity, pure quantity as well as quantity of dimension one)[1] is a quantity to which no physical dimension is assigned. Dimensionless quantities are widely used in many fields, such as mathematics, physics, chemistry, engineering, and economics. Dimensionless quantities are distinct from quantities that have associated dimensions, such as time (measured in seconds).

The corresponding unit of measurement is one (symbol 1),[2][3] which is not explicitly shown. For any system of units, the number one is considered a base unit.[4] Dimensionless units are special names that serve as units of measurement for expressing other dimensionless quantities. For example, in the SI, radians (rad) and steradians (sr) are dimensionless units for plane angles and solid angles, respectively.[2] For example, optical extent is defined as having units of metres multiplied by steradians.[5]

Some dimensionless quantities are called dimensionless numbers or characteristic numbers; they result from the product or quotient of other general quantities (e.g., characteristic lengths) and serve as parameters in equations and models. Characteristic numbers often carry the term "number" in their names (e.g., "Reynolds number") and may be denoted mathematically with a capitalized two-letter acronym (e.g., "Re" or "Re", italicized or not).[6] Several such numbers are defined as part of the International System of Quantities (ISQ), as standardized in ISO 80000-11.[7]

Dimensionless physical constants (e.g., fine-structure constant) and dimensionless material constants (e.g., refractive index) are dimensionless quantities having a fixed value for the whole universe or for a given material, respectively

Dimensionless quantities are often obtained as ratios, the quotient resulting from the division of quantities of the same kind ― that are not dimensionless, but whose dimensions cancel out in the mathematical operation.[4][16] Examples of quotients of dimension one include calculating slopes or some unit conversion factors. A more complex example of such a ratio is engineering strain, a measure of physical deformation defined as a change in length divided by the initial length. Another set of examples is mass fractions or mole fractions, often written using parts-per notation such as ppm (= 10−6), ppb (= 10−9), and ppt (= 10−12), or perhaps confusingly as ratios of two identical units (kg/kg or mol/mol). For example, alcohol by volume, which characterizes the concentration of ethanol in an alcoholic beverage, could be written as mL / 100 mL.

Other common proportions are percentages % (= 0.01),  ‰ (= 0.001). Some angle units such as turn, radian, and steradian are defined as ratios of quantities of the same kind. In statistics the coefficient of variation is the ratio of the standard deviation to the mean and is used to measure the dispersion in the data.

It has been argued that quantities defined as ratios Q = A/B having equal dimensions in numerator and denominator are actually only unitless quantities and still have physical dimension defined as dim Q = dim A × dim B−1.[17] For example, moisture content may be defined as a ratio of volumes (volumetric moisture, m3⋅m−3, dimension L3⋅L−3) or as a ratio of masses (gravimetric moisture, units kg⋅kg−1, dimension M⋅M−1); both would be unitless quantities, but of different dimension.
$
10
Question 1: A dimensionless quantity has which characteristic?
A: No physical dimension assigned
B: Always equals one
C: Only applicable to physics
D: Can only be used in equations
E: Always measured in seconds
Answer: A

Question 2: What is the corresponding unit of measurement for a dimensionless quantity?
A: Second
B: Kilogram
C: One
D: Metre
E: Radian
Answer: C

Question 3: In the SI, what are the dimensionless units for plane angles and solid angles, respectively?
A: Degrees and minutes
B: Revolutions and rotations
C: Gradians and revolutions
D: Radians and steradians
E: Pitches and rolls
Answer: D

Question 4: The term "Reynolds number" is an example of what?
A: A base unit
B: A unitless quantity
C: A characteristic number
D: A dimensioned quantity
E: A mathematical constant
Answer: C

Question 5: How is a dimensionless physical constant different from a dimensionless material constant?
A: One varies over time, the other doesn't.
B: One has a fixed value for the universe, the other for a given material.
C: One can be measured, the other cannot.
D: One is derived from SI units, the other from non-SI units.
E: One relates to chemistry, the other to physics.
Answer: B

Question 6: Dimensionless quantities often result from the division of quantities that:
A: Are both dimensionless
B: Are of the same kind and their dimensions cancel out
C: Vary with time
D: Have no real-world application
E: Are always measured in SI units
Answer: B

Question 7: Engineering strain is a measure of what?
A: Refraction index
B: Moisture content
C: Fine-structure constant
D: Physical deformation
E: Mass fraction
Answer: D

Question 8: In statistics, which measure is the ratio of the standard deviation to the mean?
A: Reynolds number
B: Moisture content
C: Coefficient of variation
D: Characteristic length
E: Alcohol by volume
Answer: C

Question 9: What do percentages % and ‰ represent?
A: 0.1 and 0.01
B: 0.01 and 0.001
C: 10 and 100
D: 100 and 1000
E: 1 and 10
Answer: B

Question 10: Moisture content can be defined as a ratio of:
A: Volumes or masses
B: Lengths or widths
C: Densities or weights
D: Temperatures or pressures
E: Heights or depths
Answer: A
@
In computer science, a data structure is a data organization, management, and storage format that is usually chosen for efficient access to data.[1][2][3] More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data,[4] i.e., it is an algebraic structure about data.

Data structures serve as the basis for abstract data types (ADT). The ADT defines the logical form of the data type. The data structure implements the physical form of the data type.[5]

Different types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, relational databases commonly use B-tree indexes for data retrieval,[6] while compiler implementations usually use hash tables to look up identifiers.[7]

Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Data structures can be used to organize the storage and retrieval of information stored in both main memory and secondary memory.[8]

Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by a pointer—a bit string, representing a memory address, that can be itself stored in memory and manipulated by the program. Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations, while the linked data structures are based on storing addresses of data items within the structure itself.

The implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).[9]
$
10
Question 1: What does a data structure enable in terms of data?
A: Efficient network connectivity
B: Instantaneous processing speed
C: Efficient access to data
D: Enhanced graphic rendering
E: Improved disk speed
Answer: C

Question 2: A data structure is a collection of data values and what else?
A: Only the relationships among them
B: Only the functions applied to the data
C: Both relationships among them and functions applied to the data
D: The speed of access to them
E: The storage capacity for them
Answer: C

Question 3: Abstract data types (ADT) define the:
A: Physical form of the data type
B: Logical form of the data type
C: Efficiency of data retrieval
D: Storage capacity
E: Memory address specifications
Answer: B

Question 4: Which data structure is commonly used in relational databases for data retrieval?
A: Linked lists
B: Hash tables
C: Arrays
D: B-tree indexes
E: Queues
Answer: D

Question 5: For what purpose are data structures particularly efficient?
A: For improving the speed of the CPU
B: For enhancing graphic rendering
C: For managing large amounts of data
D: For boosting the internet speed
E: For data encryption
Answer: C

Question 6: The design of which element often depends on the efficiency of data structures?
A: User interfaces
B: Graphic engines
C: Algorithms
D: Hardware circuits
E: Power supply
Answer: C

Question 7: What can data structures be used to organize in terms of storage?
A: Only main memory
B: Only secondary memory
C: Both main memory and secondary memory
D: Disk speed
E: Network bandwidth
Answer: C

Question 8: Based on what ability are data structures generally designed?
A: Ability of a computer to boost its speed
B: Ability of a computer to connect to the internet
C: Ability of a computer to fetch and store data at any place in its memory
D: Ability of a computer to increase its storage capacity
E: Ability of a computer to support multiple users
Answer: C

Question 9: The array and record data structures compute the addresses of data items using:
A: Memory addresses directly
B: Arithmetic operations
C: Linked lists
D: Secondary memory
E: Network protocols
Answer: B

Question 10: The efficiency of a data structure is tied to what other aspect?
A: The size of the data
B: The type of programming language used
C: The operations that manipulate the structure
D: The kind of user interface designed
E: The hardware configuration of the computer
Answer: C
@
The philosophy of artificial intelligence is a branch of the philosophy of mind and the philosophy of computer science[1] that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology, and free will.[2][3] Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers.[4] These factors contributed to the emergence of the philosophy of artificial intelligence.

The philosophy of artificial intelligence attempts to answer such questions as follows:[5]

Can a machine act intelligently? Can it solve any problem that a person would solve by thinking?
Are human intelligence and machine intelligence the same? Is the human brain essentially a computer?
Can a machine have a mind, mental states, and consciousness in the same sense that a human being can? Can it feel how things are?
Questions like these reflect the divergent interests of AI researchers, cognitive scientists and philosophers respectively. The scientific answers to these questions depend on the definition of "intelligence" and "consciousness" and exactly which "machines" are under discussion.

Important propositions in the philosophy of AI include some of the following:

Turing's "polite convention": If a machine behaves as intelligently as a human being, then it is as intelligent as a human being.[6]
The Dartmouth proposal: "Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it."[7]
Allen Newell and Herbert A. Simon's physical symbol system hypothesis: "A physical symbol system has the necessary and sufficient means of general intelligent action."[8]
John Searle's strong AI hypothesis: "The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds."[9]
Hobbes' mechanism: "For 'reason' ... is nothing but 'reckoning,' that is adding and subtracting, of the consequences of general names agreed upon for the 'marking' and 'signifying' of our thoughts..."[10]

Is it possible to create a machine that can solve all the problems humans solve using their intelligence? This question defines the scope of what machines could do in the future and guides the direction of AI research. It only concerns the behavior of machines and ignores the issues of interest to psychologists, cognitive scientists and philosophers; to answer this question: Does it matter whether a machine is really thinking, as a person thinks, rather than just producing outcomes that appear to result from thinking?[11]

The basic position of most AI researchers is summed up in this statement, which appeared in the proposal for the Dartmouth workshop of 1956:

"Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it."[7]
Arguments against the basic premise must show that building a working AI system is impossible because there is some practical limit to the abilities of computers or that there is some special quality of the human mind that is necessary for intelligent behavior and yet cannot be duplicated by a machine (or by the methods of current AI research). Arguments in favor of the basic premise must show that such a system is possible.

It is also possible to sidestep the connection between the two parts of the above proposal. For instance, machine learning, beginning with Turing's infamous child machine proposal[12] essentially achieves the desired feature of intelligence without a precise design-time description as to how it would exactly work. The account on robot tacit knowledge[13] eliminates the need for a precise description altogether.

The first step to answering the question is to clearly define "intelligence".
$
10
Question 1: What two branches of philosophy does the philosophy of artificial intelligence stem from?
A: Philosophy of science and philosophy of ethics
B: Philosophy of mind and philosophy of computer science
C: Philosophy of nature and philosophy of thought
D: Philosophy of life and philosophy of machines
E: Philosophy of evolution and philosophy of technology
Answer: B

Question 2: The philosophy of artificial intelligence is concerned with the creation of:
A: Only artificial animals
B: Only artificial people
C: Both artificial animals and artificial people
D: Artificial emotions
E: Artificial nature
Answer: C

Question 3: The philosophy of AI poses the question, "Can a machine have a mind and consciousness in the same sense as a human?" This is similar to asking if a machine can:
A: Reproduce
B: Operate automatically
C: Feel how things are
D: Evolve over time
E: Learn new languages
Answer: C

Question 4: Turing's "polite convention" suggests that if a machine behaves:
A: In an unpredictable manner, it is malfunctioning
B: As intelligently as a human, it is as intelligent as a human
C: Without emotion, it is truly artificial
D: With constant learning, it surpasses human intelligence
E: In a repetitive pattern, it lacks true intelligence
Answer: B

Question 5: What does the Dartmouth proposal state?
A: Every intelligent machine must have a learning ability.
B: Every machine can display intelligence if it has the right programming.
C: Every aspect of learning or intelligence can be precisely described for a machine to simulate.
D: Every machine, regardless of its complexity, can never replicate human thought.
E: Every intelligent act is the result of learned behaviors.
Answer: C

Question 6: According to John Searle's strong AI hypothesis, a computer with the right programming:
A: Can surpass human intelligence
B: Can replicate only specific aspects of the human mind
C: Will always remain inferior to the human mind
D: Would have a mind in the same sense humans have minds
E: Can never truly understand emotions
Answer: D

Question 7: The question about whether machines can solve problems using intelligence like humans primarily concerns the:
A: True understanding of machines
B: Behavior of machines
C: Emotional capability of machines
D: Future of human intelligence
E: Ethical considerations of AI
Answer: B

Question 8: What is the underlying assumption held by most AI researchers as per the Dartmouth workshop of 1956?
A: Machines can never truly learn
B: Machines can exhibit certain intelligent behaviors but not all
C: Every feature of intelligence can be described precisely for a machine to simulate
D: Human minds have a special quality that cannot be replicated by machines
E: Intelligence in machines is always a result of pre-defined behaviors
Answer: C

Question 9: Arguments against the premise of AI must show that:
A: Machines can think better than humans
B: Building an AI system is practically impossible
C: Machines have feelings and emotions
D: Humans can always outperform machines
E: AI can solve only a limited set of problems
Answer: B

Question 10: Before answering whether machines can be as intelligent as humans, it is essential to clearly define:
A: The scope of AI
B: The limitations of human intelligence
C: Machine learning algorithms
D: "Intelligence"
E: Technological advancements
Answer: D
@
Philosophy of mind is a branch of philosophy that studies the ontology and nature of the mind and its relationship with the body. The mind–body problem is a paradigmatic issue in philosophy of mind, although a number of other issues are addressed, such as the hard problem of consciousness and the nature of particular mental states.[1][2][3] Aspects of the mind that are studied include mental events, mental functions, mental properties, consciousness and its neural correlates, the ontology of the mind, the nature of cognition and of thought, and the relationship of the mind to the body.

Dualism and monism are the two central schools of thought on the mind–body problem, although nuanced views have arisen that do not fit one or the other category neatly.

Dualism finds its entry into Western philosophy thanks to René Descartes in the 17th century.[4] Substance dualists like Descartes argue that the mind is an independently existing substance, whereas property dualists maintain that the mind is a group of independent properties that emerge from and cannot be reduced to the brain, but that it is not a distinct substance.[5]
Monism is the position that mind and body are ontologically indiscernible entities, not dependent substances. This view was espoused by the 17th-century rationalist Baruch Spinoza.[6] Physicalists argue that only entities postulated by physical theory exist, and that mental processes will eventually be explained in terms of these entities as physical theory continues to evolve. Physicalists maintain various positions on the prospects of reducing mental properties to physical properties (many of whom adopt compatible forms of property dualism),[7][8][9][10][11][12] and the ontological status of such mental properties remains unclear.[11][13][14] Idealists maintain that the mind is all that exists and that the external world is either mental itself, or an illusion created by the mind. Neutral monists such as Ernst Mach and William James argue that events in the world can be thought of as either mental (psychological) or physical depending on the network of relationships into which they enter, and dual-aspect monists such as Spinoza adhere to the position that there is some other, neutral substance, and that both matter and mind are properties of this unknown substance. The most common monisms in the 20th and 21st centuries have all been variations of physicalism; these positions include behaviorism, the type identity theory, anomalous monism and functionalism.[15]
Most modern philosophers of mind adopt either a reductive physicalist or non-reductive physicalist position, maintaining in their different ways that the mind is not something separate from the body.[15] These approaches have been particularly influential in the sciences, especially in the fields of sociobiology, computer science (specifically, artificial intelligence), evolutionary psychology and the various neurosciences.[16][17][18][19] Reductive physicalists assert that all mental states and properties will eventually be explained by scientific accounts of physiological processes and states.[20][21][22] Non-reductive physicalists argue that although the mind is not a separate substance, mental properties supervene on physical properties, or that the predicates and vocabulary used in mental descriptions and explanations are indispensable, and cannot be reduced to the language and lower-level explanations of physical science.[23][24] Continued neuroscientific progress has helped to clarify some of these issues; however, they are far from being resolved. Modern philosophers of mind continue to ask how the subjective qualities and the intentionality of mental states and properties can be explained in naturalistic terms.[25][26]

However, a number of issues have been recognized with non-reductive physicalism. First, it is irreconcilable with self-identity over time. Secondly, intentional states of consciousness do not make sense on non-reductive physicalism. Thirdly, free will is impossible to reconcile with either reductive or non-reductive physicalism. Fourthly, it fails to properly explain the phenomenon of mental causation.[27]
$
10
Question 1: The philosophy of mind primarily investigates the relationship between which two entities?
A: Mind and universe
B: Soul and consciousness
C: Mind and body
D: Thought and reality
E: Emotion and perception
Answer: C

Question 2: Which problem is considered paradigmatic in the philosophy of mind?
A: Mind–society problem
B: Mind–emotion problem
C: Mind–body problem
D: Mind–reality problem
E: Mind–perception problem
Answer: C

Question 3: Dualism in Western philosophy was introduced by which philosopher in the 17th century?
A: Baruch Spinoza
B: Ernst Mach
C: William James
D: René Descartes
E: John Locke
Answer: D

Question 4: Substance dualists argue that the mind:
A: Is dependent on the body
B: Is an illusion created by the mind
C: Is an independently existing substance
D: Can be thought of as either mental or physical
E: Is just a set of physical processes
Answer: C

Question 5: Which view suggests that the mind and body are not distinguishable entities but are the same?
A: Dualism
B: Monism
C: Functionalism
D: Anomalous monism
E: Neutral monism
Answer: B

Question 6: Physicalists believe:
A: The mind is separate from the body
B: Only entities postulated by physical theory exist
C: The external world is an illusion created by the mind
D: The mind is an independent substance from the brain
E: Both matter and mind are properties of an unknown substance
Answer: B

Question 7: Idealists maintain that:
A: The mind is all that exists
B: The external world is both mental and physical
C: Mental states will be explained by physiological processes
D: Mental properties are reducible to physical properties
E: The mind supervenes on physical properties
Answer: A

Question 8: Modern philosophers of mind primarily adopt which position about the relationship between the mind and body?
A: Dualistic physicalist
B: Reductive physicalist or non-reductive physicalist
C: Monistic idealist
D: Neutral monist
E: Dual-aspect monist
Answer: B

Question 9: Reductive physicalists claim that:
A: The mind is a separate substance from the body
B: Mental properties cannot be reduced to physical properties
C: All mental states and properties will be explained by physiological processes
D: The mind is an illusion created by the external world
E: Mental descriptions cannot be reduced to physical science language
Answer: C

Question 10: One criticism of non-reductive physicalism is that it:
A: Supports the mind–body dualism
B: Reconciles with free will
C: Properly explains the phenomenon of mental causation
D: Makes intentional states of consciousness logical
E: Fails to properly explain the phenomenon of mental causation
Answer: E
@
The mind–body problem is a philosophical problem concerning the relationship between thought and consciousness in the human mind, and the body.[1][2]

It is not obvious how the concept of the mind and the concept of the body relate. For example, feelings of sadness (which are mental events) cause people to cry (which is a physical state of the body). Finding a joke funny (a mental event) causes one to laugh (another bodily state). Feelings of pain (in the mind) cause avoidance behaviours (in the body), and so on.

Similarly, changing the chemistry of the body (and the brain especially) via drugs (such as antipsychotics, SSRIs, or alcohol) can change one's state of mind in nontrivial ways. Alternatively, therapeutic interventions like cognitive behavioural therapy can change cognition in ways that have downstream effects on the bodily health.

In general, the existence of these mind–body connections seems unproblematic. Issues arise, however, once one considers what exactly we should make of these relations from a metaphysical or scientific perspective. Such reflections quickly raise a number of questions like:

Are the mind and body two distinct entities, or a single entity?
If the mind and body are two distinct entities, do the two of them causally interact?
Is it possible for these two distinct entities to causally interact?
What is the nature of this interaction?
Can this interaction ever be an object of empirical study?
If the mind and body are a single entity, then are mental events explicable in terms of physical events, or vice versa?
Is the relation between mental and physical events something that arises de novo at a certain point in development?
And so on. These and other questions that discuss the relation between mind and body are questions that all fall under the banner of the 'mind–body problem'.
$
10
Question 1: The mind-body problem primarily revolves around the relationship between what two elements?
A: Consciousness and body
B: Emotion and behavior
C: Thought and consciousness
D: Sensation and action
E: Brain and heart
Answer: C

Question 2: What physical reaction might result from a mental event of finding a joke humorous?
A: Crying
B: Jumping
C: Laughing
D: Avoiding
E: Yawning
Answer: C

Question 3: How might altering the body's chemistry, especially in the brain, affect an individual?
A: It changes one's physical appearance
B: It changes one's state of mind in nontrivial ways
C: It enhances an individual's strength
D: It causes immediate sleep
E: It decreases the size of the brain
Answer: B

Question 4: Cognitive behavioural therapy primarily targets:
A: Physical strength
B: Heart rate
C: Cognition
D: Digestive system
E: Immune response
Answer: C

Question 5: What general observation can be made about the connections between the mind and body?
A: They are always problematic
B: They seem unproblematic
C: They are irrelevant
D: They have no significant impact on behavior
E: They are solely driven by external stimuli
Answer: B

Question 6: One question arising from the mind-body problem debates whether:
A: The mind and body can exist without each other
B: Mental events are explicable in terms of physical events
C: Mental events can be stored in physical objects
D: Physical events can trigger immediate mental responses
E: The mind is a byproduct of bodily reactions
Answer: B

Question 7: The mind-body problem seeks to understand:
A: The physical dimensions of the mind
B: The mind's ability to control all body functions
C: How the mind and body relate from metaphysical or scientific perspectives
D: The evolutionary aspects of mind development
E: The dominance of the body over the mind
Answer: C

Question 8: If someone believes that the mind and body are a single entity, they might question if:
A: Mental events occur before physical events
B: Physical events can be controlled by mental thoughts
C: Mental events arise at a specific point in development
D: Physical events have no impact on mental state
E: Mental states can be transferred to other bodies
Answer: C

Question 9: The use of drugs such as SSRIs can primarily affect:
A: Physical strength
B: Muscle coordination
C: State of mind
D: Bodily appearance
E: Heart rate variability
Answer: C

Question 10: A primary question within the mind-body problem is whether:
A: The mind and body are one or two distinct entities
B: The mind is more powerful than the body
C: The body can function without the mind
D: Mental events are more significant than physical events
E: The body has its own consciousness independent of the mind
Answer: A
@
Consciousness, at its simplest, is awareness of internal and external existence.[1] However, its nature has led to millennia of analyses, explanations and debates by philosophers, theologians, linguists, and scientists. Opinions differ about what exactly needs to be studied or even considered consciousness. In some explanations, it is synonymous with the mind, and at other times, an aspect of mind. In the past, it was one's "inner life", the world of introspection, of private thought, imagination and volition.[2] Today, it often includes any kind of cognition, experience, feeling or perception. It may be awareness, awareness of awareness, or self-awareness either continuously changing or not.[3][4] The disparate range of research, notions and speculations raises a curiosity about whether the right questions are being asked.[5]

Examples of the range of descriptions, definitions or explanations are: simple wakefulness, one's sense of selfhood or soul explored by "looking within"; being a metaphorical "stream" of contents, or being a mental state, mental event or mental process of the brain.

The dictionary definitions of the word consciousness extend through several centuries and reflect a range of seemingly related meanings, with some differences that have been controversial, such as the distinction between 'inward awareness' and 'perception' of the physical world, or the distinction between 'conscious' and 'unconscious', or the notion of a "mental entity" or "mental activity" that is not physical.

The common usage definitions of consciousness in Webster's Third New International Dictionary (1966 edition, Volume 1, page 482) are as follows:

awareness or perception of an inward psychological or spiritual fact; intuitively perceived knowledge of something in one's inner self
inward awareness of an external object, state, or fact
concerned awareness; INTEREST, CONCERN—often used with an attributive noun [e.g. class consciousness]
the state or activity that is characterized by sensation, emotion, volition, or thought; mind in the broadest possible sense; something in nature that is distinguished from the physical
the totality in psychology of sensations, perceptions, ideas, attitudes, and feelings of which an individual or a group is aware at any given time or within a particular time span—compare STREAM OF CONSCIOUSNESS
waking life (as that to which one returns after sleep, trance, fever) wherein all one's mental powers have returned . . .
the part of mental life or psychic content in psychoanalysis that is immediately available to the ego—compare PRECONSCIOUS, UNCONSCIOUS
The Cambridge Dictionary defines consciousness as "the state of understanding and realizing something."[20] The Oxford Living Dictionary defines consciousness as "The state of being aware of and responsive to one's surroundings.", "A person's awareness or perception of something." and "The fact of awareness by the mind of itself and the world."[21]

Philosophers have attempted to clarify technical distinctions by using a jargon of their own. The Routledge Encyclopedia of Philosophy in 1998 defines consciousness as follows:

Consciousness—Philosophers have used the term 'consciousness' for four main topics: knowledge in general, intentionality, introspection (and the knowledge it specifically generates) and phenomenal experience... Something within one's mind is 'introspectively conscious' just in case one introspects it (or is poised to do so). Introspection is often thought to deliver one's primary knowledge of one's mental life. An experience or other mental entity is 'phenomenally conscious' just in case there is 'something it is like' for one to have it. The clearest examples are: perceptual experience, such as tastings and seeings; bodily-sensational experiences, such as those of pains, tickles and itches; imaginative experiences, such as those of one's own actions or perceptions; and streams of thought, as in the experience of thinking 'in words' or 'in images'. Introspection and phenomenality seem independent, or dissociable, although this is controversial.[22]

Many philosophers and scientists have been unhappy about the difficulty of producing a definition that does not involve circularity or fuzziness.[23] In The Macmillan Dictionary of Psychology (1989 edition), Stuart Sutherland expressed a skeptical attitude more than a definition:

Consciousness—The having of perceptions, thoughts, and feelings; awareness. The term is impossible to define except in terms that are unintelligible without a grasp of what consciousness means. Many fall into the trap of equating consciousness with self-consciousness—to be conscious it is only necessary to be aware of the external world. Consciousness is a fascinating but elusive phenomenon: it is impossible to specify what it is, what it does, or why it has evolved. Nothing worth reading has been written on it.[23]

A partisan definition such as Sutherland's can hugely affect researchers' assumptions and the direction of their work:

If awareness of the environment . . . is the criterion of consciousness, then even the protozoans are conscious. If awareness of awareness is required, then it is doubtful whether the great apes and human infants are conscious.[24]

Many philosophers have argued that consciousness is a unitary concept that is understood by the majority of people despite the difficulty philosophers have had defining it.[25] Others, though, have argued that the level of disagreement about the meaning of the word indicates that it either means different things to different people (for instance, the objective versus subjective aspects of consciousness), that it encompasses a variety of distinct meanings with no simple element in common,[26] or that we should eliminate this concept from our understanding of the mind, a position known as consciousness semanticism.[27]
$
10
Question 1: What, at its most basic, is consciousness defined as?
A: Sensation and thought
B: Awareness of internal and external existence
C: Perception of the external world
D: Introspection and thought
E: Imagination and volition
Answer: B

Question 2: In some explanations, consciousness is seen as:
A: Independent of the mind
B: A separate entity from the soul
C: An aspect of the mind
D: A byproduct of thought
E: Exclusively about the external world
Answer: C

Question 3: Today, consciousness often includes aspects like:
A: Memory and reflection
B: Dreaming and sleeping
C: Imagination, volition, and inner life
D: Cognition, experience, feeling, or perception
E: Bodily sensations and external stimuli
Answer: D

Question 4: What is one dictionary definition of consciousness from Webster's Third New International Dictionary?
A: Awareness of surrounding nature
B: Intuitively perceived knowledge of something in one's inner self
C: Perceptual experience of the world
D: Thought process and analysis
E: Reaction to external stimuli
Answer: B

Question 5: How does The Oxford Living Dictionary define consciousness?
A: The state of introspection and reflection
B: The process of thinking and analyzing
C: The state of understanding and realizing something
D: The state of being aware of and responsive to one's surroundings
E: The knowledge of one's self-worth
Answer: D

Question 6: According to The Routledge Encyclopedia of Philosophy, something within one's mind is 'introspectively conscious' if:
A: It is perceived by external sources
B: One introspects it
C: It is phenomenally conscious
D: It is analytically examined
E: It correlates with physical sensations
Answer: B

Question 7: In The Macmillan Dictionary of Psychology, Stuart Sutherland expressed that consciousness is:
A: Clearly definable and understood
B: Only related to self-awareness
C: Impossible to specify what it is or why it has evolved
D: Only related to human perception
E: Dependent on one's introspection and reflection
Answer: C

Question 8: If the criterion of consciousness is awareness of the environment, which organism might be considered conscious?
A: Rocks
B: Computers
C: Protozoans
D: Machines
E: Vegetables
Answer: C

Question 9: Some philosophers believe that the disagreements about consciousness mean that:
A: It is universally understood by everyone
B: It should be eliminated from our understanding of the mind
C: It has only one clear and defined meaning
D: It is solely based on introspection
E: It is unrelated to self-awareness
Answer: B

Question 10: Which term represents the idea that consciousness is a unitary concept widely understood by most people?
A: Consciousness semanticism
B: Introspective distinction
C: Phenomenal agreement
D: Conscious clarity
E: Unitary consciousness
Answer: E
@
Between 600 and 200 BCE, the Vaisheshika school of Hindu philosophy, founded by the ancient Indian philosopher Kanada, accepted perception and inference as the only two reliable sources of knowledge.[12][13][14] This is enumerated in his work Vaiśeṣika Sūtra. The Charvaka school held similar beliefs, asserting that perception is the only reliable source of knowledge while inference obtains knowledge with uncertainty.

The earliest Western proto-empiricists were the empiric school of ancient Greek medical practitioners, founded in 330 BCE.[15] Its members rejected the doctrines of the dogmatic school, preferring to rely on the observation of phantasiai (i.e., phenomena, the appearances).[16] The Empiric school was closely allied with the Pyrrhonist school of philosophy, which made the philosophical case for their proto-empiricism.

The notion of tabula rasa ("clean slate" or "blank tablet") connotes a view of mind as an originally blank or empty recorder (Locke used the words "white paper") on which experience leaves marks. This denies that humans have innate ideas. The notion dates back to Aristotle, c. 350 BC:

What the mind (nous) thinks must be in it in the same sense as letters are on a tablet (grammateion) which bears no actual writing (grammenon); this is just what happens in the case of the mind. (Aristotle, On the Soul, 3.4.430a1).

Aristotle's explanation of how this was possible was not strictly empiricist in a modern sense, but rather based on his theory of potentiality and actuality, and experience of sense perceptions still requires the help of the active nous. These notions contrasted with Platonic notions of the human mind as an entity that pre-existed somewhere in the heavens, before being sent down to join a body on Earth (see Plato's Phaedo and Apology, as well as others). Aristotle was considered to give a more important position to sense perception than Plato, and commentators in the Middle Ages summarized one of his positions as "nihil in intellectu nisi prius fuerit in sensu" (Latin for "nothing in the intellect without first being in the senses").

This idea was later developed in ancient philosophy by the Stoic school, from about 330 BCE. Stoic epistemology generally emphasized that the mind starts blank, but acquires knowledge as the outside world is impressed upon it.[17] The doxographer Aetius summarizes this view as "When a man is born, the Stoics say, he has the commanding part of his soul like a sheet of paper ready for writing upon."[18]
$
10
Question 1: What were the two reliable sources of knowledge according to the Vaisheshika school of Hindu philosophy?
A: Perception and imagination
B: Inference and deduction
C: Perception and inference
D: Deduction and imagination
E: Inference and imagination
Answer: C

Question 2: Which ancient school held the belief that perception is the sole reliable source of knowledge?
A: Stoic school
B: Charvaka school
C: Pyrrhonist school
D: Empiric school
E: Dogmatic school
Answer: B

Question 3: Who were the earliest Western proto-empiricists?
A: The Stoic philosophers
B: The Charvaka school
C: The empiric school of ancient Greek medical practitioners
D: The Pyrrhonist school of philosophy
E: The dogmatic school
Answer: C

Question 4: The term "tabula rasa" refers to the concept of the mind as:
A: A recorder filled with innate ideas
B: An entity pre-existing in the heavens
C: A blank slate where experience leaves marks
D: A place of active thought and decision making
E: An entity that evolves over time
Answer: C

Question 5: Which philosopher related the mind to a tablet bearing no actual writing?
A: Plato
B: Aristotle
C: Locke
D: Kanada
E: Pyrrho
Answer: B

Question 6: Aristotle’s explanation of the mind's ability to think was based on his theory of:
A: Tabula rasa and empiricism
B: Perception and inference
C: Potentiality and actuality
D: Sense perceptions and active nous
E: Stoicism and doxography
Answer: C

Question 7: Which philosopher is associated with the idea that the human mind pre-existed in the heavens before joining a body on Earth?
A: Locke
B: Aristotle
C: Kanada
D: Plato
E: Pyrrho
Answer: D

Question 8: The summarized position of Aristotle in the Middle Ages was:
A: Nothing in the intellect without first being in imagination
B: Everything in the intellect is innate and predetermined
C: Nothing in the intellect without first being in the senses
D: All knowledge is derived from empirical observation
E: The mind is a passive recipient of experiences
Answer: C

Question 9: Which school emphasized that the human mind starts blank and gains knowledge as the world impresses upon it?
A: Charvaka school
B: Dogmatic school
C: Stoic school
D: Empiric school
E: Pyrrhonist school
Answer: C

Question 10: According to the Stoics, when a man is born, the commanding part of his soul is like:
A: A book filled with stories
B: A well of potential knowledge
C: A vessel ready to be filled
D: A sheet of paper ready for writing upon
E: A mirror reflecting the external world
Answer: D
@
In computer science, a graph is an abstract data type that is meant to implement the undirected graph and directed graph concepts from the field of graph theory within mathematics.

A graph data structure consists of a finite (and possibly mutable) set of vertices (also called nodes or points), together with a set of unordered pairs of these vertices for an undirected graph or a set of ordered pairs for a directed graph. These pairs are known as edges (also called links or lines), and for a directed graph are also known as edges but also sometimes arrows or arcs. The vertices may be part of the graph structure, or may be external entities represented by integer indices or references.

A graph data structure may also associate to each edge some edge value, such as a symbolic label or a numeric attribute (cost, capacity, length, etc.).

The basic operations provided by a graph data structure G usually include:[1]

adjacent(G, x, y): tests whether there is an edge from the vertex x to the vertex y;
neighbors(G, x): lists all vertices y such that there is an edge from the vertex x to the vertex y;
add_vertex(G, x): adds the vertex x, if it is not there;
remove_vertex(G, x): removes the vertex x, if it is there;
add_edge(G, x, y, z): adds the edge z from the vertex x to the vertex y, if it is not there;
remove_edge(G, x, y): removes the edge from the vertex x to the vertex y, if it is there;
get_vertex_value(G, x): returns the value associated with the vertex x;
set_vertex_value(G, x, v): sets the value associated with the vertex x to v.
Structures that associate values to the edges usually also provide:[1]

get_edge_value(G, x, y): returns the value associated with the edge (x, y);
set_edge_value(G, x, y, v): sets the value associated with the edge (x, y) to v.
$
10
Question 1: In computer science, what is a graph designed to implement?
A: Concepts from algebra
B: The directed graph and undirected graph concepts from graph theory
C: Principles of calculus
D: Concepts from set theory
E: The binary tree structure from algorithms
Answer: B

Question 2: In a graph data structure, what are the unordered pairs of vertices called in an undirected graph?
A: Arrows
B: Arcs
C: Nodes
D: Edges
E: Points
Answer: D

Question 3: In a directed graph, in addition to being called edges, what else can the pairs be referred to as?
A: Lines and links
B: Nodes and vertices
C: Arrows and arcs
D: Lines and points
E: Links and nodes
Answer: C

Question 4: The vertices in a graph data structure can be:
A: Only integer indices
B: Part of the graph structure or external entities represented by indices or references
C: Always external entities
D: Only symbolic labels
E: Always part of the graph structure
Answer: B

Question 5: A graph data structure may associate what type of value to each edge?
A: Only numeric attributes
B: Only symbolic labels
C: Both symbolic labels and numeric attributes
D: Neither symbolic labels nor numeric attributes
E: Only the edge's direction
Answer: C

Question 6: Which operation tests if there's an edge from vertex x to vertex y?
A: add_edge(G, x, y, z)
B: set_vertex_value(G, x, v)
C: adjacent(G, x, y)
D: neighbors(G, x)
E: remove_edge(G, x, y)
Answer: C

Question 7: What operation would you use to list all vertices y that have an edge from vertex x?
A: get_vertex_value(G, x)
B: neighbors(G, x)
C: add_edge(G, x, y, z)
D: remove_vertex(G, x)
E: set_edge_value(G, x, y, v)
Answer: B

Question 8: How can you set the value associated with an edge (x, y) in a graph?
A: add_edge(G, x, y, z)
B: get_edge_value(G, x, y)
C: remove_edge(G, x, y)
D: set_vertex_value(G, x, v)
E: set_edge_value(G, x, y, v)
Answer: E

Question 9: If you want to return the value associated with vertex x, which operation should you use?
A: add_vertex(G, x)
B: get_vertex_value(G, x)
C: set_vertex_value(G, x, v)
D: adjacent(G, x, y)
E: remove_edge(G, x, y)
Answer: B

Question 10: Which operation is used to remove an edge from vertex x to vertex y?
A: remove_vertex(G, x)
B: add_vertex(G, x)
C: set_edge_value(G, x, y, v)
D: remove_edge(G, x, y)
E: adjacent(G, x, y)
Answer: D
@
In computer science, a tree is a widely used abstract data type that represents a hierarchical tree structure with a set of connected nodes. Each node in the tree can be connected to many children (depending on the type of tree), but must be connected to exactly one parent,[1] except for the root node, which has no parent (i.e., the root node as the top-most node in the tree hierarchy). These constraints mean there are no cycles or "loops" (no node can be its own ancestor), and also that each child can be treated like the root node of its own subtree, making recursion a useful technique for tree traversal. In contrast to linear data structures, many trees cannot be represented by relationships between neighboring nodes (parent and children nodes of a node under consideration if they exists) in a single straight line (called edge or link between two adjacent nodes).

Binary trees are a commonly used type, which constrain the number of children for each parent to at most two. When the order of the children is specified, this data structure corresponds to an ordered tree in graph theory. A value or pointer to other data may be associated with every node in the tree, or sometimes only with the leaf nodes, which have no children nodes.

The Abstract Data Type (ADT) can be represented in a number of ways, including a list of parents with pointers to children, a list of children with pointers to parents, or a list of nodes and a separate list of parent-child relations (a specific type of adjacency list). Representations might also be more complicated, for example using indexes or ancestor lists for performance.

Trees as used in computing are similar to but can be different from mathematical constructs of trees in graph theory, trees in set theory, and trees in descriptive set theory.
$
10
Question 1: In the context of computer science, what does a tree represent?
A: A linear structure with nodes.
B: A hierarchical structure with a set of connected nodes.
C: A network of interrelated databases.
D: A flat list of elements.
E: A grid-based layout system.
Answer: B

Question 2: Which node in a tree does not have a parent?
A: Any leaf node.
B: The second node.
C: The root node.
D: Child nodes.
E: The last node.
Answer: C

Question 3: Which of the following constraints is true for a tree structure in computer science?
A: Each node can be its own ancestor.
B: The root node must have a parent.
C: There are no cycles or "loops" in the tree.
D: Every node must connect to three other nodes.
E: The tree must always be represented in a straight line.
Answer: C

Question 4: What is unique about a binary tree?
A: It allows each parent to connect to at most five children.
B: It allows cycles and loops.
C: It does not differentiate between parents and children.
D: Each parent can connect to at most two children.
E: Every node must be a leaf node.
Answer: D

Question 5: If only the order of children is specified in a tree, what does it correspond to in graph theory?
A: Cyclic tree.
B: Unordered tree.
C: Graph network.
D: Ordered tree.
E: Mesh network.
Answer: D

Question 6: Which nodes in a tree might be associated with a value or pointer to other data?
A: Only root nodes.
B: Only nodes in the middle of the tree.
C: Only the nodes with the most children.
D: Every node or sometimes only leaf nodes.
E: Nodes with two parents.
Answer: D

Question 7: Which traversal technique is particularly useful for trees because of their hierarchical nature?
A: Iteration.
B: Recursion.
C: Duplication.
D: Serialization.
E: Merging.
Answer: B

Question 8: How can the Abstract Data Type (ADT) of a tree be represented?
A: Only as a list of parents with pointers to children.
B: Solely as an adjacency matrix.
C: As a list of nodes and a separate list of parent-child relations.
D: Through a single straight line.
E: With a series of loops.
Answer: C

Question 9: Trees used in computing might differ from which of the following?
A: Trees in software engineering.
B: Trees in descriptive set theory.
C: Trees in application development.
D: Trees in operating systems.
E: Trees in file management.
Answer: B

Question 10: Unlike linear data structures, many trees cannot be represented by relationships between which of the following?
A: Nodes and roots.
B: Parents and children nodes in a straight line.
C: Every node in the tree.
D: The first and last node.
E: Binary and ternary nodes.
Answer: B
@
In computer science, an abstract data type (ADT) is a mathematical model for data types, defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. This mathematical model contrasts with data structures, which are concrete representations of data, and are the point of view of an implementer, not a user.

Formally, an ADT may be defined as a "class of objects whose logical behavior is defined by a set of values and a set of operations";[1] this is analogous to an algebraic structure in mathematics. What is meant by "behaviour" varies by author, with the two main types of formal specifications for behavior being axiomatic (algebraic) specification and an abstract model;[2] these correspond to axiomatic semantics and operational semantics of an abstract machine, respectively. Some authors also include the computational complexity ("cost"), both in terms of time (for computing operations) and space (for representing values). In practice, many common data types are not ADTs, as the abstraction is not perfect, and users must be aware of issues like arithmetic overflow that are due to the representation. For example, integers are often stored as fixed-width values (32-bit or 64-bit binary numbers), and thus experience integer overflow if the maximum value is exceeded.

ADTs are a theoretical concept, in computer science, used in the design and analysis of algorithms, data structures, and software systems, and do not correspond to specific features of computer languages—mainstream computer languages do not directly support formally specified ADTs. However, various language features correspond to certain aspects of ADTs, and are easily confused with ADTs proper; these include abstract types, opaque data types, protocols, and design by contract. ADTs were first proposed by Barbara Liskov and Stephen N. Zilles in 1974, as part of the development of the CLU language.[3]

An abstract data type is defined as a mathematical model of the data objects that make up a data type as well as the functions that operate on these objects. There are no standard conventions for defining them. A broad division may be drawn between "imperative" (or "operational") and "functional" (or "axiomatic") definition styles.

Imperative-style definition
In the theory of imperative programming languages, an abstract data structure is conceived as an entity that is mutable—meaning that it may be in different states at different times. Some operations may change the state of the ADT; therefore, the order in which operations are evaluated is important, and the same operation on the same entities may have different effects if executed at different times. This is analogous to the instructions of a computer or the commands and procedures of an imperative language. To underscore this view, it is customary to say that the operations are executed or applied, rather than evaluated, similar to the imperative style often used when describing abstract algorithms. (See The Art of Computer Programming by Donald Knuth for more details).

Functional-style definition
Another way to define an ADT, closer to the spirit of functional programming, is to consider each state of the structure as a separate entity. In this view, any operation that modifies the ADT is modelled as a mathematical function that takes the old state as an argument and returns the new state as part of the result. Unlike the imperative operations, these functions have no side effects. Therefore, the order in which they are evaluated is immaterial, and the same operation applied to the same arguments (including the same input states) will always return the same results (and output states).

In the functional view, in particular, there is no way (or need) to define an "abstract variable" with the semantics of imperative variables (namely, with fetch and store operations). Instead of storing values into variables, one passes them as arguments to functions.
$
10
Question 1: What is the primary distinction between an abstract data type (ADT) and data structures?
A: ADT focuses on the user's perspective, while data structures focus on the implementer's perspective.
B: ADT is used in software systems, while data structures are used in hardware systems.
C: ADT is a type of data structure.
D: Data structures are a subset of ADTs.
E: ADT describes the syntax, while data structures describe the semantics.
Answer: A

Question 2: What defines the behavior of an ADT?
A: Possible values, possible operations, and the behavior of these operations.
B: Its size and memory allocation.
C: Its physical representation in memory.
D: Its interaction with other data types.
E: Its speed and computational power.
Answer: A

Question 3: Which of the following best defines an ADT, drawing a comparison to mathematics?
A: A set of procedures.
B: A logarithmic function.
C: An algebraic structure.
D: A linear equation.
E: A geometric shape.
Answer: C

Question 4: How do some authors expand the concept of an ADT's "behavior"?
A: By including visual representation.
B: By including computational complexity.
C: By including historical context.
D: By including references to other ADTs.
E: By including user reviews.
Answer: B

Question 5: Why are integers not perfect examples of ADTs?
A: They can only represent real numbers.
B: They experience integer overflow if the maximum value is exceeded.
C: They cannot be combined with other ADTs.
D: They are too simple to represent ADTs.
E: They are not mutable.
Answer: B

Question 6: In which field are ADTs primarily a theoretical concept?
A: Physics.
B: Economics.
C: Computer science.
D: Biology.
E: Literature.
Answer: C

Question 7: What does NOT directly support formally specified ADTs among mainstream computer languages?
A: Specific algorithms.
B: Computational power.
C: Specific features of computer languages.
D: Design by contract.
E: Opaque data types.
Answer: C

Question 8: Who first proposed ADTs?
A: Donald Knuth and Alan Turing.
B: Ada Lovelace and Charles Babbage.
C: John McCarthy and Dennis Ritchie.
D: Barbara Liskov and Stephen N. Zilles.
E: Grace Hopper and Richard Stallman.
Answer: D

Question 9: In the imperative-style definition of ADTs, what is emphasized?
A: The state of the ADT may change over time.
B: The state remains constant and immutable.
C: The operations have side effects.
D: Operations are executed only once.
E: Operations are applied based on specific conditions.
Answer: A

Question 10: How does the functional-style definition view operations that modify the ADT?
A: As functions with side effects.
B: As commands with irreversible changes.
C: As mathematical functions taking the old state and returning a new one.
D: As procedures that change the data type.
E: As operations that need constant monitoring.
Answer: C
@
A planet is a large, rounded astronomical body that is neither a star nor its remnant. The best available theory of planet formation is the nebular hypothesis, which posits that an interstellar cloud collapses out of a nebula to create a young protostar orbited by a protoplanetary disk. Planets grow in this disk by the gradual accumulation of material driven by gravity, a process called accretion. The Solar System has at least eight planets: the terrestrial planets Mercury, Venus, Earth and Mars, and the giant planets Jupiter, Saturn, Uranus and Neptune. These planets each rotate around an axis tilted with respect to its orbital pole. All planets of the Solar System other than Mercury possess a considerable atmosphere, and some share such features as ice caps, seasons, volcanism, hurricanes, tectonics, and even hydrology. Apart from Venus and Mars, the Solar System planets generate magnetic fields, and all except Venus and Mercury have natural satellites. The giant planets bear planetary rings, the most prominent being those of Saturn.

The word planet probably comes from the Greek planḗtai, meaning "wanderers". In antiquity, this word referred to the Sun, Moon, and five points of light visible by the naked eye that moved across the background of the stars—namely, Mercury, Venus, Mars, Jupiter and Saturn. Planets have historically had religious associations: multiple cultures identified celestial bodies with gods, and these connections with mythology and folklore persist in the schemes for naming newly discovered Solar System bodies. Earth itself was recognized as a planet when heliocentrism supplanted geocentrism during the 16th and 17th centuries.

With the development of the telescope, the meaning of planet broadened to include objects only visible with assistance: the moons of the planets beyond Earth; the ice giants Uranus and Neptune; Ceres and other bodies later recognized to be part of the asteroid belt; and Pluto, later found to be the largest member of the collection of icy bodies known as the Kuiper belt. The discovery of other large objects in the Kuiper belt, particularly Eris, spurred debate about how exactly to define a planet. The International Astronomical Union (IAU) adopted a standard by which the four terrestrials and four giants qualify, placing Ceres, Pluto and Eris in the category of dwarf planet,[1][2][3] although many planetary scientists have continued to apply the term planet more broadly.[4]

Further advances in astronomy led to the discovery of over five thousand planets outside the Solar System, termed exoplanets. These include hot Jupiters—giant planets that orbit close to their parent stars—like 51 Pegasi b, super-Earths like Gliese 581c that have masses in between that of Earth and Neptune; and planets smaller than Earth, like Kepler-20e. Multiple exoplanets have been found to orbit in the habitable zones of their stars, but Earth remains the only planet known to support life.
$
10
Question 1: What is the primary theory describing planet formation?
A: The stellar convergence theory.
B: The cosmic evolution theory.
C: The nebular hypothesis.
D: The big bang planetary theory.
E: The celestial distribution principle.
Answer: C

Question 2: How do planets form according to the nebular hypothesis?
A: Via interstellar cloud collapses to create a young protostar and protoplanetary disk.
B: Through fusion reactions in stars.
C: Via collisions of asteroids and other space debris.
D: Through gravitational pulls from nearby stars.
E: By branching off from already existing planets.
Answer: A

Question 3: How many planets are there in the Solar System?
A: Five.
B: Six.
C: Seven.
D: Eight.
E: Nine.
Answer: D

Question 4: Which planet in the Solar System does not possess a considerable atmosphere?
A: Venus.
B: Mars.
C: Mercury.
D: Neptune.
E: Saturn.
Answer: C

Question 5: Which planets in the Solar System do not generate magnetic fields?
A: Jupiter and Saturn.
B: Earth and Mars.
C: Venus and Mercury.
D: Neptune and Uranus.
E: Earth and Venus.
Answer: C

Question 6: From which language does the word "planet" likely originate?
A: Latin.
B: German.
C: Sanskrit.
D: Greek.
E: Arabic.
Answer: D

Question 7: During which centuries was Earth recognized as a planet due to the transition from geocentrism to heliocentrism?
A: 14th and 15th centuries.
B: 16th and 17th centuries.
C: 18th and 19th centuries.
D: 12th and 13th centuries.
E: 20th and 21st centuries.
Answer: B

Question 8: Which body is considered the largest member of the Kuiper belt?
A: Eris.
B: Neptune.
C: Ceres.
D: Pluto.
E: Saturn.
Answer: D

Question 9: How did the International Astronomical Union (IAU) classify Pluto?
A: As a moon.
B: As a planet.
C: As a comet.
D: As a star.
E: As a dwarf planet.
Answer: E

Question 10: What type of exoplanets orbits close to their parent stars?
A: Dwarf planets.
B: Super-Earths.
C: Hot Jupiters.
D: Cold Neptunes.
E: Goldilocks planets.
Answer: C
@
In astronomy, the geocentric model (also known as geocentrism, often exemplified specifically by the Ptolemaic system) is a superseded description of the Universe with Earth at the center. Under most geocentric models, the Sun, Moon, stars, and planets all orbit Earth.[1] The geocentric model was the predominant description of the cosmos in many European ancient civilizations, such as those of Aristotle in Classical Greece and Ptolemy in Roman Egypt. Ptolemy’s geocentric model was adopted and refined during the Islamic Golden Age, which Muslims believed correlated with the teachings of Islam.[2][3][4]

Two observations supported the idea that Earth was the center of the Universe:

First, from anywhere on Earth, the Sun appears to revolve around Earth once per day. While the Moon and the planets have their own motions, they also appear to revolve around Earth about once per day. The stars appeared to be fixed on a celestial sphere rotating once each day about an axis through the geographic poles of Earth.[5]
Second, Earth seems to be unmoving from the perspective of an earthbound observer; it feels solid, stable, and stationary.
Ancient Greek, ancient Roman, and medieval philosophers usually combined the geocentric model with a spherical Earth, in contrast to the older flat-Earth model implied in some mythology.[n 1][n 2][8] The ancient Jewish Babylonian uranography pictured a flat Earth with a dome-shaped, rigid canopy called the firmament placed over it (רקיע- rāqîa').[n 3][n 4][n 5][n 6][n 7][n 8] However, the Greek astronomer and mathematician Aristarchus of Samos (c. 310 – c. 230 BC) developed a heliocentric model placing all of the then-known planets in their correct order around the Sun.[15] The ancient Greeks believed that the motions of the planets were circular, a view that was not challenged in Western culture until the 17th century, when Johannes Kepler postulated that orbits were heliocentric and elliptical (Kepler's first law of planetary motion). In 1687 Newton showed that elliptical orbits could be derived from his laws of gravitation.

The astronomical predictions of Ptolemy's geocentric model, developed in the 2nd century CE, served as the basis for preparing astrological and astronomical charts for over 1,500 years. The geocentric model held sway into the early modern age, but from the late 16th century onward, it was gradually superseded by the heliocentric model of Copernicus (1473–1543), Galileo (1564–1642), and Kepler (1571–1630). There was much resistance to the transition between these two theories. Some felt that a new, unknown theory could not subvert an accepted consensus for geocentrism.
$
10
Question 1: What did the geocentric model describe?
A: Earth revolving around the Sun.
B: Planets orbiting each other.
C: The Universe with the Sun at the center.
D: The Universe with Earth at the center.
E: The Moon as the central point in the Universe.
Answer: D

Question 2: Which two figures are often associated with the geocentric model?
A: Kepler and Galileo.
B: Aristotle and Ptolemy.
C: Newton and Copernicus.
D: Aristarchus and Kepler.
E: Galileo and Newton.
Answer: B

Question 3: In which age was Ptolemy’s geocentric model refined and adopted?
A: The Renaissance Age.
B: The Bronze Age.
C: The Islamic Golden Age.
D: The Dark Ages.
E: The Medieval Age.
Answer: C

Question 4: How did the stars appear in relation to Earth in the geocentric model?
A: Revolving around the Moon.
B: Fixed on a celestial sphere rotating daily.
C: Moving in an elliptical orbit.
D: Positioned static without any movement.
E: Orbiting Earth in an unpredictable manner.
Answer: B

Question 5: Why did ancient observers believe Earth was the center of the Universe?
A: Stars moved in an elliptical pattern.
B: The Moon appeared larger than the Sun.
C: Earth felt solid, stable, and unmoving.
D: They believed the Sun was a minor star.
E: All celestial bodies appeared square-shaped.
Answer: C

Question 6: What shape did ancient Greek, ancient Roman, and medieval philosophers commonly attribute to the Earth?
A: Flat.
B: Triangular.
C: Cylindrical.
D: Spherical.
E: Cube.
Answer: D

Question 7: Who developed a heliocentric model placing planets in their correct order around the Sun?
A: Galileo.
B: Kepler.
C: Newton.
D: Ptolemy.
E: Aristarchus of Samos.
Answer: E

Question 8: Who proposed that orbits were heliocentric and elliptical?
A: Aristarchus of Samos.
B: Ptolemy.
C: Johannes Kepler.
D: Newton.
E: Copernicus.
Answer: C

Question 9: For how long did the predictions of Ptolemy's geocentric model serve as the basis for preparing astrological and astronomical charts?
A: Over 500 years.
B: Over 750 years.
C: Over 1,000 years.
D: Over 1,500 years.
E: Over 2,000 years.
Answer: D

Question 10: Who were three primary figures associated with the shift from the geocentric model to the heliocentric model?
A: Aristarchus, Galileo, and Kepler.
B: Ptolemy, Newton, and Galileo.
C: Kepler, Aristotle, and Copernicus.
D: Copernicus, Galileo, and Kepler.
E: Newton, Copernicus, and Ptolemy.
Answer: D
@
A star is an astronomical object comprising a luminous spheroid of plasma held together by self-gravity. The nearest star to Earth is the Sun. Many other stars are visible to the naked eye at night; their immense distances from Earth make them appear as fixed points of light. The most prominent stars have been categorised into constellations and asterisms, and many of the brightest stars have proper names. Astronomers have assembled star catalogues that identify the known stars and provide standardized stellar designations. The observable universe contains an estimated 1022 to 1024 stars. Only about 4,000 of these stars are visible to the naked eye—all within the Milky Way galaxy.[1]

A star's life begins with the gravitational collapse of a gaseous nebula of material largely comprising hydrogen, helium, and trace heavier elements. Its total mass mainly determines its evolution and eventual fate. A star shines for most of its active life due to the thermonuclear fusion of hydrogen into helium in its core. This process releases energy that traverses the star's interior and radiates into outer space. At the end of a star's lifetime, its core becomes a stellar remnant: a white dwarf, a neutron star, or—if it is sufficiently massive—a black hole.

Stellar nucleosynthesis in stars or their remnants creates almost all naturally occurring chemical elements heavier than lithium. Stellar mass loss or supernova explosions return chemically enriched material to the interstellar medium. These elements are then recycled into new stars. Astronomers can determine stellar properties—including mass, age, metallicity (chemical composition), variability, distance, and motion through space—by carrying out observations of a star's apparent brightness, spectrum, and changes in its position in the sky over time.

Stars can form orbital systems with other astronomical objects, as in planetary systems and star systems with two or more stars. When two such stars orbit closely, their gravitational interaction can significantly impact their evolution. Stars can form part of a much larger gravitationally bound structure, such as a star cluster or a galaxy.
$
10
Question 1: What is the nearest star to Earth?
A: Alpha Centauri.
B: Polaris.
C: Proxima Centauri.
D: Sirius.
E: The Sun.
Answer: E

Question 2: How do many stars appear to the naked eye at night due to their immense distances from Earth?
A: As moving points of light.
B: As luminous trails.
C: As fixed points of light.
D: As flashing lights.
E: As colored orbs.
Answer: C

Question 3: What do astronomers use to identify known stars and provide standardized designations?
A: Telescopes.
B: Star maps.
C: Star catalogues.
D: Gravitational calculations.
E: Spectrographs.
Answer: C

Question 4: How does a star primarily shine during its active life?
A: By reflecting light from nearby stars.
B: Due to the thermonuclear fusion of hydrogen into helium in its core.
C: By absorbing energy from its surroundings.
D: Due to its rotation.
E: By emitting energy from its surface.
Answer: B

Question 5: What can become of a star's core at the end of its life?
A: A gas giant.
B: A white dwarf.
C: A brown dwarf.
D: A planet.
E: A comet.
Answer: B

Question 6: Which process in stars or their remnants creates almost all naturally occurring chemical elements heavier than lithium?
A: Planetary accretion.
B: Gravitational interaction.
C: Stellar nucleosynthesis.
D: Core fusion.
E: Supernova implosion.
Answer: C

Question 7: How is chemically enriched material returned to the interstellar medium?
A: Via planetary collisions.
B: Through black holes.
C: By being sucked into galaxies.
D: Stellar mass loss or supernova explosions.
E: By cosmic winds.
Answer: D

Question 8: Which property of a star is NOT mentioned in the text as being determinable by astronomers?
A: Age.
B: Temperature.
C: Mass.
D: Metallicity.
E: Variability.
Answer: B

Question 9: In what type of systems can stars form an orbital relationship with other astronomical objects?
A: Only in galaxies.
B: In black holes.
C: In planetary systems and star systems.
D: Only in star clusters.
E: Only with comets.
Answer: C

Question 10: What can happen when two stars orbit closely?
A: They merge into a single star.
B: Their gravitational interaction can impact their evolution.
C: They can form a new galaxy.
D: They repel each other and move apart.
E: They can form a planetary system.
Answer: B
@
In astronomy, metallicity is the abundance of elements present in an object that are heavier than hydrogen and helium. Most of the normal currently detectable (i.e. non-dark) matter in the universe is either hydrogen or helium, and astronomers use the word "metals" as a convenient short term for "all elements except hydrogen and helium". This word-use is distinct from the conventional chemical or physical definition of a metal as an electrically conducting solid. Stars and nebulae with relatively high abundances of heavier elements are called "metal-rich" in astrophysical terms, even though many of those elements are nonmetals in chemistry.

The presence of heavier elements results from stellar nucleosynthesis, where the majority of elements heavier than hydrogen and helium in the Universe (metals, hereafter) are formed in the cores of stars as they evolve. Over time, stellar winds and supernovae deposit the metals into the surrounding environment, enriching the interstellar medium and providing recycling materials for the birth of new stars. It follows that older generations of stars, which formed in the metal-poor early Universe, generally have lower metallicities than those of younger generations, which formed in a more metal-rich Universe.

Observed changes in the chemical abundances of different types of stars, based on the spectral peculiarities that were later attributed to metallicity, led astronomer Walter Baade in 1944 to propose the existence of two different populations of stars.[1] These became commonly known as population I (metal-rich) and population II (metal-poor) stars. A third, earliest stellar population was hypothesized in 1978, known as population III stars.[2][3][4] These "extremely metal-poor" (XMP) stars are theorized to have been the "first-born" stars created in the Universe.

Astronomers use several different methods to describe and approximate metal abundances, depending on the available tools and the object of interest. Some methods include determining the fraction of mass that is attributed to gas versus metals, or measuring the ratios of the number of atoms of two different elements as compared to the ratios found in the Sun.
$
10
Question 1: What does the term "metallicity" refer to in astronomy?
A: The amount of metal present in a celestial body.
B: The ability of an object to conduct electricity.
C: The abundance of elements in an object heavier than hydrogen and helium.
D: The brightness of a star.
E: The age of a celestial body.
Answer: C

Question 2: How do astronomers define "metals"?
A: Elements that can conduct electricity.
B: Elements that are shiny and solid at room temperature.
C: Only elements found on the periodic table's metal section.
D: Elements that can be magnetized.
E: All elements except hydrogen and helium.
Answer: E

Question 3: How are the majority of elements heavier than hydrogen and helium formed?
A: Through the collision of asteroids.
B: In the cores of stars during their evolution.
C: Through the interaction of dark matter.
D: By gravitational pull from black holes.
E: Through cosmic radiation.
Answer: B

Question 4: Older generations of stars have generally ________ compared to younger stars.
A: Larger sizes
B: Lower metallicities
C: Higher temperatures
D: Shorter lifespans
E: More hydrogen content
Answer: B

Question 5: What does the term "metal-rich" mean in astrophysical terms?
A: A celestial body that has a lot of metallic ores.
B: A star or nebula with a high abundance of elements heavier than hydrogen and helium.
C: A celestial body that can conduct electricity.
D: A celestial body that contains only metals from the periodic table.
E: A star with a solid core.
Answer: B

Question 6: Who proposed the existence of two different populations of stars in 1944?
A: Edwin Hubble
B: Carl Sagan
C: Galileo Galilei
D: Walter Baade
E: Isaac Newton
Answer: D

Question 7: Which stellar population is considered "extremely metal-poor"?
A: Population I
B: Population II
C: Population III
D: Population IV
E: Population V
Answer: C

Question 8: What is theorized about population III stars?
A: They are the most recent stars formed.
B: They contain only hydrogen and helium.
C: They have the highest metallicity.
D: They are the "first-born" stars created in the Universe.
E: They are the largest stars in size.
Answer: D

Question 9: Astronomers use different methods to approximate metal abundances. One method involves:
A: Calculating the age of the star.
B: Observing the color of the star.
C: Measuring the ratios of the number of atoms of two different elements.
D: Calculating the distance from Earth.
E: Observing the number of moons around a planet.
Answer: C

Question 10: Where are metals deposited over time due to stellar winds and supernovae?
A: Into the core of stars.
B: Into black holes.
C: Into the interstellar medium.
D: Away from galaxies.
E: Into other universes.
Answer: C
@
In physical cosmology, Big Bang nucleosynthesis (also known as primordial nucleosynthesis, and abbreviated as BBN)[1] is the production of nuclei other than those of the lightest isotope of hydrogen (hydrogen-1, 1H, having a single proton as a nucleus) during the early phases of the universe. This type of nucleosynthesis is thought by most cosmologists to have occurred from 10 seconds to 20 minutes after the Big Bang.[2] It is thought to be responsible for the formation of most of the universe's helium (as isotope helium-4 (4He)), along with small amounts of the hydrogen isotope deuterium (2H or D), the helium isotope helium-3 (3He), and a very small amount of the lithium isotope lithium-7 (7Li). In addition to these stable nuclei, two unstable or radioactive isotopes were produced: the heavy hydrogen isotope tritium (3H or T) and the beryllium isotope beryllium-7 (7Be). These unstable isotopes later decayed into 3He and 7Li, respectively, as above.

Elements heavier than lithium are thought to have been created later in the life of the Universe by stellar nucleosynthesis, through the formation, evolution and death of stars.

There are several important characteristics of Big Bang nucleosynthesis (BBN):

The initial conditions (neutron–proton ratio) were set in the first second after the Big Bang.
The universe was very close to homogeneous at this time, and strongly radiation-dominated.
The fusion of nuclei occurred between roughly 10 seconds to 20 minutes after the Big Bang; this corresponds to the temperature range when the universe was cool enough for deuterium to survive, but hot and dense enough for fusion reactions to occur at a significant rate.[1]
It was widespread, encompassing the entire observable universe.
The key parameter which allows one to calculate the effects of Big Bang nucleosynthesis is the baryon/photon number ratio, which is a small number of order 6 × 10−10. This parameter corresponds to the baryon density and controls the rate at which nucleons collide and react; from this it is possible to calculate element abundances after nucleosynthesis ends. Although the baryon per photon ratio is important in determining element abundances, the precise value makes little difference to the overall picture. Without major changes to the Big Bang theory itself, BBN will result in mass abundances of about 75% of hydrogen-1, about 25% helium-4, about 0.01% of deuterium and helium-3, trace amounts (on the order of 10−10) of lithium, and negligible heavier elements. That the observed abundances in the universe are generally consistent with these abundance numbers is considered strong evidence for the Big Bang theory.

In this field, for historical reasons it is customary to quote the helium-4 fraction by mass, symbol Y, so that 25% helium-4 means that helium-4 atoms account for 25% of the mass, but less than 8% of the nuclei would be helium-4 nuclei. Other (trace) nuclei are usually expressed as number ratios to hydrogen. The first detailed calculations of the primordial isotopic abundances came in 1966[3][4] and have been refined over the years using updated estimates of the input nuclear reaction rates. The first systematic Monte Carlo study of how nuclear reaction rate uncertainties impact isotope predictions, over the relevant temperature range, was carried out in 1993.[5]
$
10
Question 1: Big Bang nucleosynthesis refers to the production of nuclei other than which isotope of hydrogen?
A: Hydrogen-2
B: Hydrogen-3
C: Hydrogen-1
D: Helium-4
E: Deuterium
Answer: C

Question 2: The majority of the universe's helium was formed as which isotope?
A: Helium-1
B: Helium-2
C: Helium-3
D: Helium-4
E: Helium-5
Answer: D

Question 3: Within what time frame is Big Bang nucleosynthesis believed to have occurred?
A: 1 second to 10 minutes after the Big Bang
B: 5 minutes to 1 hour after the Big Bang
C: 10 seconds to 20 minutes after the Big Bang
D: Immediately after the Big Bang
E: 30 minutes to 1 hour after the Big Bang
Answer: C

Question 4: Which of the following is NOT a product of Big Bang nucleosynthesis?
A: Deuterium
B: Helium-3
C: Lithium-7
D: Oxygen-8
E: Tritium
Answer: D

Question 5: Elements heavier than lithium were primarily created through what process?
A: Big Bang nucleosynthesis
B: Black hole absorption
C: Cosmic radiation
D: Stellar nucleosynthesis
E: Quantum fluctuations
Answer: D

Question 6: During which phase was the neutron-proton ratio established for Big Bang nucleosynthesis?
A: First hour after the Big Bang
B: First week after the Big Bang
C: First month after the Big Bang
D: First year after the Big Bang
E: First second after the Big Bang
Answer: E

Question 7: When referring to the helium-4 fraction by mass, which symbol is used?
A: H
B: He
C: Li
D: D
E: Y
Answer: E

Question 8: Approximately what percentage of mass abundances will result from BBN for hydrogen-1?
A: 8%
B: 25%
C: 50%
D: 75%
E: 90%
Answer: D

Question 9: Which parameter controls the rate at which nucleons collide and react during Big Bang nucleosynthesis?
A: Photon/neutron number ratio
B: Neutron/proton ratio
C: Baryon/photon number ratio
D: Photon/electron number ratio
E: Electron/neutron number ratio
Answer: C

Question 10: When were the first detailed calculations of the primordial isotopic abundances made?
A: 1944
B: 1956
C: 1966
D: 1978
E: 1993
Answer: C
@
Stellar nucleosynthesis is the creation (nucleosynthesis) of chemical elements by nuclear fusion reactions within stars. Stellar nucleosynthesis has occurred since the original creation of hydrogen, helium and lithium during the Big Bang. As a predictive theory, it yields accurate estimates of the observed abundances of the elements. It explains why the observed abundances of elements change over time and why some elements and their isotopes are much more abundant than others. The theory was initially proposed by Fred Hoyle in 1946,[1] who later refined it in 1954.[2] Further advances were made, especially to nucleosynthesis by neutron capture of the elements heavier than iron, by Margaret and Geoffrey Burbidge, William Alfred Fowler and Fred Hoyle in their famous 1957 B2FH paper,[3] which became one of the most heavily cited papers in astrophysics history.

Stars evolve because of changes in their composition (the abundance of their constituent elements) over their lifespans, first by burning hydrogen (main sequence star), then helium (horizontal branch star), and progressively burning higher elements. However, this does not by itself significantly alter the abundances of elements in the universe as the elements are contained within the star. Later in its life, a low-mass star will slowly eject its atmosphere via stellar wind, forming a planetary nebula, while a higher–mass star will eject mass via a sudden catastrophic event called a supernova. The term supernova nucleosynthesis is used to describe the creation of elements during the explosion of a massive star or white dwarf.

The advanced sequence of burning fuels is driven by gravitational collapse and its associated heating, resulting in the subsequent burning of carbon, oxygen and silicon. However, most of the nucleosynthesis in the mass range A = 28–56 (from silicon to nickel) is actually caused by the upper layers of the star collapsing onto the core, creating a compressional shock wave rebounding outward. The shock front briefly raises temperatures by roughly 50%, thereby causing furious burning for about a second. This final burning in massive stars, called explosive nucleosynthesis or supernova nucleosynthesis, is the final epoch of stellar nucleosynthesis.

A stimulus to the development of the theory of nucleosynthesis was the discovery of variations in the abundances of elements found in the universe. The need for a physical description was already inspired by the relative abundances of the chemical elements in the solar system. Those abundances, when plotted on a graph as a function of the atomic number of the element, have a jagged sawtooth shape that varies by factors of tens of millions (see history of nucleosynthesis theory).[4] This suggested a natural process that is not random. A second stimulus to understanding the processes of stellar nucleosynthesis occurred during the 20th century, when it was realized that the energy released from nuclear fusion reactions accounted for the longevity of the Sun as a source of heat and light.[5]
$
10
Question 1: Stellar nucleosynthesis refers to the creation of chemical elements by which process within stars?
A: Nuclear fission
B: Radioactive decay
C: Photoelectric effect
D: Nuclear fusion
E: Solar flaring
Answer: D

Question 2: Which theory yields accurate estimates of the observed abundances of the elements?
A: Quantum mechanics
B: General relativity
C: Stellar nucleosynthesis
D: Quantum field theory
E: Planetary formation theory
Answer: C

Question 3: Who initially proposed the theory of stellar nucleosynthesis?
A: William Alfred Fowler
B: Margaret Burbidge
C: Geoffrey Burbidge
D: Fred Hoyle
E: James Chadwick
Answer: D

Question 4: The famous 1957 B2FH paper, heavily cited in astrophysics, was a collaborative work by which individuals?
A: Fred Hoyle, Geoffrey Burbidge, and Margaret Burbidge
B: Fred Hoyle, James Chadwick, and William Alfred Fowler
C: Geoffrey Burbidge, Margaret Burbidge, William Alfred Fowler, and Fred Hoyle
D: William Alfred Fowler, James Chadwick, and Geoffrey Burbidge
E: Margaret Burbidge, Geoffrey Burbidge, and James Chadwick
Answer: C

Question 5: In the life cycle of a star, what event leads to the creation of elements during the explosion of a massive star or white dwarf?
A: Planetary nebula formation
B: Main sequence burning
C: Stellar wind
D: Supernova nucleosynthesis
E: Horizontal branch burning
Answer: D

Question 6: The final epoch of stellar nucleosynthesis, involving the upper layers of a star collapsing onto the core, is referred to as?
A: Compressive nucleosynthesis
B: Explosive nucleosynthesis
C: Core fusion nucleosynthesis
D: Shockwave nucleosynthesis
E: Gravitational nucleosynthesis
Answer: B

Question 7: A graph plotting the abundances of elements in the universe as a function of their atomic number presents a pattern resembling what shape?
A: Linear gradient
B: Perfect circle
C: Sawtooth
D: Parabola
E: Hyperbola
Answer: C

Question 8: What was a significant realization in the 20th century regarding nuclear fusion reactions in relation to the Sun?
A: It caused solar flares.
B: It affected Earth's magnetism.
C: It accounted for the Sun's longevity as a heat and light source.
D: It led to the formation of black holes.
E: It determined the Sun's position in the Milky Way.
Answer: C

Question 9: Which fuel burning sequence in stars is primarily driven by gravitational collapse and its associated heating?
A: Hydrogen burning
B: Helium burning
C: Carbon, oxygen, and silicon burning
D: Lithium and boron burning
E: Neutron and proton burning
Answer: C

Question 10: What major event in the universe's history was responsible for the initial creation of hydrogen, helium, and lithium?
A: Stellar supernovae
B: Formation of galaxies
C: The Big Bang
D: Formation of black holes
E: Cosmic radiation events
Answer: C
@
A chemical element is a chemical substance that cannot be broken down into other substances. The basic particle that constitutes a chemical element is the atom, and each chemical element is distinguished by the number of protons in the nuclei of its atoms, known as its atomic number. For example, oxygen has an atomic number of 8, meaning that each oxygen atom has 8 protons in its nucleus. This is in contrast to chemical compounds and mixtures, which contain atoms with more than one atomic number.

Almost all of the baryonic matter of the universe is composed of chemical elements (among rare exceptions are neutron stars). When different elements undergo chemical reactions, atoms are rearranged into new compounds held together by chemical bonds. Only a minority of elements, such as silver and gold, are found uncombined as relatively pure native element minerals. Nearly all other naturally occurring elements occur in the Earth as compounds or mixtures. Air is primarily a mixture of the elements nitrogen, oxygen, and argon, though it does contain compounds including carbon dioxide and water.

The history of the discovery and use of the elements began with primitive human societies that discovered native minerals like carbon, sulfur, copper and gold (though the concept of a chemical element was not yet understood). Attempts to classify materials such as these resulted in the concepts of classical elements, alchemy, and various similar theories throughout human history. Much of the modern understanding of elements developed from the work of Dmitri Mendeleev, a Russian chemist who published the first recognizable periodic table in 1869. This table organizes the elements by increasing atomic number into rows ("periods") in which the columns ("groups") share recurring ("periodic") physical and chemical properties. The periodic table summarizes various properties of the elements, allowing chemists to derive relationships between them and to make predictions about compounds and potential new ones.

By November 2016, the International Union of Pure and Applied Chemistry had recognized a total of 118 elements. The first 94 occur naturally on Earth, and the remaining 24 are synthetic elements produced in nuclear reactions. Save for unstable radioactive elements (radionuclides) which decay quickly, nearly all of the elements are available industrially in varying amounts. The discovery and synthesis of further new elements is an ongoing area of scientific study.
$
10
Question 1: What is the distinguishing feature of a chemical element based on the structure of its atoms?
A: Number of electrons
B: Atomic mass
C: Number of neutrons
D: Atomic number (number of protons)
E: Orbital configuration
Answer: D

Question 2: Which of the following substances consists of atoms with more than one atomic number?
A: Chemical element
B: Isotope
C: Chemical compound
D: Radionuclide
E: Ion
Answer: C

Question 3: What kind of celestial body is an exception to being composed of chemical elements?
A: Black hole
B: Galaxy
C: Comet
D: Neutron star
E: Planet
Answer: D

Question 4: Gold and silver are examples of elements that can be found in what form?
A: Gaseous state
B: As a compound
C: As radionuclides
D: In a mixed state
E: As pure native element minerals
Answer: E

Question 5: Air primarily consists of which three elements?
A: Hydrogen, oxygen, and carbon
B: Carbon, argon, and helium
C: Nitrogen, oxygen, and argon
D: Hydrogen, helium, and argon
E: Oxygen, carbon dioxide, and neon
Answer: C

Question 6: Who is credited with publishing the first recognizable periodic table?
A: Albert Einstein
B: Marie Curie
C: Dmitri Mendeleev
D: Isaac Newton
E: Niels Bohr
Answer: C

Question 7: What are the columns in the periodic table called?
A: Lanes
B: Intervals
C: Series
D: Periods
E: Groups
Answer: E

Question 8: As of November 2016, how many elements were recognized by the International Union of Pure and Applied Chemistry?
A: 90
B: 100
C: 104
D: 112
E: 118
Answer: E

Question 9: How many of the recognized elements naturally occur on Earth?
A: 82
B: 88
C: 94
D: 102
E: 110
Answer: C

Question 10: The discovery and synthesis of new elements is what type of scientific endeavor?
A: Completed field with no further research
B: Discontinued as of 1900s
C: An ongoing area of study
D: Limited to natural occurrences on Earth
E: Focused solely on radioactive elements
Answer: C
@
A synthetic element is one of 24 known chemical elements that do not occur naturally on Earth: they have been created by human manipulation of fundamental particles in a nuclear reactor, a particle accelerator, or the explosion of an atomic bomb; thus, they are called "synthetic", "artificial", or "man-made". The synthetic elements are those with atomic numbers 95–118, as shown in purple on the accompanying periodic table:[1] these 24 elements were first created between 1944 and 2010. The mechanism for the creation of a synthetic element is to force additional protons into the nucleus of an element with an atomic number lower than 95. All known (see: Island of stability) synthetic elements are unstable, but they decay at widely varying rates: the half-lives of their longest-lived isotopes range from microseconds to millions of years.

Five more elements that were first created artificially are strictly speaking not synthetic because they were later found in nature in trace quantities: 43Tc, 61Pm, 85At, 93Np, and 94Pu, though are sometimes classified as synthetic alongside exclusively artificial elements.[2] The first, technetium, was created in 1937.[3] Plutonium (Pu, atomic number 94), first synthesized in 1940, is another such element. It is the element with the largest number of protons (atomic number) to occur in nature, but it does so in such tiny quantities that it is far more practical to synthesize it. Plutonium is known mainly for its use in atomic bombs and nuclear reactors.[4]

No elements with atomic numbers greater than 99 have any uses outside of scientific research, since they have extremely short half-lives, and thus have never been produced in large quantities.

All elements with atomic number greater than 94 decay quickly enough into lighter elements such that any atoms of these that may have existed when the Earth formed (about 4.6 billion years ago) have long since decayed.[5][6] Synthetic elements now present on Earth are the product of atomic bombs or experiments that involve nuclear reactors or particle accelerators, via nuclear fusion or neutron absorption.[7]

Atomic mass for natural elements is based on weighted average abundance of natural isotopes in Earth's crust and atmosphere. For synthetic elements, there is no "natural isotope abundance". Therefore, for synthetic elements the total nucleon count (protons plus neutrons) of the most stable isotope, i.e., the isotope with the longest half-life—is listed in brackets as the atomic mass.
$
10
Question 1: What defines a synthetic element?
A: Elements that occur naturally on Earth.
B: Elements found only in specific regions of the Earth.
C: Elements that have been discovered after 1900.
D: Elements created by human manipulation and do not occur naturally on Earth.
E: Elements that can be found in both natural and synthetic forms.
Answer: D

Question 2: Between which years were the synthetic elements with atomic numbers 95-118 first created?
A: 1900-1937
B: 1937-1944
C: 1944-2010
D: 2010-2020
E: 1800-1900
Answer: C

Question 3: How are synthetic elements primarily created?
A: By removing electrons from natural elements.
B: By combining two natural elements.
C: By forcing additional protons into the nucleus of an element.
D: By exposing elements to high temperatures.
E: By diluting natural elements.
Answer: C

Question 4: Which of the following elements was first created artificially but was later found in nature?
A: 95Am
B: 92U
C: 94Pu
D: 98Cf
E: 96Cm
Answer: C

Question 5: For what is Plutonium mainly known?
A: As a fuel for cars.
B: Its use in atomic bombs and nuclear reactors.
C: Its presence in natural minerals.
D: Its wide use in chemical industries.
E: Its role in solar panels.
Answer: B

Question 6: What is common among elements with atomic numbers greater than 99?
A: They are used in medical treatments.
B: They are abundant in nature.
C: They have no uses outside of scientific research.
D: They have long half-lives.
E: They are primarily used in industries.
Answer: C

Question 7: All elements with an atomic number greater than 94...
A: Are found naturally on Earth.
B: Have existed on Earth since its formation.
C: Have long since decayed from when the Earth formed.
D: Are stable and don't decay.
E: Are found in the Earth's core.
Answer: C

Question 8: How is the atomic mass for synthetic elements determined?
A: Based on weighted average abundance of natural isotopes.
B: By counting the number of electrons.
C: Based on the total nucleon count of the most stable isotope.
D: By the number of protons only.
E: By the element's position in the periodic table.
Answer: C

Question 9: Synthetic elements now present on Earth are mainly products of...
A: Natural chemical reactions.
B: Atmospheric changes.
C: Cosmic radiations.
D: Atomic bombs or experiments involving nuclear reactors or particle accelerators.
E: Evolution over millions of years.
Answer: D

Question 10: Technetium, one of the elements first created artificially but later found in nature, was synthesized in which year?
A: 1930
B: 1937
C: 1940
D: 1944
E: 1950
Answer: B
@
A particle accelerator is a machine that uses electromagnetic fields to propel charged particles to very high speeds and energies, and to contain them in well-defined beams.[1][2]

Large accelerators are used for fundamental research in particle physics. The largest accelerator currently active is the Large Hadron Collider (LHC) near Geneva, Switzerland, operated by CERN. It is a collider accelerator, which can accelerate two beams of protons to an energy of 6.5 TeV and cause them to collide head-on, creating center-of-mass energies of 13 TeV. Other powerful accelerators are, RHIC at Brookhaven National Laboratory in New York and, formerly, the Tevatron at Fermilab, Batavia, Illinois. Accelerators are also used as synchrotron light sources for the study of condensed matter physics. Smaller particle accelerators are used in a wide variety of applications, including particle therapy for oncological purposes, radioisotope production for medical diagnostics, ion implanters for the manufacture of semiconductors, and accelerator mass spectrometers for measurements of rare isotopes such as radiocarbon. There are currently more than 30,000 accelerators in operation around the world.[3]

There are two basic classes of accelerators: electrostatic and electrodynamic (or electromagnetic) accelerators.[4] Electrostatic particle accelerators use static electric fields to accelerate particles. The most common types are the Cockcroft–Walton generator and the Van de Graaff generator. A small-scale example of this class is the cathode ray tube in an ordinary old television set. The achievable kinetic energy for particles in these devices is determined by the accelerating voltage, which is limited by electrical breakdown. Electrodynamic or electromagnetic accelerators, on the other hand, use changing electromagnetic fields (either magnetic induction or oscillating radio frequency fields) to accelerate particles. Since in these types the particles can pass through the same accelerating field multiple times, the output energy is not limited by the strength of the accelerating field. This class, which was first developed in the 1920s, is the basis for most modern large-scale accelerators.

Rolf Widerøe, Gustav Ising, Leó Szilárd, Max Steenbeck, and Ernest Lawrence are considered pioneers of this field, having conceived and built the first operational linear particle accelerator,[5] the betatron, and the cyclotron.

Because the target of the particle beams of early accelerators was usually the atoms of a piece of matter, with the goal being to create collisions with their nuclei in order to investigate nuclear structure, accelerators were commonly referred to as atom smashers in the 20th century.[6] The term persists despite the fact that many modern accelerators create collisions between two subatomic particles, rather than a particle and an atomic nucleus.[

Beams of high-energy particles are useful for fundamental and applied research in the sciences, and also in many technical and industrial fields unrelated to fundamental research.[10] It has been estimated that there are approximately 30,000 accelerators worldwide. Of these, only about 1% are research machines with energies above 1 GeV, while about 44% are for radiotherapy, 41% for ion implantation, 9% for industrial processing and research, and 4% for biomedical and other low-energy research.[11]
$
10
Question 1: What is the primary function of a particle accelerator?
A: To slow down charged particles.
B: To generate electrical energy.
C: To use electromagnetic fields to speed up and direct charged particles.
D: To create electromagnetic fields.
E: To study the behavior of neutral particles.
Answer: C

Question 2: Where is the largest active accelerator, the Large Hadron Collider (LHC), located?
A: New York, USA
B: Batavia, Illinois, USA
C: Geneva, Switzerland
D: Paris, France
E: Tokyo, Japan
Answer: C

Question 3: What is the primary application of the RHIC accelerator?
A: Production of semiconductors.
B: Oncological purposes.
C: Fundamental research in particle physics.
D: Radioisotope production.
E: Biomedical research.
Answer: C

Question 4: What are accelerators that use static electric fields to propel particles called?
A: Electrodynamic accelerators.
B: Cyclotrons.
C: Betatrons.
D: Electrostatic accelerators.
E: Linear accelerators.
Answer: D

Question 5: The cathode ray tube in an old television set is an example of which type of accelerator?
A: Large Hadron Collider.
B: Cyclotron.
C: Electrodynamic accelerator.
D: Electrostatic accelerator.
E: Betatron.
Answer: D

Question 6: Which type of accelerators allow particles to pass through the same accelerating field multiple times?
A: Electrostatic accelerators.
B: Electrodynamic or electromagnetic accelerators.
C: Cyclotrons.
D: Betatrons.
E: Linear accelerators.
Answer: B

Question 7: Who is NOT considered a pioneer in the field of particle accelerators?
A: Max Steenbeck.
B: Rolf Widerøe.
C: Ernest Lawrence.
D: Albert Einstein.
E: Gustav Ising.
Answer: D

Question 8: In the 20th century, what were accelerators commonly referred to due to their target?
A: Particle enhancers.
B: Atom creators.
C: Atom smashers.
D: Subatomic colliders.
E: Nucleus generators.
Answer: C

Question 9: What percentage of accelerators worldwide is used for ion implantation?
A: 1%
B: 4%
C: 9%
D: 41%
E: 44%
Answer: D

Question 10: Which accelerator was operational first?
A: Electrodynamic accelerator.
B: Linear particle accelerator.
C: Cyclotron.
D: Electrostatic accelerator.
E: Betatron.
Answer: B
@
Physics is the natural science of matter, involving the study of matter,[a] its fundamental constituents, its motion and behavior through space and time, and the related entities of energy and force.[2] Physics is one of the most fundamental scientific disciplines, with its main goal being to understand how the universe behaves.[b][3][4][5] A scientist who specializes in the field of physics is called a physicist.

Physics is one of the oldest academic disciplines and, through its inclusion of astronomy, perhaps the oldest.[6] Over much of the past two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the Scientific Revolution in the 17th century these natural sciences emerged as unique research endeavors in their own right.[c] Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences[3] and suggest new avenues of research in these and other academic disciplines such as mathematics and philosophy.

Advances in physics often enable new technologies. For example, advances in the understanding of electromagnetism, solid-state physics, and nuclear physics led directly to the development of new products that have dramatically transformed modern-day society, such as television, computers, domestic appliances, and nuclear weapons;[3] advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus.
$
10
Question: What is the primary objective of physics?
A: To predict future events.
B: To understand how the universe behaves.
C: To explore the depths of the ocean.
D: To study biological systems.
E: To classify elements.
Answer: B

Question: What is the title given to a scientist who specializes in physics?
A: Chemist
B: Biologist
C: Mathematician
D: Philosopher
E: Physicist
Answer: E

Question: Physics is one of the oldest academic disciplines and, through its inclusion, is perhaps the oldest of what?
A: Philosophy
B: Mathematics
C: Astronomy
D: Biology
E: Chemistry
Answer: C

Question: During which period did natural sciences like physics, chemistry, and biology emerge as unique research endeavors?
A: Renaissance
B: Middle Ages
C: Scientific Revolution in the 17th century
D: Industrial Revolution
E: Age of Discovery
Answer: C

Question: Physics intersects with many interdisciplinary areas of research. Which of the following is NOT one of these areas?
A: Biophysics
B: Quantum chemistry
C: Geology
D: Quantum mechanics
E: Astronomy
Answer: C

Question: Which of the following is a direct consequence of advances in physics?
A: The creation of literature
B: Development of new dance forms
C: The establishment of new languages
D: The development of television and computers
E: The discovery of new planets
Answer: D

Question: Advances in what field of physics led to the development of industrialization?
A: Quantum mechanics
B: Electromagnetism
C: Thermodynamics
D: Solid-state physics
E: Nuclear physics
Answer: C

Question: Which field of study was inspired by advances in mechanics?
A: Biology
B: Astronomy
C: Literature
D: Calculus
E: Quantum mechanics
Answer: D

Question: Over the past two millennia, physics, chemistry, biology, and some branches of mathematics were part of what?
A: Quantum studies
B: Interdisciplinary sciences
C: Natural philosophy
D: Classical studies
E: Modern sciences
Answer: C

Question: New ideas in physics often explain fundamental mechanisms in which of the following ways?
A: By rejecting all previous scientific theories
B: By contradicting other sciences
C: By suggesting new avenues of research in academic disciplines
D: By limiting the scope of mathematical studies
E: By focusing solely on astronomical events
Answer: C
@
Physical causality is a physical relationship between causes and effects.[1][2] It is considered to be fundamental to all natural sciences and behavioural sciences, especially physics. Causality is also a topic studied from the perspectives of philosophy, statistics and logic. Causality means that an effect can not occur from a cause that is not in the back (past) light cone of that event. Similarly, a cause can not have an effect outside its front (future) light cone.

As a physical concept
In classical physics, an effect cannot occur before its cause which is why solutions such as the advanced time solutions of the Liénard–Wiechert potential are discarded as physically meaningless. In both Einstein's theory of special and general relativity, causality means that an effect cannot occur from a cause that is not in the back (past) light cone of that event. Similarly, a cause cannot have an effect outside its front (future) light cone. These restrictions are consistent with the constraint that mass and energy that act as causal influences cannot travel faster than the speed of light and/or backwards in time. In quantum field theory, observables of events with a spacelike relationship, "elsewhere", have to commute, so the order of observations or measurements of such observables do not impact each other.

Another requirement of causality is that cause and effect be mediated across space and time (requirement of contiguity). This requirement has been very influential in the past, in the first place as a result of direct observation of causal processes (like pushing a cart), in the second place as a problematic aspect of Newton's theory of gravitation (attraction of the earth by the sun by means of action at a distance) replacing mechanistic proposals like Descartes' vortex theory; in the third place as an incentive to develop dynamic field theories (e.g., Maxwell's electrodynamics and Einstein's general theory of relativity) restoring contiguity in the transmission of influences in a more successful way than in Descartes' theory.

In modern physics, the notion of causality had to be clarified. The word simultaneous is observer-dependent in special relativity.[3] The principle is relativity of simultaneity. Consequently, the relativistic principle of causality says that the cause must precede its effect according to all inertial observers. This is equivalent to the statement that the cause and its effect are separated by a timelike interval, and the effect belongs to the future of its cause. If a timelike interval separates the two events, this means that a signal could be sent between them at less than the speed of light. On the other hand, if signals could move faster than the speed of light, this would violate causality because it would allow a signal to be sent across spacelike intervals, which means that at least to some inertial observers the signal would travel backward in time. For this reason, special relativity does not allow communication faster than the speed of light.

In the theory of general relativity, the concept of causality is generalized in the most straightforward way: the effect must belong to the future light cone of its cause, even if the spacetime is curved. New subtleties must be taken into account when we investigate causality in quantum mechanics and relativistic quantum field theory in particular. In those two theories, causality is closely related to the principle of locality. However, the principle of locality is disputed: whether it strictly holds depends on the interpretation of quantum mechanics chosen, especially for experiments involving quantum entanglement that satisfy Bell's Theorem.

Despite these subtleties, causality remains an important and valid concept in physical theories. For example, the notion that events can be ordered into causes and effects is necessary to prevent (or at least outline) causality paradoxes such as the grandfather paradox, which asks what happens if a time-traveler kills his own grandfather before he ever meets the time-traveler's grandmother. See also Chronology protection conjecture.
$
10
Question: Physical causality relates to the relationship between what two entities?
A: Causes and actions
B: Action and reactions
C: Causes and effects
D: Effects and results
E: Action and consequences
Answer: C

Question: In classical physics, can an effect occur before its cause?
A: Always
B: Sometimes
C: Rarely
D: Never
E: Only in specific conditions
Answer: D

Question: According to Einstein's theory of relativity, a cause cannot have an effect outside its what?
A: Spacetime
B: Future light cone
C: Past light cone
D: Present moment
E: Physical boundary
Answer: B

Question: In quantum field theory, if two events have a spacelike relationship, what must their observables do?
A: Interact
B: Cancel out
C: Commute
D: Collide
E: Exist simultaneously
Answer: C

Question: The requirement that cause and effect be mediated across space and time is also known as what?
A: Requirement of continuity
B: Requirement of simultaneity
C: Requirement of contiguity
D: Requirement of concurrency
E: Requirement of coexistence
Answer: C

Question: Newton's theory of gravitation, which involved action at a distance, replaced which theory?
A: Einstein's theory of relativity
B: Descartes' vortex theory
C: Maxwell's electrodynamics
D: Quantum field theory
E: Liénard–Wiechert potential
Answer: B

Question: In special relativity, what is observer-dependent?
A: Causality
B: Simultaneity
C: Quantum entanglement
D: Time-travel
E: Contiguity
Answer: B

Question: If signals could move faster than the speed of light, what would it violate?
A: Quantum entanglement
B: Principle of simultaneity
C: Principle of contiguity
D: Causality
E: Principle of locality
Answer: D

Question: In the theory of general relativity, the effect must belong to what of its cause?
A: The past light cone
B: The future shadow
C: The present spectrum
D: The future light cone
E: The past radius
Answer: D

Question: The grandfather paradox revolves around which concept?
A: Simultaneity
B: Quantum entanglement
C: Time-travel
D: Principle of locality
E: Quantum field theory
Answer: C
@
Continuous spacetime symmetries are symmetries involving transformations of space and time. These may be further classified as spatial symmetries, involving only the spatial geometry associated with a physical system; temporal symmetries, involving only changes in time; or spatio-temporal symmetries, involving changes in both space and time.

Time translation: A physical system may have the same features over a certain interval of time Δt; this is expressed mathematically as invariance under the transformation t → t + a for any real parameters t and t + a in the interval. For example, in classical mechanics, a particle solely acted upon by gravity will have gravitational potential energy mgh when suspended from a height h above the Earth's surface. Assuming no change in the height of the particle, this will be the total gravitational potential energy of the particle at all times. In other words, by considering the state of the particle at some time t0 and also at t0 + a, the particle's total gravitational potential energy will be preserved.
Spatial translation: These spatial symmetries are represented by transformations of the form r→ → r→ + a→ and describe those situations where a property of the system does not change with a continuous change in location. For example, the temperature in a room may be independent of where the thermometer is located in the room.
Spatial rotation: These spatial symmetries are classified as proper rotations and improper rotations. The former are just the 'ordinary' rotations; mathematically, they are represented by square matrices with unit determinant. The latter are represented by square matrices with determinant −1 and consist of a proper rotation combined with a spatial reflection (inversion). For example, a sphere has proper rotational symmetry. Other types of spatial rotations are described in the article Rotation symmetry.
Poincaré transformations: These are spatio-temporal symmetries which preserve distances in Minkowski spacetime, i.e. they are isometries of Minkowski space. They are studied primarily in special relativity. Those isometries that leave the origin fixed are called Lorentz transformations and give rise to the symmetry known as Lorentz covariance.
Projective symmetries: These are spatio-temporal symmetries which preserve the geodesic structure of spacetime. They may be defined on any smooth manifold, but find many applications in the study of exact solutions in general relativity.
Inversion transformations: These are spatio-temporal symmetries which generalise Poincaré transformations to include other conformal one-to-one transformations on the space-time coordinates. Lengths are not invariant under inversion transformations but there is a cross-ratio on four points that is invariant.
$
10
Question: Which type of symmetry involves only changes in the spatial geometry associated with a physical system?
A: Spatial translation
B: Time translation
C: Poincaré transformations
D: Projective symmetries
E: Temporal symmetries
Answer: A

Question: When does a physical system have time translation symmetry?
A: When it preserves geodesic structure of spacetime
B: When it has the same features over a certain interval of time Δt
C: When it preserves distances in Minkowski spacetime
D: When it involves only changes in time
E: When there is a cross-ratio on four points that is invariant
Answer: B

Question: What type of symmetry describes situations where a property of the system remains constant despite a continuous change in location?
A: Spatial translation
B: Spatial rotation
C: Temporal symmetries
D: Poincaré transformations
E: Projective symmetries
Answer: A

Question: Which symmetry is represented by square matrices with unit determinant?
A: Proper rotations
B: Improper rotations
C: Poincaré transformations
D: Lorentz transformations
E: Spatial translation
Answer: A

Question: What transformation combines a proper rotation with a spatial reflection?
A: Spatial rotation
B: Spatial translation
C: Improper rotation
D: Poincaré transformations
E: Projective symmetries
Answer: C

Question: What type of symmetries are primarily studied in special relativity and preserve distances in Minkowski spacetime?
A: Time translation
B: Spatial rotation
C: Poincaré transformations
D: Inversion transformations
E: Projective symmetries
Answer: C

Question: What is another term for the isometries that leave the origin fixed in Minkowski spacetime?
A: Temporal symmetries
B: Spatial translation
C: Proper rotations
D: Lorentz transformations
E: Improper rotations
Answer: D

Question: Which symmetries preserve the geodesic structure of spacetime?
A: Poincaré transformations
B: Projective symmetries
C: Spatial rotation
D: Inversion transformations
E: Time translation
Answer: B

Question: What is not invariant under inversion transformations?
A: Time
B: Proper rotations
C: The geodesic structure of spacetime
D: Lengths
E: Space-time coordinates
Answer: D

Question: Which symmetry has an invariant cross-ratio on four points?
A: Spatial rotation
B: Temporal symmetries
C: Poincaré transformations
D: Inversion transformations
E: Projective symmetries
Answer: D
@
In physics, a symmetry of a physical system is a physical or mathematical feature of the system (observed or intrinsic) that is preserved or remains unchanged under some transformation.

A family of particular transformations may be continuous (such as rotation of a circle) or discrete (e.g., reflection of a bilaterally symmetric figure, or rotation of a regular polygon). Continuous and discrete transformations give rise to corresponding types of symmetries. Continuous symmetries can be described by Lie groups while discrete symmetries are described by finite groups (see Symmetry group).

These two concepts, Lie and finite groups, are the foundation for the fundamental theories of modern physics. Symmetries are frequently amenable to mathematical formulations such as group representations and can, in addition, be exploited to simplify many problems.

Arguably the most important example of a symmetry in physics is that the speed of light has the same value in all frames of reference, which is described in special relativity by a group of transformations of the spacetime known as the Poincaré group. Another important example is the invariance of the form of physical laws under arbitrary differentiable coordinate transformations, which is an important idea in general relativity.

As a kind of invariance
Invariance is specified mathematically by transformations that leave some property (e.g. quantity) unchanged. This idea can apply to basic real-world observations. For example, temperature may be homogeneous throughout a room. Since the temperature does not depend on the position of an observer within the room, we say that the temperature is invariant under a shift in an observer's position within the room.

Similarly, a uniform sphere rotated about its center will appear exactly as it did before the rotation. The sphere is said to exhibit spherical symmetry. A rotation about any axis of the sphere will preserve how the sphere "looks".
$
10
Question: What remains unchanged under a symmetry in a physical system?
A: A mathematical model
B: A transformation type
C: A physical or mathematical feature
D: A Lie group representation
E: The type of transformation
Answer: C

Question: What type of transformations might a bilaterally symmetric figure undergo?
A: Rotation of a circle
B: Transformation of spacetime
C: Shift in observer's position
D: Reflection
E: Rotation about any axis
Answer: D

Question: How are continuous symmetries typically described?
A: Finite groups
B: Poincaré groups
C: Differentiable coordinate transformations
D: Lie groups
E: Basic real-world observations
Answer: D

Question: Which kind of group is used to describe discrete symmetries?
A: Lie groups
B: Poincaré groups
C: Continuous groups
D: Differentiable groups
E: Finite groups
Answer: E

Question: What is the basis for the fundamental theories of modern physics?
A: Continuous and discrete transformations
B: Group representations
C: Lie and finite groups
D: Invariance under shift
E: The speed of light
Answer: C

Question: In special relativity, which group of transformations describes the symmetry that the speed of light remains constant in all frames of reference?
A: Lie group
B: Finite group
C: Continuous group
D: Poincaré group
E: General relativity group
Answer: D

Question: Invariance in physics is specified by transformations that leave what unchanged?
A: Symmetry group
B: Method of observation
C: Fundamental theory
D: Some property or quantity
E: Type of transformation
Answer: D

Question: If the temperature is consistent throughout a space, what kind of invariance is it demonstrating?
A: Spherical symmetry
B: Bilateral symmetry
C: Positional shift of an observer
D: Continuous transformation
E: Discrete transformation
Answer: C

Question: When a uniform sphere is rotated about its center and appears the same as before, it exhibits which symmetry?
A: Continuous symmetry
B: Bilateral symmetry
C: Poincaré symmetry
D: Spherical symmetry
E: Lie group symmetry
Answer: D

Question: What is an example of a symmetry that relates to the form of physical laws being consistent regardless of coordinate transformations?
A: Lie group symmetry
B: Rotation of a polygon
C: Special relativity symmetry
D: Reflection of a symmetric figure
E: Idea in general relativity
Answer: E
@
Particles can also be classified according to composition. Composite particles refer to particles that have composition – that is particles which are made of other particles.[16] For example, a carbon-14 atom is made of six protons, eight neutrons, and six electrons. By contrast, elementary particles (also called fundamental particles) refer to particles that are not made of other particles.[17] According to our current understanding of the world, only a very small number of these exist, such as leptons, quarks, and gluons. However it is possible that some of these might turn up to be composite particles after all, and merely appear to be elementary for the moment.[18] While composite particles can very often be considered point-like, elementary particles are truly punctual.[19]

Stability
Both elementary (such as muons) and composite particles (such as uranium nuclei), are known to undergo particle decay. Those that do not are called stable particles, such as the electron or a helium-4 nucleus. The lifetime of stable particles can be either infinite or large enough to hinder attempts to observe such decays. In the latter case, those particles are called "observationally stable". In general, a particle decays from a high-energy state to a lower-energy state by emitting some form of radiation, such as the emission of photons.
$
10
Question: What are composite particles made of?
A: Photons
B: High-energy states
C: Other particles
D: Radiation
E: Stable particles
Answer: C

Question: What term is used to describe particles that are not made of other particles?
A: Punctual particles
B: Composite particles
C: Stable particles
D: Observationally stable particles
E: Elementary particles
Answer: E

Question: How are elementary particles different in terms of composition when compared to composite particles?
A: They decay faster
B: They are made of other particles
C: They emit more radiation
D: They are not made of other particles
E: They can never decay
Answer: D

Question: According to the current understanding, which among the following is considered an elementary particle?
A: Carbon-14 atom
B: Helium-4 nucleus
C: Muon
D: Proton
E: Neutron
Answer: C

Question: What can be said about the size of elementary particles compared to composite particles?
A: Both are point-like
B: Composite particles are truly punctual
C: Elementary particles can be larger
D: Elementary particles are truly punctual
E: Both are of the same size
Answer: D

Question: What term is given to particles that do not undergo decay?
A: High-energy particles
B: Emissive particles
C: Observationally stable particles
D: Stable particles
E: Composite particles
Answer: D

Question: What characterizes "observationally stable" particles?
A: They never decay
B: They decay rapidly
C: Their decay cannot be observed due to an extremely long lifetime
D: They emit high amounts of radiation
E: They are all composite particles
Answer: C

Question: In general, why does a particle decay?
A: It undergoes fusion
B: It moves from a low-energy state to a high-energy state
C: It absorbs radiation
D: It moves from a high-energy state to a lower-energy state
E: It splits into other elementary particles
Answer: D

Question: During particle decay, what form of radiation might a particle emit?
A: Neutrons
B: Protons
C: Photons
D: Electrons
E: Quarks
Answer: C

Question: Which of the following is NOT an example of an elementary particle?
A: Helium-4 nucleus
B: Lepton
C: Quark
D: Gluon
E: Muon
Answer: A
@
The Standard Model of particle physics is the theory describing three of the four known fundamental forces (electromagnetic, weak and strong interactions – excluding gravity) in the universe and classifying all known elementary particles. It was developed in stages throughout the latter half of the 20th century, through the work of many scientists worldwide,[1] with the current formulation being finalized in the mid-1970s upon experimental confirmation of the existence of quarks. Since then, proof of the top quark (1995), the tau neutrino (2000), and the Higgs boson (2012) have added further credence to the Standard Model. In addition, the Standard Model has predicted various properties of weak neutral currents and the W and Z bosons with great accuracy.

Although the Standard Model is believed to be theoretically self-consistent[note 1] and has demonstrated some success in providing experimental predictions, it leaves some physical phenomena unexplained and so falls short of being a complete theory of fundamental interactions.[3] For example, it does not fully explain baryon asymmetry, incorporate the full theory of gravitation[4] as described by general relativity, or account for the universe's accelerating expansion as possibly described by dark energy. The model does not contain any viable dark matter particle that possesses all of the required properties deduced from observational cosmology. It also does not incorporate neutrino oscillations and their non-zero masses.

The development of the Standard Model was driven by theoretical and experimental particle physicists alike. The Standard Model is a paradigm of a quantum field theory for theorists, exhibiting a wide range of phenomena, including spontaneous symmetry breaking, anomalies, and non-perturbative behavior. It is used as a basis for building more exotic models that incorporate hypothetical particles, extra dimensions, and elaborate symmetries (such as supersymmetry) to explain experimental results at variance with the Standard Model, such as the existence of dark matter and neutrino oscillations.
$
10
Question: Which fundamental force is NOT described by the Standard Model?
A: Electromagnetic force
B: Weak interaction
C: Strong interaction
D: Gravitational force
E: Neutral force
Answer: D

Question: When was the current formulation of the Standard Model finalized?
A: Early 20th century
B: Late 20th century
C: Mid-1970s
D: 1995
E: 2012
Answer: C

Question: Which particle's existence was confirmed in 2012 that added further support to the Standard Model?
A: Top quark
B: Tau neutrino
C: W boson
D: Z boson
E: Higgs boson
Answer: E

Question: What does the Standard Model not fully explain?
A: The existence of the top quark
B: Properties of the W and Z bosons
C: Baryon asymmetry
D: The prediction of weak neutral currents
E: Properties of the Higgs boson
Answer: C

Question: Which theory related to gravitation is not fully incorporated into the Standard Model?
A: Quantum mechanics
B: Special relativity
C: Quantum field theory
D: Electromagnetic theory
E: General relativity
Answer: E

Question: Which of the following phenomena related to the universe's expansion is not accounted for by the Standard Model?
A: Gravitational waves
B: Black holes
C: Dark matter
D: Accelerating expansion possibly due to dark energy
E: Spontaneous symmetry breaking
Answer: D

Question: What does the Standard Model not contain with regard to dark matter?
A: A particle that has all the properties from observational cosmology
B: An explanation for dark energy
C: A description of black holes
D: A theory for gravitational waves
E: An understanding of quantum fluctuations
Answer: A

Question: Which phenomenon related to neutrinos is not incorporated in the Standard Model?
A: Neutrino emission
B: Neutrino absorption
C: Neutrino symmetry
D: Neutrino oscillations with non-zero masses
E: Neutrino field theories
Answer: D

Question: The Standard Model serves as a foundation for building models that consider:
A: Historical particles
B: Supernovas
C: Black holes
D: Extra dimensions and hypothetical particles
E: Stellar phenomena
Answer: D

Question: What type of symmetry is explored in exotic models that deviate from the Standard Model to explain certain experimental results?
A: Electromagnetic symmetry
B: Gravitational symmetry
C: Weak force symmetry
D: Spontaneous symmetry
E: Supersymmetry
Answer: E
@
In physical cosmology, the age of the universe is the time elapsed since the Big Bang. Astronomers have derived two different measurements of the age of the universe:[1] a measurement based on direct observations of an early state of the universe, which indicate an age of 13.787±0.020 billion years as interpreted with the Lambda-CDM concordance model as of 2021;[2] and a measurement based on the observations of the local, modern universe, which suggest a younger age.[3][4][5] The uncertainty of the first kind of measurement has been narrowed down to 20 million years, based on a number of studies that all show similar figures for the age. These studies include researches of the microwave background radiation by the Planck spacecraft, the Wilkinson Microwave Anisotropy Probe and other space probes. Measurements of the cosmic background radiation give the cooling time of the universe since the Big Bang,[6] and measurements of the expansion rate of the universe can be used to calculate its approximate age by extrapolating backwards in time. The range of the estimate is also within the range of the estimate for the oldest observed star in the universe.
$
10
Question: What is the primary event marking the beginning of the universe in physical cosmology?
A: The Black Hole Event
B: The Universal Expansion
C: The Cosmic Contraction
D: The Big Freeze
E: The Big Bang
Answer: E

Question: As of 2021, based on direct observations of an early state of the universe, what is the estimated age of the universe according to the Lambda-CDM concordance model?
A: 13.567±0.020 billion years
B: 12.787±0.020 billion years
C: 14.787±0.020 billion years
D: 13.787±0.020 billion years
E: 15.787±0.020 billion years
Answer: D

Question: What is the uncertainty range of the first measurement method?
A: 50 million years
B: 30 million years
C: 10 million years
D: 20 million years
E: 40 million years
Answer: D

Question: Which spacecraft provided data on the microwave background radiation that contributed to determining the age of the universe?
A: Hubble Space Telescope
B: Kepler Spacecraft
C: Planck spacecraft
D: Voyager 1
E: Mars Rover
Answer: C

Question: The Wilkinson Microwave Anisotropy Probe was involved in research related to what aspect of the universe?
A: The study of black holes
B: The research on universal contraction
C: The study of exoplanets
D: Researches of the microwave background radiation
E: Observations of distant galaxies
Answer: D

Question: Measurements of the cosmic background radiation primarily provide information on:
A: The density of stars in the universe
B: The temperature fluctuations in distant galaxies
C: The cooling time of the universe since the Big Bang
D: The number of black holes in the universe
E: The rate of star formation in the Milky Way
Answer: C

Question: By observing the expansion rate of the universe, scientists can:
A: Predict the number of galaxies in the universe
B: Determine the density of dark matter
C: Calculate its approximate age by extrapolating backwards in time
D: Calculate the future rate of cosmic expansion
E: Predict the eventual contraction of the universe
Answer: C

Question: The estimated age of the universe based on the first method of measurement is:
A: Older than the oldest observed star
B: The same as the age of the Milky Way galaxy
C: Younger than the oldest observed star
D: Within the range of the estimate for the oldest observed star
E: Inconsistent with any observed cosmic phenomena
Answer: D

Question: Observations of the local, modern universe suggest:
A: An older age of the universe than direct observations of an early state
B: The same age as direct observations of an early state
C: A younger age of the universe than direct observations of an early state
D: An inconclusive age of the universe
E: A contradictory age that does not match any models
Answer: C

Question: Which of the following is NOT mentioned as a source or method to determine the age of the universe?
A: Study of universal contractions
B: Observations of the expansion rate
C: Measurements of the cosmic background radiation
D: Data from the Planck spacecraft
E: Wilkinson Microwave Anisotropy Probe research
Answer: A
@
The first reasonably accurate measurement of the rate of expansion of the universe, a numerical value now known as the Hubble constant, was made in 1958 by astronomer Allan Sandage.[9] His measured value for the Hubble constant came very close to the value range generally accepted today.

Sandage, like Einstein, did not believe his own results at the time of discovery. Sandage proposed new theories of cosmogony to explain this discrepancy. This issue was more or less resolved by improvements in the theoretical models used for estimating the ages of stars. As of 2013, using the latest models for stellar evolution, the estimated age of the oldest known star is 14.46±0.8 billion years.[10]

The discovery of microwave cosmic background radiation announced in 1965[11] finally brought an effective end to the remaining scientific uncertainty over the expanding universe. It was a chance result from work by two teams less than 60 miles apart. In 1964, Arno Penzias and Robert Wilson were trying to detect radio wave echoes with a supersensitive antenna. The antenna persistently detected a low, steady, mysterious noise in the microwave region that was evenly spread over the sky, and was present day and night. After testing, they became certain that the signal did not come from the Earth, the Sun, or our galaxy, but from outside our own galaxy, but could not explain it. At the same time another team, Robert H. Dicke, Jim Peebles, and David Wilkinson, were attempting to detect low level noise that might be left over from the Big Bang and could prove whether the Big Bang theory was correct. The two teams realized that the detected noise was in fact radiation left over from the Big Bang, and that this was strong evidence that the theory was correct. Since then, a great deal of other evidence has strengthened and confirmed this conclusion, and refined the estimated age of the universe to its current figure.

The space probes WMAP, launched in 2001, and Planck, launched in 2009, produced data that determines the Hubble constant and the age of the universe independent of galaxy distances, removing the largest source of error.[12]
$
10
Question: What numerical value did Allan Sandage accurately measure in 1958?
A: The age of the oldest star
B: The microwave cosmic background radiation
C: The rate of expansion of the universe
D: The distance between galaxies
E: The density of the universe
Answer: C

Question: How did Sandage initially react to his own discovery?
A: He readily accepted his results.
B: He questioned the accuracy of his tools.
C: He was indifferent to his findings.
D: He did not believe his own results.
E: He immediately proposed it to the scientific community.
Answer: D

Question: Using the latest models for stellar evolution as of 2013, what is the estimated age of the oldest known star?
A: 12.46±0.8 billion years
B: 13.46±0.8 billion years
C: 14.46±0.8 billion years
D: 15.46±0.8 billion years
E: 16.46±0.8 billion years
Answer: C

Question: In 1965, what discovery played a crucial role in settling scientific uncertainties about the expanding universe?
A: Detection of gravitational waves
B: Discovery of black holes
C: Detection of gamma-ray bursts
D: Discovery of microwave cosmic background radiation
E: Observation of a supermassive black hole
Answer: D

Question: Who were trying to detect radio wave echoes with a supersensitive antenna in 1964?
A: Robert H. Dicke and Jim Peebles
B: David Wilkinson and Allan Sandage
C: Arno Penzias and Robert Wilson
D: Einstein and Sandage
E: Peebles and Wilkinson
Answer: C

Question: The mysterious noise detected by Penzias and Wilson was believed to originate from:
A: Within our galaxy
B: The Sun
C: The Earth
D: Outside our galaxy
E: The Milky Way's black hole
Answer: D

Question: What were Robert H. Dicke, Jim Peebles, and David Wilkinson aiming to detect?
A: Gravitational waves
B: Radiation from distant galaxies
C: Noise potentially left over from the Big Bang
D: Signals from extraterrestrial life
E: Pulsar frequencies
Answer: C

Question: The detection of the low, steady noise in the microwave region provided evidence for which theory?
A: The Steady State theory
B: The Big Crunch theory
C: The Oscillating Universe theory
D: The Big Freeze theory
E: The Big Bang theory
Answer: E

Question: What was the primary purpose of the space probes WMAP and Planck?
A: To study black holes in the universe
B: To detect potential alien signals
C: To measure the density of stars
D: To determine the Hubble constant and the age of the universe
E: To explore the possibility of life on other planets
Answer: D

Question: Which of the following was NOT a source of error that WMAP and Planck aimed to eliminate?
A: Uncertainty in radiation detection
B: Error in measuring the density of black holes
C: Galaxy distances
D: Inaccuracies in the previous tools
E: Variations in gravitational force measurements
Answer: B
@
The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[10][11] The synonym self-teaching computers was also used in this time period.[12][13]

By the early 1960s an experimental "learning machine" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively "trained" by a human operator/teacher to recognize patterns and equipped with a "goof" button to cause it to re-evaluate incorrect decisions.[14] A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[15] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[16] In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[17]

Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."[18] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper "Computing Machinery and Intelligence", in which the question "Can machines think?" is replaced with the question "Can machines do what we (as thinking entities) can do?".[19]

Modern-day machine learning has two objectives, one is to classify data based on models which have been developed, the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.[20]
$
10
Question: Who coined the term "machine learning"?
A: Alan Turing
B: Tom M. Mitchell
C: Duda and Hart
D: Arthur Samuel
E: Nilsson
Answer: D

Question: What was the synonym used for machine learning during its early period?
A: Data classification
B: Reinforcement training
C: Self-teaching computers
D: Neural networking
E: Cognitive computing
Answer: C

Question: What was the purpose of the experimental "learning machine" called Cybertron developed by Raytheon Company in the 1960s?
A: Data storage
B: Stock trading prediction
C: Analyzing sonar signals, electrocardiograms, and speech patterns
D: Classifying computer viruses
E: Game development
Answer: C

Question: Who authored the book "Learning Machines" that focused primarily on machine learning for pattern classification in the 1960s?
A: Tom M. Mitchell
B: Duda and Hart
C: Arthur Samuel
D: Alan Turing
E: Nilsson
Answer: E

Question: In 1981, a report described teaching a neural network to recognize how many characters from a computer terminal?
A: 20 characters
B: 25 characters
C: 30 characters
D: 40 characters
E: 50 characters
Answer: D

Question: Tom M. Mitchell's formal definition of machine learning revolves around which key elements?
A: Experience, tasks, and performance measure
B: Learning, data, and algorithms
C: Computers, humans, and data
D: Patterns, recognition, and classification
E: Networks, systems, and recognition
Answer: A

Question: Alan Turing proposed replacing the question "Can machines think?" with:
A: Why do machines think?
B: How do machines think?
C: Can machines learn from us?
D: What can machines think about?
E: Can machines do what we (as thinking entities) can do?
Answer: E

Question: What are the two primary objectives of modern-day machine learning?
A: Data storage and retrieval
B: Pattern recognition and data sorting
C: Classifying data and making predictions for future outcomes
D: Supervised and unsupervised learning
E: Reinforcement learning and neural networking
Answer: C

Question: A hypothetical algorithm that uses computer vision of moles along with supervised learning aims to:
A: Predict the growth rate of moles
B: Classify the cancerous moles
C: Analyze the age of moles
D: Measure the size of moles
E: Study the genetic makeup of moles
Answer: B

Question: What is a potential application of a machine learning algorithm in the finance sector?
A: Designing new currencies
B: Data storage of bank transactions
C: Informing a trader of future potential predictions in stock trading
D: Analyzing customer support interactions
E: Designing banking software interfaces
Answer: C
@
As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[22] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[23]: 488 

However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[23]: 488  By 1980, expert systems had come to dominate AI, and statistics was out of favor.[24] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[23]: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[23]: 25 

Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[24]
$
10
Question: Machine learning originally grew out of which scientific quest?
A: Artificial general intelligence
B: Quantum computing
C: Artificial intelligence (AI)
D: Cybernetics
E: Bioinformatics
Answer: C

Question: In the early stages of AI, what models were mostly perceptrons and were later identified as reinventions from statistics?
A: Fuzzy logic systems
B: Expert systems
C: Generalized linear models
D: Inductive logic programming
E: Symbolic methods
Answer: C

Question: Which reasoning was especially employed in automated medical diagnosis in the early days?
A: Inductive reasoning
B: Symbolic reasoning
C: Fuzzy logic reasoning
D: Probabilistic reasoning
E: Deductive reasoning
Answer: D

Question: By 1980, which system dominated AI, leading to a decline in interest in statistics?
A: Neural networks
B: Fuzzy logic systems
C: Expert systems
D: Connectionism
E: Probabilistic systems
Answer: C

Question: What was a primary challenge faced by probabilistic systems?
A: Speed of computation
B: Data acquisition and representation
C: Integration with neural networks
D: Interaction with symbolic methods
E: Compatibility with inductive logic programming
Answer: B

Question: Who were among the researchers that continued the line of research on "connectionism"?
A: Hopfield, Rumelhart, and Hinton
B: Turing, Shannon, and Neumann
C: Samuel, Nilsson, and Mitchell
D: Einstein, Feynman, and Dirac
E: Turing, Hopfield, and Samuel
Answer: A

Question: What significant development in connectionism took place in the mid-1980s?
A: Development of fuzzy logic
B: Introduction of expert systems
C: Reinvention of backpropagation
D: Inception of inductive logic programming
E: Creation of probabilistic systems
Answer: C

Question: In the 1990s, machine learning shifted its focus from achieving AI to:
A: Reinventing neural networks
B: Expanding on probabilistic systems
C: Tackling solvable, practical problems
D: Achieving quantum computing
E: Perfecting symbolic methods
Answer: C

Question: From which areas did machine learning start borrowing methods and models in the 1990s?
A: Cybernetics, quantum physics, and chemistry
B: Biology, genetics, and bioinformatics
C: Statistics, fuzzy logic, and probability theory
D: Calculus, geometry, and linear algebra
E: Neurology, psychology, and cognitive science
Answer: C

Question: Neural networks research was abandoned by AI and computer science and was continued as "connectionism" by disciplines outside AI/CS around:
A: Early 1970s
B: Late 1970s
C: Early 1980s
D: Mid-1980s
E: Late 1980s
Answer: C
@
As a machine-learning algorithm, backpropagation performs a backward pass to adjust a neural network model's parameters, aiming to minimize the mean squared error (MSE).[1][2] In a multi-layered network, backpropagation uses the following steps:

Propagate training data through the model from input to predicted output by computing the successive hidden layers' outputs and finally the final layer's output (the feedforward step).
Adjust the model weights to reduce the error relative to the weights.
The error is typically the squared difference between prediction and target.
For each weight, the slope or derivative of the error is found, and the weight adjusted by a negative multiple of this derivative, so as to go downslope toward the minimum-error configuration.
This derivative is easy to calculate for final layer weights, and possible to calculate for one layer given the next layer's derivatives. Starting at the end, then, the derivatives are calculated layer by layer toward the beginning -- thus "backpropagation".
Repeatedly update the weights until they converge or the model has undergone enough iterations.
It is an efficient application of the Leibniz chain rule (1673)[3] to such networks.[4] It is also known as the reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970).[5][6][7][8][9][10][11] The term "back-propagating error correction" was introduced in 1962 by Frank Rosenblatt,[12][4] but he did not know how to implement this, even though Henry J. Kelley had a continuous precursor of backpropagation[13] already in 1960 in the context of control theory.[4]
$
10
Question: What is the primary objective of backpropagation in a neural network model?
A: To maximize the output values.
B: To minimize the variance.
C: To reduce computation time.
D: To minimize the mean squared error (MSE).
E: To increase the complexity of the model.
Answer: D

Question: In a multi-layered network, what is the first step of backpropagation?
A: Adjust the model weights.
B: Calculate the error.
C: Propagate training data through the model from input to predicted output.
D: Update the weights repeatedly.
E: Compute the derivative of the error.
Answer: C

Question: How is the error typically represented in the context of backpropagation?
A: Sum of absolute differences.
B: Difference between prediction and target squared.
C: Ratio of prediction to target.
D: Maximum difference between prediction and target.
E: Mean of prediction values.
Answer: B

Question: For adjusting the model weights in backpropagation, what is considered regarding each weight?
A: The sum of weights.
B: The squared value of the weight.
C: The slope or derivative of the error.
D: The inverse of the weight.
E: The logarithm of the weight.
Answer: C

Question: Why is the term "backpropagation" used?
A: Weights are propagated from the beginning to the end.
B: Error correction is always backwards.
C: Weights are adjusted from the last layer to the first.
D: Derivatives are calculated layer by layer starting from the end toward the beginning.
E: It was a term given due to its reverse accumulation method.
Answer: D

Question: To which rule, dating back to 1673, is backpropagation an efficient application?
A: Newton's law
B: Pythagoras theorem
C: Bayes' theorem
D: Leibniz chain rule
E: Fermat's last theorem
Answer: D

Question: What is another term for backpropagation due to Seppo Linnainmaa?
A: Forward differentiation
B: Reverse accumulation
C: Sideways propagation
D: Differential backtracking
E: Inverse calculation
Answer: B

Question: Who introduced the term "back-propagating error correction" in 1962?
A: Seppo Linnainmaa
B: Leibniz
C: Frank Rosenblatt
D: Henry J. Kelley
E: James Maxwell
Answer: C

Question: What did Frank Rosenblatt not know regarding "back-propagating error correction"?
A: The significance of the method.
B: How to teach it.
C: How to implement it.
D: How to document it.
E: The mathematical foundation behind it.
Answer: C

Question: Who had a continuous precursor of backpropagation in the context of control theory in 1960?
A: Seppo Linnainmaa
B: Frank Rosenblatt
C: Henry J. Kelley
D: Leibniz
E: James Maxwell
Answer: C
@
In mathematical optimization and decision theory, a loss function or cost function (sometimes also called an error function) [1] is a function that maps an event or values of one or more variables onto a real number intuitively representing some "cost" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. The loss function could include terms from several levels of the hierarchy.

In statistics, typically a loss function is used for parameter estimation, and the event in question is some function of the difference between estimated and true values for an instance of data. The concept, as old as Laplace, was reintroduced in statistics by Abraham Wald in the middle of the 20th century.[2] In the context of economics, for example, this is usually economic cost or regret. In classification, it is the penalty for an incorrect classification of an example. In actuarial science, it is used in an insurance context to model benefits paid over premiums, particularly since the works of Harald Cramér in the 1920s.[3] In optimal control, the loss is the penalty for failing to achieve a desired value. In financial risk management, the function is mapped to a monetary loss.

In many applications, objective functions, including loss functions as a particular case, are determined by the problem formulation. In other situations, the decision maker’s preference must be elicited and represented by a scalar-valued function (called also utility function) in a form suitable for optimization — the problem that Ragnar Frisch has highlighted in his Nobel Prize lecture.[4] The existing methods for constructing objective functions are collected in the proceedings of two dedicated conferences.[5][6] In particular, Andranik Tangian showed that the most usable objective functions — quadratic and additive — are determined by a few indifference points. He used this property in the models for constructing these objective functions from either ordinal or cardinal data that were elicited through computer-assisted interviews with decision makers.[7][8] Among other things, he constructed objective functions to optimally distribute budgets for 16 Westfalian universities[9] and the European subsidies for equalizing unemployment rates among 271 German regions.[10]
$
10
Question: In mathematical optimization, what is the primary aim concerning a loss function?
A: To maximize it.
B: To equate it to zero.
C: To minimize it.
D: To differentiate it.
E: To equate it to one.
Answer: C

Question: An objective function can be which of the following?
A: A minimized reward function.
B: An opposite of a loss function.
C: The same as a loss function.
D: Both B and C.
E: A transformation of a utility function.
Answer: D

Question: In statistics, what does a loss function usually represent?
A: The sum of estimated values.
B: The difference between estimated and true values.
C: The multiplication of estimated and true values.
D: The division of estimated by true values.
E: The absolute value of true values.
Answer: B

Question: Who reintroduced the concept of the loss function in statistics during the mid-20th century?
A: Ragnar Frisch
B: Harald Cramér
C: Laplace
D: Andranik Tangian
E: Abraham Wald
Answer: E

Question: In the realm of economics, a loss function typically signifies what?
A: Monetary gain.
B: Economic growth.
C: Economic cost or regret.
D: Economic stability.
E: Economic inflation.
Answer: C

Question: In actuarial science, what was the application of the loss function mainly related to after the works of Harald Cramér in the 1920s?
A: Modelling economic downturns.
B: Predicting stock market trends.
C: Modelling benefits paid over premiums.
D: Estimating life expectancy.
E: Evaluating investment risks.
Answer: C

Question: What is the function used in financial risk management often mapped to?
A: A monetary benefit.
B: An economic forecast.
C: A monetary loss.
D: An insurance premium.
E: An investment gain.
Answer: C

Question: Who highlighted the problem of representing the decision maker’s preference by a scalar-valued function in a Nobel Prize lecture?
A: Harald Cramér
B: Abraham Wald
C: Ragnar Frisch
D: Andranik Tangian
E: Laplace
Answer: C

Question: Andranik Tangian used certain properties to construct objective functions from which types of data?
A: Binary data.
B: Nominal data.
C: Ordinal or cardinal data.
D: Interval data.
E: Ratio data.
Answer: C

Question: For what purpose did Andranik Tangian construct objective functions in relation to 271 German regions?
A: For predicting economic growth.
B: For assessing natural resources.
C: For determining housing needs.
D: For equalizing unemployment rates.
E: For evaluating regional educational standards.
Answer: D
@
Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data on expected output values.[1] An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.

To solve a given problem of supervised learning, one has to perform the following steps:

Determine the type of training examples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, an entire sentence of handwriting or perhaps a full paragraph of handwriting.
Gather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements.
Determine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output.
Determine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose to use support-vector machines or decision trees.
Complete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation.
Evaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set.
$
10
Question: What does Supervised Learning (SL) require as part of its training data?
A: Unlabeled input objects.
B: Only output values.
C: Input objects and a human-labeled supervisory signal.
D: Generalized error measurements.
E: Inductive bias characteristics.
Answer: C

Question: What is the desired outcome of Supervised Learning when presented with unseen instances?
A: The algorithm predicts with some errors.
B: The algorithm produces a new training set.
C: The algorithm correctly determines output values.
D: The algorithm asks for human intervention.
E: The algorithm reframes the input data.
Answer: C

Question: Which quality of a learning algorithm in SL is gauged by the generalization error?
A: Input representation.
B: Supervisory signal accuracy.
C: Statistical quality.
D: Dimensionality of the data.
E: Training set size.
Answer: C

Question: What should be the primary consideration when deciding on the type of training examples?
A: The generalization error.
B: The learning algorithm.
C: The real-world use of the function.
D: The number of input objects.
E: The type of data to be used as a training set.
Answer: E

Question: Why is it crucial for the input object to be accurately represented in SL?
A: It dictates the type of training examples.
B: It determines the size of the training set.
C: It strongly influences the accuracy of the learned function.
D: It helps in gathering a training set.
E: It sets the generalization error.
Answer: C

Question: What usually describes an input object in Supervised Learning?
A: A control parameter.
B: A human expert’s opinion.
C: A feature vector.
D: A training set sample.
E: A generalization error.
Answer: C

Question: What is a potential pitfall if the number of features for an input object is excessively large?
A: Increased accuracy.
B: Reduced learning time.
C: The curse of dimensionality.
D: Improved generalization.
E: Enhanced inductive bias.
Answer: C

Question: Which of the following might be chosen to determine the structure of the learned function?
A: Feature vector.
B: Handwriting analysis.
C: Support-vector machines.
D: Generalization error.
E: Input object representation.
Answer: C

Question: Some supervised learning algorithms might require the user to set certain parameters. How might these be adjusted?
A: By changing the training set entirely.
B: By solely relying on human expertise.
C: Using a validation set from the training set or via cross-validation.
D: By transforming the input object.
E: By switching the learning algorithm.
Answer: C

Question: What is an essential step after learning and parameter adjustment in Supervised Learning?
A: Re-gathering the training set.
B: Changing the feature vector.
C: Measuring the function’s performance on a separate test set.
D: Re-determining the input feature representation.
E: Altering the structure of the learned function.
Answer: C
@
In machine learning, support vector machines (SVMs, also support vector networks[1]) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995,[1] Vapnik et al., 1997[citation needed]) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.

In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.

The support vector clustering[2] algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data.[citation needed] These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data to groups and, then, to map new data according to these clusters.
$
10
Question: What is the primary function of support vector machines (SVMs) in machine learning?
A: They offer clustering solutions for data.
B: They handle unsupervised learning exclusively.
C: They are models for classification and regression analysis.
D: They replace traditional learning frameworks.
E: They deal only with probabilistic classification.
Answer: C

Question: Where were SVMs developed?
A: Stanford University.
B: Massachusetts Institute of Technology.
C: AT&T Bell Laboratories.
D: Harvard University.
E: Google Labs.
Answer: C

Question: Who is primarily associated with the development of SVMs?
A: Hava Siegelmann.
B: Chervonenkis.
C: Vladimir Vapnik.
D: John Platt.
E: Guyon.
Answer: C

Question: SVMs are renowned for being based on which framework or theory?
A: Linear separation theory.
B: Neural network theory.
C: Statistical learning frameworks or VC theory.
D: Probabilistic framework.
E: Binary categorization principle.
Answer: C

Question: How does SVM categorize training examples for a binary classification?
A: By analyzing the volume of data.
B: By maximizing the distance between data clusters.
C: By predicting the probability of each category.
D: By placing them in a probabilistic setting.
E: By using the kernel trick exclusively.
Answer: B

Question: SVMs can handle which type of classification apart from linear?
A: Multidimensional.
B: Probabilistic.
C: Recursive.
D: Non-linear.
E: Hierarchical.
Answer: D

Question: Which technique allows SVMs to perform non-linear classification?
A: The clustering technique.
B: The statistical mapping technique.
C: The high-dimensional separation.
D: The kernel trick.
E: The VC categorization.
Answer: D

Question: The support vector clustering algorithm was primarily introduced by which duo?
A: Guyon and Boser.
B: Chervonenkis and Vapnik.
C: Hava Siegelmann and Vladimir Vapnik.
D: Cortes and Guyon.
E: Platt and Siegelmann.
Answer: C

Question: What is the main purpose of unsupervised learning approaches like support vector clustering?
A: To classify labeled data into one of two categories.
B: To find a natural clustering of the data into groups.
C: To maximize the gap between data categories.
D: To map inputs into a probabilistic setting.
E: To replace traditional SVMs in most tasks.
Answer: B

Question: In the SVM context, what does "Platt scaling" relate to?
A: Mapping training examples to points in space.
B: Using SVM for probabilistic classification.
C: Efficiently performing non-linear classification.
D: Assigning new examples to a specific category.
E: Maximizing the width of the gap between two categories.
Answer: B
@
A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers.[10][13] There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions.[137] These components as a whole function similarly to a human brain, and can be trained like any other ML algorithm.[citation needed]

For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer,[citation needed] and complex DNN have many layers, hence the name "deep" networks.

DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[138] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[10] For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.[139]

Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.

DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or "weights", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.[140] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.

Recurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling.[141][142][143][144][145] Long short-term memory is particularly effective for this use.[75][146]

Convolutional deep neural networks (CNNs) are used in computer vision.[147] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[148]
$
10
Question: What is a deep neural network (DNN) characterized by?
A: The ability to recognize only dog breeds.
B: Having multiple layers between input and output layers.
C: The usage of only convolutional architectures.
D: Lack of weights and biases.
E: Exclusively functioning for vision tasks.
Answer: B

Question: In neural networks, what components contribute to its function that resembles a human brain?
A: Algorithms, data, predictions, layers, and labels.
B: Neurons, synapses, weights, biases, and functions.
C: Thresholds, layers, weights, predictions, and inputs.
D: Synapses, algorithms, labels, data, and predictions.
E: Layers, predictions, biases, algorithms, and thresholds.
Answer: B

Question: When a DNN is trained to recognize dog breeds, what does it compute for each breed?
A: The size of the image.
B: The clarity of the image.
C: The number of pixels.
D: The probability that the dog is of that breed.
E: The number of layers required to identify the breed.
Answer: D

Question: In the context of DNNs, what is referred to as a 'layer'?
A: A single neuron.
B: A single input or output.
C: A mathematical manipulation on the data.
D: A threshold set by the user.
E: A specific dog breed.
Answer: C

Question: Why are deep architectures termed "deep"?
A: They have complex algorithms.
B: They delve deep into data to fetch details.
C: They have many mathematical manipulations or layers.
D: They have a deep understanding of the data.
E: Their performance is deep and comprehensive.
Answer: C

Question: What advantage do DNN architectures offer over shallow networks?
A: They are faster in processing.
B: They require more data.
C: They model complex data with potentially fewer units.
D: They are more reliable.
E: They have fewer layers.
Answer: C

Question: In a typical feedforward DNN, how does data travel?
A: It loops back and forth.
B: It flows only from the output layer to the input layer.
C: It flows in random directions.
D: It flows from the input layer to the output layer without looping back.
E: It remains static within the layers.
Answer: D

Question: What adjustment does a DNN make if it did not accurately recognize a pattern?
A: It changes the structure of the neurons.
B: It modifies the number of layers.
C: It adjusts the weights of connections.
D: It increases the number of synapses.
E: It completely resets the network.
Answer: C

Question: For which application are Recurrent neural networks (RNNs) notably used?
A: Image recognition.
B: Audio processing.
C: Language modeling.
D: Spatial mapping.
E: Handwriting analysis.
Answer: C

Question: What application domain is a convolutional deep neural network (CNN) majorly associated with?
A: Time-series forecasting.
B: Language translation.
C: Computer vision.
D: Tabular data analysis.
E: Text summarization.
Answer: C
@
Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters (or kernel) optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections.[1][2] For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels,[3][4] only 25 neurons are required to process 5x5-sized tiles[5][6] Higher-layer features are extracted from wider context windows, compared to lower-layer features.

They have applications in:

image and video recognition,[7]
recommender systems,[8]
image classification,
image segmentation,
medical image analysis,
natural language processing,[9]
brain–computer interfaces,[10] and
financial time series.[11]
CNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps.[12][13] Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.[14]

Feed-forward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The "full connectivity" of these networks make them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increases the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.[15]

Convolutional networks were inspired by biological processes[16][17][18][19] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.
$
10
Question: What does a convolutional neural network (CNN) optimize to learn feature engineering?
A: Neurons in the layers.
B: Regularized weights.
C: Filters or kernels.
D: Image sizes.
E: Translation-equivariant responses.
Answer: C

Question: Why are fewer weights required in CNNs as compared to a fully-connected layer?
A: Because they process smaller-sized images.
B: Because of the utilization of cascaded convolution kernels.
C: They have a more compact architecture.
D: They skip certain connections.
E: They penalize parameters during training.
Answer: B

Question: What do higher-layer features extract from compared to lower-layer features?
A: Smaller context windows.
B: Cascaded convolution kernels.
C: Wider context windows.
D: Neural network connections.
E: Image sizes.
Answer: C

Question: In which application is a CNN NOT mentioned to have a notable use?
A: Audio synthesis.
B: Image and video recognition.
C: Recommender systems.
D: Medical image analysis.
E: Brain-computer interfaces.
Answer: A

Question: What alternative name is given to CNNs based on the shared-weight architecture of the convolution kernels?
A: Fully Invariant Artificial Neural Networks.
B: Space Invariant Artificial Neural Networks (SIANN).
C: Cascaded Correlation Neural Networks.
D: Regularized Feed-Forward Networks.
E: Visual Field Neural Networks.
Answer: B

Question: Why are most CNNs not invariant to translation?
A: Because of the use of regularized weights.
B: Due to the optimization of filters.
C: Because they slide along input features.
D: Owing to the downsampling operation they apply to the input.
E: Due to the use of penalizing parameters during training.
Answer: D

Question: What is a characteristic of feed-forward neural networks compared to CNNs?
A: They lack connectivity.
B: They are always translation-equivariant.
C: They are usually fully connected networks.
D: They use only convolutional kernels.
E: They always apply downsampling.
Answer: C

Question: What is a common method of preventing overfitting in neural networks?
A: Increasing the number of layers.
B: Trimming connectivity.
C: Increasing the number of neurons.
D: Cascading convolutional kernels.
E: Focusing on smaller images.
Answer: B

Question: What does a poorly-populated dataset risk teaching CNNs?
A: The fundamentals of the dataset.
B: Generalized principles of the dataset.
C: The biases of the dataset.
D: The full connectivity of the network.
E: The translation-equivariant responses.
Answer: C

Question: What was the inspiration behind the design of convolutional networks?
A: The structure of the human auditory system.
B: The organization of the animal visual cortex.
C: The mechanics of the human muscular system.
D: The functioning of the human digestive system.
E: The structure of the human skeletal system.
Answer: B
@
































































































