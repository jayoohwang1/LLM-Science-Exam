{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MACHINE = \"JAYOO_PC\"\n",
    "# MACHINE = \"KAGGLE\"\n",
    "\n",
    "DEVICE = \"GPU\"\n",
    "\n",
    "if MACHINE == \"JAYOO_PC\":\n",
    "    ROOT = '/jayoo'\n",
    "else:\n",
    "    ROOT = '/'\n",
    "    \n",
    "SAVE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenBook DeBERTaV3-Large with an updated model\n",
    "\n",
    "This work is based on the great [work](https://www.kaggle.com/code/nlztrk/openbook-debertav3-large-baseline-single-model) of [nlztrk](https://www.kaggle.com/nlztrk).\n",
    "\n",
    "I trained a model offline using the dataset I shared [here](https://www.kaggle.com/datasets/mgoksu/llm-science-exam-dataset-w-context). I just added my model to the original notebook. The model is available [here](https://www.kaggle.com/datasets/mgoksu/llm-science-run-context-2).\n",
    "\n",
    "I also addressed the problem of [CSV Not Found at submission](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/434228) with this notebook by clipping the context like so:\n",
    "\n",
    "`test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:1500]) + \" #### \" +  test_df[\"prompt\"]`\n",
    "\n",
    "You can probably get more than 1500 without getting an OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 126.809817,
     "end_time": "2023-08-14T10:09:22.925969",
     "exception": false,
     "start_time": "2023-08-14T10:07:16.116152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# installing offline dependencies\n",
    "# !pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "# !cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n",
    "# !pip install -U /kaggle/working/sentence-transformers\n",
    "!pip install faiss-gpu\n",
    "!pip install sentence_transformers\n",
    "\n",
    "!pip install -U /jayoo/kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n",
    "# !pip install --no-index --no-deps /jayoo/kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /jayoo/kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n",
    "# !pip install --no-index --no-deps /jayoo/kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /jayoo/kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 8.534957,
     "end_time": "2023-08-14T10:09:31.474781",
     "exception": false,
     "start_time": "2023-08-14T10:09:22.939824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import blingfire as bf\n",
    "from __future__ import annotations\n",
    "import pickle\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import faiss\n",
    "from faiss import write_index, read_index\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from IPython.display import FileLink, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.034054,
     "end_time": "2023-08-14T10:09:31.574046",
     "exception": false,
     "start_time": "2023-08-14T10:09:31.539992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_documents(documents: Iterable[str],\n",
    "                      document_ids: Iterable,\n",
    "                      split_sentences: bool = True,\n",
    "                      filter_len: int = 3,\n",
    "                      disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main helper function to process documents from the EMR.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param document_type: String denoting the document type to be processed\n",
    "    :param document_sections: List of sections for a given document type to process\n",
    "    :param split_sentences: Flag to determine whether to further split sections into sentences\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "    \n",
    "    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n",
    "\n",
    "    if split_sentences:\n",
    "        df = sentencize(df.text.values, \n",
    "                        df.document_id.values,\n",
    "                        df.offset.values, \n",
    "                        filter_len, \n",
    "                        disable_progress_bar)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sectionize_documents(documents: Iterable[str],\n",
    "                         document_ids: Iterable,\n",
    "                         disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtains the sections of the imaging reports and returns only the \n",
    "    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n",
    "    \"\"\"\n",
    "    processed_documents = []\n",
    "    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n",
    "        row = {}\n",
    "        text, start, end = (document, 0, len(document))\n",
    "        row['document_id'] = document_id\n",
    "        row['text'] = text\n",
    "        row['offset'] = (start, end)\n",
    "\n",
    "        processed_documents.append(row)\n",
    "\n",
    "    _df = pd.DataFrame(processed_documents)\n",
    "    if _df.shape[0] > 0:\n",
    "        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n",
    "    else:\n",
    "        return _df\n",
    "\n",
    "\n",
    "def sentencize(documents: Iterable[str],\n",
    "               document_ids: Iterable,\n",
    "               offsets: Iterable[tuple[int, int]],\n",
    "               filter_len: int = 3,\n",
    "               disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a document into sentences. Can be used with `sectionize_documents`\n",
    "    to further split documents into more manageable pieces. Takes in offsets\n",
    "    to ensure that after splitting, the sentences can be matched to the\n",
    "    location in the original documents.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param offsets: Iterable tuple of the start and end indices\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "\n",
    "    document_sentences = []\n",
    "    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n",
    "        try:\n",
    "            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n",
    "            for o in sentence_offsets:\n",
    "                if o[1]-o[0] > filter_len:\n",
    "                    sentence = document[o[0]:o[1]]\n",
    "                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n",
    "                    row = {}\n",
    "                    row['document_id'] = document_id\n",
    "                    row['text'] = sentence\n",
    "                    row['offset'] = abs_offsets\n",
    "                    document_sentences.append(row)\n",
    "                    \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    document_df = pd.DataFrame(document_sentences)\n",
    "    return document_df\n",
    "\n",
    "\n",
    "# fully clear memory\n",
    "def clear_mem():\n",
    "    gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def download_file(path, file_name):\n",
    "    os.chdir('/kaggle/working/')\n",
    "    zip = f\"{file_name}.zip\"\n",
    "    command = f\"zip {zip} {path} -r\"\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(\"Unable to run zip command!\")\n",
    "        print(result.stderr)\n",
    "        return\n",
    "    display(FileLink(f'{file_name}.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.036342,
     "end_time": "2023-08-14T10:09:31.623595",
     "exception": false,
     "start_time": "2023-08-14T10:09:31.587253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "SIM_MODEL = 'BAAI/bge-small-en-v1.5'\n",
    "DEVICE = 0\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "WIKI_PATH = ROOT+\"/kaggle/input/wikipedia-20230701\"\n",
    "wiki_files = os.listdir(WIKI_PATH)\n",
    "\n",
    "FILE_DIR = ROOT+\"/kaggle/input/bge/prefix\"\n",
    "\n",
    "ngpus = faiss.get_num_gpus()\n",
    "print(ngpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant Title Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.058533,
     "end_time": "2023-08-14T10:09:31.695383",
     "exception": false,
     "start_time": "2023-08-14T10:09:31.63685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trn = pd.read_csv(\"/kaggle/input/53k-cleaned/53k_cleaned.csv\").drop(\"id\", 1)\n",
    "trn = pd.read_csv(ROOT+\"/kaggle/input/chris_data/54k_nota.csv\") #.drop(\"id\", 1)\n",
    "trn = trn.drop(\"context\", axis=1)\n",
    "trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Combine all answers\n",
    "trn['answer_all'] = trn.apply(lambda x: \" \".join([str(x['A']), str(x['B']), str(x['C']), str(x['D']), str(x['E'])]), axis=1)\n",
    "\n",
    "\n",
    "## Search using the prompt and answers to guide the search\n",
    "trn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']\n",
    "\n",
    "# add prefix for bge retrieval\n",
    "prefix = 'Represent this sentence for searching relevant passages: '\n",
    "trn['prompt_answer_stem'] = prefix + trn['prompt_answer_stem']\n",
    "trn['prompt_answer_stem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 13.282604,
     "end_time": "2023-08-14T10:09:44.992949",
     "exception": false,
     "start_time": "2023-08-14T10:09:31.710345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer(SIM_MODEL) #,device='cuda'\n",
    "model = model.cuda().half()\n",
    "model.max_seq_length = MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 95.926417,
     "end_time": "2023-08-14T10:11:20.934445",
     "exception": false,
     "start_time": "2023-08-14T10:09:45.008028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")\n",
    "sentence_index = read_index(ROOT+\"/kaggle/input/faiss-index/bge_wikiAbstract.index\")\n",
    "\n",
    "# move index to gpu\n",
    "# if ngpus > 1:\n",
    "    # sentence_index = faiss.index_cpu_to_all_gpus(sentence_index)\n",
    "# if ngpus == 1:\n",
    "#     res = faiss.StandardGpuResources()\n",
    "#     sentence_index = faiss.index_cpu_to_gpu(res, 0, sentence_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 10.891104,
     "end_time": "2023-08-14T10:11:31.84869",
     "exception": false,
     "start_time": "2023-08-14T10:11:20.957586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=32, device=DEVICE,\n",
    "                                 show_progress_bar=True, normalize_embeddings=True) #convert_to_tensor=True\n",
    "# prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n",
    "_ = gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 23.339585,
     "end_time": "2023-08-14T10:11:55.247556",
     "exception": false,
     "start_time": "2023-08-14T10:11:31.907971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get the top 3 pages that are likely to contain the topic of interest\n",
    "search_score, search_index = sentence_index.search(np.float32(prompt_embeddings), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.877305,
     "end_time": "2023-08-14T10:11:56.145444",
     "exception": false,
     "start_time": "2023-08-14T10:11:55.268139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Save memory - delete sentence_index since it is no longer necessary\n",
    "del sentence_index\n",
    "del prompt_embeddings\n",
    "_ = gc.collect()\n",
    "libc.malloc_trim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save\n",
    "\n",
    "# load\n",
    "# search_index = np.load('search_index.np.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Sentences from the Relevant Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.737408,
     "end_time": "2023-08-14T10:12:01.897408",
     "exception": false,
     "start_time": "2023-08-14T10:11:56.16",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(ROOT+\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n",
    "                     columns=['id', 'file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.799872,
     "end_time": "2023-08-14T10:12:02.712752",
     "exception": false,
     "start_time": "2023-08-14T10:12:01.91288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get the article and associated file location using the index\n",
    "wikipedia_file_data = []\n",
    "\n",
    "for i, idx in enumerate(search_index):\n",
    "    scr_idx = idx\n",
    "    _df = df.loc[scr_idx].copy()\n",
    "    _df['prompt_id'] = i\n",
    "    wikipedia_file_data.append(_df)\n",
    "wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n",
    "wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "if SAVE is True:\n",
    "    wikipedia_file_data.to_csv('wiki_file_data.csv', index=False)\n",
    "\n",
    "\n",
    "## Save memory - delete df since it is no longer necessary\n",
    "del df\n",
    "_ = gc.collect()\n",
    "libc.malloc_trim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 303.981049,
     "end_time": "2023-08-14T10:17:06.710072",
     "exception": false,
     "start_time": "2023-08-14T10:12:02.729023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get the full text data\n",
    "wiki_text_data = []\n",
    "\n",
    "for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n",
    "    _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n",
    "    _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n",
    "\n",
    "    _df_temp = _df[_df['id'].isin(_id)].copy()\n",
    "    del _df\n",
    "    _ = gc.collect()\n",
    "    libc.malloc_trim(0)\n",
    "    wiki_text_data.append(_df_temp)\n",
    "wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n",
    "_ = gc.collect()\n",
    "\n",
    "if SAVE is True:\n",
    "    # save wiki_text_data\n",
    "    wiki_text_data.to_csv('wiki_text_data.csv', index=False)\n",
    "    download_file(f\"wiki_text_data.csv\", f\"wiki_text_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 4.491281,
     "end_time": "2023-08-14T10:17:11.220342",
     "exception": false,
     "start_time": "2023-08-14T10:17:06.729061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SAVE is True:\n",
    "    wiki_text_data = pd.read_csv('wiki_text_data.csv')\n",
    "\n",
    "## Parse documents into sentences\n",
    "processed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)\n",
    "\n",
    "if SAVE is True:\n",
    "    # Save for later\n",
    "    processed_wiki_text_data.to_csv('processed_wiki_text_data.csv', index=False)\n",
    "    download_file(f\"processed_wiki_text_data.csv\", f\"processed_wiki_text_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save\n",
    "# processed_wiki_text_data.to_csv('processed_wiki_text_data.csv', index=False)\n",
    "# wikipedia_file_data.to_csv('wiki_file_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del wiki_text_data\n",
    "_ = gc.collect()\n",
    "libc.malloc_trim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load\n",
    "processed_wiki_text_data = pd.read_csv(FILE_DIR+'/processed_wiki_text_data.csv')\n",
    "\n",
    "# Get first half of embeddings of wiki text data\n",
    "half_index = len(processed_wiki_text_data) // 2\n",
    "processed_text_half = processed_wiki_text_data['text'].iloc[half_index:].to_numpy()\n",
    "processed_text_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 25.110593,
     "end_time": "2023-08-14T10:17:36.348422",
     "exception": false,
     "start_time": "2023-08-14T10:17:11.237829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "half = 2\n",
    "wiki_data_embeddings2 = model.encode(processed_text_half,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    device=DEVICE,\n",
    "                                    show_progress_bar=True,\n",
    "                                    # convert_to_tensor=True,\n",
    "                                    normalize_embeddings=True)  #.half()\n",
    "# wiki_data_embeddings1 = wiki_data_embeddings1.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # pickle your list of embeddings\n",
    "# with open(f\"wiki_data_embs{half}.pkl\", \"wb\") as fp: \n",
    "#     pickle.dump(wiki_data_embeddings1, fp)  \n",
    "# # download_file(f\"wiki_data_embs{half}.pkl\", f\"wiki_data_embs{half}\")\n",
    "\n",
    "np.save(FILE_DIR+'/wiki_data_embs2', wiki_data_embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del processed_text_half\n",
    "del processed_wiki_text_data\n",
    "del wiki_data_embeddings2\n",
    "_ = gc.collect()\n",
    "libc.malloc_trim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine wiki_data_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Get embeddings of the wiki text data\n",
    "# wiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n",
    "#                                     batch_size=BATCH_SIZE,\n",
    "#                                     device=DEVICE,\n",
    "#                                     show_progress_bar=True,\n",
    "# #                                     convert_to_tensor=True,\n",
    "#                                     normalize_embeddings=True)#.half()\n",
    "# # wiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()\n",
    "\n",
    "# _ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Matching Prompt-Sentence Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load saved wiki embeddings\n",
    "half_embs1 = np.load(FILE_DIR+'/wiki_data_embs1.npy')\n",
    "half_embs2 = np.load(FILE_DIR+'/wiki_data_embs2.npy')\n",
    "wiki_data_embeddings = np.concatenate((half_embs1, half_embs2))\n",
    "\n",
    "# save whole embeddings\n",
    "np.save(FILE_DIR+'/full_wiki_embs', wiki_data_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load wiki data\n",
    "wikipedia_file_data = pd.read_csv('wiki_file_data.csv')\n",
    "processed_wiki_text_data = pd.read_csv('processed_wiki_text_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clear mem\n",
    "del half_embs1\n",
    "del half_embs2\n",
    "_ = gc.collect()\n",
    "libc.malloc_trim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.431343,
     "end_time": "2023-08-14T10:17:37.177862",
     "exception": false,
     "start_time": "2023-08-14T10:17:36.746519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE,\n",
    "                                   show_progress_bar=True, normalize_embeddings=True)  #convert_to_tensor=True\n",
    "# question_embeddings = question_embeddings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.609553,
     "end_time": "2023-08-14T10:17:38.836268",
     "exception": false,
     "start_time": "2023-08-14T10:17:37.226715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "## Parameter to determine how many relevant sentences to include\n",
    "NUM_SENTENCES_INCLUDE = 22\n",
    "\n",
    "## List containing just context\n",
    "contexts = []\n",
    "\n",
    "for r in tqdm(trn.itertuples(), total=len(trn)):\n",
    "\n",
    "    prompt_id = r.Index\n",
    "\n",
    "    prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values)].index.values\n",
    "\n",
    "    if prompt_indices.shape[0] > 0:\n",
    "        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n",
    "        prompt_index.add(np.float32(wiki_data_embeddings[prompt_indices]))\n",
    "        \n",
    "        prompt_index = faiss.index_cpu_to_gpu(res, 0, prompt_index)\n",
    "\n",
    "        context = \"\"\n",
    "        \n",
    "        ## Get the top matches\n",
    "        ss, ii = prompt_index.search(np.float32(question_embeddings), NUM_SENTENCES_INCLUDE)\n",
    "        for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n",
    "            context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n",
    "        \n",
    "    contexts.append(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.024188,
     "end_time": "2023-08-14T10:17:38.878394",
     "exception": false,
     "start_time": "2023-08-14T10:17:38.854206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trn['context'] = contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trn[[\"prompt\", \"A\", \"B\", \"C\", \"D\", \"E\", \"answer\", \"context\", \"source\"]].to_csv(\"53k_bge_wikiAbstract.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn.loc[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015828,
     "end_time": "2023-08-14T10:17:39.007683",
     "exception": false,
     "start_time": "2023-08-14T10:17:38.991855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
