{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# How To Train Model for Open Book Q&A Technique\n",
    "In this notebook we demonstrate how to train a model to be used with top scoring Open Book Q&A method. The Open Book method was first presented by JJ (@jjinho) [here][1], then Quangteo (@quangbk) improved RAM usage [here][2], and Anil (@nlztrk) combined with Q&A [here][3]. Radek (@radek1) demonstrated the strength of Q&A [here][5]. Next Mgoksu (@mgoksu) demonstrated how to achieve top public LB=0.807 using this method [here][4] by finetuning DeBerta large on this method.\n",
    "\n",
    "In order to train a model for use with Open Book Q&A, we need a CSV that contains; `prompt` (i.e. question), `A, B, C, D, E` (i.e. answer choices), and we need a column of `context` extracted from wikipedia pages for each question. To generate the `context` column, we run Mgoksu's notebook [here][4]. In code cell #5, we load our CSV without `context` column with code `trn = pd.read_csv(OUR_DATASET.CSV)`. Then in code cell #21 our dataset is saved to disk as `test_context.csv` with the column `context` added.\n",
    "\n",
    "I have searched and concatenated all publicly shared datasets into one 60k CSV and then ran Mgoksu's notebook with `NUM_TITLES_INCLUDE = 5` and `NUM_SENTENCES_INCLUDE = 20`. This added an additional `context` column. I uploaded the resultant CSV file to a Kaggle dataset [here][6]. If you enjoy the notebook you are reading, please upvote the dataset too. Thanks! \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:800/format:webp/1*bTGY3fKIgNefQxNsOYpnBw.png)\n",
    " \n",
    "(image source [here][7])\n",
    "\n",
    "[1]: https://www.kaggle.com/code/jjinho/open-book-llm-science-exam\n",
    "[2]: https://www.kaggle.com/code/quangbk/open-book-llm-science-exam-reduced-ram-usage\n",
    "[3]: https://www.kaggle.com/code/nlztrk/openbook-debertav3-large-baseline-single-model\n",
    "[4]: https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model\n",
    "[5]: https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training\n",
    "[6]: https://www.kaggle.com/datasets/cdeotte/60k-data-with-context-v2\n",
    "[7]: https://blog.gopenai.com/enrich-llms-with-retrieval-augmented-generation-rag-17b82a96b6f0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load CSV\n",
    "We will load 60k CSV of `prompts`, `A,B,C,D,E`, and `context` from my Kaggle dataset [here][1]. This dataset is all publicly shared datasets concatenated then processed with Mgoksu's notebook [here][2] to create a `context` column. (To learn more about the datasets within read my discussion post). This Kaggle dataset also contains competition `train.csv` with added `context` column (to be used as a validation dataset).\n",
    "\n",
    "In this train notebook, we have internet turned on and can choose whatever model we wish to download and train. After we finetune this model, we will create a second notebook with the Open Book Q&A technique and load the finetuned model from the output of this notebook. The second notebook will have internet turned off so that it can be submitted to Kaggle's competition.\n",
    "\n",
    "[1]: https://www.kaggle.com/datasets/cdeotte/60k-data-with-context-v2\n",
    "[2]: https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to initialize NVML: Unknown Error\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine: JAYOO_PC, device: GPU, root: /jayoo\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "###---- Environment config ----###\n",
    "\n",
    "# MACHINE = \"COLAB\"\n",
    "# device = \"TPU\"\n",
    "\n",
    "# MACHINE = \"KAGGLE\"\n",
    "# device = \"TPU-VM\"\n",
    "# device = \"GPU\"\n",
    "\n",
    "MACHINE = \"JAYOO_PC\"\n",
    "device = \"GPU\"\n",
    "\n",
    "\n",
    "# DEBUG = True\n",
    "DEBUG = False\n",
    "if DEBUG == True:\n",
    "    print(\"IN DEBUG MODE\")\n",
    "    # device = \"CPU\"\n",
    "    \n",
    "# Set root directory\n",
    "if MACHINE == \"JAYOO_PC\":\n",
    "    ROOT = '/jayoo'  # local\n",
    "elif MACHINE == \"COLAB\":\n",
    "    ROOT = './drive/MyDrive/colab_env'\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !pwd\n",
    "else:\n",
    "    ROOT = ''  # Kaggle\n",
    "\n",
    "print(f\"Machine: {MACHINE}, device: {device}, root: {ROOT}\")\n",
    "\n",
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if MACHINE == \"KAGGLE\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "import random\n",
    "from typing import Optional, Union\n",
    "import pandas as pd, numpy as np, torch\n",
    "from datasets import Dataset\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "import gc\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# randomly shuffle order of answers\n",
    "def shuffle_answers(row):\n",
    "#     correct = row['answer']\n",
    "    new_row = row.copy()\n",
    "    answers = ['A', 'B', 'C', 'D', 'E']\n",
    "    #shuffle answers\n",
    "    shuffled_ans = answers.copy()\n",
    "    random.shuffle(shuffled_ans)\n",
    "    \n",
    "    for i in range(len(answers)):\n",
    "        target = shuffled_ans[i]\n",
    "        new_row[answers[i]] = row[target]\n",
    "        if target == row['answer']:\n",
    "            new_row['answer'] = answers[i]\n",
    "        \n",
    "    return new_row\n",
    "\n",
    "\n",
    "# shuffle all rows in df\n",
    "def shuffle_df(df):\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i] = shuffle_answers(df.loc[i])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# replace nan answer with a random incorrect answer\n",
    "def fix_nan(row):\n",
    "    nan_option = None\n",
    "    options = []\n",
    "    answers = ['A', 'B', 'C', 'D', 'E']\n",
    "    for char in answers:\n",
    "        if (len(row[char]) > 0):\n",
    "            if (char != row['answer']):\n",
    "                options.append(char)\n",
    "        else:\n",
    "            nan_option = char\n",
    "    \n",
    "    if (nan_option != None):\n",
    "        copy = random.choice(options)\n",
    "        copy_text = row[copy]\n",
    "        row[nan_option] = copy_text\n",
    "    \n",
    "    return row\n",
    "\n",
    "\n",
    "# replace all nan answers\n",
    "def replace_nans(df):\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i] = fix_nan(df.loc[i])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VER=2\n",
    "# TRAIN WITH SUBSET OF 60K\n",
    "# NUM_TRAIN_SAMPLES = 52984 #1024\n",
    "if DEBUG is True:\n",
    "    NUM_TRAIN_SAMPLES = 1024\n",
    "\n",
    "# PARAMETER EFFICIENT FINE TUNING\n",
    "# PEFT REQUIRES 1XP100 GPU NOT 2XT4\n",
    "USE_PEFT = False\n",
    "# NUMBER OF LAYERS TO FREEZE \n",
    "# DEBERTA LARGE HAS TOTAL OF 24 LAYERS\n",
    "FREEZE_LAYERS = 0  #18\n",
    "# BOOLEAN TO FREEZE EMBEDDINGS\n",
    "FREEZE_EMBEDDINGS = False  #True\n",
    "# LENGTH OF CONTEXT PLUS QUESTION ANSWER\n",
    "MAX_INPUT = 768  # 256\n",
    "\n",
    "# HUGGING FACE MODEL\n",
    "MODEL = 'microsoft/deberta-v3-large'\n",
    "# MODEL = ROOT + '/deberta-v3_model'\n",
    "# TOK_DIR = ROOT + '/deberta-v3_tokenizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data size: (500, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>context</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the method of transcription in the lif...</td>\n",
       "      <td>-There are three different replication systems...</td>\n",
       "      <td>RNA-templated transcription is the method of t...</td>\n",
       "      <td>Transcription occurs through a unique mechanis...</td>\n",
       "      <td>Reverse transcription is the method of transcr...</td>\n",
       "      <td>DNA-templated transcription is the method of t...</td>\n",
       "      <td>Transcription does not occur in the life cycle...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the role of the viral fiber glycoprote...</td>\n",
       "      <td>-ASFV is a large (175–215 nm), icosahedral, do...</td>\n",
       "      <td>The viral fiber glycoproteins are involved in ...</td>\n",
       "      <td>The viral fiber glycoproteins code for 40 prot...</td>\n",
       "      <td>The viral fiber glycoproteins are responsible ...</td>\n",
       "      <td>The viral fiber glycoproteins mediate endocyto...</td>\n",
       "      <td>The viral fiber glycoproteins are responsible ...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the significance of the faint Hα emiss...</td>\n",
       "      <td>-Single antenna detections Radio observations ...</td>\n",
       "      <td>The emission lines indicate that 3 Geminorum i...</td>\n",
       "      <td>The emission lines indicate that 3 Geminorum i...</td>\n",
       "      <td>The emission lines indicate that 3 Geminorum i...</td>\n",
       "      <td>The emission lines indicate that 3 Geminorum i...</td>\n",
       "      <td>The emission lines indicate that 3 Geminorum i...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the significance of the pedicellariae ...</td>\n",
       "      <td>-Structure The three basic segments of the typ...</td>\n",
       "      <td>They are used for climbing on corals.</td>\n",
       "      <td>They resemble the traps of the Venus fly trap ...</td>\n",
       "      <td>They are covered by short and stout spines.</td>\n",
       "      <td>They are found on the central disc of the sea ...</td>\n",
       "      <td>They are a characteristic feature of the Gonia...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the role of the microprocessor complex...</td>\n",
       "      <td>-The microprocessor complex is a protein compl...</td>\n",
       "      <td>The microprocessor complex is responsible for ...</td>\n",
       "      <td>The microprocessor complex is responsible for ...</td>\n",
       "      <td>The microprocessor complex is involved in the ...</td>\n",
       "      <td>The microprocessor complex is involved in the ...</td>\n",
       "      <td>The microprocessor complex is responsible for ...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  What is the method of transcription in the lif...   \n",
       "1  What is the role of the viral fiber glycoprote...   \n",
       "2  What is the significance of the faint Hα emiss...   \n",
       "3  What is the significance of the pedicellariae ...   \n",
       "4  What is the role of the microprocessor complex...   \n",
       "\n",
       "                                             context  \\\n",
       "0  -There are three different replication systems...   \n",
       "1  -ASFV is a large (175–215 nm), icosahedral, do...   \n",
       "2  -Single antenna detections Radio observations ...   \n",
       "3  -Structure The three basic segments of the typ...   \n",
       "4  -The microprocessor complex is a protein compl...   \n",
       "\n",
       "                                                   A  \\\n",
       "0  RNA-templated transcription is the method of t...   \n",
       "1  The viral fiber glycoproteins are involved in ...   \n",
       "2  The emission lines indicate that 3 Geminorum i...   \n",
       "3              They are used for climbing on corals.   \n",
       "4  The microprocessor complex is responsible for ...   \n",
       "\n",
       "                                                   B  \\\n",
       "0  Transcription occurs through a unique mechanis...   \n",
       "1  The viral fiber glycoproteins code for 40 prot...   \n",
       "2  The emission lines indicate that 3 Geminorum i...   \n",
       "3  They resemble the traps of the Venus fly trap ...   \n",
       "4  The microprocessor complex is responsible for ...   \n",
       "\n",
       "                                                   C  \\\n",
       "0  Reverse transcription is the method of transcr...   \n",
       "1  The viral fiber glycoproteins are responsible ...   \n",
       "2  The emission lines indicate that 3 Geminorum i...   \n",
       "3        They are covered by short and stout spines.   \n",
       "4  The microprocessor complex is involved in the ...   \n",
       "\n",
       "                                                   D  \\\n",
       "0  DNA-templated transcription is the method of t...   \n",
       "1  The viral fiber glycoproteins mediate endocyto...   \n",
       "2  The emission lines indicate that 3 Geminorum i...   \n",
       "3  They are found on the central disc of the sea ...   \n",
       "4  The microprocessor complex is involved in the ...   \n",
       "\n",
       "                                                   E answer  \n",
       "0  Transcription does not occur in the life cycle...      D  \n",
       "1  The viral fiber glycoproteins are responsible ...      D  \n",
       "2  The emission lines indicate that 3 Geminorum i...      A  \n",
       "3  They are a characteristic feature of the Gonia...      B  \n",
       "4  The microprocessor complex is responsible for ...      A  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = pd.read_csv(ROOT+'/kaggle/input/bge/science_only/500val_bgeSci_5context.csv')\n",
    "\n",
    "# bge_1 = pd.read_csv(ROOT+'/kaggle/input/bge/science_only/500val_bgeSci_5context.csv')\n",
    "# bge_2 = pd.read_csv(ROOT+'/kaggle/input/bge/science_only/bgeSci_data2/500val_bgeSci_data2.csv')\n",
    "# df_valid = pd.concat([bge_1, bge_2])\n",
    "\n",
    "print('Validation data size:', df_valid.shape )\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_c1_v1 = pd.read_csv(ROOT+'/kaggle/input/tf-idf_context/v1/500val_tfidf_context1_v1.csv')\n",
    "# tf_c2_v1 = pd.read_csv(ROOT+'/kaggle/input/tf-idf_context/v1/500val_tfidf_context2_v1.csv')\n",
    "\n",
    "# tf_c1_v3 = pd.read_csv(ROOT+'/kaggle/input/tf-idf_context/v3/500val_tfidf_context1_v3.csv')\n",
    "# tf_c2_v3 = pd.read_csv(ROOT+'/kaggle/input/tf-idf_context/v3/500val_tfidf_context2_v3.csv')\n",
    "\n",
    "# sci_val = pd.read_csv(ROOT+'/kaggle/input/bge/science_only/500val_bgeSci_5context.csv')\n",
    "# wiki_val = pd.read_csv(ROOT+'/kaggle/input/bge/prefix/500val_prefix_bge_wikiAbstract.csv')\n",
    "\n",
    "# all_val = pd.concat([tf_c1_v1, tf_c2_v1, tf_c1_v3, tf_c2_v3, sci_val, wiki_val])\n",
    "# all_val.to_csv('3k_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12721/3206462274.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  row[nan_option] = copy_text\n"
     ]
    }
   ],
   "source": [
    "# 60k dataset\n",
    "# # df_train = pd.read_csv(ROOT+'/kaggle/input/60k-data-with-context-v2/all_12_with_context2.csv')\n",
    "\n",
    "# # bge no prefix\n",
    "# # df_53k = pd.read_csv(ROOT+'/kaggle/input/bge/no_prefix/53k_bge_wikiAbstract.csv')\n",
    "# # df_53k = df_53k.fillna('')\n",
    "# # df_53k = replace_nans(df_53k)\n",
    "# # df_53k = shuffle_df(df_53k)\n",
    "\n",
    "# # # bge prefix\n",
    "df_54k = pd.read_csv(ROOT+'/kaggle/input/bge/prefix/54k_prefix_bge_wikiAbstract.csv')\n",
    "df_54k = df_54k.fillna('')\n",
    "df_54k = replace_nans(df_54k)\n",
    "df_54k = shuffle_df(df_54k)\n",
    "non_sci = df_54k[(df_54k['source'] != 6) & (df_54k['source'] != 10) & (df_54k['source'] != 11) & (df_54k['source'] != 12)]\n",
    "\n",
    "# # Combined data\n",
    "combined = pd.read_csv(ROOT+'/kaggle/input/bge/science_only/177k_sci_context.csv')\n",
    "# combined = combined.iloc[:118123]\n",
    "\n",
    "# # 3k val\n",
    "# all_val = pd.read_csv('3k_val.csv')\n",
    "# all_val = shuffle_df(all_val)\n",
    "\n",
    "# # # # 15k gpt original context\n",
    "# # # # df_train = pd.read_csv(ROOT+'/kaggle/input/datasets/15kgpt_cleaned.csv')\n",
    "# # # # 15k gpt bge wiki context\n",
    "# # # gpt_wiki = pd.read_csv(ROOT+'/kaggle/input/bge/prefix/GPT_prefix_bge_wikiAbstract.csv')\n",
    "# # # gpt_wiki = shuffle_df(gpt_wiki)\n",
    "# # # # 15k gpt bge sci context\n",
    "# # gpt_sci = pd.read_csv(ROOT+'/kaggle/input/bge/science_only/GPT_bge_science_v1.csv')\n",
    "\n",
    "# gpt_tfidf = pd.read_csv(ROOT+'/kaggle/input/tf-idf_context/v3/15k_gpt/15kgpt_tfidf_context2_v3.csv')\n",
    "# gpt_tfidf = shuffle_df(gpt_tfidf)\n",
    "# # # gpt_long_10k = pd.read_csv(ROOT+'/kaggle/input/tf-idf_context/v3/15k_gpt/10k_long_tfidf.csv')\n",
    "# # # gpt_long_10k = shuffle_df(gpt_long_10k)\n",
    "\n",
    "# tfidf_20k = pd.read_csv(ROOT+'/kaggle/input/tf-idf_context/v3/20k_sci_tfidf/20k_tfidf_v3_fixed.csv')\n",
    "# tfidf_20k = shuffle_df(tfidf_20k)\n",
    "# # # sci_20k = pd.read_csv(ROOT+'/kaggle/input/bge/science_only/20k_bgeSci_noNaN.csv')\n",
    "# # # sci_20k = shuffle_df(sci_20k)\n",
    "\n",
    "\n",
    "# # from new 70k gpt3\n",
    "# long_14k_2 = pd.read_csv(ROOT+'/kaggle/input/tf-idf_context/v3/70k_gpt3/14klong_tfidf_context2_v3.csv')\n",
    "# long_14k_1 = pd.read_csv(ROOT+'/kaggle/input/tf-idf_context/v3/70k_gpt3/14klong_tfidf_context1_v3.csv')\n",
    "# long_14k_1 = shuffle_df(long_14k_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create three datasets with shuffled answers\n",
    "# df1 = pd.read_csv(ROOT+'/kaggle/input/bge/no_prefix/53k_bge_wikiAbstract.csv')\n",
    "# df2 = pd.read_csv(ROOT+'/kaggle/input/bge/prefix/54k_prefix_bge_wikiAbstract.csv')\n",
    "# df3 = pd.read_csv(ROOT+'/kaggle/input/chris_data/54k_nota.csv')\n",
    "\n",
    "# df1 = shuffle_df(df1)\n",
    "# df2 = shuffle_df(df2)\n",
    "# df3 = shuffle_df(df3)\n",
    "\n",
    "# df_train = pd.concat([gpt_sci, df_54k, gpt_tfidf, tfidf_20k, long_14k_2, long_14k_1])\n",
    "\n",
    "df_train = pd.concat([df_54k, combined])\n",
    "\n",
    "# df_train = pd.read_csv(ROOT+'/kaggle/input/bge/science_only/bgeSci_data2/sci_mix_128k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: (231602, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48973</th>\n",
       "      <td>What are biofilms resistant to many common for...</td>\n",
       "      <td>termination</td>\n",
       "      <td>sterilization</td>\n",
       "      <td>termination</td>\n",
       "      <td>assimilation</td>\n",
       "      <td>vaccination</td>\n",
       "      <td>B</td>\n",
       "      <td>Moreover, from an evolutionary point of view, ...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11578</th>\n",
       "      <td>What is the notable characteristic of Bristol ...</td>\n",
       "      <td>In 2010, the Liberal Democrat candidate achiev...</td>\n",
       "      <td>The winning candidate in every election from 1...</td>\n",
       "      <td>Party positions in the constituency remained u...</td>\n",
       "      <td>Bristol North West is known for its high voter...</td>\n",
       "      <td>The constituency has consistently leaned towar...</td>\n",
       "      <td>B</td>\n",
       "      <td>Bristol North West is a constituency represent...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58940</th>\n",
       "      <td>What is quantum engineering?</td>\n",
       "      <td>The development of technology that capitalizes...</td>\n",
       "      <td>The development of technology that capitalizes...</td>\n",
       "      <td>The development of technology that capitalizes...</td>\n",
       "      <td>The development of technology that capitalizes...</td>\n",
       "      <td>The development of technology that capitalizes...</td>\n",
       "      <td>B</td>\n",
       "      <td>Quantum engineering is the development of tech...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149643</th>\n",
       "      <td>What are some major components of modern theor...</td>\n",
       "      <td>All of the above</td>\n",
       "      <td>Reaction networks</td>\n",
       "      <td>Theories of electrolyte solutions</td>\n",
       "      <td>Statistical thermodynamics</td>\n",
       "      <td>Molecular dynamics</td>\n",
       "      <td>A</td>\n",
       "      <td>In recent years, it has consisted primarily of...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123740</th>\n",
       "      <td>What is the main reason why shales emit more g...</td>\n",
       "      <td>Presence of clay</td>\n",
       "      <td>Presence of uranium and thorium</td>\n",
       "      <td>Presence of dolomite and limestone</td>\n",
       "      <td>Presence of gypsum and coal</td>\n",
       "      <td>Presence of radioactive potassium</td>\n",
       "      <td>E</td>\n",
       "      <td>How are gamma rays and neutrons produced by co...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   prompt  \\\n",
       "48973   What are biofilms resistant to many common for...   \n",
       "11578   What is the notable characteristic of Bristol ...   \n",
       "58940                        What is quantum engineering?   \n",
       "149643  What are some major components of modern theor...   \n",
       "123740  What is the main reason why shales emit more g...   \n",
       "\n",
       "                                                        A  \\\n",
       "48973                                         termination   \n",
       "11578   In 2010, the Liberal Democrat candidate achiev...   \n",
       "58940   The development of technology that capitalizes...   \n",
       "149643                                   All of the above   \n",
       "123740                                   Presence of clay   \n",
       "\n",
       "                                                        B  \\\n",
       "48973                                       sterilization   \n",
       "11578   The winning candidate in every election from 1...   \n",
       "58940   The development of technology that capitalizes...   \n",
       "149643                                  Reaction networks   \n",
       "123740                    Presence of uranium and thorium   \n",
       "\n",
       "                                                        C  \\\n",
       "48973                                         termination   \n",
       "11578   Party positions in the constituency remained u...   \n",
       "58940   The development of technology that capitalizes...   \n",
       "149643                  Theories of electrolyte solutions   \n",
       "123740                 Presence of dolomite and limestone   \n",
       "\n",
       "                                                        D  \\\n",
       "48973                                        assimilation   \n",
       "11578   Bristol North West is known for its high voter...   \n",
       "58940   The development of technology that capitalizes...   \n",
       "149643                         Statistical thermodynamics   \n",
       "123740                        Presence of gypsum and coal   \n",
       "\n",
       "                                                        E answer  \\\n",
       "48973                                         vaccination      B   \n",
       "11578   The constituency has consistently leaned towar...      B   \n",
       "58940   The development of technology that capitalizes...      B   \n",
       "149643                                 Molecular dynamics      A   \n",
       "123740                  Presence of radioactive potassium      E   \n",
       "\n",
       "                                                  context source  \n",
       "48973   Moreover, from an evolutionary point of view, ...   10.0  \n",
       "11578   Bristol North West is a constituency represent...    3.0  \n",
       "58940   Quantum engineering is the development of tech...         \n",
       "149643  In recent years, it has consisted primarily of...         \n",
       "123740  How are gamma rays and neutrons produced by co...         "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess data\n",
    "NUM_TRAIN_SAMPLES = len(df_train)\n",
    "# df_train = df_train.drop(columns=\"source\")\n",
    "df_train = df_train.fillna('').sample(NUM_TRAIN_SAMPLES)\n",
    "print('Train data size:', df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df_train.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 231602 entries, 48973 to 23609\n",
      "Data columns (total 9 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   prompt   231602 non-null  object\n",
      " 1   A        231602 non-null  object\n",
      " 2   B        231602 non-null  object\n",
      " 3   C        231602 non-null  object\n",
      " 4   D        231602 non-null  object\n",
      " 5   E        231602 non-null  object\n",
      " 6   answer   231602 non-null  object\n",
      " 7   context  231602 non-null  object\n",
      " 8   source   231602 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 17.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Loader\n",
    "Code is from Radek's notebook [here][1] with modifications to the tokenization process.\n",
    "\n",
    "[1]: https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
    "index_to_option = {v: k for k,v in option_to_index.items()}\n",
    "\n",
    "def preprocess(example):\n",
    "    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n",
    "    second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation='only_first', \n",
    "                                  max_length=MAX_INPUT, add_special_tokens=False)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    \n",
    "    return tokenized_example\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60771712013c4bd5bd84f824780ac4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1709cff9a44801ac6b833a730ff249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d92246bded4d85b6710316dfee6989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'context', 'source'],\n",
       "    num_rows: 231602\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "dataset_valid = Dataset.from_pandas(df_valid)\n",
    "dataset = Dataset.from_pandas(df_train)\n",
    "dataset = dataset.remove_columns([\"__index_level_0__\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebb51a3eaf2476ba9aea8596da035fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a029cee53a452c9a47515cf50c3704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231602 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       "    num_rows: 231602\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_dataset = dataset.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "We will use a Hugging Face AutoModelForMultipleChoice. For the list of possible models, see Hugging Face's repository [here][1]. We can optionally use PEFT to accelerate training and use less memory. However i have noticed that validation accuracy is less. (Note that PEFT requires us to use 1xP100 not 2xT4 GPU. I'm not sure why). We can also optionally freeze layers. This also accelerates training and uses less memory. However validation accuracy may become less.\n",
    "\n",
    "[1]: https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49865321e1a4520b2e24c06ef71f38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMultipleChoice.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-output": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE PEFT REQUIRES US TO USE 1XP100 NOT 2XT4. I'M NOT SURE WHY.\n",
    "if USE_PEFT:\n",
    "    !pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_PEFT:\n",
    "    print('We are using PEFT.')\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    peft_config = LoraConfig(\n",
    "        r=8, lora_alpha=4, task_type=TaskType.SEQ_CLS, lora_dropout=0.1, \n",
    "        bias=\"none\", inference_mode=False, \n",
    "        target_modules=[\"query_proj\", \"value_proj\"],\n",
    "        modules_to_save=['classifier','pooler'],\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if FREEZE_EMBEDDINGS:\n",
    "    print('Freezing embeddings.')\n",
    "    for param in model.deberta.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "if FREEZE_LAYERS>0:\n",
    "    print(f'Freezing {FREEZE_LAYERS} layers.')\n",
    "    for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP@3 Metric\n",
    "The competition metric is MAP@3 therefore we will make a custom code to add to Hugging Face's trainer. Discussion [here][1]\n",
    "\n",
    "[1]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/435602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_at_3(predictions, labels):\n",
    "    map_sum = 0\n",
    "    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n",
    "    for x,y in zip(pred,labels):\n",
    "        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n",
    "        map_sum += np.sum(z)\n",
    "    return map_sum / len(predictions)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions = p.predictions.tolist()\n",
    "    labels = p.label_ids.tolist()\n",
    "    return {\"map@3\": map_at_3(predictions, labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train and Save \n",
    "We will now train and save our model using Hugging Face's easy to use trainer. By adjusting the parameters in this notebook, we can achieve `CV MAP@3 = 0.915+` and corresponding single model `LB MAP@3 = 0.830+` wow!\n",
    "\n",
    "In we run this notebook outside of Kaggle then we can train longer and with more RAM. If we run this notebook on Kaggle, then we need to use tricks to train models efficiently. Here are some ideas:\n",
    "* use fp16 (this speeds up T4 not P100)\n",
    "* use gradient_accumlation_steps (this simulates larger batch sizes)\n",
    "* use gradient_checkpointing (this uses disk to save RAM)\n",
    "* use 2xT4 instead of 1xP100 (this doubles GPUs)\n",
    "* freeze model embeddings (this reduces weights to train)\n",
    "* freeze some model layers (this reduces weights to train)\n",
    "* use PEFT (this reduces weights to train)\n",
    "* increase LR and decrease epochs (this reduces work)\n",
    "* use smaller models (this reduces weights to train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del trainer\n",
    "# del model\n",
    "# clean_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     SAVE_STEPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000000\u001b[39m\n\u001b[1;32m      5\u001b[0m     EVAL_STEPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 7\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#2e-5\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#1\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#2\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#2\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./checkpoints_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mVER\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 8\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEVAL_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEVAL_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmap@3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#'cosine'\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# checkpoint = None  # train from scratch\u001b[39;00m\n\u001b[1;32m     33\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ROOT \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/checkpoints_2/checkpoint-5200\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# resume checkpoint\u001b[39;00m\n",
      "File \u001b[0;32m<string>:114\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1405\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1398\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1399\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1404\u001b[0m ):\n\u001b[0;32m-> 1405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or NPU devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1408\u001b[0m     )\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1412\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1418\u001b[0m ):\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1420\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--bf16_full_eval`) can only be used on CUDA or CPU/TPU/NeuronCore devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1422\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices."
     ]
    }
   ],
   "source": [
    "SAVE_STEPS = 200\n",
    "EVAL_STEPS = 200\n",
    "if DEBUG is True:  # don't save\n",
    "    SAVE_STEPS = 1000000\n",
    "    EVAL_STEPS = 10\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=2e-6, #2e-5\n",
    "    per_device_train_batch_size=8, #1\n",
    "    per_device_eval_batch_size=8,  #2\n",
    "    num_train_epochs=2,  #2\n",
    "    report_to='none',\n",
    "    output_dir = f'./checkpoints_{VER}',\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4, # 8\n",
    "    logging_steps=EVAL_STEPS,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model='map@3',\n",
    "    lr_scheduler_type='cosine', #'cosine'\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "\n",
    "# checkpoint = None  # train from scratch\n",
    "checkpoint = ROOT + \"/checkpoints_2/checkpoint-5200\"  # resume checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset_valid,\n",
    "    compute_metrics = compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=checkpoint)\n",
    "trainer.save_model(f'model_v{VER}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Verify Saved Model\n",
    "During training, we see the MAP@3 validation score above. Let's load the saved model and compute it again here to verify that our model is saved correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del model, trainer\n",
    "# if USE_PEFT:\n",
    "#     model = AutoModelForMultipleChoice.from_pretrained(MODEL)\n",
    "#     model = get_peft_model(model, peft_config)\n",
    "#     checkpoint = torch.load(f'model_v{VER}/pytorch_model.bin')\n",
    "#     model.load_state_dict(checkpoint)\n",
    "# else:\n",
    "#     model = AutoModelForMultipleChoice.from_pretrained(f'model_v{VER}')\n",
    "# trainer = Trainer(model=model)\n",
    "\n",
    "# test_df = pd.read_csv(ROOT+'/kaggle/input/60k-data-with-context-v2/train_with_context2.csv')\n",
    "# tokenized_test_dataset = Dataset.from_pandas(test_df).map(\n",
    "#         preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E'])\n",
    "\n",
    "test_predictions = trainer.predict(tokenized_dataset_valid).predictions\n",
    "test_df = df_valid.copy()\n",
    "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
    "predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
    "predictions_as_string = test_df['prediction'] = [\n",
    "    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "]\n",
    "\n",
    "# Compute Validation Score\n",
    "\n",
    "# https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking\n",
    "import numpy as np\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Precision at k\"\"\"\n",
    "    assert k <= len(r)\n",
    "    assert k != 0\n",
    "    return sum(int(x) for x in r[:k]) / k\n",
    "\n",
    "def MAP_at_3(predictions, true_items):\n",
    "    \"\"\"Score is mean average precision at 3\"\"\"\n",
    "    U = len(predictions)\n",
    "    map_at_3 = 0.0\n",
    "    for u in range(U):\n",
    "        user_preds = predictions[u].split()\n",
    "        user_true = true_items[u]\n",
    "        user_results = [1 if item == user_true else 0 for item in user_preds]\n",
    "        for k in range(min(len(user_preds), 3)):\n",
    "            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n",
    "    return map_at_3 / U\n",
    "\n",
    "m = MAP_at_3(test_df.prediction.values, test_df.answer.values)\n",
    "print( 'CV MAP@3 =',m )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del tokenized_test_dataset\n",
    "# # del trainer\n",
    "# # del dataset\n",
    "# clean_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from scipy.special import softmax\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# def preprocess(example):\n",
    "#     first_sentence = [example['prompt']] * 5\n",
    "#     second_sentence = []\n",
    "#     for option in options:\n",
    "#         second_sentence.append(example[option])\n",
    "    \n",
    "#     tokenized_example = tokenizer(first_sentence, second_sentence, truncation='only_first')\n",
    "#     tokenized_example['label'] = option_to_index[example['answer']]\n",
    "#     return tokenized_example\n",
    "\n",
    "# # Compute Validation Score\n",
    "# # https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking\n",
    "# def precision_at_k(r, k):\n",
    "#     \"\"\"Precision at k\"\"\"\n",
    "#     assert k <= len(r)\n",
    "#     assert k != 0\n",
    "#     return sum(int(x) for x in r[:k]) / k\n",
    "\n",
    "# def MAP_at_3(predictions, true_items):\n",
    "#     \"\"\"Score is mean average precision at 3\"\"\"\n",
    "#     U = len(predictions)\n",
    "#     map_at_3 = 0.0\n",
    "#     for u in range(U):\n",
    "#         user_preds = predictions[u].split()\n",
    "#         user_true = true_items[u]\n",
    "#         user_results = [1 if item == user_true else 0 for item in user_preds]\n",
    "#         for k in range(min(len(user_preds), 3)):\n",
    "#             map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n",
    "#     return map_at_3 / U\n",
    "\n",
    "# # for formatting predictions as strings\n",
    "# options = 'ABCDE'\n",
    "# indices = list(range(5))\n",
    "# option_to_index = {option: index for option, index in zip(options, indices)}\n",
    "# index_to_option = {index: option for option, index in zip(options, indices)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_df = pd.read_csv(ROOT+'/kaggle/input/bge/science_only/500val_bgeSci_5context.csv')\n",
    "# print('Validation data size:', test_df.shape )\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:2300]) + \" #### \" +  test_df[\"prompt\"]  \n",
    "# data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "# tokenized_test_dataset = Dataset.from_pandas(test_df[['prompt', 'A', 'B', 'C', 'D', 'E', 'answer']]).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n",
    "# # tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\n",
    "# test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # predict\n",
    "# test_predictions = []\n",
    "# for batch in test_dataloader:\n",
    "#     for k in batch.keys():\n",
    "#         batch[k] = batch[k].to(torch.device('cuda:0'))\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**batch)\n",
    "#     test_predictions.append(outputs.logits.cpu().detach())\n",
    "\n",
    "# test_predictions = torch.cat(test_predictions)\n",
    "# test_predictions = softmax(test_predictions, axis=1).numpy()\n",
    "\n",
    "# bge_preds = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combined_predictions = test_predictions\n",
    "# predictions_as_ids = np.argsort(-combined_predictions, 1)\n",
    "# predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
    "# predictions_as_string = test_df['prediction'] = [\n",
    "#     ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "# ]\n",
    "# # print MAP score\n",
    "# m = MAP_at_3(test_df.prediction.values, test_df.answer.values)\n",
    "# print( 'CV MAP@3 =',m )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combined_predictions = (tfidf_preds_1 + tfidf_preds_2 + bge_preds) / 3\n",
    "# predictions_as_ids = np.argsort(-combined_predictions, 1)\n",
    "# predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
    "# predictions_as_string = test_df['prediction'] = [\n",
    "#     ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "# ]\n",
    "\n",
    "# # print MAP score\n",
    "# m = MAP_at_3(test_df.prediction.values, test_df.answer.values)\n",
    "# print( 'CV MAP@3 =',m )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
